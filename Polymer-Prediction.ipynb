{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61979795",
   "metadata": {},
   "source": [
    "# HOMO-LUMO Gap Predictions\n",
    "\n",
    "### Problem Statement & Motivation\n",
    "\n",
    "Accurately predicting quantum chemical properties like the HOMO–LUMO energy gap is essential for advancing materials science, drug discovery, and electronic design. The HOMO–LUMO gap is particularly informative for assessing molecular reactivity and stability. While Density Functional Theory (DFT) provides precise estimates, its high computational cost makes it impractical for large-scale screening of molecular libraries. This notebook explores machine learning alternatives that are fast, scalable, and interpretable, offering solutions that are accessible even on modest hardware.\n",
    "\n",
    "### Related Work & Key Gap\n",
    "\n",
    "Past work has shown that:\n",
    "\n",
    "* DFT is accurate but computationally intensive\n",
    "* ML models like kernel methods and GNNs show promise, but often require large models and expensive hardware\n",
    "\n",
    "Key Gap: A need for lightweight, high-performing models that can run locally and integrate with user-friendly tools for deployment in research or education.\n",
    "\n",
    "### Methodology & Evaluation\n",
    "\n",
    "This notebook:\n",
    "\n",
    "* Benchmarks a variety of 2D-based models using RDKit descriptors, Coulomb matrices, and graph neural networks (GNNs) on a 5k molecule subset\n",
    "* Progresses to a hybrid GNN architecture combining OGB-standard graphs with SMILES-derived cheminformatics features\n",
    "* Achieves **MAE = 0.159 eV**\n",
    "* Visualizes results using parity plots, error inspection, and predicted-vs-true comparisons\n",
    "* Evaluates both random and high-error cases to better understand model behavior\n",
    "\n",
    "| Metric   | Best Model (Hybrid GNN) |\n",
    "| -------- | ----------------------- |\n",
    "| **MAE**  | 0.159 eV                |\n",
    "| **RMSE** | 0.234 eV                |\n",
    "| **R²**   | 0.965                   |\n",
    "\n",
    "\n",
    "### Deployment & Accessibility\n",
    "\n",
    "To make the model practically useful, an **interactive web app** was developed:\n",
    "\n",
    "**Live App**: [HOMO–LUMO Gap Predictor on Hugging Face](https://huggingface.co/spaces/MooseML/homo-lumo-gap-predictor)\n",
    "\n",
    "Features:\n",
    "\n",
    "* **SMILES input** for any organic molecule\n",
    "* **Real-time prediction** of the HOMO–LUMO gap\n",
    "* **Molecular visualization**\n",
    "* Simple **CSV logging** for result tracking\n",
    "\n",
    "GitHub Repository: [MooseML/homo-lumo-gap-models](https://github.com/MooseML/homo-lumo-gap-models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ace_tools_open as tools\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import pickle\n",
    "import joblib\n",
    "import os \n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Module, Sequential, Dropout\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# OGB dataset \n",
    "from ogb.lsc import PygPCQM4Mv2Dataset, PCQM4Mv2Dataset\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# RDKit\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "\n",
    "# ChemML\n",
    "from chemml.chem import Molecule, RDKitFingerprint, CoulombMatrix, tensorise_molecules\n",
    "from chemml.models import MLP, NeuralGraphHidden, NeuralGraphOutput\n",
    "from chemml.utils import regression_metrics\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589db70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Built with CUDA: True\n",
      "CUDA available: True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device: /physical_device:GPU:0\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: data\n",
      "LMDB directory: data\\processed_chunks\n",
      "Train LMDB: data\\processed_chunks\\polymer_train3d_dist.lmdb\n",
      "Test LMDB: data\\processed_chunks\\polymer_test3d_dist.lmdb\n",
      "LMDBs already exist.\n"
     ]
    }
   ],
   "source": [
    "# Paths - Fixed for Kaggle environment\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  # Writable directory\n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/best_gnn_transformer_hybrid.pt'\n",
    "else:\n",
    "    DATA_ROOT = 'data'\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = 'best_gnn_transformer_hybrid.pt'\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"LMDB directory: {CHUNK_DIR}\")\n",
    "print(f\"Train LMDB: {TRAIN_LMDB}\")\n",
    "print(f\"Test LMDB: {TEST_LMDB}\")\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    # Run the LMDB builders\n",
    "    !python build_polymer_lmdb_fixed.py train\n",
    "    !python build_polymer_lmdb_fixed.py test\n",
    "    print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c34b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV with shape: (7973, 6)\n",
      "                                                 SMILES  Tg       FFV  \\\n",
      "7560  *C=Cc1ccc2c3ccc(*)cc3n(-c3ccc(OCCCCCCCCCC)c(OC... NaN  0.386695   \n",
      "1405                  *CC(=O)NCCCCCCNC(=O)Cc1ccc(O*)cc1 NaN  0.335504   \n",
      "5196                              *CC(*)c1ccccc1C(=O)NC NaN  0.355580   \n",
      "2087  *c1ccc2c(c1)C(=O)N(c1ccc(Oc3ccc(N4C(=O)c5ccc(-... NaN  0.401573   \n",
      "3337                    *CC(*)OC(=O)c1ccc(-c2ccccc2)cc1 NaN  0.353609   \n",
      "\n",
      "            Tc  Density  Rg  \n",
      "7560       NaN      NaN NaN  \n",
      "1405       NaN      NaN NaN  \n",
      "5196  0.183667      NaN NaN  \n",
      "2087       NaN      NaN NaN  \n",
      "3337       NaN      NaN NaN  \n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(DATA_ROOT, 'train.csv')\n",
    "train_df   = pd.read_csv(train_path)\n",
    "\n",
    "#  Keep only the columns we care about \n",
    "target_cols = ['SMILES', 'Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "train_df = train_df[target_cols]        # drops id and any other columns\n",
    "\n",
    "#  Sample a subset (optional) \n",
    "n = len(train_df)\n",
    "subset_size = n                         # change to whatever you need\n",
    "subset_df = train_df.sample(subset_size, random_state=42)\n",
    "\n",
    "#  Save the subset as a CSV \n",
    "subset_path = os.path.join(DATA_ROOT, 'train_subset.csv')\n",
    "subset_df.to_csv(subset_path, index=False)\n",
    "\n",
    "print(f\"Saved CSV with shape: {subset_df.shape}\")\n",
    "print(subset_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f557b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7973 molecules.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(subset_path)\n",
    "print(f\"Loaded {len(df)} molecules.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125f621",
   "metadata": {},
   "source": [
    "The only property that appears will succeed with a simple imputation strategy is FFV. All other properties contain very high percent missing. Therefore, I will impute median for FFV, train a model for FFV, and train separate models for other properties. I will attempt to filter out missing values for each property. If this yields uncessful, I may explore sampling techniques or use the trained model to impute values to train a secondaery model. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe69f3",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47dc5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Tg DataFrame shape: (7973, 2)\n",
      "Initial Tg Missing Values:\n",
      "SMILES       0\n",
      "Tg        7462\n",
      "dtype: int64\n",
      "\n",
      "Cleaned Tg DataFrame shape: (511, 2)\n",
      "Cleaned Tg Missing Values:\n",
      "SMILES    0\n",
      "Tg        0\n",
      "dtype: int64\n",
      "Initial Density DataFrame shape: (7973, 2)\n",
      "Initial Density Missing Values:\n",
      "SMILES        0\n",
      "Density    7360\n",
      "dtype: int64\n",
      "\n",
      "Cleaned Density DataFrame shape: (613, 2)\n",
      "Cleaned Density Missing Values:\n",
      "SMILES     0\n",
      "Density    0\n",
      "dtype: int64\n",
      "Initial FFV DataFrame shape: (7973, 2)\n",
      "Initial FFV Missing Values:\n",
      "SMILES      0\n",
      "FFV       943\n",
      "dtype: int64\n",
      "\n",
      "Cleaned FFV DataFrame shape: (7030, 2)\n",
      "Cleaned FFV Missing Values:\n",
      "SMILES    0\n",
      "FFV       0\n",
      "dtype: int64\n",
      "Initial Tc DataFrame shape: (7973, 2)\n",
      "Initial Tc Missing Values:\n",
      "SMILES       0\n",
      "Tc        7236\n",
      "dtype: int64\n",
      "\n",
      "Cleaned Tc DataFrame shape: (737, 2)\n",
      "Cleaned Tc Missing Values:\n",
      "SMILES    0\n",
      "Tc        0\n",
      "dtype: int64\n",
      "Initial Rg DataFrame shape: (7973, 2)\n",
      "Initial Rg Missing Values:\n",
      "SMILES       0\n",
      "Rg        7359\n",
      "dtype: int64\n",
      "\n",
      "Cleaned Rg DataFrame shape: (614, 2)\n",
      "Cleaned Rg Missing Values:\n",
      "SMILES    0\n",
      "Rg        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def build_target_df(df, target_col: str):\n",
    "    \"\"\"Return a DataFrame with only SMILES and the given target, dropping missing targets.\"\"\"\n",
    "    out = df[['SMILES', target_col]].copy()\n",
    "    print(f\"Initial {target_col} DataFrame shape:\", out.shape)\n",
    "    print(f\"Initial {target_col} Missing Values:\\n{out.isnull().sum()}\")\n",
    "\n",
    "    out = out.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\nCleaned {target_col} DataFrame shape:\", out.shape)\n",
    "    print(f\"Cleaned {target_col} Missing Values:\\n{out.isnull().sum()}\")\n",
    "    return out\n",
    "\n",
    "# Build all five\n",
    "df_tg      = build_target_df(df, 'Tg')\n",
    "df_density = build_target_df(df, 'Density')\n",
    "df_ffv     = build_target_df(df, 'FFV')   # per instructions: drop missing, no imputation\n",
    "df_tc      = build_target_df(df, 'Tc')\n",
    "df_rg      = build_target_df(df, 'Rg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d169da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import AllChem, Descriptors, HybridizationType, SanitizeFlags\n",
    "def rdkit_ogb_agree(smi: str) -> bool:\n",
    "    m = Chem.MolFromSmiles(smi)\n",
    "    if m is None:\n",
    "        return False\n",
    "    return m.GetNumAtoms() == smiles2graph(smi)[\"num_nodes\"]\n",
    "\n",
    "def canonicalize_polymer_smiles(smiles: str, cap_atomic_num: int = 6) -> str:\n",
    "    \"\"\"\n",
    "    Turn every '*' (dummy atom) into a real atom (default C) in the RDKit graph,\n",
    "    preserving existing bond orders/stereo; sanitize, remove explicit Hs, and\n",
    "    return canonical isomeric SMILES.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles, sanitize=False)\n",
    "    if mol is None:\n",
    "        raise ValueError(f\"RDKit could not parse SMILES: {smiles}\")\n",
    "\n",
    "    rw = Chem.RWMol(mol)\n",
    "    for a in rw.GetAtoms():\n",
    "        if a.GetAtomicNum() == 0:   # '*'\n",
    "            a.SetAtomicNum(cap_atomic_num)  # 6 = carbon\n",
    "            a.SetFormalCharge(0)\n",
    "            a.SetIsAromatic(False)\n",
    "            a.SetNoImplicit(False)\n",
    "            a.SetNumExplicitHs(0)\n",
    "\n",
    "    mol2 = rw.GetMol()\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol2)\n",
    "    except Exception:\n",
    "        Chem.SanitizeMol(mol2, sanitizeOps=SanitizeFlags.SANITIZE_ALL ^ SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "        Chem.Kekulize(mol2, clearAromaticFlags=True)\n",
    "\n",
    "    mol2 = Chem.RemoveHs(mol2)\n",
    "    return Chem.MolToSmiles(mol2, isomericSmiles=True, canonical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff48e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "\n",
    "def prepare_target_data(\n",
    "    df_target: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    *,\n",
    "    do_3d: bool = True,\n",
    "    optimizer: str = \"MMFF\",\n",
    "    max_iters: int = 200,\n",
    "    add_hydrogens: bool = True,\n",
    "    fp_type: str = \"morgan\",\n",
    "    fp_vector: str = \"bit\",\n",
    "    fp_bits: int = 1024,\n",
    "    fp_radius: int = 3,\n",
    "    save_csv_path: Optional[str] = None,\n",
    "    show_progress: bool = True,\n",
    ") -> Tuple[pd.DataFrame, List, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build molecules, drop missing targets, compute RDKit fingerprints.\n",
    "    Returns: (df_clean, valid_mol_objs, y, X_fp)\n",
    "\n",
    "    df_clean columns: ['SMILES', target_col]\n",
    "    y shape: (N,)\n",
    "    X_fp shape: (N, fp_bits) for bit vector\n",
    "    \"\"\"\n",
    "    assert {\"SMILES\", target_col}.issubset(df_target.columns), \"df_target must have SMILES and target column\"\n",
    "\n",
    "    # 1) Drop rows with missing targets (no label imputation)\n",
    "    df_work = df_target[[\"SMILES\", target_col]].copy()\n",
    "    before = len(df_work)\n",
    "    df_work = df_work.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    after = len(df_work)\n",
    "    print(f\"[{target_col}] dropped {before - after} rows with missing target; kept {after}\")\n",
    "\n",
    "    valid_mol_objs: List = []\n",
    "    valid_targets: List[float] = []\n",
    "\n",
    "    it = df_work.itertuples(index=False)\n",
    "    if show_progress:\n",
    "        it = tqdm(it, total=len(df_work), desc=f\"Build 3D + collect {target_col}\")\n",
    "\n",
    "    for row in it:\n",
    "        smi = row.SMILES\n",
    "        tval = getattr(row, target_col)\n",
    "\n",
    "        try:\n",
    "            clean = canonicalize_polymer_smiles(smi)\n",
    "            if not isinstance(clean, str) or not clean:\n",
    "                continue\n",
    "\n",
    "            mol = Molecule(clean, input_type=\"smiles\")\n",
    "            if add_hydrogens:\n",
    "                mol.hydrogens(\"add\")\n",
    "            if do_3d:\n",
    "                mol.to_xyz(optimizer=optimizer, maxIters=max_iters)\n",
    "                if mol.xyz is None:\n",
    "                    continue\n",
    "\n",
    "            valid_mol_objs.append(mol)\n",
    "            valid_targets.append(float(tval))\n",
    "\n",
    "        except Exception as e:\n",
    "            # skip problematic molecules; keep going\n",
    "            # print(f\"Skipped {smi}: {e}\")  # uncomment if you want verbose logging\n",
    "            continue\n",
    "\n",
    "    print(f\"[{target_col}] kept {len(valid_mol_objs)} molecules after 3D/cleaning.\")\n",
    "\n",
    "    # 2) Build cleaned DataFrame and target vector\n",
    "    df_clean = pd.DataFrame({\n",
    "        \"SMILES\": [m.smiles for m in valid_mol_objs],\n",
    "        target_col: np.array(valid_targets, dtype=float)\n",
    "    })\n",
    "    if save_csv_path:\n",
    "        df_clean.to_csv(save_csv_path, index=False)\n",
    "        print(f\"[{target_col}] saved cleaned dataset -> {save_csv_path}\")\n",
    "\n",
    "    y = np.asarray(valid_targets, dtype=float)\n",
    "\n",
    "    # 3) Compute RDKit fingerprint features\n",
    "    fp_featurizer = RDKitFingerprint(\n",
    "        fingerprint_type=fp_type,\n",
    "        vector=fp_vector,     # 'bit' or 'count'\n",
    "        n_bits=fp_bits,\n",
    "        radius=fp_radius\n",
    "    )\n",
    "    X_fp = fp_featurizer.represent(valid_mol_objs)\n",
    "\n",
    "    print(f\"[{target_col}] X_fp shape: {X_fp.shape} | y shape: {y.shape}\")\n",
    "    return df_clean, valid_mol_objs, y, X_fp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f37942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] dropped 0 rows with missing target; kept 511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build 3D + collect Tg: 100%|██████████| 511/511 [00:35<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] kept 504 molecules after 3D/cleaning.\n",
      "[Tg] saved cleaned dataset -> cleaned_tg_dataset.csv\n",
      "[Tg] X_fp shape: (504, 1024) | y shape: (504,)\n",
      "[Density] dropped 0 rows with missing target; kept 613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build 3D + collect Density: 100%|██████████| 613/613 [00:35<00:00, 17.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Density] kept 609 molecules after 3D/cleaning.\n",
      "[Density] saved cleaned dataset -> cleaned_density_dataset.csv\n",
      "[Density] X_fp shape: (609, 1024) | y shape: (609,)\n",
      "[FFV] dropped 0 rows with missing target; kept 7030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build 3D + collect FFV: 100%|██████████| 7030/7030 [00:04<00:00, 1587.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FFV] kept 7030 molecules after 3D/cleaning.\n",
      "[FFV] saved cleaned dataset -> cleaned_ffv_dataset.csv\n",
      "[FFV] X_fp shape: (7030, 1024) | y shape: (7030,)\n",
      "[Tc] dropped 0 rows with missing target; kept 737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build 3D + collect Tc: 100%|██████████| 737/737 [00:41<00:00, 17.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tc] kept 734 molecules after 3D/cleaning.\n",
      "[Tc] saved cleaned dataset -> cleaned_tc_dataset.csv\n",
      "[Tc] X_fp shape: (734, 1024) | y shape: (734,)\n",
      "[Rg] dropped 0 rows with missing target; kept 614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build 3D + collect Rg: 100%|██████████| 614/614 [00:34<00:00, 17.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rg] kept 611 molecules after 3D/cleaning.\n",
      "[Rg] saved cleaned dataset -> cleaned_rg_dataset.csv\n",
      "[Rg] X_fp shape: (611, 1024) | y shape: (611,)\n"
     ]
    }
   ],
   "source": [
    "# Tg\n",
    "df_clean_tg, mols_tg, y_tg, X_tg = prepare_target_data(\n",
    "    df_tg, \"Tg\",\n",
    "    do_3d=True, optimizer=\"MMFF\", max_iters=200,\n",
    "    fp_type=\"morgan\", fp_vector=\"bit\", fp_bits=1024, fp_radius=3,\n",
    "    save_csv_path=\"cleaned_tg_dataset.csv\"\n",
    ")\n",
    "\n",
    "# Density\n",
    "df_clean_density, mols_density, y_density, X_density = prepare_target_data(\n",
    "    df_density, \"Density\",\n",
    "    do_3d=True, optimizer=\"MMFF\", max_iters=200,\n",
    "    fp_type=\"morgan\", fp_vector=\"bit\", fp_bits=1024, fp_radius=3,\n",
    "    save_csv_path=\"cleaned_density_dataset.csv\"\n",
    ")\n",
    "\n",
    "# FFV\n",
    "# If you want to skip 3D for a tabular-first target:\n",
    "df_clean_ffv, mols_ffv, y_ffv, X_ffv = prepare_target_data(\n",
    "    df_ffv, \"FFV\", \n",
    "    do_3d=False,\n",
    "    save_csv_path=\"cleaned_ffv_dataset.csv\")\n",
    "\n",
    "\n",
    "# Tc\n",
    "df_clean_tc, mols_tc, y_tc, X_tc = prepare_target_data(\n",
    "    df_tc, \"Tc\",\n",
    "    do_3d=True, optimizer=\"MMFF\", max_iters=200,\n",
    "    fp_type=\"morgan\", fp_vector=\"bit\", fp_bits=1024, fp_radius=3,\n",
    "    save_csv_path=\"cleaned_tc_dataset.csv\"\n",
    ")\n",
    "\n",
    "# Rg\n",
    "df_clean_rg, mols_rg, y_rg, X_rg = prepare_target_data(\n",
    "    df_rg, \"Rg\",\n",
    "    do_3d=True, optimizer=\"MMFF\", max_iters=200,\n",
    "    fp_type=\"morgan\", fp_vector=\"bit\", fp_bits=1024, fp_radius=3,\n",
    "    save_csv_path=\"cleaned_rg_dataset.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff620911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "@dataclass\n",
    "class TabularSplits:\n",
    "    # unscaled (use for RF)\n",
    "    X_train: np.ndarray\n",
    "    X_test:  np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    y_test:  np.ndarray\n",
    "    # scaled (use for KRR/MLP)\n",
    "    X_train_scaled: Optional[np.ndarray] = None\n",
    "    X_test_scaled:  Optional[np.ndarray] = None\n",
    "    y_train_scaled: Optional[np.ndarray] = None  # shape (N,1)\n",
    "    y_test_scaled:  Optional[np.ndarray] = None\n",
    "    x_scaler: Optional[StandardScaler] = None\n",
    "    y_scaler: Optional[StandardScaler] = None\n",
    "\n",
    "def _make_regression_stratify_bins(y: np.ndarray, n_bins: int = 10) -> np.ndarray:\n",
    "    \"\"\"Return integer bins for approximate stratification in regression.\"\"\"\n",
    "    y = y.ravel()\n",
    "    # handle degenerate case\n",
    "    if np.unique(y).size < n_bins:\n",
    "        n_bins = max(2, np.unique(y).size)\n",
    "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "    bins = np.unique(np.quantile(y, quantiles))\n",
    "    # ensure strictly increasing\n",
    "    bins = np.unique(bins)\n",
    "    # np.digitize expects right-open intervals by default\n",
    "    strat = np.digitize(y, bins[1:-1], right=False)\n",
    "    return strat\n",
    "\n",
    "def make_tabular_splits(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    scale_X: bool = True,\n",
    "    scale_y: bool = True,\n",
    "    stratify_regression: bool = False,\n",
    "    n_strat_bins: int = 10,\n",
    "    # if you already decided splits (e.g., scaffold split), pass indices:\n",
    "    train_idx: Optional[np.ndarray] = None,\n",
    "    test_idx: Optional[np.ndarray] = None,\n",
    ") -> TabularSplits:\n",
    "    \"\"\"\n",
    "    Split and (optionally) scale tabular features/targets for a single target.\n",
    "    Returns both scaled and unscaled arrays, plus fitted scalers.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    if train_idx is not None and test_idx is not None:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "    else:\n",
    "        strat = None\n",
    "        if stratify_regression:\n",
    "            strat = _make_regression_stratify_bins(y, n_bins=n_strat_bins)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "        )\n",
    "\n",
    "    # Unscaled outputs (for RF, tree models)\n",
    "    splits = TabularSplits(\n",
    "        X_train=X_train, X_test=X_test,\n",
    "        y_train=y_train, y_test=y_test\n",
    "    )\n",
    "\n",
    "    # Scaled versions (for KRR/MLP)\n",
    "    if scale_X:\n",
    "        xscaler = StandardScaler()\n",
    "        splits.X_train_scaled = xscaler.fit_transform(X_train)\n",
    "        splits.X_test_scaled  = xscaler.transform(X_test)\n",
    "        splits.x_scaler = xscaler\n",
    "    if scale_y:\n",
    "        yscaler = StandardScaler()\n",
    "        splits.y_train_scaled = yscaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        splits.y_test_scaled  = yscaler.transform(y_test.reshape(-1, 1))\n",
    "        splits.y_scaler = yscaler\n",
    "\n",
    "    # Shapes summary\n",
    "    print(\"Splits:\")\n",
    "    print(\"  X_train:\", splits.X_train.shape, \"| X_test:\", splits.X_test.shape)\n",
    "    if splits.X_train_scaled is not None:\n",
    "        print(\"  X_train_scaled:\", splits.X_train_scaled.shape, \"| X_test_scaled:\", splits.X_test_scaled.shape)\n",
    "    print(\"  y_train:\", splits.y_train.shape, \"| y_test:\", splits.y_test.shape)\n",
    "    if splits.y_train_scaled is not None:\n",
    "        print(\"  y_train_scaled:\", splits.y_train_scaled.shape, \"| y_test_scaled:\", splits.y_test_scaled.shape)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c284cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def train_eval_rf(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    rf_params: Dict[str, Any],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    save_dir: str = \"saved_models/rf\",\n",
    "    tag: str = \"model\",\n",
    ") -> Tuple[RandomForestRegressor, Dict[str, float], TabularSplits, str]:\n",
    "    \"\"\"\n",
    "    Trains a RandomForest on unscaled features; returns (model, metrics, splits, path).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False,                 # RF doesn't need scaling\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=n_strat_bins\n",
    "    )\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=random_state, n_jobs=-1, **rf_params)\n",
    "    rf.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "    pred_tr = rf.predict(splits.X_train)\n",
    "    pred_te = rf.predict(splits.X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_MAE\": mean_absolute_error(splits.y_train, pred_tr),\n",
    "        \"train_RMSE\": mean_squared_error(splits.y_train, pred_tr, squared=False),\n",
    "        \"train_R2\": r2_score(splits.y_train, pred_tr),\n",
    "        \"val_MAE\": mean_absolute_error(splits.y_test, pred_te),\n",
    "        \"val_RMSE\": mean_squared_error(splits.y_test, pred_te, squared=False),\n",
    "        \"val_R2\": r2_score(splits.y_test, pred_te),\n",
    "    }\n",
    "    print(f\"[RF/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"rf_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": rf, \"metrics\": metrics, \"rf_params\": rf_params}, path)\n",
    "    return rf, metrics, splits, path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d95126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:\n",
      "  X_train: (5624, 1024) | X_test: (1406, 1024)\n",
      "  y_train: (5624,) | y_test: (1406,)\n",
      "[RF/FFV] val_MAE=0.008940  val_RMSE=0.015961  val_R2=0.7148\n",
      "Splits:\n",
      "  X_train: (587, 1024) | X_test: (147, 1024)\n",
      "  y_train: (587,) | y_test: (147,)\n",
      "[RF/Tc] val_MAE=0.030779  val_RMSE=0.043855  val_R2=0.7543\n",
      "Splits:\n",
      "  X_train: (488, 1024) | X_test: (123, 1024)\n",
      "  y_train: (488,) | y_test: (123,)\n",
      "[RF/Rg] val_MAE=2.042348  val_RMSE=2.931961  val_R2=0.6087\n"
     ]
    }
   ],
   "source": [
    "rf_cfg = {\n",
    "    \"FFV\": {\"n_estimators\": 100, \"max_depth\": 60},\n",
    "    \"Tc\":  {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "    \"Rg\":  {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "}\n",
    "\n",
    "rf_ffv, m_ffv, splits_ffv, p_ffv = train_eval_rf(X_ffv, y_ffv, rf_params=rf_cfg[\"FFV\"], tag=\"FFV\")\n",
    "rf_tc,  m_tc,  splits_tc,  p_tc  = train_eval_rf(X_tc,  y_tc,  rf_params=rf_cfg[\"Tc\"],  tag=\"Tc\")\n",
    "rf_rg,  m_rg,  splits_rg,  p_rg  = train_eval_rf(X_rg,  y_rg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Rg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a64af289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-06 08:44:29,910] A new study created in memory with name: ET_FFV\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:\n",
      "  X_train: (5624, 1024) | X_test: (1406, 1024)\n",
      "  y_train: (5624,) | y_test: (1406,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6e03681e1c4dc49750a3a6fce52c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-06 08:44:30,717] Trial 0 finished with value: 0.010924571875946127 and parameters: {'n_estimators': 500, 'max_depth': 60, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 0 with value: 0.010924571875946127.\n",
      "[I 2025-09-06 08:44:43,810] Trial 1 finished with value: 0.009919091298697894 and parameters: {'n_estimators': 1100, 'max_depth': 90, 'min_samples_split': 12, 'min_samples_leaf': 7, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 1 with value: 0.009919091298697894.\n",
      "[I 2025-09-06 08:45:04,622] Trial 2 finished with value: 0.009391311156532672 and parameters: {'n_estimators': 1100, 'max_depth': 30, 'min_samples_split': 13, 'min_samples_leaf': 1, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 2 with value: 0.009391311156532672.\n",
      "[I 2025-09-06 08:45:05,452] Trial 3 finished with value: 0.012489161825258601 and parameters: {'n_estimators': 600, 'max_depth': None, 'min_samples_split': 6, 'min_samples_leaf': 9, 'max_features': 'sqrt', 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 2 with value: 0.009391311156532672.\n",
      "[I 2025-09-06 08:53:07,128] Trial 4 finished with value: 0.009273816275733447 and parameters: {'n_estimators': 400, 'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 4 with value: 0.009273816275733447.\n",
      "[I 2025-09-06 08:54:39,406] Trial 5 finished with value: 0.009339544116353383 and parameters: {'n_estimators': 700, 'max_depth': 60, 'min_samples_split': 11, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 4 with value: 0.009273816275733447.\n",
      "[I 2025-09-06 08:54:57,608] Trial 6 finished with value: 0.00915398881276298 and parameters: {'n_estimators': 1000, 'max_depth': 100, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 6 with value: 0.00915398881276298.\n",
      "[I 2025-09-06 08:54:58,248] Trial 7 finished with value: 0.01202115097455331 and parameters: {'n_estimators': 400, 'max_depth': 20, 'min_samples_split': 12, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 6 with value: 0.00915398881276298.\n",
      "[I 2025-09-06 08:55:12,138] Trial 8 finished with value: 0.009506783047893954 and parameters: {'n_estimators': 1000, 'max_depth': 40, 'min_samples_split': 17, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 6 with value: 0.00915398881276298.\n",
      "[I 2025-09-06 09:36:30,041] Trial 9 finished with value: 0.00945013762077704 and parameters: {'n_estimators': 1400, 'max_depth': 20, 'min_samples_split': 12, 'min_samples_leaf': 6, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 6 with value: 0.00915398881276298.\n",
      "[I 2025-09-06 10:37:42,446] Trial 10 finished with value: 0.011341879227793152 and parameters: {'n_estimators': 1400, 'max_depth': 100, 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 6 with value: 0.00915398881276298.\n",
      "[I 2025-09-06 10:53:30,632] Trial 11 finished with value: 0.009178550732050135 and parameters: {'n_estimators': 800, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 6 with value: 0.00915398881276298.\n",
      "[I 2025-09-06 11:07:48,169] Trial 12 finished with value: 0.009213891418872243 and parameters: {'n_estimators': 800, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 6 with value: 0.00915398881276298.\n",
      "[I 2025-09-06 11:25:43,193] Trial 13 finished with value: 0.009128210139208932 and parameters: {'n_estimators': 900, 'max_depth': 80, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 13 with value: 0.009128210139208932.\n",
      "[I 2025-09-06 11:25:55,858] Trial 14 finished with value: 0.009498763832019058 and parameters: {'n_estimators': 1200, 'max_depth': 80, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 13 with value: 0.009128210139208932.\n",
      "[I 2025-09-06 11:42:01,550] Trial 15 finished with value: 0.009273358893833948 and parameters: {'n_estimators': 900, 'max_depth': 80, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 13 with value: 0.009128210139208932.\n",
      "[I 2025-09-06 11:42:26,568] Trial 16 finished with value: 0.008968199158872547 and parameters: {'n_estimators': 1200, 'max_depth': 70, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 16 with value: 0.008968199158872547.\n",
      "[I 2025-09-06 12:06:37,512] Trial 17 finished with value: 0.009217452133969763 and parameters: {'n_estimators': 1300, 'max_depth': 70, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 16 with value: 0.008968199158872547.\n",
      "[I 2025-09-06 13:02:32,281] Trial 18 finished with value: 0.010097301872715077 and parameters: {'n_estimators': 1200, 'max_depth': 70, 'min_samples_split': 6, 'min_samples_leaf': 7, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 16 with value: 0.008968199158872547.\n",
      "[I 2025-09-06 13:03:15,306] Trial 19 finished with value: 0.008810385738539955 and parameters: {'n_estimators': 900, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 19 with value: 0.008810385738539955.\n",
      "[I 2025-09-06 13:03:50,338] Trial 20 finished with value: 0.009762583253994622 and parameters: {'n_estimators': 1200, 'max_depth': 50, 'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 19 with value: 0.008810385738539955.\n",
      "[I 2025-09-06 13:04:33,367] Trial 21 finished with value: 0.008818443476620166 and parameters: {'n_estimators': 900, 'max_depth': 110, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 19 with value: 0.008810385738539955.\n",
      "[I 2025-09-06 13:05:12,985] Trial 22 finished with value: 0.008788039942067482 and parameters: {'n_estimators': 800, 'max_depth': 110, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 22 with value: 0.008788039942067482.\n",
      "[I 2025-09-06 13:05:38,293] Trial 23 finished with value: 0.009293535420283315 and parameters: {'n_estimators': 700, 'max_depth': 110, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 22 with value: 0.008788039942067482.\n",
      "[I 2025-09-06 13:06:17,654] Trial 24 finished with value: 0.008788039942067484 and parameters: {'n_estimators': 800, 'max_depth': 110, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 22 with value: 0.008788039942067482.\n",
      "[I 2025-09-06 13:06:41,092] Trial 25 finished with value: 0.009508438785581235 and parameters: {'n_estimators': 700, 'max_depth': 120, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 22 with value: 0.008788039942067482.\n",
      "[I 2025-09-06 13:07:40,700] Trial 26 finished with value: 0.00862498326061378 and parameters: {'n_estimators': 800, 'max_depth': 50, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 26 with value: 0.00862498326061378.\n",
      "[I 2025-09-06 13:08:15,503] Trial 27 finished with value: 0.009084937277003002 and parameters: {'n_estimators': 600, 'max_depth': 110, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 26 with value: 0.00862498326061378.\n",
      "[I 2025-09-06 13:09:06,579] Trial 28 finished with value: 0.008785544110669084 and parameters: {'n_estimators': 800, 'max_depth': 110, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 26 with value: 0.00862498326061378.\n",
      "[I 2025-09-06 13:09:40,680] Trial 29 finished with value: 0.009087213241724724 and parameters: {'n_estimators': 600, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 26 with value: 0.00862498326061378.\n",
      "[I 2025-09-06 13:10:12,939] Trial 30 finished with value: 0.008792219314031501 and parameters: {'n_estimators': 500, 'max_depth': 110, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 26 with value: 0.00862498326061378.\n",
      "[I 2025-09-06 13:10:52,269] Trial 31 finished with value: 0.008628368761019268 and parameters: {'n_estimators': 800, 'max_depth': 110, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 26 with value: 0.00862498326061378.\n",
      "[I 2025-09-06 13:11:16,268] Trial 32 finished with value: 0.008524582856822649 and parameters: {'n_estimators': 800, 'max_depth': 90, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:11:45,700] Trial 33 finished with value: 0.008526533481791287 and parameters: {'n_estimators': 1000, 'max_depth': 90, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:12:09,868] Trial 34 finished with value: 0.00865492477703728 and parameters: {'n_estimators': 1000, 'max_depth': 90, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:12:53,471] Trial 35 finished with value: 0.009384500677669638 and parameters: {'n_estimators': 1100, 'max_depth': 90, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:12:55,219] Trial 36 finished with value: 0.00879749388965116 and parameters: {'n_estimators': 700, 'max_depth': 90, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:13:09,213] Trial 37 finished with value: 0.009786381236839763 and parameters: {'n_estimators': 1000, 'max_depth': None, 'min_samples_split': 3, 'min_samples_leaf': 8, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:13:32,456] Trial 38 finished with value: 0.008609433604557355 and parameters: {'n_estimators': 900, 'max_depth': 90, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:13:34,688] Trial 39 finished with value: 0.008979411019369192 and parameters: {'n_estimators': 1000, 'max_depth': 90, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:14:02,334] Trial 40 finished with value: 0.009493562482426686 and parameters: {'n_estimators': 900, 'max_depth': 90, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:14:18,749] Trial 41 finished with value: 0.008634348473698527 and parameters: {'n_estimators': 700, 'max_depth': 60, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:14:51,047] Trial 42 finished with value: 0.008528519832234575 and parameters: {'n_estimators': 1100, 'max_depth': 90, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:15:23,163] Trial 43 finished with value: 0.008528519832234571 and parameters: {'n_estimators': 1100, 'max_depth': 90, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:15:52,107] Trial 44 finished with value: 0.008606924511769722 and parameters: {'n_estimators': 1100, 'max_depth': 90, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:16:27,581] Trial 45 finished with value: 0.008547893170778975 and parameters: {'n_estimators': 1100, 'max_depth': 90, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:16:41,985] Trial 46 finished with value: 0.010035874169268074 and parameters: {'n_estimators': 1100, 'max_depth': 90, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:16:44,657] Trial 47 finished with value: 0.009234179730211952 and parameters: {'n_estimators': 1300, 'max_depth': 40, 'min_samples_split': 14, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:17:07,228] Trial 48 finished with value: 0.008781656629825054 and parameters: {'n_estimators': 1100, 'max_depth': 90, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:17:33,152] Trial 49 finished with value: 0.009194689367976076 and parameters: {'n_estimators': 1300, 'max_depth': 90, 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:17:59,771] Trial 50 finished with value: 0.010005680503284748 and parameters: {'n_estimators': 1100, 'max_depth': 120, 'min_samples_split': 18, 'min_samples_leaf': 2, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:18:31,626] Trial 51 finished with value: 0.008528519832234573 and parameters: {'n_estimators': 1100, 'max_depth': 90, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:19:04,101] Trial 52 finished with value: 0.008552754952709568 and parameters: {'n_estimators': 1000, 'max_depth': 90, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:19:29,866] Trial 53 finished with value: 0.009060766925627746 and parameters: {'n_estimators': 1200, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 13:19:50,589] Trial 54 finished with value: 0.008750867478407502 and parameters: {'n_estimators': 1000, 'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 14:16:50,185] Trial 55 finished with value: 0.008559692333345277 and parameters: {'n_estimators': 1200, 'max_depth': 90, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 14:17:12,123] Trial 56 finished with value: 0.008781656629825055 and parameters: {'n_estimators': 1100, 'max_depth': 90, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 14:17:14,361] Trial 57 finished with value: 0.009115812303341462 and parameters: {'n_estimators': 1000, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 14:59:23,481] Trial 58 finished with value: 0.008549224423167365 and parameters: {'n_estimators': 1300, 'max_depth': 100, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[I 2025-09-06 14:59:49,367] Trial 59 finished with value: 0.008630746510721363 and parameters: {'n_estimators': 1100, 'max_depth': 60, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 32 with value: 0.008524582856822649.\n",
      "[ET/Optuna/FFV] best params: {'n_estimators': 800, 'max_depth': 90, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}\n",
      "[ET/Optuna/FFV] val_MAE=0.008525  val_RMSE=0.015131  val_R2=0.7437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-06 15:00:12,369] A new study created in memory with name: ET_Tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:\n",
      "  X_train: (587, 1024) | X_test: (147, 1024)\n",
      "  y_train: (587,) | y_test: (147,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cca3b6c7004701b1507f22cdc38c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-06 15:00:13,004] Trial 0 finished with value: 0.03505347611730708 and parameters: {'n_estimators': 1000, 'max_depth': 120, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 0 with value: 0.03505347611730708.\n",
      "[I 2025-09-06 15:00:13,530] Trial 1 finished with value: 0.03486280541383221 and parameters: {'n_estimators': 400, 'max_depth': 70, 'min_samples_split': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 1 with value: 0.03486280541383221.\n",
      "[I 2025-09-06 15:00:16,280] Trial 2 finished with value: 0.03147888819241985 and parameters: {'n_estimators': 1000, 'max_depth': 70, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 2 with value: 0.03147888819241985.\n",
      "[I 2025-09-06 15:00:17,794] Trial 3 finished with value: 0.03240693206265789 and parameters: {'n_estimators': 1000, 'max_depth': 70, 'min_samples_split': 17, 'min_samples_leaf': 6, 'max_features': 1.0, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 2 with value: 0.03147888819241985.\n",
      "[I 2025-09-06 15:00:18,899] Trial 4 finished with value: 0.03182474394428752 and parameters: {'n_estimators': 700, 'max_depth': 100, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 2 with value: 0.03147888819241985.\n",
      "[I 2025-09-06 15:00:23,217] Trial 5 finished with value: 0.031210817934656923 and parameters: {'n_estimators': 1400, 'max_depth': None, 'min_samples_split': 12, 'min_samples_leaf': 9, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 5 with value: 0.031210817934656923.\n",
      "[I 2025-09-06 15:00:40,619] Trial 6 finished with value: 0.030263665702947713 and parameters: {'n_estimators': 1000, 'max_depth': 40, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 6 with value: 0.030263665702947713.\n",
      "[I 2025-09-06 15:00:41,308] Trial 7 finished with value: 0.030497075771371635 and parameters: {'n_estimators': 600, 'max_depth': 70, 'min_samples_split': 7, 'min_samples_leaf': 10, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 6 with value: 0.030263665702947713.\n",
      "[I 2025-09-06 15:00:46,757] Trial 8 finished with value: 0.030287016753725318 and parameters: {'n_estimators': 800, 'max_depth': 70, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 6 with value: 0.030263665702947713.\n",
      "[I 2025-09-06 15:00:47,687] Trial 9 finished with value: 0.033866882926943925 and parameters: {'n_estimators': 1100, 'max_depth': 70, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 6 with value: 0.030263665702947713.\n",
      "[I 2025-09-06 15:01:18,058] Trial 10 finished with value: 0.030469872947347394 and parameters: {'n_estimators': 1300, 'max_depth': 40, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 6 with value: 0.030263665702947713.\n",
      "[I 2025-09-06 15:01:35,271] Trial 11 finished with value: 0.030426998279073417 and parameters: {'n_estimators': 800, 'max_depth': 40, 'min_samples_split': 13, 'min_samples_leaf': 1, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 6 with value: 0.030263665702947713.\n",
      "[I 2025-09-06 15:01:47,891] Trial 12 finished with value: 0.02975954223356006 and parameters: {'n_estimators': 1200, 'max_depth': 90, 'min_samples_split': 16, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:02:00,391] Trial 13 finished with value: 0.029834682708400784 and parameters: {'n_estimators': 1200, 'max_depth': 90, 'min_samples_split': 20, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:02:12,903] Trial 14 finished with value: 0.02983468270840079 and parameters: {'n_estimators': 1200, 'max_depth': 90, 'min_samples_split': 20, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:02:27,693] Trial 15 finished with value: 0.02981869368087367 and parameters: {'n_estimators': 1400, 'max_depth': 90, 'min_samples_split': 16, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:02:42,428] Trial 16 finished with value: 0.029765701559535354 and parameters: {'n_estimators': 1400, 'max_depth': 90, 'min_samples_split': 15, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:02:55,937] Trial 17 finished with value: 0.029770984531907997 and parameters: {'n_estimators': 1300, 'max_depth': 20, 'min_samples_split': 15, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:03:10,415] Trial 18 finished with value: 0.031521507180649985 and parameters: {'n_estimators': 1200, 'max_depth': 110, 'min_samples_split': 18, 'min_samples_leaf': 7, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:03:22,859] Trial 19 finished with value: 0.029961844254708693 and parameters: {'n_estimators': 1400, 'max_depth': 50, 'min_samples_split': 14, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:03:41,414] Trial 20 finished with value: 0.029947032113577954 and parameters: {'n_estimators': 1300, 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:03:51,972] Trial 21 finished with value: 0.030025737945727746 and parameters: {'n_estimators': 1300, 'max_depth': 20, 'min_samples_split': 15, 'min_samples_leaf': 4, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:04:08,774] Trial 22 finished with value: 0.029834238108735492 and parameters: {'n_estimators': 1200, 'max_depth': 80, 'min_samples_split': 18, 'min_samples_leaf': 2, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:04:20,077] Trial 23 finished with value: 0.029989796126613937 and parameters: {'n_estimators': 1400, 'max_depth': 20, 'min_samples_split': 14, 'min_samples_leaf': 4, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:04:35,605] Trial 24 finished with value: 0.02981665372382714 and parameters: {'n_estimators': 1100, 'max_depth': 60, 'min_samples_split': 16, 'min_samples_leaf': 2, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:04:41,788] Trial 25 finished with value: 0.03070791003214474 and parameters: {'n_estimators': 1300, 'max_depth': 90, 'min_samples_split': 15, 'min_samples_leaf': 7, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:04:43,480] Trial 26 finished with value: 0.03451814938798616 and parameters: {'n_estimators': 1100, 'max_depth': 20, 'min_samples_split': 18, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:04:46,900] Trial 27 finished with value: 0.03165560965961477 and parameters: {'n_estimators': 1300, 'max_depth': 90, 'min_samples_split': 11, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:04:59,672] Trial 28 finished with value: 0.029869785558100832 and parameters: {'n_estimators': 1400, 'max_depth': 30, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:05:00,305] Trial 29 finished with value: 0.03306141821906286 and parameters: {'n_estimators': 900, 'max_depth': 120, 'min_samples_split': 13, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:05:11,685] Trial 30 finished with value: 0.029827446491150586 and parameters: {'n_estimators': 1100, 'max_depth': 60, 'min_samples_split': 19, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:05:27,166] Trial 31 finished with value: 0.02981665372382714 and parameters: {'n_estimators': 1100, 'max_depth': 60, 'min_samples_split': 16, 'min_samples_leaf': 2, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:05:44,067] Trial 32 finished with value: 0.029828532299427654 and parameters: {'n_estimators': 1200, 'max_depth': 60, 'min_samples_split': 16, 'min_samples_leaf': 2, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:05:52,909] Trial 33 finished with value: 0.030365324647716196 and parameters: {'n_estimators': 400, 'max_depth': None, 'min_samples_split': 17, 'min_samples_leaf': 1, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:06:10,106] Trial 34 finished with value: 0.030436619740488943 and parameters: {'n_estimators': 900, 'max_depth': 110, 'min_samples_split': 15, 'min_samples_leaf': 4, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:06:12,664] Trial 35 finished with value: 0.032584895866038764 and parameters: {'n_estimators': 1300, 'max_depth': 100, 'min_samples_split': 17, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:06:14,127] Trial 36 finished with value: 0.029964260916159644 and parameters: {'n_estimators': 1100, 'max_depth': 80, 'min_samples_split': 14, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:06:30,255] Trial 37 finished with value: 0.030477078174603277 and parameters: {'n_estimators': 1000, 'max_depth': 50, 'min_samples_split': 13, 'min_samples_leaf': 5, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:06:36,378] Trial 38 finished with value: 0.03034419571590539 and parameters: {'n_estimators': 1000, 'max_depth': 120, 'min_samples_split': 19, 'min_samples_leaf': 6, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:06:38,167] Trial 39 finished with value: 0.030775296402679477 and parameters: {'n_estimators': 1200, 'max_depth': 20, 'min_samples_split': 12, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:06:43,993] Trial 40 finished with value: 0.030894107906427874 and parameters: {'n_estimators': 1400, 'max_depth': 60, 'min_samples_split': 16, 'min_samples_leaf': 8, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:06:59,875] Trial 41 finished with value: 0.029869638881526578 and parameters: {'n_estimators': 1100, 'max_depth': 60, 'min_samples_split': 17, 'min_samples_leaf': 2, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:07:14,414] Trial 42 finished with value: 0.029772668002915382 and parameters: {'n_estimators': 1000, 'max_depth': 60, 'min_samples_split': 16, 'min_samples_leaf': 2, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 12 with value: 0.02975954223356006.\n",
      "[I 2025-09-06 15:07:24,932] Trial 43 finished with value: 0.029722508325234813 and parameters: {'n_estimators': 1000, 'max_depth': 60, 'min_samples_split': 15, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:07:33,371] Trial 44 finished with value: 0.0297792479551344 and parameters: {'n_estimators': 800, 'max_depth': 90, 'min_samples_split': 15, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:07:39,120] Trial 45 finished with value: 0.029956882722476715 and parameters: {'n_estimators': 700, 'max_depth': 100, 'min_samples_split': 14, 'min_samples_leaf': 4, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:07:46,661] Trial 46 finished with value: 0.02976275072886296 and parameters: {'n_estimators': 900, 'max_depth': 90, 'min_samples_split': 11, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:07:51,647] Trial 47 finished with value: 0.03031163064643847 and parameters: {'n_estimators': 900, 'max_depth': 90, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:07:52,863] Trial 48 finished with value: 0.03236882267561416 and parameters: {'n_estimators': 800, 'max_depth': 90, 'min_samples_split': 11, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:07:56,934] Trial 49 finished with value: 0.029942835155490743 and parameters: {'n_estimators': 600, 'max_depth': 90, 'min_samples_split': 12, 'min_samples_leaf': 4, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:08:02,926] Trial 50 finished with value: 0.02979395012494789 and parameters: {'n_estimators': 700, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:08:13,435] Trial 51 finished with value: 0.02972976460155487 and parameters: {'n_estimators': 1000, 'max_depth': 40, 'min_samples_split': 13, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:08:21,881] Trial 52 finished with value: 0.029905797343699362 and parameters: {'n_estimators': 1000, 'max_depth': 40, 'min_samples_split': 13, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:08:30,030] Trial 53 finished with value: 0.029969946721016435 and parameters: {'n_estimators': 900, 'max_depth': 40, 'min_samples_split': 13, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:08:38,656] Trial 54 finished with value: 0.030274163203010164 and parameters: {'n_estimators': 1300, 'max_depth': 40, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:08:40,638] Trial 55 finished with value: 0.03227970973257027 and parameters: {'n_estimators': 900, 'max_depth': 70, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:08:52,321] Trial 56 finished with value: 0.02998633335068721 and parameters: {'n_estimators': 1400, 'max_depth': 90, 'min_samples_split': 15, 'min_samples_leaf': 4, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:09:16,250] Trial 57 finished with value: 0.030452939004967004 and parameters: {'n_estimators': 1200, 'max_depth': 20, 'min_samples_split': 14, 'min_samples_leaf': 1, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:09:24,717] Trial 58 finished with value: 0.029754764253320343 and parameters: {'n_estimators': 800, 'max_depth': 110, 'min_samples_split': 11, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 43 with value: 0.029722508325234813.\n",
      "[I 2025-09-06 15:09:33,175] Trial 59 finished with value: 0.029705897129089703 and parameters: {'n_estimators': 800, 'max_depth': 110, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 59 with value: 0.029705897129089703.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-06 15:09:41,751] A new study created in memory with name: ET_Rg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ET/Optuna/Tc] best params: {'n_estimators': 800, 'max_depth': 110, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}\n",
      "[ET/Optuna/Tc] val_MAE=0.029706  val_RMSE=0.043069  val_R2=0.7630\n",
      "Splits:\n",
      "  X_train: (488, 1024) | X_test: (123, 1024)\n",
      "  y_train: (488,) | y_test: (123,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f2834d134549edbfbf0df1c661f76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-06 15:09:45,459] Trial 0 finished with value: 2.1853420373234624 and parameters: {'n_estimators': 1100, 'max_depth': 100, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 0 with value: 2.1853420373234624.\n",
      "[I 2025-09-06 15:09:53,965] Trial 1 finished with value: 2.192977698872571 and parameters: {'n_estimators': 1200, 'max_depth': 30, 'min_samples_split': 18, 'min_samples_leaf': 8, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 0 with value: 2.1853420373234624.\n",
      "[I 2025-09-06 15:09:55,238] Trial 2 finished with value: 2.4298811882841673 and parameters: {'n_estimators': 1100, 'max_depth': 50, 'min_samples_split': 6, 'min_samples_leaf': 9, 'max_features': 1.0, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 0 with value: 2.1853420373234624.\n",
      "[I 2025-09-06 15:09:57,255] Trial 3 finished with value: 2.020266916122951 and parameters: {'n_estimators': 1000, 'max_depth': 70, 'min_samples_split': 15, 'min_samples_leaf': 1, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 3 with value: 2.020266916122951.\n",
      "[I 2025-09-06 15:09:58,861] Trial 4 finished with value: 1.9644994311208543 and parameters: {'n_estimators': 800, 'max_depth': 110, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 4 with value: 1.9644994311208543.\n",
      "[I 2025-09-06 15:10:00,944] Trial 5 finished with value: 2.0591641105536387 and parameters: {'n_estimators': 1400, 'max_depth': 50, 'min_samples_split': 17, 'min_samples_leaf': 3, 'max_features': 1.0, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 4 with value: 1.9644994311208543.\n",
      "[I 2025-09-06 15:10:03,237] Trial 6 finished with value: 2.049377015402918 and parameters: {'n_estimators': 1400, 'max_depth': 50, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 1.0, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 4 with value: 1.9644994311208543.\n",
      "[I 2025-09-06 15:10:05,220] Trial 7 finished with value: 2.3288137797038986 and parameters: {'n_estimators': 1200, 'max_depth': 110, 'min_samples_split': 11, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 4 with value: 1.9644994311208543.\n",
      "[I 2025-09-06 15:10:08,082] Trial 8 finished with value: 2.211304205588355 and parameters: {'n_estimators': 500, 'max_depth': 120, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 4 with value: 1.9644994311208543.\n",
      "[I 2025-09-06 15:10:20,267] Trial 9 finished with value: 2.257984244470785 and parameters: {'n_estimators': 1400, 'max_depth': 50, 'min_samples_split': 16, 'min_samples_leaf': 9, 'max_features': 1.0, 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 4 with value: 1.9644994311208543.\n",
      "[I 2025-09-06 15:10:21,049] Trial 10 finished with value: 2.1670172331956716 and parameters: {'n_estimators': 700, 'max_depth': 110, 'min_samples_split': 2, 'min_samples_leaf': 7, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 4 with value: 1.9644994311208543.\n",
      "[I 2025-09-06 15:10:22,756] Trial 11 finished with value: 2.0220927272715064 and parameters: {'n_estimators': 800, 'max_depth': 70, 'min_samples_split': 14, 'min_samples_leaf': 1, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 4 with value: 1.9644994311208543.\n",
      "[I 2025-09-06 15:10:24,256] Trial 12 finished with value: 2.002291379285887 and parameters: {'n_estimators': 900, 'max_depth': 70, 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 4 with value: 1.9644994311208543.\n",
      "[I 2025-09-06 15:10:25,030] Trial 13 finished with value: 1.9629276988102007 and parameters: {'n_estimators': 600, 'max_depth': None, 'min_samples_split': 20, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 13 with value: 1.9629276988102007.\n",
      "[I 2025-09-06 15:10:25,653] Trial 14 finished with value: 1.9316516403299322 and parameters: {'n_estimators': 400, 'max_depth': None, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 14 with value: 1.9316516403299322.\n",
      "[I 2025-09-06 15:10:26,310] Trial 15 finished with value: 1.9316516403299322 and parameters: {'n_estimators': 400, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 14 with value: 1.9316516403299322.\n",
      "[I 2025-09-06 15:10:26,836] Trial 16 finished with value: 2.0216823500898036 and parameters: {'n_estimators': 400, 'max_depth': None, 'min_samples_split': 3, 'min_samples_leaf': 6, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 14 with value: 1.9316516403299322.\n",
      "[I 2025-09-06 15:10:27,346] Trial 17 finished with value: 1.9709928424343173 and parameters: {'n_estimators': 400, 'max_depth': None, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 14 with value: 1.9316516403299322.\n",
      "[I 2025-09-06 15:10:28,127] Trial 18 finished with value: 2.044690018204127 and parameters: {'n_estimators': 600, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 14 with value: 1.9316516403299322.\n",
      "[I 2025-09-06 15:10:28,517] Trial 19 finished with value: 2.1502886460794195 and parameters: {'n_estimators': 500, 'max_depth': 60, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 14 with value: 1.9316516403299322.\n",
      "[I 2025-09-06 15:10:29,217] Trial 20 finished with value: 2.007805846727613 and parameters: {'n_estimators': 400, 'max_depth': 40, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 14 with value: 1.9316516403299322.\n",
      "[I 2025-09-06 15:10:29,954] Trial 21 finished with value: 1.9681141356383514 and parameters: {'n_estimators': 600, 'max_depth': None, 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 14 with value: 1.9316516403299322.\n",
      "[I 2025-09-06 15:10:30,896] Trial 22 finished with value: 2.0282854850064362 and parameters: {'n_estimators': 600, 'max_depth': 90, 'min_samples_split': 13, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 14 with value: 1.9316516403299322.\n",
      "[I 2025-09-06 15:10:31,470] Trial 23 finished with value: 2.0239358698042684 and parameters: {'n_estimators': 500, 'max_depth': 80, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 14 with value: 1.9316516403299322.\n",
      "[I 2025-09-06 15:10:32,565] Trial 24 finished with value: 2.017102914024652 and parameters: {'n_estimators': 700, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 14 with value: 1.9316516403299322.\n",
      "[I 2025-09-06 15:10:33,216] Trial 25 finished with value: 1.931651640329932 and parameters: {'n_estimators': 400, 'max_depth': None, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 25 with value: 1.931651640329932.\n",
      "[I 2025-09-06 15:10:35,454] Trial 26 finished with value: 2.1515478021121837 and parameters: {'n_estimators': 400, 'max_depth': None, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 25 with value: 1.931651640329932.\n",
      "[I 2025-09-06 15:10:36,185] Trial 27 finished with value: 2.0048287764452644 and parameters: {'n_estimators': 500, 'max_depth': None, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 25 with value: 1.931651640329932.\n",
      "[I 2025-09-06 15:10:36,730] Trial 28 finished with value: 2.07105889537583 and parameters: {'n_estimators': 700, 'max_depth': 80, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 25 with value: 1.931651640329932.\n",
      "[I 2025-09-06 15:10:37,537] Trial 29 finished with value: 2.2564540048306307 and parameters: {'n_estimators': 400, 'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 25 with value: 1.931651640329932.\n",
      "[I 2025-09-06 15:10:38,053] Trial 30 finished with value: 2.222138012268274 and parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 8, 'min_samples_leaf': 10, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 25 with value: 1.931651640329932.\n",
      "[I 2025-09-06 15:10:38,951] Trial 31 finished with value: 1.9367254679243497 and parameters: {'n_estimators': 600, 'max_depth': None, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 25 with value: 1.931651640329932.\n",
      "[I 2025-09-06 15:10:39,508] Trial 32 finished with value: 1.9495188696816597 and parameters: {'n_estimators': 400, 'max_depth': None, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 25 with value: 1.931651640329932.\n",
      "[I 2025-09-06 15:10:40,407] Trial 33 finished with value: 2.0054651416223743 and parameters: {'n_estimators': 500, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 25 with value: 1.931651640329932.\n",
      "[I 2025-09-06 15:10:41,307] Trial 34 finished with value: 1.9367254679243497 and parameters: {'n_estimators': 600, 'max_depth': 120, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 25 with value: 1.931651640329932.\n",
      "[I 2025-09-06 15:10:41,824] Trial 35 finished with value: 1.9709928424343182 and parameters: {'n_estimators': 400, 'max_depth': 40, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 25 with value: 1.931651640329932.\n",
      "[I 2025-09-06 15:11:03,550] Trial 36 finished with value: 2.188211760127235 and parameters: {'n_estimators': 800, 'max_depth': 60, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 1.0, 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 25 with value: 1.931651640329932.\n",
      "[I 2025-09-06 15:11:04,295] Trial 37 finished with value: 1.9093505131962545 and parameters: {'n_estimators': 500, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:05,759] Trial 38 finished with value: 2.073369444271017 and parameters: {'n_estimators': 1000, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:06,684] Trial 39 finished with value: 1.9293854461914701 and parameters: {'n_estimators': 500, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:16,251] Trial 40 finished with value: 2.150428689269105 and parameters: {'n_estimators': 500, 'max_depth': 20, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 1.0, 'bootstrap': True, 'criterion': 'absolute_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:17,103] Trial 41 finished with value: 1.93593156910629 and parameters: {'n_estimators': 400, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:17,849] Trial 42 finished with value: 1.909350513196255 and parameters: {'n_estimators': 500, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:18,929] Trial 43 finished with value: 1.9570500848083683 and parameters: {'n_estimators': 700, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:19,520] Trial 44 finished with value: 2.145688081741653 and parameters: {'n_estimators': 500, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:20,293] Trial 45 finished with value: 1.9093505131962545 and parameters: {'n_estimators': 500, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:21,202] Trial 46 finished with value: 2.0695863577703664 and parameters: {'n_estimators': 1200, 'max_depth': 20, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:22,722] Trial 47 finished with value: 1.9666517563077799 and parameters: {'n_estimators': 900, 'max_depth': 20, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 0.9, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:33,317] Trial 48 finished with value: 2.469663907446484 and parameters: {'n_estimators': 600, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': False, 'criterion': 'absolute_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:34,748] Trial 49 finished with value: 2.1544887812450613 and parameters: {'n_estimators': 1300, 'max_depth': 90, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:35,714] Trial 50 finished with value: 1.9942424669062104 and parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:36,459] Trial 51 finished with value: 1.9359061327263838 and parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:37,131] Trial 52 finished with value: 1.9316516403299326 and parameters: {'n_estimators': 400, 'max_depth': 110, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:37,920] Trial 53 finished with value: 1.9376907674756094 and parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:38,667] Trial 54 finished with value: 1.9718009334460054 and parameters: {'n_estimators': 600, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:39,604] Trial 55 finished with value: 1.9492487779485321 and parameters: {'n_estimators': 700, 'max_depth': 120, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:40,380] Trial 56 finished with value: 2.0124020804856797 and parameters: {'n_estimators': 400, 'max_depth': 70, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:41,277] Trial 57 finished with value: 1.9104314334687897 and parameters: {'n_estimators': 600, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:42,429] Trial 58 finished with value: 1.9118936871611096 and parameters: {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[I 2025-09-06 15:11:43,327] Trial 59 finished with value: 2.2029153983518377 and parameters: {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 8, 'max_features': 0.9, 'bootstrap': True, 'criterion': 'squared_error'}. Best is trial 37 with value: 1.9093505131962545.\n",
      "[ET/Optuna/Rg] best params: {'n_estimators': 500, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'squared_error'}\n",
      "[ET/Optuna/Rg] val_MAE=1.909351  val_RMSE=2.786025  val_R2=0.6466\n"
     ]
    }
   ],
   "source": [
    "# If needed:\n",
    "# !pip install -q optuna\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import joblib, os\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def _rmse_safe(y_true, y_pred):\n",
    "    # works on older sklearn (no squared=)\n",
    "    try:\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)\n",
    "    except TypeError:\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def optuna_tune_extratrees(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    n_trials: int = 60,\n",
    "    timeout: Optional[int] = None,     # seconds; or None\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    metric: str = \"mae\",               # \"mae\" | \"rmse\" | \"mse\" | \"r2\" (maximize r2)\n",
    "    save_dir: str = \"saved_models/et_optuna\",\n",
    "    tag: str = \"model\",\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[ExtraTreesRegressor, Dict[str, Any], Dict[str, float], str, optuna.Study]:\n",
    "    \"\"\"\n",
    "    Runs Optuna to tune ExtraTreesRegressor on a single holdout split.\n",
    "    Returns (best_model, best_params, best_metrics, path_to_joblib, study).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # One fixed split for fair comparison with your RF helper\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False,\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=n_strat_bins\n",
    "    )\n",
    "\n",
    "    def objective(trial: optuna.Trial):\n",
    "        # Search space (kept tight to be Kaggle-friendly)\n",
    "        params = {\n",
    "            \"n_estimators\":      trial.suggest_int(\"n_estimators\", 400, 1400, step=100),\n",
    "            \"max_depth\":         trial.suggest_categorical(\"max_depth\", [None] + list(range(20, 121, 10))),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "            \"min_samples_leaf\":  trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "            \"max_features\":      trial.suggest_categorical(\"max_features\", [1.0, 0.9, 0.7, \"sqrt\"]),\n",
    "            \"bootstrap\":         trial.suggest_categorical(\"bootstrap\", [False, True]),\n",
    "            \"criterion\":         trial.suggest_categorical(\"criterion\", [\"squared_error\", \"absolute_error\"]),\n",
    "            \"random_state\":      random_state,\n",
    "            \"n_jobs\":            -1,\n",
    "        }\n",
    "\n",
    "        model = ExtraTreesRegressor(**params)\n",
    "        model.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "        pred_tr = model.predict(splits.X_train)\n",
    "        pred_te = model.predict(splits.X_test)\n",
    "\n",
    "        # Compute metrics on VAL split\n",
    "        mse  = mean_squared_error(splits.y_test, pred_te)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae  = mean_absolute_error(splits.y_test, pred_te)\n",
    "        r2   = r2_score(splits.y_test, pred_te)\n",
    "\n",
    "        # Choose objective\n",
    "        if metric == \"mae\":\n",
    "            score = mae\n",
    "        elif metric == \"rmse\":\n",
    "            score = rmse\n",
    "        elif metric == \"mse\":\n",
    "            score = mse\n",
    "        elif metric == \"r2\":\n",
    "            score = -r2  # maximize R2 by minimizing -R2\n",
    "        else:\n",
    "            score = mae  # default\n",
    "\n",
    "        # Report for potential pruning (single step)\n",
    "        trial.report(score, step=0)\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", study_name=f\"ET_{tag}\")\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=verbose)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    # Refit best model on the SAME training split (you can also refit on all data later)\n",
    "    best_model = ExtraTreesRegressor(**{\n",
    "        **best_params, \"random_state\": random_state, \"n_jobs\": -1\n",
    "    }).fit(splits.X_train, splits.y_train)\n",
    "\n",
    "    # Final validation metrics for the chosen params\n",
    "    pred_tr = best_model.predict(splits.X_train)\n",
    "    pred_te = best_model.predict(splits.X_test)\n",
    "    best_metrics = {\n",
    "        \"train_MAE\":  mean_absolute_error(splits.y_train, pred_tr),\n",
    "        \"train_RMSE\": _rmse_safe(splits.y_train, pred_tr),\n",
    "        \"train_R2\":   r2_score(splits.y_train, pred_tr),\n",
    "        \"val_MAE\":    mean_absolute_error(splits.y_test,  pred_te),\n",
    "        \"val_RMSE\":   _rmse_safe(splits.y_test,  pred_te),\n",
    "        \"val_R2\":     r2_score(splits.y_test,  pred_te),\n",
    "    }\n",
    "    if verbose:\n",
    "        print(f\"[ET/Optuna/{tag}] best params: {best_params}\")\n",
    "        print(f\"[ET/Optuna/{tag}] val_MAE={best_metrics['val_MAE']:.6f}  \"\n",
    "              f\"val_RMSE={best_metrics['val_RMSE']:.6f}  val_R2={best_metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"et_{tag}_best.joblib\")\n",
    "    joblib.dump({\"model\": best_model, \"params\": best_params, \"metrics\": best_metrics}, path)\n",
    "    return best_model, best_params, best_metrics, path, study\n",
    "\n",
    "\n",
    "# FFV (lots of data → good candidate)\n",
    "best_et_ffv, bp_ffv, met_ffv, p_et_ffv, study_ffv = optuna_tune_extratrees(\n",
    "    X_ffv, y_ffv, n_trials=60, metric=\"mae\", tag=\"FFV\"\n",
    ")\n",
    "\n",
    "# Tc\n",
    "best_et_tc, bp_tc, met_tc, p_et_tc, study_tc = optuna_tune_extratrees(\n",
    "    X_tc, y_tc, n_trials=60, metric=\"mae\", tag=\"Tc\"\n",
    ")\n",
    "\n",
    "# Rg\n",
    "best_et_rg, bp_rg, met_rg, p_et_rg, study_rg = optuna_tune_extratrees(\n",
    "    X_rg, y_rg, n_trials=60, metric=\"mae\", tag=\"Rg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe1cabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np, joblib, os\n",
    "\n",
    "def _rmse_safe(y_true, y_pred):\n",
    "    # Kaggle-safe RMSE (handles older sklearn)\n",
    "    try:\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)\n",
    "    except TypeError:\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def train_eval_extratrees(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    et_params: Dict[str, Any],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    save_dir: str = \"saved_models/et\",\n",
    "    tag: str = \"model\",\n",
    ") -> Tuple[ExtraTreesRegressor, Dict[str, float], \"TabularSplits\", str]:\n",
    "    \"\"\"\n",
    "    Trains an ExtraTreesRegressor on unscaled features; returns (model, metrics, splits, path).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False,\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=n_strat_bins\n",
    "    )\n",
    "\n",
    "    params = dict(et_params) if et_params else {}\n",
    "    params.setdefault(\"random_state\", random_state)\n",
    "    params.setdefault(\"n_jobs\", -1)\n",
    "\n",
    "    et = ExtraTreesRegressor(**params)\n",
    "    et.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "    pred_tr = et.predict(splits.X_train)\n",
    "    pred_te = et.predict(splits.X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_MAE\":  mean_absolute_error(splits.y_train, pred_tr),\n",
    "        \"train_RMSE\": _rmse_safe(splits.y_train, pred_tr),\n",
    "        \"train_R2\":   r2_score(splits.y_train, pred_tr),\n",
    "        \"val_MAE\":    mean_absolute_error(splits.y_test,  pred_te),\n",
    "        \"val_RMSE\":   _rmse_safe(splits.y_test,  pred_te),\n",
    "        \"val_R2\":     r2_score(splits.y_test,  pred_te),\n",
    "    }\n",
    "    print(f\"[ET/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"et_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": et, \"metrics\": metrics, \"params\": params}, path)\n",
    "    return et, metrics, splits, path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e736596b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:\n",
      "  X_train: (5624, 1024) | X_test: (1406, 1024)\n",
      "  y_train: (5624,) | y_test: (1406,)\n",
      "[ET/FFV] val_MAE=0.011200  val_RMSE=0.020049  val_R2=0.5501\n",
      "Splits:\n",
      "  X_train: (587, 1024) | X_test: (147, 1024)\n",
      "  y_train: (587,) | y_test: (147,)\n",
      "[ET/Tc] val_MAE=0.036053  val_RMSE=0.056604  val_R2=0.5906\n",
      "Splits:\n",
      "  X_train: (488, 1024) | X_test: (123, 1024)\n",
      "  y_train: (488,) | y_test: (123,)\n",
      "[ET/Rg] val_MAE=2.306750  val_RMSE=3.373014  val_R2=0.4821\n"
     ]
    }
   ],
   "source": [
    "et_cfg = {\n",
    "    \"FFV\": {\n",
    "        \"n_estimators\": 1000, \"max_depth\": None,\n",
    "        \"min_samples_split\": 2, \"min_samples_leaf\": 1,\n",
    "        \"max_features\": 1.0,   # <- often strong for ExtraTrees\n",
    "        \"bootstrap\": False,\n",
    "    },\n",
    "    \"Tc\": {\n",
    "        \"n_estimators\": 800, \"max_depth\": 40,\n",
    "        \"min_samples_split\": 2, \"min_samples_leaf\": 1,\n",
    "        \"max_features\": 0.9,\n",
    "        \"bootstrap\": False,\n",
    "    },\n",
    "    \"Rg\": {\n",
    "        \"n_estimators\": 1200, \"max_depth\": 60,\n",
    "        \"min_samples_split\": 2, \"min_samples_leaf\": 2,  # a touch more regularization\n",
    "        \"max_features\": 1.0,\n",
    "        \"bootstrap\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "et_ffv, m_et_ffv, splits_et_ffv, p_et_ffv = train_eval_extratrees(X_ffv, y_ffv, et_params=et_cfg[\"FFV\"], tag=\"FFV\")\n",
    "et_tc,  m_et_tc,  splits_et_tc,  p_et_tc  = train_eval_extratrees(X_tc,  y_tc,  et_params=et_cfg[\"Tc\"],  tag=\"Tc\")\n",
    "et_rg,  m_et_rg,  splits_et_rg,  p_et_rg  = train_eval_extratrees(X_rg,  y_rg,  et_params=et_cfg[\"Rg\"],  tag=\"Rg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77f7ec",
   "metadata": {},
   "source": [
    "## ChemML GNN Model Results\n",
    "| Model Type             | Featurization        |   MAE |  RMSE |   R² | Notes             |\n",
    "|------------------------|----------------------|-------|-------|------|-------------------|\n",
    "| GNN (Tuned)            | tensorise_molecules Graph   | 0.302 | 0.411 | 0.900 | Best performance across all metrics   |\n",
    "| GNN (Untuned)          | tensorise_molecules Graph   | 0.400 | 0.519 | 0.841 | Good overall|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42db218",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Model Training\n",
    "\n",
    "Having explored different molecular graph representations and model architectures, I am now moving to training what is expected to be the best-performing model using the full dataset. The earlier GNN model was based on `tensorise_molecules` (ChemML) graphs and had strong performance with a **mean absolute error (MAE) around 0.30**. These graphs are based on RDKit's internal descriptors and do not reflect the original PCQM4Mv2 graph structure used in the Open Graph Benchmark (OGB). Therefore, I will shift focus to the `smiles2graph` representation provided by OGB, which aligns more directly with the benchmark's evaluation setup and top-performing models on the leaderboard.\n",
    "\n",
    "\n",
    "| Source                         | Atom/Bond Features                                                 | Format                                          | Customizable?     | Alignment with PCQM4Mv2?  |\n",
    "| ------------------------------ | ------------------------------------------------------------------ | ----------------------------------------------- | ----------------- | ---------------------- |\n",
    "| `tensorise_molecules` (ChemML) | RDKit-based descriptors (ex: atom number, degree, hybridization) | NumPy tensors (`X_atoms`, `X_bonds`, `X_edges`) | Limited           |  Not aligned          |\n",
    "| `smiles2graph` (OGB / PyG)     | Predefined categorical features from PCQM4Mv2                      | PyTorch Geometric `Data` objects                |  Highly flexible |  Matches OGB standard |\n",
    "\n",
    "By using `smiles2graph`, we:\n",
    "\n",
    "* Use OGB-standard graph construction and feature encoding for fair comparisons with leaderboard models\n",
    "* Include learnable AtomEncoder and BondEncoder embeddings from `ogb.graphproppred.mol_encoder`, which improve model expressiveness\n",
    "* Maintain compatibility with PyTorch Geometric, DGL, and OGB tools\n",
    "\n",
    "I will also concatenate GNN-derived embeddings with SMILES-based RDKit descriptors, feeding this hybrid representation into MLP head. This allows you to combine structural and cheminformatics perspectives for improved prediction accuracy. With this setup, I aim to improve upon the MAE of \\~0.30 achieved earlier and push closer toward state-of-the-art performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced601e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Descriptors, rdMolDescriptors as rdmd\n",
    "# def compute_rdkit_descriptors(smiles: str):\n",
    "#     s = canonicalize_polymer_smiles(smiles)\n",
    "#     m = Chem.MolFromSmiles(s)\n",
    "#     if m is None or m.GetNumAtoms() == 0:\n",
    "#         return [np.nan]*9\n",
    "#     return [\n",
    "#         Descriptors.MolWt(m),\n",
    "#         Descriptors.NumRotatableBonds(m),\n",
    "#         Descriptors.TPSA(m),\n",
    "#         Descriptors.NumHAcceptors(m),\n",
    "#         Descriptors.NumHDonors(m),\n",
    "#         Descriptors.RingCount(m),\n",
    "#         Descriptors.FractionCSP3(m),\n",
    "#         Descriptors.MolLogP(m),\n",
    "#         Descriptors.NumSaturatedRings(m),\n",
    "#     ]\n",
    "\n",
    "\n",
    "def compute_rdkit_descriptors(m):\n",
    "    s = canonicalize_polymer_smiles(m)\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    heavy = Descriptors.HeavyAtomCount(m)\n",
    "    arom_atoms = sum(a.GetIsAromatic() for a in m.GetAtoms())\n",
    "    hetero = sum(a.GetAtomicNum() not in (1,6) for a in m.GetAtoms())\n",
    "    aromatic_prop = (arom_atoms / max(1, heavy))\n",
    "    hetero_frac   = (hetero / max(1, heavy))\n",
    "\n",
    "    return [\n",
    "        Descriptors.MolMR(m),                    # 1\n",
    "        Descriptors.MolLogP(m),                  # 2 (SlogP)\n",
    "        rdmd.CalcFractionCSP3(m),                # 3\n",
    "        rdmd.CalcNumAromaticRings(m),            # 4\n",
    "        aromatic_prop,                           # 5\n",
    "        Descriptors.TPSA(m),                     # 6\n",
    "        Descriptors.NumRotatableBonds(m),        # 7\n",
    "        hetero_frac,                             # 8\n",
    "        rdmd.CalcLabuteASA(m),                   # 9\n",
    "        # rdmd.CalcBalabanJ(m),                    # 10\n",
    "        rdmd.CalcKappa1(m),                      # 11\n",
    "        rdmd.CalcKappa2(m),                      # 12\n",
    "    ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3efce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tg descriptors: (504, 6) labels: (504,)\n",
      "Density descriptors: (609, 6) labels: (609,)\n"
     ]
    }
   ],
   "source": [
    "# -- Tg --\n",
    "df_tg = pd.read_csv('cleaned_tg_dataset.csv')\n",
    "smiles_tg = df_tg['SMILES'].tolist()\n",
    "y_tg = df_tg['Tg'].to_numpy(dtype=float)\n",
    "\n",
    "X_tg_desc = np.array([compute_rdkit_descriptors(s) for s in smiles_tg])\n",
    "mask_tg = ~np.isnan(X_tg_desc).any(axis=1)\n",
    "smiles_tg = list(np.array(smiles_tg)[mask_tg])\n",
    "y_tg = y_tg[mask_tg]\n",
    "X_tg_desc = X_tg_desc[mask_tg]\n",
    "print(\"Tg descriptors:\", X_tg_desc.shape, \"labels:\", y_tg.shape)\n",
    "\n",
    "# -- Density --\n",
    "df_density = pd.read_csv('cleaned_density_dataset.csv')\n",
    "smiles_density = df_density['SMILES'].tolist()\n",
    "y_density = df_density['Density'].to_numpy(dtype=float)\n",
    "\n",
    "X_density_desc = np.array([compute_rdkit_descriptors(s) for s in smiles_density])\n",
    "mask_den = ~np.isnan(X_density_desc).any(axis=1)\n",
    "smiles_density = list(np.array(smiles_density)[mask_den])\n",
    "y_density = y_density[mask_den]\n",
    "X_density_desc = X_density_desc[mask_den]\n",
    "print(\"Density descriptors:\", X_density_desc.shape, \"labels:\", y_density.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65a87050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_pyg_dataset(smiles: list, X_desc: np.ndarray, y: np.ndarray) -> list:\n",
    "    assert len(smiles) == len(y) == X_desc.shape[0]\n",
    "    out = []\n",
    "    for i, smi in enumerate(smiles):\n",
    "        g = smiles2graph(smi)\n",
    "        out.append(Data(\n",
    "            x=torch.tensor(g['node_feat'], dtype=torch.long),\n",
    "            edge_index=torch.tensor(g['edge_index'], dtype=torch.long),\n",
    "            edge_attr=torch.tensor(g['edge_feat'], dtype=torch.long),\n",
    "            rdkit_feats=torch.tensor(X_desc[i], dtype=torch.float32),\n",
    "            y=torch.tensor([y[i]], dtype=torch.float32),\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "def make_loaders(data_list, test_size=0.2, batch_size=32, seed=42):\n",
    "    tr, va = train_test_split(data_list, test_size=test_size, random_state=seed)\n",
    "    return (\n",
    "        GeoDataLoader(tr, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True),\n",
    "        GeoDataLoader(va, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True),\n",
    "    )\n",
    "\n",
    "data_tg = build_pyg_dataset(smiles_tg, X_tg_desc, y_tg)\n",
    "train_loader_tg, val_loader_tg = make_loaders(data_tg, test_size=0.2, batch_size=32, seed=42)\n",
    "\n",
    "data_density = build_pyg_dataset(smiles_density, X_density_desc, y_density)\n",
    "train_loader_den, val_loader_den = make_loaders(data_density, test_size=0.2, batch_size=32, seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983db98",
   "metadata": {},
   "source": [
    "## Step 5: Define the Hybrid GNN Model\n",
    "\n",
    "The final architecture uses both structural and cheminformatics data by combining GNN-learned graph embeddings with SMILES-derived RDKit descriptors. This Hybrid GNN model uses `smiles2graph` for graph construction and augments it with RDKit-based molecular features for improved prediction accuracy.\n",
    "\n",
    "### Model Components:\n",
    "\n",
    "* **AtomEncoder / BondEncoder**\n",
    "  Transforms categorical atom and bond features (provided by OGB) into learnable embeddings using the encoders from `ogb.graphproppred.mol_encoder`. These provide a strong foundation for expressive graph learning.\n",
    "\n",
    "* **GINEConv Layers (x2)**\n",
    "  I use two stacked GINEConv layers (Graph Isomorphism Network with Edge features). These layers perform neighborhood aggregation based on edge attributes, allowing the model to capture localized chemical environments.\n",
    "\n",
    "* **Global Mean Pooling**\n",
    "  After message passing, node level embeddings are aggregated into a fixed size graph level representation using `global_mean_pool`.\n",
    "\n",
    "* **Concatenation with RDKit Descriptors**\n",
    "  The pooled GNN embedding is concatenated with external RDKit descriptors, which capture global molecular properties not easily inferred from graph data alone.\n",
    "\n",
    "* **MLP Prediction Head**\n",
    "  A multilayer perceptron processes the combined feature vector with ReLU activations, dropout regularization, and linear layers to predict the HOMO–LUMO gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0946f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def _act(name: str):\n",
    "    name = (name or \"ReLU\").lower()\n",
    "    if name in (\"relu\",):   return nn.ReLU()\n",
    "    if name in (\"gelu\",):   return nn.GELU()\n",
    "    if name in (\"swish\",\"silu\"): return nn.SiLU()\n",
    "    return nn.ReLU()\n",
    "\n",
    "class HybridGNN(Module):\n",
    "    def __init__(self, gnn_dim: int, rdkit_dim: int, hidden_dim: int, dropout_rate: float=0.2, activation: str=\"ReLU\"):\n",
    "        super().__init__()\n",
    "        self.gnn_dim = gnn_dim\n",
    "        self.rdkit_dim = rdkit_dim\n",
    "        act = _act(activation)\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=gnn_dim)\n",
    "        self.bond_encoder = BondEncoder(emb_dim=gnn_dim)\n",
    "\n",
    "        self.conv1 = GINEConv(Sequential(Linear(gnn_dim, gnn_dim), act, Linear(gnn_dim, gnn_dim)))\n",
    "        self.conv2 = GINEConv(Sequential(Linear(gnn_dim, gnn_dim), act, Linear(gnn_dim, gnn_dim)))\n",
    "        self.pool = global_mean_pool\n",
    "\n",
    "        self.mlp = Sequential(\n",
    "            Linear(gnn_dim + rdkit_dim, hidden_dim), act, Dropout(dropout_rate),\n",
    "            Linear(hidden_dim, hidden_dim // 2), act, Dropout(dropout_rate),\n",
    "            Linear(hidden_dim // 2, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # encode atoms and bonds\n",
    "        x = self.atom_encoder(data.x)\n",
    "        edge_attr = self.bond_encoder(data.edge_attr)\n",
    "\n",
    "        # GNN convolutions\n",
    "        x = self.conv1(x, data.edge_index, edge_attr)\n",
    "        x = self.conv2(x, data.edge_index, edge_attr)\n",
    "        x = self.pool(x, data.batch)\n",
    "\n",
    "        # handle RDKit features\n",
    "        rdkit_feats = getattr(data, 'rdkit_feats', None)\n",
    "        if rdkit_feats is not None:\n",
    "            # Reshape the RDKit features tensor to be (batch_size, rdkit_dim)\n",
    "            # The number of samples in the batch is given by x.shape[0] after pooling\n",
    "            reshaped_rdkit_feats = rdkit_feats.view(x.shape[0], self.rdkit_dim)\n",
    "            \n",
    "            if x.shape[0] != reshaped_rdkit_feats.shape[0]:\n",
    "                raise ValueError(f\"Shape mismatch: GNN output ({x.shape[0]}) vs rdkit_feats ({reshaped_rdkit_feats.shape[0]})\")\n",
    "            \n",
    "            x = torch.cat([x, reshaped_rdkit_feats], dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"RDKit features not found in the data object\")\n",
    "\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc992041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid_gnn(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    *,\n",
    "    lr: float,\n",
    "    optimizer: str = \"Adam\",\n",
    "    weight_decay: float = 0.0,\n",
    "    epochs: int = 100,\n",
    "    patience: int = 10,\n",
    "    save_dir: str = \"saved_models/gnn\",\n",
    "    tag: str = \"model\",\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "    if optimizer.lower() == \"adamw\":\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best, bad = float(\"inf\"), 0\n",
    "    best_path = os.path.join(save_dir, f\"{tag}.pt\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_once(loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        for b in loader:\n",
    "            b = b.to(device)\n",
    "            p = model(b)\n",
    "            preds.append(p.cpu())\n",
    "            trues.append(b.y.view(-1,1).cpu())\n",
    "        preds = torch.cat(preds).numpy(); trues = torch.cat(trues).numpy()\n",
    "        mse = np.mean((preds - trues)**2)\n",
    "        return mse, preds, trues\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total, count = 0.0, 0\n",
    "        for b in train_loader:\n",
    "            b = b.to(device)\n",
    "            pred = model(b)\n",
    "            loss = F.mse_loss(pred, b.y.view(-1,1))\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            total += loss.item() * b.num_graphs\n",
    "            count += b.num_graphs\n",
    "        tr_mse = total / max(1, count)\n",
    "        va_mse, _, _ = eval_once(val_loader)\n",
    "        print(f\"Epoch {ep:02d} | Train MSE {tr_mse:.5f} | Val MSE {va_mse:.5f}\")\n",
    "\n",
    "        if va_mse < best - 1e-7:\n",
    "            best, bad = va_mse, 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    val_mse, val_pred, val_true = eval_once(val_loader)\n",
    "    mae = np.mean(np.abs(val_pred - val_true))\n",
    "    rmse = np.sqrt(val_mse)\n",
    "    r2 = 1 - np.sum((val_pred - val_true)**2) / np.sum((val_true - val_true.mean())**2)\n",
    "    print(f\"[{tag}] Best Val — MAE {mae:.6f} | RMSE {rmse:.6f} | R2 {r2:.4f}\")\n",
    "    return model, best_path, {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7e9cb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE 15444.07007 | Val MSE 11713.92578\n",
      "Epoch 02 | Train MSE 11183.00550 | Val MSE 10380.86719\n",
      "Epoch 03 | Train MSE 10230.25985 | Val MSE 8342.48535\n",
      "Epoch 04 | Train MSE 9003.71527 | Val MSE 7855.45605\n",
      "Epoch 05 | Train MSE 7574.64763 | Val MSE 5449.46729\n",
      "Epoch 06 | Train MSE 6721.04160 | Val MSE 5008.34277\n",
      "Epoch 07 | Train MSE 6470.01543 | Val MSE 5736.78174\n",
      "Epoch 08 | Train MSE 6253.05764 | Val MSE 4692.91650\n",
      "Epoch 09 | Train MSE 6326.17281 | Val MSE 4487.83496\n",
      "Epoch 10 | Train MSE 6046.40858 | Val MSE 4503.41943\n",
      "Epoch 11 | Train MSE 5916.84184 | Val MSE 4242.87891\n",
      "Epoch 12 | Train MSE 6213.79196 | Val MSE 4538.29443\n",
      "Epoch 13 | Train MSE 5540.59278 | Val MSE 4418.76318\n",
      "Epoch 14 | Train MSE 5703.22842 | Val MSE 4029.75366\n",
      "Epoch 15 | Train MSE 5410.52960 | Val MSE 4288.02002\n",
      "Epoch 16 | Train MSE 5124.28050 | Val MSE 4578.30908\n",
      "Epoch 17 | Train MSE 5743.87881 | Val MSE 4772.59766\n",
      "Epoch 18 | Train MSE 5931.18839 | Val MSE 6021.19434\n",
      "Epoch 19 | Train MSE 5849.53569 | Val MSE 3891.39087\n",
      "Epoch 20 | Train MSE 5193.36963 | Val MSE 3867.99341\n",
      "Epoch 21 | Train MSE 5478.89697 | Val MSE 3952.25000\n",
      "Epoch 22 | Train MSE 5048.76298 | Val MSE 4122.46729\n",
      "Epoch 23 | Train MSE 5038.29762 | Val MSE 3948.87231\n",
      "Epoch 24 | Train MSE 4763.12860 | Val MSE 3697.10376\n",
      "Epoch 25 | Train MSE 4929.79609 | Val MSE 3969.11377\n",
      "Epoch 26 | Train MSE 6027.08540 | Val MSE 3682.50659\n",
      "Epoch 27 | Train MSE 5351.14751 | Val MSE 4217.62646\n",
      "Epoch 28 | Train MSE 5589.67810 | Val MSE 3739.10742\n",
      "Epoch 29 | Train MSE 5202.31821 | Val MSE 3804.53491\n",
      "Epoch 30 | Train MSE 4823.06416 | Val MSE 3558.73364\n",
      "Epoch 31 | Train MSE 4656.82289 | Val MSE 3681.84009\n",
      "Epoch 32 | Train MSE 4563.66538 | Val MSE 4103.36523\n",
      "Epoch 33 | Train MSE 4642.52707 | Val MSE 3781.84497\n",
      "Epoch 34 | Train MSE 4561.41748 | Val MSE 3793.14136\n",
      "Epoch 35 | Train MSE 4560.70260 | Val MSE 3554.46753\n",
      "Epoch 36 | Train MSE 4446.53263 | Val MSE 3804.94360\n",
      "Epoch 37 | Train MSE 4562.30788 | Val MSE 3464.62427\n",
      "Epoch 38 | Train MSE 4801.45749 | Val MSE 4006.88037\n",
      "Epoch 39 | Train MSE 4615.34111 | Val MSE 3434.70557\n",
      "Epoch 40 | Train MSE 4750.51517 | Val MSE 4346.52930\n",
      "Epoch 41 | Train MSE 5549.08938 | Val MSE 3567.71802\n",
      "Epoch 42 | Train MSE 4845.33600 | Val MSE 3774.77637\n",
      "Epoch 43 | Train MSE 4657.54159 | Val MSE 3605.95166\n",
      "Epoch 44 | Train MSE 4559.13792 | Val MSE 3498.74414\n",
      "Epoch 45 | Train MSE 4529.65628 | Val MSE 3470.05078\n",
      "Epoch 46 | Train MSE 4424.83588 | Val MSE 3466.80005\n",
      "Epoch 47 | Train MSE 4369.87498 | Val MSE 3399.83472\n",
      "Epoch 48 | Train MSE 4295.00172 | Val MSE 3534.23853\n",
      "Epoch 49 | Train MSE 4622.50609 | Val MSE 3724.36963\n",
      "Epoch 50 | Train MSE 4539.81966 | Val MSE 3618.07715\n",
      "Epoch 51 | Train MSE 4765.30139 | Val MSE 3514.70166\n",
      "Epoch 52 | Train MSE 4133.80987 | Val MSE 3645.13550\n",
      "Epoch 53 | Train MSE 4434.95452 | Val MSE 3468.35913\n",
      "Epoch 54 | Train MSE 4414.98507 | Val MSE 3450.59692\n",
      "Epoch 55 | Train MSE 4101.03116 | Val MSE 3548.33911\n",
      "Epoch 56 | Train MSE 4281.57242 | Val MSE 3790.23267\n",
      "Epoch 57 | Train MSE 4098.86615 | Val MSE 3330.06006\n",
      "Epoch 58 | Train MSE 4098.96918 | Val MSE 3815.35645\n",
      "Epoch 59 | Train MSE 4995.44720 | Val MSE 4212.14502\n",
      "Epoch 60 | Train MSE 4995.45208 | Val MSE 4296.12549\n",
      "Epoch 61 | Train MSE 4940.77451 | Val MSE 3738.09253\n",
      "Epoch 62 | Train MSE 4190.18960 | Val MSE 3541.83765\n",
      "Epoch 63 | Train MSE 4356.33919 | Val MSE 3830.46802\n",
      "Epoch 64 | Train MSE 4098.43047 | Val MSE 3434.54443\n",
      "Epoch 65 | Train MSE 4330.35870 | Val MSE 4817.04004\n",
      "Epoch 66 | Train MSE 4406.85986 | Val MSE 3687.87988\n",
      "Epoch 67 | Train MSE 4039.41742 | Val MSE 3815.24976\n",
      "Early stopping.\n",
      "[hybridgnn_tg] Best Val — MAE 41.719727 | RMSE 57.706673 | R2 0.7326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattg\\AppData\\Local\\Temp\\ipykernel_33644\\3084240870.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "model_tg = HybridGNN(\n",
    "    gnn_dim=128, rdkit_dim=X_tg_desc.shape[1], hidden_dim=256,\n",
    "    dropout_rate=0.2, activation=\"ReLU\"\n",
    ")\n",
    "model_tg, ckpt_tg, metrics_tg = train_hybrid_gnn(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=1e-3, optimizer=\"Adam\", weight_decay=0.0,\n",
    "    epochs=100, patience=10, save_dir=\"saved_models/gnn_tg\", tag=\"hybridgnn_tg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a44f756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE 2.17636 | Val MSE 0.06613\n",
      "Epoch 02 | Train MSE 0.38810 | Val MSE 0.03597\n",
      "Epoch 03 | Train MSE 0.12518 | Val MSE 0.02702\n",
      "Epoch 04 | Train MSE 0.07666 | Val MSE 0.02018\n",
      "Epoch 05 | Train MSE 0.06099 | Val MSE 0.03023\n",
      "Epoch 06 | Train MSE 0.05001 | Val MSE 0.01728\n",
      "Epoch 07 | Train MSE 0.04629 | Val MSE 0.02006\n",
      "Epoch 08 | Train MSE 0.03975 | Val MSE 0.01823\n",
      "Epoch 09 | Train MSE 0.03617 | Val MSE 0.01693\n",
      "Epoch 10 | Train MSE 0.03190 | Val MSE 0.01054\n",
      "Epoch 11 | Train MSE 0.03053 | Val MSE 0.01393\n",
      "Epoch 12 | Train MSE 0.03381 | Val MSE 0.01481\n",
      "Epoch 13 | Train MSE 0.02940 | Val MSE 0.01107\n",
      "Epoch 14 | Train MSE 0.02420 | Val MSE 0.01387\n",
      "Epoch 15 | Train MSE 0.03742 | Val MSE 0.01703\n",
      "Epoch 16 | Train MSE 0.02867 | Val MSE 0.01163\n",
      "Epoch 17 | Train MSE 0.02569 | Val MSE 0.01301\n",
      "Epoch 18 | Train MSE 0.02469 | Val MSE 0.00796\n",
      "Epoch 19 | Train MSE 0.02491 | Val MSE 0.00744\n",
      "Epoch 20 | Train MSE 0.02079 | Val MSE 0.00660\n",
      "Epoch 21 | Train MSE 0.02265 | Val MSE 0.01368\n",
      "Epoch 22 | Train MSE 0.02465 | Val MSE 0.00674\n",
      "Epoch 23 | Train MSE 0.02112 | Val MSE 0.01164\n",
      "Epoch 24 | Train MSE 0.02148 | Val MSE 0.00631\n",
      "Epoch 25 | Train MSE 0.02125 | Val MSE 0.01001\n",
      "Epoch 26 | Train MSE 0.02023 | Val MSE 0.01439\n",
      "Epoch 27 | Train MSE 0.02130 | Val MSE 0.00720\n",
      "Epoch 28 | Train MSE 0.01792 | Val MSE 0.00377\n",
      "Epoch 29 | Train MSE 0.01843 | Val MSE 0.00683\n",
      "Epoch 30 | Train MSE 0.02062 | Val MSE 0.00359\n",
      "Epoch 31 | Train MSE 0.01636 | Val MSE 0.00405\n",
      "Epoch 32 | Train MSE 0.02332 | Val MSE 0.01040\n",
      "Epoch 33 | Train MSE 0.01907 | Val MSE 0.00481\n",
      "Epoch 34 | Train MSE 0.02061 | Val MSE 0.00674\n",
      "Epoch 35 | Train MSE 0.02064 | Val MSE 0.00653\n",
      "Epoch 36 | Train MSE 0.01719 | Val MSE 0.00456\n",
      "Epoch 37 | Train MSE 0.02093 | Val MSE 0.00674\n",
      "Epoch 38 | Train MSE 0.02150 | Val MSE 0.01134\n",
      "Epoch 39 | Train MSE 0.01817 | Val MSE 0.00838\n",
      "Epoch 40 | Train MSE 0.01917 | Val MSE 0.00632\n",
      "Epoch 41 | Train MSE 0.01693 | Val MSE 0.00625\n",
      "Epoch 42 | Train MSE 0.01607 | Val MSE 0.00629\n",
      "Epoch 43 | Train MSE 0.01562 | Val MSE 0.00704\n",
      "Epoch 44 | Train MSE 0.01512 | Val MSE 0.00526\n",
      "Epoch 45 | Train MSE 0.01348 | Val MSE 0.00672\n",
      "Early stopping.\n",
      "[hybridgnn_density] Best Val — MAE 0.040481 | RMSE 0.059955 | R2 0.8305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattg\\AppData\\Local\\Temp\\ipykernel_33644\\3084240870.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "den_cfg = {'gnn_dim': 1024, 'hidden_dim': 384, 'dropout_rate': 0.3735260731607324,\n",
    "           'lr': 5.956024201538505e-04, 'activation': 'Swish', 'optimizer': 'AdamW',\n",
    "           'weight_decay': 8.619671341229739e-06}\n",
    "\n",
    "model_den = HybridGNN(\n",
    "    gnn_dim=den_cfg['gnn_dim'],\n",
    "    rdkit_dim=X_density_desc.shape[1],\n",
    "    hidden_dim=den_cfg['hidden_dim'],\n",
    "    dropout_rate=den_cfg['dropout_rate'],\n",
    "    activation=den_cfg['activation']\n",
    ")\n",
    "model_den, ckpt_den, metrics_den = train_hybrid_gnn(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=den_cfg['lr'], optimizer=den_cfg['optimizer'],\n",
    "    weight_decay=den_cfg['weight_decay'],\n",
    "    epochs=120, patience=15,  \n",
    "    save_dir=\"saved_models/gnn_density\", tag=\"hybridgnn_density\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2606b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattg\\AppData\\Local\\Temp\\ipykernel_33644\\2426260556.py:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_tg_infer.load_state_dict(torch.load(ckpt_tg, map_location=device))\n",
      "C:\\Users\\mattg\\AppData\\Local\\Temp\\ipykernel_33644\\2426260556.py:159: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_den_infer.load_state_dict(torch.load(ckpt_den, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1109053969</td>\n",
       "      <td>130.776535</td>\n",
       "      <td>0.387567</td>\n",
       "      <td>0.208534</td>\n",
       "      <td>1.150190</td>\n",
       "      <td>18.997804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1422188626</td>\n",
       "      <td>174.627182</td>\n",
       "      <td>0.362225</td>\n",
       "      <td>0.228302</td>\n",
       "      <td>1.069831</td>\n",
       "      <td>19.638581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2032016830</td>\n",
       "      <td>59.418404</td>\n",
       "      <td>0.367133</td>\n",
       "      <td>0.202228</td>\n",
       "      <td>1.072706</td>\n",
       "      <td>18.213128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id          Tg       FFV        Tc   Density         Rg\n",
       "0  1109053969  130.776535  0.387567  0.208534  1.150190  18.997804\n",
       "1  1422188626  174.627182  0.362225  0.228302  1.069831  19.638581\n",
       "2  2032016830   59.418404  0.367133  0.202228  1.072706  18.213128"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Submission: predict each target on test set and combine =====\n",
    "import os, numpy as np, pandas as pd, joblib, torch\n",
    "from tqdm.auto import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors, Descriptors, DataStructs\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "\n",
    "# ---- 0) Load test ids & smiles (and sample for dtype of id) ----\n",
    "sample = pd.read_csv(os.path.join(DATA_ROOT, 'sample_submission.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_ROOT, 'test.csv'))\n",
    "test_ids = test_df['id'].astype(sample['id'].dtype).values\n",
    "test_smiles = test_df['SMILES'].astype(str).tolist()\n",
    "\n",
    "# ---- 1) Helpers for features ----\n",
    "def canonicalize(s):\n",
    "    try:\n",
    "        # you already have canonicalize_polymer_smiles; use that if imported:\n",
    "        return canonicalize_polymer_smiles(s)\n",
    "    except NameError:\n",
    "        # fallback: plain RDKit canonicalization\n",
    "        m = Chem.MolFromSmiles(s)\n",
    "        return Chem.MolToSmiles(m) if m is not None else \"\"\n",
    "\n",
    "def morgan_bits_from_smiles(smiles_list, n_bits=1024, radius=3):\n",
    "    \"\"\"Return (N, n_bits) uint8 array of Morgan bit vectors; no dropping.\"\"\"\n",
    "    fps = np.zeros((len(smiles_list), n_bits), dtype=np.uint8)\n",
    "    for i, smi in enumerate(smiles_list):\n",
    "        bv = np.zeros((n_bits,), dtype=np.uint8)\n",
    "        try:\n",
    "            cs = canonicalize(smi)\n",
    "            m = Chem.MolFromSmiles(cs)\n",
    "            if m is not None:\n",
    "                fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(m, radius=radius, nBits=n_bits)\n",
    "                DataStructs.ConvertToNumpyArray(fp, bv)\n",
    "        except Exception:\n",
    "            pass  # leave zeros if it fails\n",
    "        fps[i] = bv\n",
    "    return fps\n",
    "\n",
    "def compute_rdkit_descriptors(m):\n",
    "    s = canonicalize_polymer_smiles(m)\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    heavy = Descriptors.HeavyAtomCount(m)\n",
    "    arom_atoms = sum(a.GetIsAromatic() for a in m.GetAtoms())\n",
    "    hetero = sum(a.GetAtomicNum() not in (1,6) for a in m.GetAtoms())\n",
    "    aromatic_prop = (arom_atoms / max(1, heavy))\n",
    "    hetero_frac   = (hetero / max(1, heavy))\n",
    "\n",
    "    return [\n",
    "        Descriptors.MolMR(m),                    # 1\n",
    "        Descriptors.MolLogP(m),                  # 2 (SlogP)\n",
    "        rdmd.CalcFractionCSP3(m),                # 3\n",
    "        rdmd.CalcNumAromaticRings(m),            # 4\n",
    "        aromatic_prop,                           # 5\n",
    "        Descriptors.TPSA(m),                     # 6\n",
    "        Descriptors.NumRotatableBonds(m),        # 7\n",
    "        hetero_frac,                             # 8\n",
    "        rdmd.CalcLabuteASA(m),                   # 9\n",
    "        # rdmd.CalcBalabanJ(m),                    # 10\n",
    "        rdmd.CalcKappa1(m),                      # 11\n",
    "        rdmd.CalcKappa2(m),                      # 12\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_desc_matrix(smiles_list, fill_values=None):\n",
    "    X = np.array([compute_rdkit_descriptors(s) for s in smiles_list], dtype=float)\n",
    "    mask = ~np.isfinite(X)  # NaN/Inf\n",
    "    if fill_values is None:\n",
    "        # fallback: column-wise median\n",
    "        col_med = np.nanmedian(np.where(mask, np.nan, X), axis=0)\n",
    "        fill_values = np.where(np.isfinite(col_med), col_med, 0.0)\n",
    "    X[mask] = np.take(fill_values, np.where(mask)[1])\n",
    "    return X, fill_values\n",
    "\n",
    "def smiles2graph_safe(smi):\n",
    "    g = smiles2graph(smi)  # your utility\n",
    "    return Data(\n",
    "        x=torch.tensor(g['node_feat'], dtype=torch.long),\n",
    "        edge_index=torch.tensor(g['edge_index'], dtype=torch.long),\n",
    "        edge_attr=torch.tensor(g['edge_feat'], dtype=torch.long),\n",
    "    )\n",
    "\n",
    "def make_pyg_dataset_for_infer(smiles_list, X_desc):\n",
    "    data_list = []\n",
    "    for i, smi in enumerate(smiles_list):\n",
    "        try:\n",
    "            d = smiles2graph_safe(smi)\n",
    "            d.rdkit_feats = torch.tensor(X_desc[i], dtype=torch.float32)\n",
    "            # dummy y to satisfy code paths; not used at inference\n",
    "            d.y = torch.tensor([0.0], dtype=torch.float32)\n",
    "            data_list.append(d)\n",
    "        except Exception:\n",
    "            # on failure, create a 1-node empty-ish graph as fallback\n",
    "            x = torch.zeros((1,1), dtype=torch.long)\n",
    "            edge_index = torch.zeros((2,0), dtype=torch.long)\n",
    "            edge_attr = torch.zeros((0,1), dtype=torch.long)\n",
    "            d = Data(x=x, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                     rdkit_feats=torch.tensor(X_desc[i], dtype=torch.float32),\n",
    "                     y=torch.tensor([0.0], dtype=torch.float32))\n",
    "            data_list.append(d)\n",
    "    return data_list\n",
    "\n",
    "def predict_gnn(model, loader, device):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)  # [B,1]\n",
    "            outs.append(pred.squeeze(1).detach().cpu().numpy())\n",
    "    return np.concatenate(outs, axis=0)\n",
    "\n",
    "# ---- 2) RF predictions (FFV, Tc, Rg) ----\n",
    "# Use the same FP config you trained with\n",
    "FP_BITS, FP_RADIUS = 1024, 3\n",
    "X_test_fp = morgan_bits_from_smiles(test_smiles, n_bits=FP_BITS, radius=FP_RADIUS)\n",
    "\n",
    "rf_ffv_bundle = joblib.load(p_ffv); rf_ffv = rf_ffv_bundle['model']\n",
    "rf_tc_bundle  = joblib.load(p_tc);  rf_tc  = rf_tc_bundle['model']\n",
    "rf_rg_bundle  = joblib.load(p_rg);  rf_rg  = rf_rg_bundle['model']\n",
    "\n",
    "pred_ffv = rf_ffv.predict(X_test_fp).astype(float)\n",
    "pred_tc  = rf_tc.predict(X_test_fp).astype(float)\n",
    "pred_rg  = rf_rg.predict(X_test_fp).astype(float)\n",
    "\n",
    "# ---- 3) GNN predictions (Tg, Density) ----\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 3a) Tg descriptors for test (fill with TRAIN stats)\n",
    "# Assumes X_tg_desc exists from training; else set to None to use medians\n",
    "tg_fill = np.nanmean(X_tg_desc, axis=0) if 'X_tg_desc' in globals() else None\n",
    "X_tg_desc_test, _ = build_desc_matrix(test_smiles, fill_values=tg_fill)\n",
    "\n",
    "data_tg = make_pyg_dataset_for_infer(test_smiles, X_tg_desc_test)\n",
    "test_loader_tg = GeoDataLoader(data_tg, batch_size=256, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "model_tg_infer = HybridGNN(\n",
    "    gnn_dim=128,\n",
    "    rdkit_dim=X_tg_desc_test.shape[1],\n",
    "    hidden_dim=256,\n",
    "    dropout_rate=0.2,\n",
    "    activation=\"ReLU\",\n",
    ").to(device)\n",
    "model_tg_infer.load_state_dict(torch.load(ckpt_tg, map_location=device))\n",
    "pred_tg = predict_gnn(model_tg_infer, test_loader_tg, device)\n",
    "\n",
    "# 3b) Density descriptors for test (fill with TRAIN stats)\n",
    "den_fill = np.nanmean(X_density_desc, axis=0) if 'X_density_desc' in globals() else None\n",
    "X_den_desc_test, _ = build_desc_matrix(test_smiles, fill_values=den_fill)\n",
    "\n",
    "data_den = make_pyg_dataset_for_infer(test_smiles, X_den_desc_test)\n",
    "test_loader_den = GeoDataLoader(data_den, batch_size=256, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# your tuned density config\n",
    "den_cfg = {'gnn_dim': 1024, 'hidden_dim': 384, 'dropout_rate': 0.3735260731607324,\n",
    "           'lr': 5.956024201538505e-04, 'activation': 'Swish', 'optimizer': 'AdamW',\n",
    "           'weight_decay': 8.619671341229739e-06}\n",
    "\n",
    "model_den_infer = HybridGNN(\n",
    "    gnn_dim=den_cfg['gnn_dim'],\n",
    "    rdkit_dim=X_den_desc_test.shape[1],\n",
    "    hidden_dim=den_cfg['hidden_dim'],\n",
    "    dropout_rate=den_cfg['dropout_rate'],\n",
    "    activation=den_cfg['activation'],\n",
    ").to(device)\n",
    "model_den_infer.load_state_dict(torch.load(ckpt_den, map_location=device))\n",
    "pred_density = predict_gnn(model_den_infer, test_loader_den, device)\n",
    "\n",
    "# Safety: replace any NaNs/Infs with finite numbers\n",
    "pred_tg       = np.nan_to_num(pred_tg, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "pred_density  = np.nan_to_num(pred_density, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "pred_ffv      = np.nan_to_num(pred_ffv, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "pred_tc       = np.nan_to_num(pred_tc, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "pred_rg       = np.nan_to_num(pred_rg, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# ---- 4) Assemble submission in required order ----\n",
    "sub = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Tg': pred_tg,\n",
    "    'FFV': pred_ffv,\n",
    "    'Tc': pred_tc,\n",
    "    'Density': pred_density,\n",
    "    'Rg': pred_rg,\n",
    "})\n",
    "\n",
    "# Optional: clip to train ranges to avoid wild outliers\n",
    "if 'train_df' in globals():\n",
    "    for k in label_cols:\n",
    "        lo = np.nanmin(train_df[k].values)\n",
    "        hi = np.nanmax(train_df[k].values)\n",
    "        sub[k] = np.clip(sub[k].values, lo, hi)\n",
    "\n",
    "# Save\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "print('Submission file created')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673460",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656cce6e",
   "metadata": {},
   "source": [
    "## Model Performance Summary\n",
    "\n",
    "All baseline models were initially trained and evaluated on a 5,000 molecule subset of the full dataset. Below is a comparison of results across different featurization strategies and model types:\n",
    "\n",
    "### 2D Baseline Models\n",
    "\n",
    "| Model Type    | Featurization      | MAE   | RMSE  | R²    | Notes                                 |\n",
    "| ------------- | ------------------ | ----- | ----- | ----- | ------------------------------------- |\n",
    "| MLP (Tuned)   | RDKit Fingerprints | 0.426 | 0.574 | 0.798 | Strong performance across all metrics |\n",
    "| KRR (Tuned)   | RDKit Fingerprints | 0.454 | 0.593 | 0.784 | Good overall, slightly lower R²       |\n",
    "| RF (Tuned)    | RDKit Fingerprints | 0.423 | 0.583 | 0.791 | Best MAE, very competitive overall    |\n",
    "| MLP (Tuned)   | Coulomb Matrix     | 0.636 | 0.819 | 0.588 | Significantly weaker performance      |\n",
    "| MLP (Untuned) | RDKit Fingerprints | 0.467 | 0.609 | 0.772 | Solid untuned baseline                |\n",
    "| KRR (Untuned) | RDKit Fingerprints | 0.519 | 0.668 | 0.726 | Notable drop from tuned version       |\n",
    "| RF (Untuned)  | RDKit Fingerprints | 0.426 | 0.587 | 0.788 | Surprisingly close to tuned RF        |\n",
    "| MLP (Untuned) | Coulomb Matrix     | 0.663 | 0.847 | 0.559 | Consistently underperforms            |\n",
    "\n",
    "### Graph Neural Network Models (ChemML)\n",
    "\n",
    "| Model Type    | Featurization               | MAE   | RMSE  | R²    | Notes                                |\n",
    "| ------------- | --------------------------- | ----- | ----- | ----- | ------------------------------------ |\n",
    "| GNN (Tuned)   | `tensorise_molecules` Graph | 0.302 | 0.411 | 0.900 | Best results from ChemML experiments |\n",
    "| GNN (Untuned) | `tensorise_molecules` Graph | 0.400 | 0.519 | 0.841 | Strong but less optimized            |\n",
    "\n",
    "### Final Hybrid GNN Model Trained on Full Dataset (OGB-Compatible)\n",
    "\n",
    "| Model Type           | Featurization                          | MAE   | RMSE  | R²    | Notes                              |\n",
    "| -------------------- | -------------------------------------- | ----- | ----- | ----- | ---------------------------------- |\n",
    "| Hybrid GNN (Tuned)   | OGB `smiles2graph` + RDKit descriptors | 0.159 | 0.234 | 0.965 | State-of-the-art level performance |\n",
    "| Hybrid GNN (Untuned) | OGB `smiles2graph` + RDKit descriptors | 0.223 | 0.308 | 0.939 | Still very strong pre-tuning       |\n",
    "\n",
    "---\n",
    "\n",
    "## Model Error Analysis\n",
    "\n",
    "I performed qualitative evaluation by comparing predicted vs. true HOMO–LUMO gaps for both randomly selected and poorly predicted molecules. The worst performing molecules often showed rare or complex structures likely underrepresented in the training set. This highlights the importance of structural diversity and potentially more expressive 3D information to improve generalization.\n",
    "\n",
    "## Next Steps: Integrating 3D Molecular Information\n",
    "\n",
    "To push performance even further and overcome limitations of 2D graphs and hand crafted descriptors, my next step will involve:\n",
    "\n",
    "* Using **3D molecular geometries** \n",
    "* Incorporating **interatomic distances**, angles, and **spatial encoding** (SchNet, DimeNet, or SE(3)-equivariant models)\n",
    "* Comparing results against the current best MAE (\\~0.159)\n",
    "\n",
    "This direction aligns with trends in molecular property prediction where 3D aware models often outperform purely 2D approaches, especially for quantum properties like HOMO–LUMO gaps.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
