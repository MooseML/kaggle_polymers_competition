{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61979795",
   "metadata": {},
   "source": [
    "# Polymer Property Predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ace_tools_open as tools\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import pickle\n",
    "import joblib\n",
    "import os \n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Module, Sequential, Dropout\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# OGB dataset \n",
    "from ogb.lsc import PygPCQM4Mv2Dataset, PCQM4Mv2Dataset\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# RDKit\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "\n",
    "# ChemML\n",
    "from chemml.chem import Molecule, RDKitFingerprint, CoulombMatrix, tensorise_molecules\n",
    "from chemml.models import MLP, NeuralGraphHidden, NeuralGraphOutput\n",
    "from chemml.utils import regression_metrics\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589db70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Built with CUDA: True\n",
      "CUDA available: True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device: /physical_device:GPU:0\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: data\n",
      "LMDB directory: data\\processed_chunks\n",
      "Train LMDB: data\\processed_chunks\\polymer_train3d_dist.lmdb\n",
      "Test LMDB: data\\processed_chunks\\polymer_test3d_dist.lmdb\n",
      "LMDBs already exist.\n"
     ]
    }
   ],
   "source": [
    "# Paths - Fixed for Kaggle environment\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  # Writable directory\n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/best_gnn_transformer_hybrid.pt'\n",
    "else:\n",
    "    DATA_ROOT = 'data'\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = 'best_gnn_transformer_hybrid.pt'\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"LMDB directory: {CHUNK_DIR}\")\n",
    "print(f\"Train LMDB: {TRAIN_LMDB}\")\n",
    "print(f\"Test LMDB: {TEST_LMDB}\")\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    # Run the LMDB builders\n",
    "    !python build_polymer_lmdb_fixed.py train\n",
    "    !python build_polymer_lmdb_fixed.py test\n",
    "    print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDB contains 79,730 train graphs\n",
      "Global pools -> train_pool=71,757  val_pool=7,973\n",
      "     Tg:      0 rows with labels (pre-intersection with pools)\n",
      "    FFV:      0 rows with labels (pre-intersection with pools)\n",
      "     Tc:      0 rows with labels (pre-intersection with pools)\n",
      "Density:      0 rows with labels (pre-intersection with pools)\n",
      "     Rg:      0 rows with labels (pre-intersection with pools)\n"
     ]
    }
   ],
   "source": [
    "# LMDB+CSV wiring \n",
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "# 1) Columns / index mapping\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "\n",
    "# 2) Read the training labels (CSV is only used to know which IDs have labels)\n",
    "train_path = os.path.join(DATA_ROOT, 'train.csv')\n",
    "train_df   = pd.read_csv(train_path)\n",
    "assert {'id','SMILES'}.issubset(train_df.columns), \"train.csv must have id and SMILES\"\n",
    "train_df['id'] = train_df['id'].astype(int)\n",
    "\n",
    "# 3) Read the actual IDs that exist in the LMDB\n",
    "def read_lmdb_ids(lmdb_path: str) -> np.ndarray:\n",
    "    ids_txt = lmdb_path + \".ids.txt\"\n",
    "    if not os.path.exists(ids_txt):\n",
    "        raise FileNotFoundError(f\"Missing {ids_txt}. Rebuild LMDB or confirm paths.\")\n",
    "    ids = np.loadtxt(ids_txt, dtype=np.int64)\n",
    "    if ids.ndim == 0:  # single id edge case\n",
    "        ids = ids.reshape(1)\n",
    "    return ids\n",
    "\n",
    "lmdb_ids = read_lmdb_ids(TRAIN_LMDB)\n",
    "print(f\"LMDB contains {len(lmdb_ids):,} train graphs\")\n",
    "\n",
    "# 4) Helper: IDs that have a label for a given task (intersection with LMDB ids)\n",
    "def ids_with_label(task: str) -> np.ndarray:\n",
    "    col = task\n",
    "    have_label = train_df.loc[~train_df[col].isna(), 'id'].astype(int).values\n",
    "    # Only keep those that were actually written to the LMDB\n",
    "    keep = np.intersect1d(have_label, lmdb_ids, assume_unique=False)\n",
    "    return keep\n",
    "\n",
    "# 5) Make a global pool split once (reused for each task)\n",
    "rng = np.random.default_rng(123)\n",
    "perm = rng.permutation(len(lmdb_ids))\n",
    "split = int(0.9 * len(lmdb_ids))\n",
    "train_pool_ids = lmdb_ids[perm[:split]]\n",
    "val_pool_ids   = lmdb_ids[perm[split:]]\n",
    "\n",
    "print(f\"Global pools -> train_pool={len(train_pool_ids):,}  val_pool={len(val_pool_ids):,}\")\n",
    "\n",
    "# 6) Quick sanity: show available counts per task\n",
    "for t in label_cols:\n",
    "    n_task_ids = len(ids_with_label(t))\n",
    "    print(f\"{t:>7}: {n_task_ids:6d} rows with labels (pre-intersection with pools)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd3c3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def _safe_numpy(x, default_shape=None, dtype=np.float32):\n",
    "    try:\n",
    "        return torch.as_tensor(x).detach().cpu().numpy().astype(dtype)\n",
    "    except Exception:\n",
    "        if default_shape is None:\n",
    "            return np.array([], dtype=dtype)\n",
    "        return np.zeros(default_shape, dtype=dtype)\n",
    "\n",
    "def geom_features_from_rec(rec, rdkit_dim_expected=15, rbf_K=32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a fixed-length vector from a single LMDB record:\n",
    "      [rdkit(15), n_atoms, n_bonds, deg_mean, deg_max, has_xyz,\n",
    "       eig3(3), bbox_extents(3), radius_stats(3), hop_hist(3), extra_atom_mean(5),\n",
    "       edge_rbf_mean(32)]\n",
    "    ~ total len = 15 + 5 + 3 + 3 + 3 + 3 + 5 + 32 = 69\n",
    "    \"\"\"\n",
    "    # 15 RDKit descriptors stored in LMDB (your rebuilt version)\n",
    "    rd = getattr(rec, \"rdkit_feats\", None)\n",
    "    rd = _safe_numpy(rd, default_shape=(1, rdkit_dim_expected)).reshape(-1)\n",
    "    if rd.size != rdkit_dim_expected:\n",
    "        rd = np.zeros((rdkit_dim_expected,), dtype=np.float32)\n",
    "\n",
    "    # basic graph sizes & degree\n",
    "    x = torch.as_tensor(rec.x)             # [N, ...]\n",
    "    ei = torch.as_tensor(rec.edge_index)   # [2, E]\n",
    "    n = x.shape[0]\n",
    "    e = ei.shape[1] if ei.ndim == 2 else 0\n",
    "    deg = torch.bincount(ei[0], minlength=n) if e > 0 else torch.zeros(n, dtype=torch.long)\n",
    "    deg_mean = deg.float().mean().item() if n > 0 else 0.0\n",
    "    deg_max  = deg.max().item() if n > 0 else 0.0\n",
    "\n",
    "    # has_xyz flag\n",
    "    has_xyz = int(bool(getattr(rec, \"has_xyz\", torch.zeros(1, dtype=torch.bool))[0].item())) if hasattr(rec, \"has_xyz\") else 0\n",
    "\n",
    "    # pos-based features\n",
    "    eig3 = np.zeros(3, dtype=np.float32)\n",
    "    extents = np.zeros(3, dtype=np.float32)\n",
    "    rad_stats = np.zeros(3, dtype=np.float32)\n",
    "    pos = getattr(rec, \"pos\", None)\n",
    "    if pos is not None and n > 0 and has_xyz:\n",
    "        P = torch.as_tensor(pos).float()                     # [N,3]\n",
    "        center = P.mean(dim=0, keepdim=True)\n",
    "        C = P - center\n",
    "        cov = (C.T @ C) / max(1, n-1)                       # [3,3]\n",
    "        vals = torch.linalg.eigvalsh(cov).clamp_min(0).sqrt()  # length scales\n",
    "        eig3 = vals.detach().cpu().numpy()\n",
    "        mn, mx = P.min(0).values, P.max(0).values\n",
    "        extents = (mx - mn).detach().cpu().numpy()\n",
    "        r = C.norm(dim=1)\n",
    "        rad_stats = np.array([r.mean().item(), r.std().item(), r.max().item()], dtype=np.float32)\n",
    "\n",
    "    # hop-distance histogram (1,2,3 hops)\n",
    "    hop_hist = np.zeros(3, dtype=np.float32)\n",
    "    D = getattr(rec, \"dist\", None)\n",
    "    if D is not None and n > 0:\n",
    "        Dn = torch.as_tensor(D).float()[:n, :n]\n",
    "        hop_hist = np.array([\n",
    "            (Dn == 1).float().mean().item(),\n",
    "            (Dn == 2).float().mean().item(),\n",
    "            (Dn == 3).float().mean().item()\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    # extra atom features (mean over atoms, 5 dims if present)\n",
    "    extra_atom = getattr(rec, \"extra_atom_feats\", None)\n",
    "    extra_mean = np.zeros(5, dtype=np.float32)\n",
    "    if extra_atom is not None and hasattr(extra_atom, \"shape\") and extra_atom.shape[-1] == 5:\n",
    "        extra_mean = torch.as_tensor(extra_atom).float().mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    # edge RBF (last 32 channels of edge_attr were RBF(d))\n",
    "    rbf_mean = np.zeros(rbf_K, dtype=np.float32)\n",
    "    ea = getattr(rec, \"edge_attr\", None)\n",
    "    if ea is not None:\n",
    "        EA = torch.as_tensor(ea)\n",
    "        if EA.ndim == 2 and EA.shape[1] >= (3 + rbf_K):\n",
    "            rbf = EA[:, -rbf_K:].float()\n",
    "            rbf_mean = rbf.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    scalars = np.array([n, e, deg_mean, deg_max, has_xyz], dtype=np.float32)\n",
    "    return np.concatenate([rd, scalars, eig3, extents, rad_stats, hop_hist, extra_mean, rbf_mean], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e663914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors as rdmd, DataStructs\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def morgan_bits(smiles_list, n_bits=1024, radius=3):\n",
    "    X = np.zeros((len(smiles_list), n_bits), dtype=np.uint8)\n",
    "    for i, s in enumerate(smiles_list):\n",
    "        arr = np.zeros((n_bits,), dtype=np.uint8)\n",
    "        m = Chem.MolFromSmiles(s)\n",
    "        if m is not None:\n",
    "            fp = rdmd.GetMorganFingerprintAsBitVect(m, radius=radius, nBits=n_bits)\n",
    "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        X[i] = arr\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "def build_rf_features_from_lmdb(ids: np.ndarray, lmdb_path: str, smiles_list: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns X = [Morgan1024 | LMDB-3D-global(69)] for each id/smiles.\n",
    "    Assumes ids and smiles_list are aligned with the CSV used to build LMDB.\n",
    "    \"\"\"\n",
    "    base = LMDBDataset(ids, lmdb_path)\n",
    "    # 3D/global block\n",
    "    feats3d = []\n",
    "    for i in range(len(base)):\n",
    "        rec = base[i]\n",
    "        feats3d.append(geom_features_from_rec(rec))  # shape (69,)\n",
    "    X3d = np.vstack(feats3d).astype(np.float32) if feats3d else np.zeros((0, 69), dtype=np.float32)\n",
    "\n",
    "    # Morgan FP block (2D)\n",
    "    Xfp = morgan_bits(smiles_list, n_bits=1024, radius=3)   # (N,1024)\n",
    "\n",
    "    # concat\n",
    "    X = np.hstack([Xfp, X3d]).astype(np.float32)            # (N, 1024+69)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe69f3",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47dc5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Tg shape: (0, 2)\n",
      "Initial Tg missing:\n",
      "SMILES    0.0\n",
      "Tg        0.0\n",
      "dtype: float64\n",
      "Cleaned Tg shape: (0, 2)\n",
      "Cleaned Tg missing:\n",
      "SMILES    0.0\n",
      "Tg        0.0\n",
      "dtype: float64\n",
      "\n",
      "Initial Density shape: (0, 2)\n",
      "Initial Density missing:\n",
      "SMILES     0.0\n",
      "Density    0.0\n",
      "dtype: float64\n",
      "Cleaned Density shape: (0, 2)\n",
      "Cleaned Density missing:\n",
      "SMILES     0.0\n",
      "Density    0.0\n",
      "dtype: float64\n",
      "\n",
      "Initial FFV shape: (0, 2)\n",
      "Initial FFV missing:\n",
      "SMILES    0.0\n",
      "FFV       0.0\n",
      "dtype: float64\n",
      "Cleaned FFV shape: (0, 2)\n",
      "Cleaned FFV missing:\n",
      "SMILES    0.0\n",
      "FFV       0.0\n",
      "dtype: float64\n",
      "\n",
      "Initial Tc shape: (0, 2)\n",
      "Initial Tc missing:\n",
      "SMILES    0.0\n",
      "Tc        0.0\n",
      "dtype: float64\n",
      "Cleaned Tc shape: (0, 2)\n",
      "Cleaned Tc missing:\n",
      "SMILES    0.0\n",
      "Tc        0.0\n",
      "dtype: float64\n",
      "\n",
      "Initial Rg shape: (0, 2)\n",
      "Initial Rg missing:\n",
      "SMILES    0.0\n",
      "Rg        0.0\n",
      "dtype: float64\n",
      "Cleaned Rg shape: (0, 2)\n",
      "Cleaned Rg missing:\n",
      "SMILES    0.0\n",
      "Rg        0.0\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the CSV only to know which rows have labels; keep 'id' here.\n",
    "train_df = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_df[\"id\"] = train_df[\"id\"].astype(int)\n",
    "\n",
    "def build_target_df_from_ids(df: pd.DataFrame, target_col: str, keep_ids: np.ndarray):\n",
    "    \"\"\"\n",
    "    Return DataFrame with only SMILES + target, restricted to IDs present in the LMDB\n",
    "    and dropping missing targets.\n",
    "    \"\"\"\n",
    "    out = df.loc[df[\"id\"].isin(keep_ids), [\"SMILES\", target_col]].copy()\n",
    "    print(f\"Initial {target_col} shape:\", out.shape)\n",
    "    print(f\"Initial {target_col} missing:\\n{out.isnull().sum()}\")\n",
    "    out = out.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    print(f\"Cleaned {target_col} shape:\", out.shape)\n",
    "    print(f\"Cleaned {target_col} missing:\\n{out.isnull().sum()}\\n\")\n",
    "    return out\n",
    "\n",
    "# Build all five (use same LMDB id set so we only keep rows that exist in LMDB)\n",
    "df_tg      = build_target_df_from_ids(train_df, \"Tg\",      lmdb_ids)\n",
    "df_density = build_target_df_from_ids(train_df, \"Density\", lmdb_ids)\n",
    "df_ffv     = build_target_df_from_ids(train_df, \"FFV\",     lmdb_ids)\n",
    "df_tc      = build_target_df_from_ids(train_df, \"Tc\",      lmdb_ids)\n",
    "df_rg      = build_target_df_from_ids(train_df, \"Rg\",      lmdb_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff48e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morgan FP utilities (no 3D, no external descriptors) \n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def smiles_to_morgan_fp(\n",
    "    smi: str,\n",
    "    n_bits: int = 1024,\n",
    "    radius: int = 3,\n",
    "    use_counts: bool = False,\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"Return a 1D numpy array Morgan fingerprint; None if SMILES invalid.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    if use_counts:\n",
    "        fp = rdMolDescriptors.GetMorganFingerprint(mol, radius)\n",
    "        # convert to dense count vector\n",
    "        arr = np.zeros((n_bits,), dtype=np.int32)\n",
    "        for bit_id, count in fp.GetNonzeroElements().items():\n",
    "            arr[bit_id % n_bits] += count\n",
    "        return arr.astype(np.float32)\n",
    "    else:\n",
    "        bv = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "        arr = np.zeros((n_bits,), dtype=np.int8)\n",
    "        Chem.DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "        return arr.astype(np.float32)\n",
    "\n",
    "def prepare_fp_for_target(\n",
    "    df_target: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    *,\n",
    "    fp_bits: int = 1024,\n",
    "    fp_radius: int = 3,\n",
    "    use_counts: bool = False,\n",
    "    save_csv_path: Optional[str] = None,\n",
    "    show_progress: bool = True,\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Drop missing targets, compute Morgan FPs from SMILES only.\n",
    "    Returns (df_clean, y, X_fp) where:\n",
    "      df_clean: ['SMILES', target_col]\n",
    "      y: (N,)\n",
    "      X_fp: (N, fp_bits)\n",
    "    \"\"\"\n",
    "    assert {\"SMILES\", target_col}.issubset(df_target.columns)\n",
    "\n",
    "    # 1) drop missing targets (no imputation)\n",
    "    work = df_target[[\"SMILES\", target_col]].copy()\n",
    "    before = len(work)\n",
    "    work = work.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    after = len(work)\n",
    "    print(f\"[{target_col}] dropped {before - after} missing; kept {after}\")\n",
    "\n",
    "    # 2) compute FPs; skip invalid SMILES\n",
    "    fps, ys, keep_smiles = [], [], []\n",
    "    it = work.itertuples(index=False)\n",
    "    if show_progress:\n",
    "        it = tqdm(it, total=len(work), desc=f\"FPs for {target_col}\")\n",
    "\n",
    "    for row in it:\n",
    "        smi = row.SMILES\n",
    "        yv  = getattr(row, target_col)\n",
    "        arr = smiles_to_morgan_fp(smi, n_bits=fp_bits, radius=fp_radius, use_counts=use_counts)\n",
    "        if arr is None:\n",
    "            continue\n",
    "        fps.append(arr)\n",
    "        ys.append(float(yv))\n",
    "        keep_smiles.append(smi)\n",
    "\n",
    "    X_fp = np.stack(fps, axis=0) if fps else np.zeros((0, fp_bits), dtype=np.float32)\n",
    "    y = np.asarray(ys, dtype=float)\n",
    "    df_clean = pd.DataFrame({\"SMILES\": keep_smiles, target_col: y})\n",
    "\n",
    "    if save_csv_path:\n",
    "        df_clean.to_csv(save_csv_path, index=False)\n",
    "        print(f\"[{target_col}] saved cleaned CSV -> {save_csv_path}\")\n",
    "\n",
    "    print(f\"[{target_col}] X_fp: {X_fp.shape} | y: {y.shape}\")\n",
    "    return df_clean, y, X_fp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f37942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] dropped 0 missing; kept 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3853264e29a434890872c60cec4d360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Tg: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] saved cleaned CSV -> cleaned_tg_fp.csv\n",
      "[Tg] X_fp: (0, 1024) | y: (0,)\n",
      "[Density] dropped 0 missing; kept 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a8baae73b8462ca03d94e9db90d445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Density: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Density] saved cleaned CSV -> cleaned_density_fp.csv\n",
      "[Density] X_fp: (0, 1024) | y: (0,)\n",
      "[FFV] dropped 0 missing; kept 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356303aaa9014476b8462d0fbfad7d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for FFV: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FFV] saved cleaned CSV -> cleaned_ffv_fp.csv\n",
      "[FFV] X_fp: (0, 1024) | y: (0,)\n",
      "[Tc] dropped 0 missing; kept 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a569e5e4838b43f3a08e7be746c3db58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Tc: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tc] saved cleaned CSV -> cleaned_tc_fp.csv\n",
      "[Tc] X_fp: (0, 1024) | y: (0,)\n",
      "[Rg] dropped 0 missing; kept 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d128677cd146b7bb26f532a3e2b62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Rg: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rg] saved cleaned CSV -> cleaned_rg_fp.csv\n",
      "[Rg] X_fp: (0, 1024) | y: (0,)\n"
     ]
    }
   ],
   "source": [
    "# Bit vectors (1024, r=3) \n",
    "df_clean_tg,      y_tg,      X_tg      = prepare_fp_for_target(df_tg,      \"Tg\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_tg_fp.csv\")\n",
    "df_clean_density, y_density, X_density = prepare_fp_for_target(df_density, \"Density\", fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_density_fp.csv\")\n",
    "df_clean_ffv,     y_ffv,     X_ffv     = prepare_fp_for_target(df_ffv,     \"FFV\",     fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_ffv_fp.csv\")\n",
    "df_clean_tc,      y_tc,      X_tc      = prepare_fp_for_target(df_tc,      \"Tc\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_tc_fp.csv\")\n",
    "df_clean_rg,      y_rg,      X_rg      = prepare_fp_for_target(df_rg,      \"Rg\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_rg_fp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff620911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "@dataclass\n",
    "class TabularSplits:\n",
    "    # unscaled (for RF)\n",
    "    X_train: np.ndarray\n",
    "    X_test:  np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    y_test:  np.ndarray\n",
    "    # scaled (for KRR/MLP)\n",
    "    X_train_scaled: Optional[np.ndarray] = None\n",
    "    X_test_scaled:  Optional[np.ndarray] = None\n",
    "    y_train_scaled: Optional[np.ndarray] = None  # shape (N,1)\n",
    "    y_test_scaled:  Optional[np.ndarray] = None\n",
    "    x_scaler: Optional[StandardScaler] = None\n",
    "    y_scaler: Optional[StandardScaler] = None\n",
    "\n",
    "def _make_regression_stratify_bins(y: np.ndarray, n_bins: int = 10) -> np.ndarray:\n",
    "    \"\"\"Return integer bins for approximate stratification in regression.\"\"\"\n",
    "    y = y.ravel()\n",
    "    # handle degenerate case\n",
    "    if np.unique(y).size < n_bins:\n",
    "        n_bins = max(2, np.unique(y).size)\n",
    "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "    bins = np.unique(np.quantile(y, quantiles))\n",
    "    # ensure strictly increasing\n",
    "    bins = np.unique(bins)\n",
    "    # np.digitize expects right-open intervals by default\n",
    "    strat = np.digitize(y, bins[1:-1], right=False)\n",
    "    return strat\n",
    "\n",
    "def make_tabular_splits(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    scale_X: bool = True,\n",
    "    scale_y: bool = True,\n",
    "    stratify_regression: bool = False,\n",
    "    n_strat_bins: int = 10,\n",
    "    # if you already decided splits (e.g., scaffold split), pass indices:\n",
    "    train_idx: Optional[np.ndarray] = None,\n",
    "    test_idx: Optional[np.ndarray] = None,\n",
    ") -> TabularSplits:\n",
    "    \"\"\"\n",
    "    Split and (optionally) scale tabular features/targets for a single target.\n",
    "    Returns both scaled and unscaled arrays, plus fitted scalers.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    if train_idx is not None and test_idx is not None:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "    else:\n",
    "        strat = None\n",
    "        if stratify_regression:\n",
    "            strat = _make_regression_stratify_bins(y, n_bins=n_strat_bins)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "        )\n",
    "\n",
    "    # Unscaled outputs (for RF, tree models)\n",
    "    splits = TabularSplits(\n",
    "        X_train=X_train, X_test=X_test,\n",
    "        y_train=y_train, y_test=y_test\n",
    "    )\n",
    "\n",
    "    # Scaled versions (for KRR/MLP)\n",
    "    if scale_X:\n",
    "        xscaler = StandardScaler()\n",
    "        splits.X_train_scaled = xscaler.fit_transform(X_train)\n",
    "        splits.X_test_scaled  = xscaler.transform(X_test)\n",
    "        splits.x_scaler = xscaler\n",
    "    if scale_y:\n",
    "        yscaler = StandardScaler()\n",
    "        splits.y_train_scaled = yscaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        splits.y_test_scaled  = yscaler.transform(y_test.reshape(-1, 1))\n",
    "        splits.y_scaler = yscaler\n",
    "\n",
    "    # Shapes summary\n",
    "    print(\"Splits:\")\n",
    "    print(\"X_train:\", splits.X_train.shape, \"| X_test:\", splits.X_test.shape)\n",
    "    if splits.X_train_scaled is not None:\n",
    "        print(\"X_train_scaled:\", splits.X_train_scaled.shape, \"| X_test_scaled:\", splits.X_test_scaled.shape)\n",
    "    print(\"y_train:\", splits.y_train.shape, \"| y_test:\", splits.y_test.shape)\n",
    "    if splits.y_train_scaled is not None:\n",
    "        print(\"y_train_scaled:\", splits.y_train_scaled.shape, \"| y_test_scaled:\", splits.y_test_scaled.shape)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c284cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def train_eval_rf(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    rf_params: Dict[str, Any],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    save_dir: str = \"saved_models/rf\",\n",
    "    tag: str = \"model\",\n",
    ") -> Tuple[RandomForestRegressor, Dict[str, float], TabularSplits, str]:\n",
    "    \"\"\"\n",
    "    Trains a RandomForest on unscaled features; returns (model, metrics, splits, path).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # Pick a safe number of bins based on dataset size\n",
    "    if stratify_regression:\n",
    "        adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "    else:\n",
    "        adaptive_bins = n_strat_bins\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False,                 # RF doesn't need scaling\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=adaptive_bins\n",
    "    )\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=random_state, n_jobs=-1, **rf_params)\n",
    "    rf.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "    pred_tr = rf.predict(splits.X_train)\n",
    "    pred_te = rf.predict(splits.X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_MAE\": mean_absolute_error(splits.y_train, pred_tr),\n",
    "        \"train_RMSE\": mean_squared_error(splits.y_train, pred_tr, squared=False),\n",
    "        \"train_R2\": r2_score(splits.y_train, pred_tr),\n",
    "        \"val_MAE\": mean_absolute_error(splits.y_test, pred_te),\n",
    "        \"val_RMSE\": mean_squared_error(splits.y_test, pred_te, squared=False),\n",
    "        \"val_R2\": r2_score(splits.y_test, pred_te),\n",
    "    }\n",
    "    print(f\"[RF/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"rf_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": rf, \"metrics\": metrics, \"rf_params\": rf_params}, path)\n",
    "    return rf, metrics, splits, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d95126",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "cannot do a non-empty take from an empty axes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m rf_cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFFV\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m60\u001b[39m},\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTc\u001b[39m\u001b[38;5;124m\"\u001b[39m:  {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m800\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m20\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRg\u001b[39m\u001b[38;5;124m\"\u001b[39m:  {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m400\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m260\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[0;32m      5\u001b[0m }\n\u001b[1;32m----> 7\u001b[0m rf_ffv, m_ffv, splits_ffv, p_ffv \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_eval_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_ffv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ffv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrf_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrf_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFFV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFFV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m rf_tc,  m_tc,  splits_tc,  p_tc  \u001b[38;5;241m=\u001b[39m train_eval_rf(X_tc,  y_tc,  rf_params\u001b[38;5;241m=\u001b[39mrf_cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTc\u001b[39m\u001b[38;5;124m\"\u001b[39m],  tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m rf_rg,  m_rg,  splits_rg,  p_rg  \u001b[38;5;241m=\u001b[39m train_eval_rf(X_rg,  y_rg,  rf_params\u001b[38;5;241m=\u001b[39mrf_cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRg\u001b[39m\u001b[38;5;124m\"\u001b[39m],  tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m, in \u001b[0;36mtrain_eval_rf\u001b[1;34m(X, y, rf_params, test_size, random_state, stratify_regression, n_strat_bins, save_dir, tag)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     adaptive_bins \u001b[38;5;241m=\u001b[39m n_strat_bins\n\u001b[1;32m---> 29\u001b[0m splits \u001b[38;5;241m=\u001b[39m \u001b[43mmake_tabular_splits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale_X\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# RF doesn't need scaling\u001b[39;49;00m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstratify_regression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify_regression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_strat_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madaptive_bins\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39mrandom_state, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrf_params)\n\u001b[0;32m     39\u001b[0m rf\u001b[38;5;241m.\u001b[39mfit(splits\u001b[38;5;241m.\u001b[39mX_train, splits\u001b[38;5;241m.\u001b[39my_train)\n",
      "Cell \u001b[1;32mIn[10], line 63\u001b[0m, in \u001b[0;36mmake_tabular_splits\u001b[1;34m(X, y, test_size, random_state, scale_X, scale_y, stratify_regression, n_strat_bins, train_idx, test_idx)\u001b[0m\n\u001b[0;32m     61\u001b[0m     strat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify_regression:\n\u001b[1;32m---> 63\u001b[0m         strat \u001b[38;5;241m=\u001b[39m \u001b[43m_make_regression_stratify_bins\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_strat_bins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     65\u001b[0m         X, y, test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39mrandom_state, stratify\u001b[38;5;241m=\u001b[39mstrat\n\u001b[0;32m     66\u001b[0m     )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Unscaled outputs (for RF, tree models)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 29\u001b[0m, in \u001b[0;36m_make_regression_stratify_bins\u001b[1;34m(y, n_bins)\u001b[0m\n\u001b[0;32m     27\u001b[0m     n_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m2\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(y)\u001b[38;5;241m.\u001b[39msize)\n\u001b[0;32m     28\u001b[0m quantiles \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, n_bins \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# ensure strictly increasing\u001b[39;00m\n\u001b[0;32m     31\u001b[0m bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(bins)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mquantile\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\numpy\\lib\\function_base.py:4461\u001b[0m, in \u001b[0;36mquantile\u001b[1;34m(a, q, axis, out, overwrite_input, method, keepdims, interpolation)\u001b[0m\n\u001b[0;32m   4459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quantile_is_valid(q):\n\u001b[0;32m   4460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantiles must be in the range [0, 1]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 4461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_quantile_unchecked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4462\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\numpy\\lib\\function_base.py:4473\u001b[0m, in \u001b[0;36m_quantile_unchecked\u001b[1;34m(a, q, axis, out, overwrite_input, method, keepdims)\u001b[0m\n\u001b[0;32m   4465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_quantile_unchecked\u001b[39m(a,\n\u001b[0;32m   4466\u001b[0m                         q,\n\u001b[0;32m   4467\u001b[0m                         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4470\u001b[0m                         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4471\u001b[0m                         keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   4472\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4474\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_quantile_ureduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4475\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4476\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4477\u001b[0m \u001b[43m                    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4478\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4479\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4480\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\numpy\\lib\\function_base.py:3752\u001b[0m, in \u001b[0;36m_ureduce\u001b[1;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[0;32m   3749\u001b[0m             index_out \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, ) \u001b[38;5;241m*\u001b[39m nd\n\u001b[0;32m   3750\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out[(\u001b[38;5;28mEllipsis\u001b[39m, ) \u001b[38;5;241m+\u001b[39m index_out]\n\u001b[1;32m-> 3752\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\numpy\\lib\\function_base.py:4639\u001b[0m, in \u001b[0;36m_quantile_ureduce_func\u001b[1;34m(a, q, axis, out, overwrite_input, method)\u001b[0m\n\u001b[0;32m   4637\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4638\u001b[0m         arr \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m-> 4639\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_quantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4640\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4641\u001b[0m \u001b[43m                   \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4642\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4643\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\numpy\\lib\\function_base.py:4745\u001b[0m, in \u001b[0;36m_quantile\u001b[1;34m(arr, quantiles, axis, method, out)\u001b[0m\n\u001b[0;32m   4737\u001b[0m arr\u001b[38;5;241m.\u001b[39mpartition(\n\u001b[0;32m   4738\u001b[0m     np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39mconcatenate(([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   4739\u001b[0m                               previous_indexes\u001b[38;5;241m.\u001b[39mravel(),\n\u001b[0;32m   4740\u001b[0m                               next_indexes\u001b[38;5;241m.\u001b[39mravel(),\n\u001b[0;32m   4741\u001b[0m                               ))),\n\u001b[0;32m   4742\u001b[0m     axis\u001b[38;5;241m=\u001b[39mDATA_AXIS)\n\u001b[0;32m   4743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(arr\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minexact):\n\u001b[0;32m   4744\u001b[0m     slices_having_nans \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(\n\u001b[1;32m-> 4745\u001b[0m         \u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_AXIS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4746\u001b[0m     )\n\u001b[0;32m   4747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4748\u001b[0m     slices_having_nans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mtake\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:190\u001b[0m, in \u001b[0;36mtake\u001b[1;34m(a, indices, axis, out, mode)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_take_dispatcher)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtake\u001b[39m(a, indices, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    Take elements from an array along an axis.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m           [5, 7]])\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtake\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mIndexError\u001b[0m: cannot do a non-empty take from an empty axes."
     ]
    }
   ],
   "source": [
    "rf_cfg = {\n",
    "    \"FFV\": {\"n_estimators\": 100, \"max_depth\": 60},\n",
    "    \"Tc\":  {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "    \"Rg\":  {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "}\n",
    "\n",
    "rf_ffv, m_ffv, splits_ffv, p_ffv = train_eval_rf(X_ffv, y_ffv, rf_params=rf_cfg[\"FFV\"], tag=\"FFV\")\n",
    "rf_tc,  m_tc,  splits_tc,  p_tc  = train_eval_rf(X_tc,  y_tc,  rf_params=rf_cfg[\"Tc\"],  tag=\"Tc\")\n",
    "rf_rg,  m_rg,  splits_rg,  p_rg  = train_eval_rf(X_rg,  y_rg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Rg\")\n",
    "rf_tg,  m_tg,  splits_tg,  p_tg  = train_eval_rf(X_tg,  y_tg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Tg\")\n",
    "rf_density,  m_density,  splits_density,  p_density  = train_eval_rf(X_density,  y_density,  rf_params=rf_cfg[\"Rg\"],  tag=\"Density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7492bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Training RF(+3D) for FFV\n",
      "[LMDBDataset] Dropped 7030 ids not found in LMDB (ids.txt).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 7030 and the array at index 1 has size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFFV\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDensity\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m>>> Training RF(+3D) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m     m, met, sp, p \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rf_aug3d_for_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrf_cfg_aug\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_csv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlmdb_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_LMDB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaved_models/rf_aug3d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtag_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maug3D\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstratify_regression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_strat_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     rf_models[t], rf_metrics[t], rf_splits[t], rf_paths[t] \u001b[38;5;241m=\u001b[39m m, met, sp, p\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[RF+3D/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]  val_MAE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_MAE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  val_RMSE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_RMSE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  val_R2=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_R2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 22\u001b[0m, in \u001b[0;36mtrain_rf_aug3d_for_target\u001b[1;34m(target_col, rf_params, train_csv_path, lmdb_path, save_dir, tag_prefix, test_size, random_state, stratify_regression, n_strat_bins)\u001b[0m\n\u001b[0;32m     19\u001b[0m smiles_tr \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[mask, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMILES\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     20\u001b[0m y         \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[mask, target_col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 22\u001b[0m X_aug \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_rf_features_from_lmdb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlmdb_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmiles_tr\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (N, 1024 + 69)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m model, metrics, splits, path \u001b[38;5;241m=\u001b[39m train_eval_rf(\n\u001b[0;32m     25\u001b[0m     X_aug, y,\n\u001b[0;32m     26\u001b[0m     rf_params\u001b[38;5;241m=\u001b[39mrf_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, metrics, splits, path\n",
      "Cell \u001b[1;32mIn[6], line 33\u001b[0m, in \u001b[0;36mbuild_rf_features_from_lmdb\u001b[1;34m(ids, lmdb_path, smiles_list)\u001b[0m\n\u001b[0;32m     30\u001b[0m Xfp \u001b[38;5;241m=\u001b[39m morgan_bits(smiles_list, n_bits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, radius\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)   \u001b[38;5;66;03m# (N,1024)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# concat\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mXfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX3d\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)            \u001b[38;5;66;03m# (N, 1024+69)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\numpy\\core\\shape_base.py:370\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 7030 and the array at index 1 has size 0"
     ]
    }
   ],
   "source": [
    "# === helpers (uses the LMDB feature builders you already added) ===\n",
    "def train_rf_aug3d_for_target(\n",
    "    target_col: str,\n",
    "    rf_params: dict,\n",
    "    *,\n",
    "    train_csv_path: str,\n",
    "    lmdb_path: str,\n",
    "    save_dir: str = \"saved_models/rf_aug3d\",\n",
    "    tag_prefix: str = \"aug3D\",\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "):\n",
    "    \"\"\"Load rows with target, build X=[FP|3D], train RF via your train_eval_rf().\"\"\"\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    mask = ~df[target_col].isna()\n",
    "    ids_tr    = df.loc[mask, 'id'].astype(int).values\n",
    "    smiles_tr = df.loc[mask, 'SMILES'].astype(str).tolist()\n",
    "    y         = df.loc[mask, target_col].astype(float).values\n",
    "\n",
    "    X_aug = build_rf_features_from_lmdb(ids_tr, lmdb_path, smiles_tr)  # (N, 1024 + 69)\n",
    "\n",
    "    model, metrics, splits, path = train_eval_rf(\n",
    "        X_aug, y,\n",
    "        rf_params=rf_params,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=n_strat_bins,\n",
    "        save_dir=save_dir,\n",
    "        tag=f\"{target_col}_{tag_prefix}\"\n",
    "    )\n",
    "    return model, metrics, splits, path\n",
    "\n",
    "# === per-target configs (start with what worked; tweak later) ===\n",
    "rf_cfg_aug = {\n",
    "    \"FFV\":     {\"n_estimators\": 800, \"max_depth\": 30, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "    \"Tc\":      {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "    \"Rg\":      {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "    # reasonable first passes for the two GNN targets (just to A/B):\n",
    "    \"Tg\":      {\"n_estimators\": 600, \"max_depth\": 60, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "    \"Density\": {\"n_estimators\": 600, \"max_depth\": 40, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "}\n",
    "\n",
    "# === train all five with augmented features ===\n",
    "TRAIN_CSV = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "rf_models, rf_metrics, rf_splits, rf_paths = {}, {}, {}, {}\n",
    "\n",
    "for t in [\"FFV\", \"Tc\", \"Rg\", \"Tg\", \"Density\"]:\n",
    "    print(f\"\\n>>> Training RF(+3D) for {t}\")\n",
    "    m, met, sp, p = train_rf_aug3d_for_target(\n",
    "        t, rf_cfg_aug[t],\n",
    "        train_csv_path=TRAIN_CSV,\n",
    "        lmdb_path=TRAIN_LMDB,\n",
    "        save_dir=\"saved_models/rf_aug3d\",\n",
    "        tag_prefix=\"aug3D\",\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify_regression=True,\n",
    "        n_strat_bins=10,\n",
    "    )\n",
    "    rf_models[t], rf_metrics[t], rf_splits[t], rf_paths[t] = m, met, sp, p\n",
    "    print(f\"[RF+3D/{t}]  val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d599b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tg ids: (0,) Density ids: (0,)\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))  # keep 'id'!\n",
    "lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "if os.path.exists(lmdb_ids_path):\n",
    "    with open(lmdb_ids_path) as f:\n",
    "        kept_ids = set(int(x.strip()) for x in f if x.strip())\n",
    "else:\n",
    "    kept_ids = set(train_csv['id'].astype(int).tolist())\n",
    "\n",
    "def ids_for_task(task):\n",
    "    t = task2idx[task]\n",
    "    col = label_cols[t]\n",
    "    ids = train_csv.loc[~train_csv[col].isna(), 'id'].astype(int).tolist()\n",
    "    # only those that actually exist in LMDB\n",
    "    return np.array([i for i in ids if i in kept_ids], dtype=int)\n",
    "\n",
    "ids_tg  = ids_for_task(\"Tg\")\n",
    "ids_den = ids_for_task(\"Density\")\n",
    "ids_tc = ids_for_task(\"Tc\")\n",
    "ids_rg = ids_for_task(\"Rg\")\n",
    "ids_ffv = ids_for_task(\"FFV\")\n",
    "print(\"Tg ids:\", ids_tg.shape, \"Density ids:\", ids_den.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3efce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import torch, numpy as np\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def _get_rdkit_feats_from_record(rec):\n",
    "    arr = getattr(rec, \"rdkit_feats\", None)\n",
    "    if arr is None:\n",
    "        return torch.zeros(15, dtype=torch.float32)   # or 6 if thats your build\n",
    "    v = torch.as_tensor(np.asarray(arr, np.float32).reshape(-1), dtype=torch.float32)\n",
    "    return v.unsqueeze(0)  # <<< IMPORTANT: (1, D) so batch -> (B, D)\n",
    "\n",
    "\n",
    "class LMDBtoPyGSingleTask(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ids,\n",
    "        lmdb_path,\n",
    "        target_index=None,\n",
    "        *,\n",
    "        use_mixed_edges: bool = True,      # < enables 3 cat + 32 RBF continuous\n",
    "        include_extra_atom_feats: bool = True,  # < attach per-atom extras\n",
    "    ):\n",
    "        self.base = LMDBDataset(ids, lmdb_path)\n",
    "        self.t = target_index\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        self.include_extra_atom_feats = include_extra_atom_feats\n",
    "\n",
    "    def __len__(self): return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.base[idx]\n",
    "\n",
    "        x  = torch.as_tensor(rec.x, dtype=torch.long)\n",
    "        ei = torch.as_tensor(rec.edge_index, dtype=torch.long)\n",
    "\n",
    "        ea = torch.as_tensor(rec.edge_attr)              # (E, 3 + 32)\n",
    "        if self.use_mixed_edges:\n",
    "            # keep all columns; EdgeEncoderMixed will split cat vs cont\n",
    "            edge_attr = ea.to(torch.float32)\n",
    "        else:\n",
    "            # categorical-only for vanilla BondEncoder\n",
    "            edge_attr = ea[:, :3].to(torch.long)\n",
    "\n",
    "        # rdkit globals: KEEP AS (1, D) so PyG collates to (B, D)\n",
    "        rdkit_feats = _get_rdkit_feats_from_record(rec)  # (1, D)\n",
    "        d = Data(x=x, edge_index=ei, edge_attr=edge_attr, rdkit_feats=rdkit_feats)\n",
    "\n",
    "        if self.include_extra_atom_feats and hasattr(rec, \"extra_atom_feats\"):\n",
    "            d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32)  # (N,5)\n",
    "\n",
    "        if hasattr(rec, \"has_xyz\"):\n",
    "            # collates to (B,1); handy as a gating/global indicator\n",
    "            hz = np.asarray(rec.has_xyz, np.uint8).reshape(-1)\n",
    "            d.has_xyz = torch.from_numpy(hz.astype(np.float32))\n",
    "\n",
    "        if (self.t is not None) and hasattr(rec, \"y\"):\n",
    "            yv = torch.as_tensor(rec.y, dtype=torch.float32).view(-1)\n",
    "            if self.t < yv.numel():\n",
    "                d.y = yv[self.t:self.t+1]  # (1,)\n",
    "\n",
    "        # geometry & extras from LMDB (if present)\n",
    "        if hasattr(rec, \"pos\"):              # (N,3) float\n",
    "            d.pos = torch.as_tensor(rec.pos, dtype=torch.float32)\n",
    "        if hasattr(rec, \"extra_atom_feats\"): # (N,5) float\n",
    "            d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32)\n",
    "        if hasattr(rec, \"has_xyz\"):          # (1,) bool/uint8\n",
    "            d.has_xyz = torch.as_tensor(rec.has_xyz, dtype=torch.float32)\n",
    "        # LMDBtoPyGSingleTask.__getitem__  (add this near the end, after you create Data d)\n",
    "        if hasattr(rec, \"dist\"):\n",
    "            # rec.dist is (L, L) (uint8) in your LMDB\n",
    "            d.hops = torch.as_tensor(rec.dist, dtype=torch.long).unsqueeze(0)  # (1, L, L)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "694612d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "def make_loaders_for_task(task, ids, *, batch_size=64, seed=42,\n",
    "                          use_mixed_edges=True, include_extra_atom_feats=True):\n",
    "    t = task2idx[task]\n",
    "    tr_ids, va_ids = train_test_split(ids, test_size=0.2, random_state=seed)\n",
    "    tr_ds = LMDBtoPyGSingleTask(tr_ids, TRAIN_LMDB, target_index=t,\n",
    "                                use_mixed_edges=use_mixed_edges,\n",
    "                                include_extra_atom_feats=include_extra_atom_feats)\n",
    "    va_ds = LMDBtoPyGSingleTask(va_ids, TRAIN_LMDB, target_index=t,\n",
    "                                use_mixed_edges=use_mixed_edges,\n",
    "                                include_extra_atom_feats=include_extra_atom_feats)\n",
    "    tr = GeoDataLoader(tr_ds, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "    va = GeoDataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    return tr, va\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983db98",
   "metadata": {},
   "source": [
    "## Step 5: Define the Hybrid GNN Model\n",
    "\n",
    "The final architecture uses both structural and cheminformatics data by combining GNN-learned graph embeddings with SMILES-derived RDKit descriptors. This Hybrid GNN model uses `smiles2graph` for graph construction and augments it with RDKit-based molecular features for improved prediction accuracy.\n",
    "\n",
    "### Model Components:\n",
    "\n",
    "* **AtomEncoder / BondEncoder**\n",
    "  Transforms categorical atom and bond features (provided by OGB) into learnable embeddings using the encoders from `ogb.graphproppred.mol_encoder`. These provide a strong foundation for expressive graph learning.\n",
    "\n",
    "* **GINEConv Layers (x2)**\n",
    "  I use two stacked GINEConv layers (Graph Isomorphism Network with Edge features). These layers perform neighborhood aggregation based on edge attributes, allowing the model to capture localized chemical environments.\n",
    "\n",
    "* **Global Mean Pooling**\n",
    "  After message passing, node level embeddings are aggregated into a fixed size graph level representation using `global_mean_pool`.\n",
    "\n",
    "* **Concatenation with RDKit Descriptors**\n",
    "  The pooled GNN embedding is concatenated with external RDKit descriptors, which capture global molecular properties not easily inferred from graph data alone.\n",
    "\n",
    "* **MLP Prediction Head**\n",
    "  A multilayer perceptron processes the combined feature vector with ReLU activations, dropout regularization, and linear layers to predict the HOMOLUMO gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82dad355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = float(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        rand = keep + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        rand.floor_()  # 0/1\n",
    "        return x.div(keep) * rand\n",
    "\n",
    "\n",
    "def _act(name: str):\n",
    "    name = (name or \"ReLU\").lower()\n",
    "    if name == \"relu\": return nn.ReLU()\n",
    "    if name == \"gelu\": return nn.GELU()\n",
    "    if name in (\"swish\", \"silu\"): return nn.SiLU()\n",
    "    return nn.ReLU()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0946f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeEncoderMixed(nn.Module):\n",
    "    def __init__(self, emb_dim: int, cont_dim: int = 32, activation=\"GeLU\"):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        # OGB bond categorical widths: type(5), stereo(6), conjugation(2)\n",
    "        self.emb0 = nn.Embedding(5, emb_dim)\n",
    "        self.emb1 = nn.Embedding(6, emb_dim)\n",
    "        self.emb2 = nn.Embedding(2, emb_dim)\n",
    "        self.mlp_cont = nn.Sequential(\n",
    "            nn.Linear(cont_dim, emb_dim),\n",
    "            act,\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        # edge_attr: (E, 3+K)\n",
    "        cat = edge_attr[:, :3].long()\n",
    "        cont = edge_attr[:, 3:].float()\n",
    "        e_cat  = self.emb0(cat[:,0]) + self.emb1(cat[:,1]) + self.emb2(cat[:,2])\n",
    "        e_cont = self.mlp_cont(cont)\n",
    "        return e_cat + e_cont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5380b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtraAtomEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, activation=\"GeLU\"):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            act,\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, extra):\n",
    "        return self.proj(extra)  # (N, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8e379ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINEConv\n",
    "\n",
    "class GINEBlock_GNN(nn.Module):\n",
    "    def __init__(self, dim, activation=\"GeLU\", dropout=0.1, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.conv = GINEConv(nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            act,\n",
    "            nn.Linear(dim, dim),\n",
    "        ))\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dp1 = DropPath(drop_path)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, 2*dim),\n",
    "            act,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2*dim, dim),\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dp2 = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_emb):\n",
    "        # pre-norm transformer style\n",
    "        h = self.norm1(x)\n",
    "        h = self.conv(h, edge_index, edge_emb)\n",
    "        h = self.dropout1(h)\n",
    "        x = x + self.dp1(h)\n",
    "\n",
    "        h2 = self.norm2(x)\n",
    "        h2 = self.ffn(h2)\n",
    "        h2 = self.dropout2(h2)\n",
    "        x = x + self.dp2(h2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bef6fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool, global_max_pool, GlobalAttention\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "from torch import nn\n",
    "\n",
    "class HybridGNNv2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gnn_dim: int,\n",
    "        rdkit_dim: int,\n",
    "        hidden_dim: int,\n",
    "        *,\n",
    "        num_layers: int = 8,\n",
    "        activation: str = \"Swish\",\n",
    "        dropout: float = 0.2,\n",
    "        drop_path_rate: float = 0.1,\n",
    "        use_mixed_edges: bool = True,\n",
    "        cont_dim: int = 32,\n",
    "        use_extra_atom_feats: bool = True,\n",
    "        extra_atom_dim: int = 5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gnn_dim = gnn_dim\n",
    "        self.rdkit_dim = rdkit_dim\n",
    "        self.use_extra_atom_feats = use_extra_atom_feats\n",
    "\n",
    "        # encoders\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=gnn_dim)\n",
    "        if use_mixed_edges:\n",
    "            self.edge_encoder = EdgeEncoderMixed(emb_dim=gnn_dim, cont_dim=cont_dim, activation=activation)\n",
    "        else:\n",
    "            self.edge_encoder = BondEncoder(emb_dim=gnn_dim)\n",
    "\n",
    "        if use_extra_atom_feats:\n",
    "            self.extra_atom = ExtraAtomEncoder(in_dim=extra_atom_dim, out_dim=gnn_dim, activation=activation)\n",
    "            self.extra_gate = nn.Sequential(nn.Linear(2*gnn_dim, gnn_dim), _act(activation))\n",
    "\n",
    "        # backbone\n",
    "        dpr = [drop_path_rate * i / max(1, num_layers - 1) for i in range(num_layers)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GINEBlock_GNN(gnn_dim, activation=activation, dropout=dropout, drop_path=dpr[i])\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # pooling (concat of mean/max/attention)\n",
    "        self.att_pool = GlobalAttention(\n",
    "            gate_nn=nn.Sequential(\n",
    "                nn.Linear(gnn_dim, gnn_dim // 2),\n",
    "                _act(activation),\n",
    "                nn.Linear(gnn_dim // 2, 1),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        pooled_dim = 3 * gnn_dim  # mean + max + attention\n",
    "        # plus rdkit globals (+ optional has_xyz scalar)\n",
    "        self.with_has_xyz = True\n",
    "        head_in = pooled_dim + rdkit_dim + (1 if self.with_has_xyz else 0)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(head_in),\n",
    "            nn.Linear(head_in, hidden_dim),\n",
    "            _act(activation),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            _act(activation),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.atom_encoder(data.x)  # (N, D)\n",
    "\n",
    "        if self.use_extra_atom_feats and hasattr(data, \"extra_atom_feats\"):\n",
    "            xa = self.extra_atom(data.extra_atom_feats)  # (N, D)\n",
    "            x = self.extra_gate(torch.cat([x, xa], dim=1))\n",
    "\n",
    "        e = self.edge_encoder(data.edge_attr)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, data.edge_index, e)\n",
    "\n",
    "        # pool\n",
    "        mean = global_mean_pool(x, data.batch)\n",
    "        mmax = global_max_pool(x, data.batch)\n",
    "        attn = self.att_pool(x, data.batch)\n",
    "        g = torch.cat([mean, mmax, attn], dim=1)\n",
    "\n",
    "        rd = data.rdkit_feats.view(g.size(0), -1)\n",
    "        extras = [g, rd]\n",
    "\n",
    "        if self.with_has_xyz and hasattr(data, \"has_xyz\"):\n",
    "            # has_xyz collates to (B,1)\n",
    "            extras.append(data.has_xyz.view(-1, 1).float())\n",
    "\n",
    "        out = torch.cat(extras, dim=1)\n",
    "        return self.head(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc992041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numpy as np, torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, RMSprop\n",
    "from torch.amp import GradScaler, autocast\n",
    "from copy import deepcopy\n",
    "\n",
    "def train_hybrid_gnn_sota(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    *,\n",
    "    lr: float = 5e-4,\n",
    "    optimizer: str = \"AdamW\",\n",
    "    weight_decay: float = 1e-5,\n",
    "    epochs: int = 120,\n",
    "    warmup_epochs: int = 5,\n",
    "    patience: int = 15,\n",
    "    clip_norm: float = 1.0,\n",
    "    amp: bool = True,\n",
    "    loss_name: str = \"mse\",   # \"mse\" or \"huber\"\n",
    "    save_dir: str = \"saved_models/gnn\",\n",
    "    tag: str = \"model_sota\",\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    opt_name = optimizer.lower()\n",
    "    if opt_name == \"rmsprop\":\n",
    "        opt = RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.0)\n",
    "    else:\n",
    "        opt = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # cosine schedule w/ warmup\n",
    "    def lr_factor(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / max(1, warmup_epochs)\n",
    "        t = (epoch - warmup_epochs) / max(1, (epochs - warmup_epochs))\n",
    "        return 0.5 * (1 + math.cos(math.pi * t))\n",
    "    scaler = GradScaler(\"cuda\", enabled=amp)\n",
    "\n",
    "    def loss_fn(pred, target):\n",
    "        if loss_name.lower() == \"huber\":\n",
    "            return F.huber_loss(pred, target, delta=1.0)\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_once(loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        for b in loader:\n",
    "            b = b.to(device)\n",
    "            p = model(b)\n",
    "            preds.append(p.detach().cpu())\n",
    "            trues.append(b.y.view(-1,1).cpu())\n",
    "        preds = torch.cat(preds).numpy(); trues = torch.cat(trues).numpy()\n",
    "        mae = np.mean(np.abs(preds - trues))\n",
    "        rmse = float(np.sqrt(np.mean((preds - trues)**2)))\n",
    "        r2 = float(1 - np.sum((preds - trues)**2) / np.sum((trues - trues.mean())**2))\n",
    "        return mae, rmse, r2\n",
    "\n",
    "    best_mae = float(\"inf\")\n",
    "    best = None\n",
    "    best_path = os.path.join(save_dir, f\"{tag}.pt\")\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # schedule\n",
    "        for g in opt.param_groups:\n",
    "            g[\"lr\"] = lr * lr_factor(ep-1)\n",
    "\n",
    "        model.train()\n",
    "        total, count = 0.0, 0\n",
    "        for b in train_loader:\n",
    "            b = b.to(device)\n",
    "            with autocast(\"cuda\", enabled=amp):\n",
    "                pred = model(b)\n",
    "                loss = loss_fn(pred, b.y.view(-1,1))\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            if clip_norm is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "            scaler.step(opt); scaler.update()\n",
    "\n",
    "            total += loss.item() * b.num_graphs\n",
    "            count += b.num_graphs\n",
    "\n",
    "        tr_mse = total / max(1, count)\n",
    "        mae, rmse, r2 = eval_once(val_loader)\n",
    "        print(f\"Epoch {ep:03d} | tr_MSE {tr_mse:.5f} | val_MAE {mae:.5f} | val_RMSE {rmse:.5f} | R2 {r2:.4f}\")\n",
    "\n",
    "        if mae < best_mae - 1e-6:\n",
    "            best_mae = mae\n",
    "            best = deepcopy(model.state_dict())\n",
    "            torch.save(best, best_path)\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best is not None:\n",
    "        model.load_state_dict(best)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "    final_mae, final_rmse, final_r2 = eval_once(val_loader)\n",
    "    print(f\"[{tag}] Best Val  MAE {final_mae:.6f} | RMSE {final_rmse:.6f} | R2 {final_r2:.4f}\")\n",
    "    return model, best_path, {\"MAE\": final_mae, \"RMSE\": final_rmse, \"R2\": final_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0813a81",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Build loaders (now feeding mixed edges + extra atom feats)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_loader_tg,  val_loader_tg  \u001b[38;5;241m=\u001b[39m \u001b[43mmake_loaders_for_task\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[43mids_tg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                                         \u001b[49m\u001b[43muse_mixed_edges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_extra_atom_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m train_loader_ffv,  val_loader_ffv  \u001b[38;5;241m=\u001b[39m make_loaders_for_task(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFFV\u001b[39m\u001b[38;5;124m\"\u001b[39m,      ids_ffv,  batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m      5\u001b[0m                                                          use_mixed_edges\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, include_extra_atom_feats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m train_loader_tc,  val_loader_tc  \u001b[38;5;241m=\u001b[39m make_loaders_for_task(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTc\u001b[39m\u001b[38;5;124m\"\u001b[39m,      ids_tc,  batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m      7\u001b[0m                                                          use_mixed_edges\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, include_extra_atom_feats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m, in \u001b[0;36mmake_loaders_for_task\u001b[1;34m(task, ids, batch_size, seed, use_mixed_edges, include_extra_atom_feats)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_loaders_for_task\u001b[39m(task, ids, \u001b[38;5;241m*\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m      5\u001b[0m                           use_mixed_edges\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, include_extra_atom_feats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      6\u001b[0m     t \u001b[38;5;241m=\u001b[39m task2idx[task]\n\u001b[1;32m----> 7\u001b[0m     tr_ids, va_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     tr_ds \u001b[38;5;241m=\u001b[39m LMDBtoPyGSingleTask(tr_ids, TRAIN_LMDB, target_index\u001b[38;5;241m=\u001b[39mt,\n\u001b[0;32m      9\u001b[0m                                 use_mixed_edges\u001b[38;5;241m=\u001b[39muse_mixed_edges,\n\u001b[0;32m     10\u001b[0m                                 include_extra_atom_feats\u001b[38;5;241m=\u001b[39minclude_extra_atom_feats)\n\u001b[0;32m     11\u001b[0m     va_ds \u001b[38;5;241m=\u001b[39m LMDBtoPyGSingleTask(va_ids, TRAIN_LMDB, target_index\u001b[38;5;241m=\u001b[39mt,\n\u001b[0;32m     12\u001b[0m                                 use_mixed_edges\u001b[38;5;241m=\u001b[39muse_mixed_edges,\n\u001b[0;32m     13\u001b[0m                                 include_extra_atom_feats\u001b[38;5;241m=\u001b[39minclude_extra_atom_feats)\n",
      "File \u001b[1;32mc:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2649\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2646\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2648\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2649\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2305\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2302\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2309\u001b[0m     )\n\u001b[0;32m   2311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Build loaders (now feeding mixed edges + extra atom feats)\n",
    "train_loader_tg,  val_loader_tg  = make_loaders_for_task(\"Tg\",      ids_tg,  batch_size=64,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "train_loader_ffv,  val_loader_ffv  = make_loaders_for_task(\"FFV\",      ids_ffv,  batch_size=64,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "train_loader_tc,  val_loader_tc  = make_loaders_for_task(\"Tc\",      ids_tc,  batch_size=64,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "\n",
    "train_loader_den, val_loader_den = make_loaders_for_task(\"Density\", ids_den, batch_size=64,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "train_loader_rg,  val_loader_rg  = make_loaders_for_task(\"Rg\",      ids_rg,  batch_size=64,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "\n",
    "\n",
    "\n",
    "# Introspect dims from a real batch\n",
    "b_tg = next(iter(train_loader_tg))\n",
    "rd_dim = b_tg.rdkit_feats.shape[-1]           # 15 if you rebuilt with 15 globals\n",
    "print(\"rdkit_dim =\", rd_dim)\n",
    "\n",
    "# Tg \n",
    "model_tg = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "\n",
    "model_tg, ckpt_tg, metrics_tg = train_hybrid_gnn_sota(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=0.0005555079210176292, optimizer=\"RMSprop\", weight_decay=9.056299733554687e-06,\n",
    "    epochs=200, warmup_epochs=5, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gnn_tg_v2\", tag=\"hybridgnn_tg_v2\"\n",
    ")\n",
    "# # FFV\n",
    "model_ffv = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "\n",
    "model_ffv, ckpt_ffv, metrics_ffv = train_hybrid_gnn_sota(\n",
    "    model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "    lr=0.0005555079210176292, optimizer=\"RMSprop\", weight_decay=9.056299733554687e-06,\n",
    "    epochs=200, warmup_epochs=5, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gnn_ffv_v2\", tag=\"hybridgnn_ffv_v2\"\n",
    ")\n",
    "\n",
    "# Tc\n",
    "model_tc = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "\n",
    "model_tc, ckpt_tc, metrics_tc = train_hybrid_gnn_sota(\n",
    "    model_tc, train_loader_tc, val_loader_tc,\n",
    "    lr=0.0005555079210176292, optimizer=\"RMSprop\", weight_decay=9.056299733554687e-06,\n",
    "    epochs=200, warmup_epochs=5, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gnn_tc_v2\", tag=\"hybridgnn_tc_v2\"\n",
    ")\n",
    "\n",
    "# Density (use your tuned dims if you like larger backbones)\n",
    "model_den = HybridGNNv2(\n",
    "    gnn_dim=1024, rdkit_dim=rd_dim, hidden_dim=384,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.1, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "model_den, ckpt_den, metrics_den = train_hybrid_gnn_sota(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=5.956024201538505e-04, optimizer=\"AdamW\", weight_decay=8.619671341229739e-06,\n",
    "    epochs=200, warmup_epochs=8, patience=30,\n",
    "    clip_norm=0.5, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gnn_density_v2\", tag=\"hybridgnn_density_v2\"\n",
    ")\n",
    "\n",
    "# Rg (your tuned gnn_dim + swish + RMSprop work fine here)\n",
    "model_rg = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "model_rg, ckpt_rg, metrics_rg = train_hybrid_gnn_sota(\n",
    "    model_rg, train_loader_rg, val_loader_rg,\n",
    "    lr=5.6e-4, optimizer=\"RMSprop\", weight_decay=9.0e-6,\n",
    "    epochs=120, warmup_epochs=6, patience=20,\n",
    "    clip_norm=0.5, amp=True, loss_name=\"huber\",  # Huber often helps Rg\n",
    "    save_dir=\"saved_models/gnn_rg_v2\", tag=\"hybridgnn_rg_v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1bc6e",
   "metadata": {},
   "source": [
    "\n",
    "| Model Type | Feature | MAE | RMSE | R2 |\n",
    "|---|---|---|---|---|\n",
    "| RF3D | Tg | 58.315801 | 74.296699 | 0.5846 |\n",
    "| GNN2 | Tg | 47.105114 | 61.480179 | 0.6040 |\n",
    "| RF3D | Tc | 0.029937 | 0.045036 | 0.7313 |\n",
    "| GNN2 | Tc | 0.025115 | 0.041331 | 0.8000 |\n",
    "| RF3D | Density | 0.037793 | 0.070932 | 0.7847 |\n",
    "| GNN2 | Density | 0.031735 | 0.067845 | 0.7379 |\n",
    "| RF3D | FFV | 0.007621 | 0.017553 | 0.6605 |\n",
    "| GNN2 | FFV | 0.013817 | 0.023902 | 0.4473 |\n",
    "| RF3D | Rg | 1.648818 | 2.493712 | 0.7299 |\n",
    "| GNN2 | Rg | 2.115880 | 2.801481 | 0.6434 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673460",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
