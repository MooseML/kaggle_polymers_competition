{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61979795",
   "metadata": {},
   "source": [
    "# HOMO-LUMO Gap Predictions\n",
    "\n",
    "### Problem Statement & Motivation\n",
    "\n",
    "Accurately predicting quantum chemical properties like the HOMO–LUMO energy gap is essential for advancing materials science, drug discovery, and electronic design. The HOMO–LUMO gap is particularly informative for assessing molecular reactivity and stability. While Density Functional Theory (DFT) provides precise estimates, its high computational cost makes it impractical for large-scale screening of molecular libraries. This notebook explores machine learning alternatives that are fast, scalable, and interpretable, offering solutions that are accessible even on modest hardware.\n",
    "\n",
    "### Related Work & Key Gap\n",
    "\n",
    "Past work has shown that:\n",
    "\n",
    "* DFT is accurate but computationally intensive\n",
    "* ML models like kernel methods and GNNs show promise, but often require large models and expensive hardware\n",
    "\n",
    "Key Gap: A need for lightweight, high-performing models that can run locally and integrate with user-friendly tools for deployment in research or education.\n",
    "\n",
    "### Methodology & Evaluation\n",
    "\n",
    "This notebook:\n",
    "\n",
    "* Benchmarks a variety of 2D-based models using RDKit descriptors, Coulomb matrices, and graph neural networks (GNNs) on a 5k molecule subset\n",
    "* Progresses to a hybrid GNN architecture combining OGB-standard graphs with SMILES-derived cheminformatics features\n",
    "* Achieves **MAE = 0.159 eV**\n",
    "* Visualizes results using parity plots, error inspection, and predicted-vs-true comparisons\n",
    "* Evaluates both random and high-error cases to better understand model behavior\n",
    "\n",
    "| Metric   | Best Model (Hybrid GNN) |\n",
    "| -------- | ----------------------- |\n",
    "| **MAE**  | 0.159 eV                |\n",
    "| **RMSE** | 0.234 eV                |\n",
    "| **R²**   | 0.965                   |\n",
    "\n",
    "\n",
    "### Deployment & Accessibility\n",
    "\n",
    "To make the model practically useful, an **interactive web app** was developed:\n",
    "\n",
    "**Live App**: [HOMO–LUMO Gap Predictor on Hugging Face](https://huggingface.co/spaces/MooseML/homo-lumo-gap-predictor)\n",
    "\n",
    "Features:\n",
    "\n",
    "* **SMILES input** for any organic molecule\n",
    "* **Real-time prediction** of the HOMO–LUMO gap\n",
    "* **Molecular visualization**\n",
    "* Simple **CSV logging** for result tracking\n",
    "\n",
    "GitHub Repository: [MooseML/homo-lumo-gap-models](https://github.com/MooseML/homo-lumo-gap-models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ace_tools_open as tools\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import pickle\n",
    "import joblib\n",
    "import os \n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Module, Sequential, Dropout\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# OGB dataset \n",
    "from ogb.lsc import PygPCQM4Mv2Dataset, PCQM4Mv2Dataset\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# RDKit\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "\n",
    "# ChemML\n",
    "from chemml.chem import Molecule, RDKitFingerprint, CoulombMatrix, tensorise_molecules\n",
    "from chemml.models import MLP, NeuralGraphHidden, NeuralGraphOutput\n",
    "from chemml.utils import regression_metrics\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589db70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Built with CUDA: True\n",
      "CUDA available: True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device: /physical_device:GPU:0\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: data\n",
      "LMDB directory: data\\processed_chunks\n",
      "Train LMDB: data\\processed_chunks\\polymer_train3d_dist.lmdb\n",
      "Test LMDB: data\\processed_chunks\\polymer_test3d_dist.lmdb\n",
      "LMDBs already exist.\n"
     ]
    }
   ],
   "source": [
    "# Paths - Fixed for Kaggle environment\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  # Writable directory\n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/best_gnn_transformer_hybrid.pt'\n",
    "else:\n",
    "    DATA_ROOT = 'data'\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = 'best_gnn_transformer_hybrid.pt'\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"LMDB directory: {CHUNK_DIR}\")\n",
    "print(f\"Train LMDB: {TRAIN_LMDB}\")\n",
    "print(f\"Test LMDB: {TEST_LMDB}\")\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    # Run the LMDB builders\n",
    "    !python build_polymer_lmdb_fixed.py train\n",
    "    !python build_polymer_lmdb_fixed.py test\n",
    "    print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c34b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDB contains 7,973 train graphs\n",
      "Global pools -> train_pool=7,175  val_pool=798\n",
      "     Tg:    511 rows with labels (pre-intersection with pools)\n",
      "    FFV:   7030 rows with labels (pre-intersection with pools)\n",
      "     Tc:    737 rows with labels (pre-intersection with pools)\n",
      "Density:    613 rows with labels (pre-intersection with pools)\n",
      "     Rg:    614 rows with labels (pre-intersection with pools)\n"
     ]
    }
   ],
   "source": [
    "# LMDB+CSV wiring \n",
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "# 1) Columns / index mapping\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "\n",
    "# 2) Read the training labels (CSV is only used to know which IDs have labels)\n",
    "train_path = os.path.join(DATA_ROOT, 'train.csv')\n",
    "train_df   = pd.read_csv(train_path)\n",
    "assert {'id','SMILES'}.issubset(train_df.columns), \"train.csv must have id and SMILES\"\n",
    "train_df['id'] = train_df['id'].astype(int)\n",
    "\n",
    "# 3) Read the actual IDs that exist in the LMDB\n",
    "def read_lmdb_ids(lmdb_path: str) -> np.ndarray:\n",
    "    ids_txt = lmdb_path + \".ids.txt\"\n",
    "    if not os.path.exists(ids_txt):\n",
    "        raise FileNotFoundError(f\"Missing {ids_txt}. Rebuild LMDB or confirm paths.\")\n",
    "    ids = np.loadtxt(ids_txt, dtype=np.int64)\n",
    "    if ids.ndim == 0:  # single id edge case\n",
    "        ids = ids.reshape(1)\n",
    "    return ids\n",
    "\n",
    "lmdb_ids = read_lmdb_ids(TRAIN_LMDB)\n",
    "print(f\"LMDB contains {len(lmdb_ids):,} train graphs\")\n",
    "\n",
    "# 4) Helper: IDs that have a label for a given task (intersection with LMDB ids)\n",
    "def ids_with_label(task: str) -> np.ndarray:\n",
    "    col = task\n",
    "    have_label = train_df.loc[~train_df[col].isna(), 'id'].astype(int).values\n",
    "    # Only keep those that were actually written to the LMDB\n",
    "    keep = np.intersect1d(have_label, lmdb_ids, assume_unique=False)\n",
    "    return keep\n",
    "\n",
    "# 5) Make a global pool split once (reused for each task)\n",
    "rng = np.random.default_rng(123)\n",
    "perm = rng.permutation(len(lmdb_ids))\n",
    "split = int(0.9 * len(lmdb_ids))\n",
    "train_pool_ids = lmdb_ids[perm[:split]]\n",
    "val_pool_ids   = lmdb_ids[perm[split:]]\n",
    "\n",
    "print(f\"Global pools -> train_pool={len(train_pool_ids):,}  val_pool={len(val_pool_ids):,}\")\n",
    "\n",
    "# 6) Quick sanity: show available counts per task\n",
    "for t in label_cols:\n",
    "    n_task_ids = len(ids_with_label(t))\n",
    "    print(f\"{t:>7}: {n_task_ids:6d} rows with labels (pre-intersection with pools)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125f621",
   "metadata": {},
   "source": [
    "The only property that appears will succeed with a simple imputation strategy is FFV. All other properties contain very high percent missing. Therefore, I will impute median for FFV, train a model for FFV, and train separate models for other properties. I will attempt to filter out missing values for each property. If this yields uncessful, I may explore sampling techniques or use the trained model to impute values to train a secondaery model. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd3c3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def _safe_numpy(x, default_shape=None, dtype=np.float32):\n",
    "    try:\n",
    "        return torch.as_tensor(x).detach().cpu().numpy().astype(dtype)\n",
    "    except Exception:\n",
    "        if default_shape is None:\n",
    "            return np.array([], dtype=dtype)\n",
    "        return np.zeros(default_shape, dtype=dtype)\n",
    "\n",
    "def geom_features_from_rec(rec, rdkit_dim_expected=15, rbf_K=32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a fixed-length vector from a single LMDB record:\n",
    "      [rdkit(15), n_atoms, n_bonds, deg_mean, deg_max, has_xyz,\n",
    "       eig3(3), bbox_extents(3), radius_stats(3), hop_hist(3), extra_atom_mean(5),\n",
    "       edge_rbf_mean(32)]\n",
    "    ~ total len = 15 + 5 + 3 + 3 + 3 + 3 + 5 + 32 = 69\n",
    "    \"\"\"\n",
    "    # 15 RDKit descriptors stored in LMDB (your rebuilt version)\n",
    "    rd = getattr(rec, \"rdkit_feats\", None)\n",
    "    rd = _safe_numpy(rd, default_shape=(1, rdkit_dim_expected)).reshape(-1)\n",
    "    if rd.size != rdkit_dim_expected:\n",
    "        rd = np.zeros((rdkit_dim_expected,), dtype=np.float32)\n",
    "\n",
    "    # basic graph sizes & degree\n",
    "    x = torch.as_tensor(rec.x)             # [N, ...]\n",
    "    ei = torch.as_tensor(rec.edge_index)   # [2, E]\n",
    "    n = x.shape[0]\n",
    "    e = ei.shape[1] if ei.ndim == 2 else 0\n",
    "    deg = torch.bincount(ei[0], minlength=n) if e > 0 else torch.zeros(n, dtype=torch.long)\n",
    "    deg_mean = deg.float().mean().item() if n > 0 else 0.0\n",
    "    deg_max  = deg.max().item() if n > 0 else 0.0\n",
    "\n",
    "    # has_xyz flag\n",
    "    has_xyz = int(bool(getattr(rec, \"has_xyz\", torch.zeros(1, dtype=torch.bool))[0].item())) if hasattr(rec, \"has_xyz\") else 0\n",
    "\n",
    "    # pos-based features\n",
    "    eig3 = np.zeros(3, dtype=np.float32)\n",
    "    extents = np.zeros(3, dtype=np.float32)\n",
    "    rad_stats = np.zeros(3, dtype=np.float32)\n",
    "    pos = getattr(rec, \"pos\", None)\n",
    "    if pos is not None and n > 0 and has_xyz:\n",
    "        P = torch.as_tensor(pos).float()                     # [N,3]\n",
    "        center = P.mean(dim=0, keepdim=True)\n",
    "        C = P - center\n",
    "        cov = (C.T @ C) / max(1, n-1)                       # [3,3]\n",
    "        vals = torch.linalg.eigvalsh(cov).clamp_min(0).sqrt()  # length scales\n",
    "        eig3 = vals.detach().cpu().numpy()\n",
    "        mn, mx = P.min(0).values, P.max(0).values\n",
    "        extents = (mx - mn).detach().cpu().numpy()\n",
    "        r = C.norm(dim=1)\n",
    "        rad_stats = np.array([r.mean().item(), r.std().item(), r.max().item()], dtype=np.float32)\n",
    "\n",
    "    # hop-distance histogram (1,2,3 hops)\n",
    "    hop_hist = np.zeros(3, dtype=np.float32)\n",
    "    D = getattr(rec, \"dist\", None)\n",
    "    if D is not None and n > 0:\n",
    "        Dn = torch.as_tensor(D).float()[:n, :n]\n",
    "        hop_hist = np.array([\n",
    "            (Dn == 1).float().mean().item(),\n",
    "            (Dn == 2).float().mean().item(),\n",
    "            (Dn == 3).float().mean().item()\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    # extra atom features (mean over atoms, 5 dims if present)\n",
    "    extra_atom = getattr(rec, \"extra_atom_feats\", None)\n",
    "    extra_mean = np.zeros(5, dtype=np.float32)\n",
    "    if extra_atom is not None and hasattr(extra_atom, \"shape\") and extra_atom.shape[-1] == 5:\n",
    "        extra_mean = torch.as_tensor(extra_atom).float().mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    # edge RBF (last 32 channels of edge_attr were RBF(d))\n",
    "    rbf_mean = np.zeros(rbf_K, dtype=np.float32)\n",
    "    ea = getattr(rec, \"edge_attr\", None)\n",
    "    if ea is not None:\n",
    "        EA = torch.as_tensor(ea)\n",
    "        if EA.ndim == 2 and EA.shape[1] >= (3 + rbf_K):\n",
    "            rbf = EA[:, -rbf_K:].float()\n",
    "            rbf_mean = rbf.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    scalars = np.array([n, e, deg_mean, deg_max, has_xyz], dtype=np.float32)\n",
    "    return np.concatenate([rd, scalars, eig3, extents, rad_stats, hop_hist, extra_mean, rbf_mean], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e663914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors as rdmd, DataStructs\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def morgan_bits(smiles_list, n_bits=1024, radius=3):\n",
    "    X = np.zeros((len(smiles_list), n_bits), dtype=np.uint8)\n",
    "    for i, s in enumerate(smiles_list):\n",
    "        arr = np.zeros((n_bits,), dtype=np.uint8)\n",
    "        m = Chem.MolFromSmiles(s)\n",
    "        if m is not None:\n",
    "            fp = rdmd.GetMorganFingerprintAsBitVect(m, radius=radius, nBits=n_bits)\n",
    "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        X[i] = arr\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "def build_rf_features_from_lmdb(ids: np.ndarray, lmdb_path: str, smiles_list: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns X = [Morgan1024 | LMDB-3D-global(69)] for each id/smiles.\n",
    "    Assumes ids and smiles_list are aligned with the CSV used to build LMDB.\n",
    "    \"\"\"\n",
    "    base = LMDBDataset(ids, lmdb_path)\n",
    "    # 3D/global block\n",
    "    feats3d = []\n",
    "    for i in range(len(base)):\n",
    "        rec = base[i]\n",
    "        feats3d.append(geom_features_from_rec(rec))  # shape (69,)\n",
    "    X3d = np.vstack(feats3d).astype(np.float32) if feats3d else np.zeros((0, 69), dtype=np.float32)\n",
    "\n",
    "    # Morgan FP block (2D)\n",
    "    Xfp = morgan_bits(smiles_list, n_bits=1024, radius=3)   # (N,1024)\n",
    "\n",
    "    # concat\n",
    "    X = np.hstack([Xfp, X3d]).astype(np.float32)            # (N, 1024+69)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe69f3",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47dc5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Tg shape: (7973, 2)\n",
      "Initial Tg missing:\n",
      "SMILES       0\n",
      "Tg        7462\n",
      "dtype: int64\n",
      "Cleaned Tg shape: (511, 2)\n",
      "Cleaned Tg missing:\n",
      "SMILES    0\n",
      "Tg        0\n",
      "dtype: int64\n",
      "\n",
      "Initial Density shape: (7973, 2)\n",
      "Initial Density missing:\n",
      "SMILES        0\n",
      "Density    7360\n",
      "dtype: int64\n",
      "Cleaned Density shape: (613, 2)\n",
      "Cleaned Density missing:\n",
      "SMILES     0\n",
      "Density    0\n",
      "dtype: int64\n",
      "\n",
      "Initial FFV shape: (7973, 2)\n",
      "Initial FFV missing:\n",
      "SMILES      0\n",
      "FFV       943\n",
      "dtype: int64\n",
      "Cleaned FFV shape: (7030, 2)\n",
      "Cleaned FFV missing:\n",
      "SMILES    0\n",
      "FFV       0\n",
      "dtype: int64\n",
      "\n",
      "Initial Tc shape: (7973, 2)\n",
      "Initial Tc missing:\n",
      "SMILES       0\n",
      "Tc        7236\n",
      "dtype: int64\n",
      "Cleaned Tc shape: (737, 2)\n",
      "Cleaned Tc missing:\n",
      "SMILES    0\n",
      "Tc        0\n",
      "dtype: int64\n",
      "\n",
      "Initial Rg shape: (7973, 2)\n",
      "Initial Rg missing:\n",
      "SMILES       0\n",
      "Rg        7359\n",
      "dtype: int64\n",
      "Cleaned Rg shape: (614, 2)\n",
      "Cleaned Rg missing:\n",
      "SMILES    0\n",
      "Rg        0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the CSV only to know which rows have labels; keep 'id' here.\n",
    "train_df = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_df[\"id\"] = train_df[\"id\"].astype(int)\n",
    "\n",
    "def build_target_df_from_ids(df: pd.DataFrame, target_col: str, keep_ids: np.ndarray):\n",
    "    \"\"\"\n",
    "    Return DataFrame with only SMILES + target, restricted to IDs present in the LMDB\n",
    "    and dropping missing targets.\n",
    "    \"\"\"\n",
    "    out = df.loc[df[\"id\"].isin(keep_ids), [\"SMILES\", target_col]].copy()\n",
    "    print(f\"Initial {target_col} shape:\", out.shape)\n",
    "    print(f\"Initial {target_col} missing:\\n{out.isnull().sum()}\")\n",
    "    out = out.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    print(f\"Cleaned {target_col} shape:\", out.shape)\n",
    "    print(f\"Cleaned {target_col} missing:\\n{out.isnull().sum()}\\n\")\n",
    "    return out\n",
    "\n",
    "# Build all five (use same LMDB id set so we only keep rows that exist in LMDB)\n",
    "df_tg      = build_target_df_from_ids(train_df, \"Tg\",      lmdb_ids)\n",
    "df_density = build_target_df_from_ids(train_df, \"Density\", lmdb_ids)\n",
    "df_ffv     = build_target_df_from_ids(train_df, \"FFV\",     lmdb_ids)\n",
    "df_tc      = build_target_df_from_ids(train_df, \"Tc\",      lmdb_ids)\n",
    "df_rg      = build_target_df_from_ids(train_df, \"Rg\",      lmdb_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff48e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morgan FP utilities (no 3D, no external descriptors) \n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def smiles_to_morgan_fp(\n",
    "    smi: str,\n",
    "    n_bits: int = 1024,\n",
    "    radius: int = 3,\n",
    "    use_counts: bool = False,\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"Return a 1D numpy array Morgan fingerprint; None if SMILES invalid.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    if use_counts:\n",
    "        fp = rdMolDescriptors.GetMorganFingerprint(mol, radius)\n",
    "        # convert to dense count vector\n",
    "        arr = np.zeros((n_bits,), dtype=np.int32)\n",
    "        for bit_id, count in fp.GetNonzeroElements().items():\n",
    "            arr[bit_id % n_bits] += count\n",
    "        return arr.astype(np.float32)\n",
    "    else:\n",
    "        bv = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "        arr = np.zeros((n_bits,), dtype=np.int8)\n",
    "        Chem.DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "        return arr.astype(np.float32)\n",
    "\n",
    "def prepare_fp_for_target(\n",
    "    df_target: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    *,\n",
    "    fp_bits: int = 1024,\n",
    "    fp_radius: int = 3,\n",
    "    use_counts: bool = False,\n",
    "    save_csv_path: Optional[str] = None,\n",
    "    show_progress: bool = True,\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Drop missing targets, compute Morgan FPs from SMILES only.\n",
    "    Returns (df_clean, y, X_fp) where:\n",
    "      df_clean: ['SMILES', target_col]\n",
    "      y: (N,)\n",
    "      X_fp: (N, fp_bits)\n",
    "    \"\"\"\n",
    "    assert {\"SMILES\", target_col}.issubset(df_target.columns)\n",
    "\n",
    "    # 1) drop missing targets (no imputation)\n",
    "    work = df_target[[\"SMILES\", target_col]].copy()\n",
    "    before = len(work)\n",
    "    work = work.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    after = len(work)\n",
    "    print(f\"[{target_col}] dropped {before - after} missing; kept {after}\")\n",
    "\n",
    "    # 2) compute FPs; skip invalid SMILES\n",
    "    fps, ys, keep_smiles = [], [], []\n",
    "    it = work.itertuples(index=False)\n",
    "    if show_progress:\n",
    "        it = tqdm(it, total=len(work), desc=f\"FPs for {target_col}\")\n",
    "\n",
    "    for row in it:\n",
    "        smi = row.SMILES\n",
    "        yv  = getattr(row, target_col)\n",
    "        arr = smiles_to_morgan_fp(smi, n_bits=fp_bits, radius=fp_radius, use_counts=use_counts)\n",
    "        if arr is None:\n",
    "            continue\n",
    "        fps.append(arr)\n",
    "        ys.append(float(yv))\n",
    "        keep_smiles.append(smi)\n",
    "\n",
    "    X_fp = np.stack(fps, axis=0) if fps else np.zeros((0, fp_bits), dtype=np.float32)\n",
    "    y = np.asarray(ys, dtype=float)\n",
    "    df_clean = pd.DataFrame({\"SMILES\": keep_smiles, target_col: y})\n",
    "\n",
    "    if save_csv_path:\n",
    "        df_clean.to_csv(save_csv_path, index=False)\n",
    "        print(f\"[{target_col}] saved cleaned CSV -> {save_csv_path}\")\n",
    "\n",
    "    print(f\"[{target_col}] X_fp: {X_fp.shape} | y: {y.shape}\")\n",
    "    return df_clean, y, X_fp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f37942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] dropped 0 missing; kept 511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ec976e0c2846b1bf138ea686a87b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Tg:   0%|          | 0/511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] saved cleaned CSV -> cleaned_tg_fp.csv\n",
      "[Tg] X_fp: (511, 1024) | y: (511,)\n",
      "[Density] dropped 0 missing; kept 613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5add1693f5f44b2e9c9ce97c025122e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Density:   0%|          | 0/613 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Density] saved cleaned CSV -> cleaned_density_fp.csv\n",
      "[Density] X_fp: (613, 1024) | y: (613,)\n",
      "[FFV] dropped 0 missing; kept 7030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a947a84a3094119a27494a85628a7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for FFV:   0%|          | 0/7030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FFV] saved cleaned CSV -> cleaned_ffv_fp.csv\n",
      "[FFV] X_fp: (7030, 1024) | y: (7030,)\n",
      "[Tc] dropped 0 missing; kept 737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e444a6ae0574f169834f9902837c56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Tc:   0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tc] saved cleaned CSV -> cleaned_tc_fp.csv\n",
      "[Tc] X_fp: (737, 1024) | y: (737,)\n",
      "[Rg] dropped 0 missing; kept 614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51bcaa1718f40129c3d37590b96e715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Rg:   0%|          | 0/614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rg] saved cleaned CSV -> cleaned_rg_fp.csv\n",
      "[Rg] X_fp: (614, 1024) | y: (614,)\n"
     ]
    }
   ],
   "source": [
    "# Bit vectors (1024, r=3) \n",
    "df_clean_tg,      y_tg,      X_tg      = prepare_fp_for_target(df_tg,      \"Tg\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_tg_fp.csv\")\n",
    "df_clean_density, y_density, X_density = prepare_fp_for_target(df_density, \"Density\", fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_density_fp.csv\")\n",
    "df_clean_ffv,     y_ffv,     X_ffv     = prepare_fp_for_target(df_ffv,     \"FFV\",     fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_ffv_fp.csv\")\n",
    "df_clean_tc,      y_tc,      X_tc      = prepare_fp_for_target(df_tc,      \"Tc\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_tc_fp.csv\")\n",
    "df_clean_rg,      y_rg,      X_rg      = prepare_fp_for_target(df_rg,      \"Rg\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_rg_fp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff620911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "@dataclass\n",
    "class TabularSplits:\n",
    "    # unscaled (for RF)\n",
    "    X_train: np.ndarray\n",
    "    X_test:  np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    y_test:  np.ndarray\n",
    "    # scaled (for KRR/MLP)\n",
    "    X_train_scaled: Optional[np.ndarray] = None\n",
    "    X_test_scaled:  Optional[np.ndarray] = None\n",
    "    y_train_scaled: Optional[np.ndarray] = None  # shape (N,1)\n",
    "    y_test_scaled:  Optional[np.ndarray] = None\n",
    "    x_scaler: Optional[StandardScaler] = None\n",
    "    y_scaler: Optional[StandardScaler] = None\n",
    "\n",
    "def _make_regression_stratify_bins(y: np.ndarray, n_bins: int = 10) -> np.ndarray:\n",
    "    \"\"\"Return integer bins for approximate stratification in regression.\"\"\"\n",
    "    y = y.ravel()\n",
    "    # handle degenerate case\n",
    "    if np.unique(y).size < n_bins:\n",
    "        n_bins = max(2, np.unique(y).size)\n",
    "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "    bins = np.unique(np.quantile(y, quantiles))\n",
    "    # ensure strictly increasing\n",
    "    bins = np.unique(bins)\n",
    "    # np.digitize expects right-open intervals by default\n",
    "    strat = np.digitize(y, bins[1:-1], right=False)\n",
    "    return strat\n",
    "\n",
    "def make_tabular_splits(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    scale_X: bool = True,\n",
    "    scale_y: bool = True,\n",
    "    stratify_regression: bool = False,\n",
    "    n_strat_bins: int = 10,\n",
    "    # if you already decided splits (e.g., scaffold split), pass indices:\n",
    "    train_idx: Optional[np.ndarray] = None,\n",
    "    test_idx: Optional[np.ndarray] = None,\n",
    ") -> TabularSplits:\n",
    "    \"\"\"\n",
    "    Split and (optionally) scale tabular features/targets for a single target.\n",
    "    Returns both scaled and unscaled arrays, plus fitted scalers.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    if train_idx is not None and test_idx is not None:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "    else:\n",
    "        strat = None\n",
    "        if stratify_regression:\n",
    "            strat = _make_regression_stratify_bins(y, n_bins=n_strat_bins)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "        )\n",
    "\n",
    "    # Unscaled outputs (for RF, tree models)\n",
    "    splits = TabularSplits(\n",
    "        X_train=X_train, X_test=X_test,\n",
    "        y_train=y_train, y_test=y_test\n",
    "    )\n",
    "\n",
    "    # Scaled versions (for KRR/MLP)\n",
    "    if scale_X:\n",
    "        xscaler = StandardScaler()\n",
    "        splits.X_train_scaled = xscaler.fit_transform(X_train)\n",
    "        splits.X_test_scaled  = xscaler.transform(X_test)\n",
    "        splits.x_scaler = xscaler\n",
    "    if scale_y:\n",
    "        yscaler = StandardScaler()\n",
    "        splits.y_train_scaled = yscaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        splits.y_test_scaled  = yscaler.transform(y_test.reshape(-1, 1))\n",
    "        splits.y_scaler = yscaler\n",
    "\n",
    "    # Shapes summary\n",
    "    print(\"Splits:\")\n",
    "    print(\"X_train:\", splits.X_train.shape, \"| X_test:\", splits.X_test.shape)\n",
    "    if splits.X_train_scaled is not None:\n",
    "        print(\"X_train_scaled:\", splits.X_train_scaled.shape, \"| X_test_scaled:\", splits.X_test_scaled.shape)\n",
    "    print(\"y_train:\", splits.y_train.shape, \"| y_test:\", splits.y_test.shape)\n",
    "    if splits.y_train_scaled is not None:\n",
    "        print(\"y_train_scaled:\", splits.y_train_scaled.shape, \"| y_test_scaled:\", splits.y_test_scaled.shape)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c284cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def train_eval_rf(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    rf_params: Dict[str, Any],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    save_dir: str = \"saved_models/rf\",\n",
    "    tag: str = \"model\",\n",
    ") -> Tuple[RandomForestRegressor, Dict[str, float], TabularSplits, str]:\n",
    "    \"\"\"\n",
    "    Trains a RandomForest on unscaled features; returns (model, metrics, splits, path).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # Pick a safe number of bins based on dataset size\n",
    "    if stratify_regression:\n",
    "        adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "    else:\n",
    "        adaptive_bins = n_strat_bins\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False,                 # RF doesn't need scaling\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=adaptive_bins\n",
    "    )\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=random_state, n_jobs=-1, **rf_params)\n",
    "    rf.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "    pred_tr = rf.predict(splits.X_train)\n",
    "    pred_te = rf.predict(splits.X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_MAE\": mean_absolute_error(splits.y_train, pred_tr),\n",
    "        \"train_RMSE\": mean_squared_error(splits.y_train, pred_tr, squared=False),\n",
    "        \"train_R2\": r2_score(splits.y_train, pred_tr),\n",
    "        \"val_MAE\": mean_absolute_error(splits.y_test, pred_te),\n",
    "        \"val_RMSE\": mean_squared_error(splits.y_test, pred_te, squared=False),\n",
    "        \"val_R2\": r2_score(splits.y_test, pred_te),\n",
    "    }\n",
    "    print(f\"[RF/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"rf_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": rf, \"metrics\": metrics, \"rf_params\": rf_params}, path)\n",
    "    return rf, metrics, splits, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d95126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:\n",
      "X_train: (5624, 1024) | X_test: (1406, 1024)\n",
      "y_train: (5624,) | y_test: (1406,)\n",
      "[RF/FFV] val_MAE=0.009095  val_RMSE=0.019753  val_R2=0.5701\n",
      "Splits:\n",
      "X_train: (589, 1024) | X_test: (148, 1024)\n",
      "y_train: (589,) | y_test: (148,)\n",
      "[RF/Tc] val_MAE=0.029866  val_RMSE=0.045109  val_R2=0.7304\n",
      "Splits:\n",
      "X_train: (491, 1024) | X_test: (123, 1024)\n",
      "y_train: (491,) | y_test: (123,)\n",
      "[RF/Rg] val_MAE=1.715067  val_RMSE=2.664982  val_R2=0.6916\n",
      "Splits:\n",
      "X_train: (408, 1024) | X_test: (103, 1024)\n",
      "y_train: (408,) | y_test: (103,)\n",
      "[RF/Tg] val_MAE=61.738193  val_RMSE=78.750171  val_R2=0.5333\n",
      "Splits:\n",
      "X_train: (490, 1024) | X_test: (123, 1024)\n",
      "y_train: (490,) | y_test: (123,)\n",
      "[RF/Density] val_MAE=0.054697  val_RMSE=0.092855  val_R2=0.6311\n"
     ]
    }
   ],
   "source": [
    "rf_cfg = {\n",
    "    \"FFV\": {\"n_estimators\": 100, \"max_depth\": 60},\n",
    "    \"Tc\":  {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "    \"Rg\":  {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "}\n",
    "\n",
    "rf_ffv, m_ffv, splits_ffv, p_ffv = train_eval_rf(X_ffv, y_ffv, rf_params=rf_cfg[\"FFV\"], tag=\"FFV\")\n",
    "rf_tc,  m_tc,  splits_tc,  p_tc  = train_eval_rf(X_tc,  y_tc,  rf_params=rf_cfg[\"Tc\"],  tag=\"Tc\")\n",
    "rf_rg,  m_rg,  splits_rg,  p_rg  = train_eval_rf(X_rg,  y_rg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Rg\")\n",
    "rf_tg,  m_tg,  splits_tg,  p_tg  = train_eval_rf(X_tg,  y_tg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Tg\")\n",
    "rf_density,  m_density,  splits_density,  p_density  = train_eval_rf(X_density,  y_density,  rf_params=rf_cfg[\"Rg\"],  tag=\"Density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7492bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Training RF(+3D) for FFV\n",
      "Splits:\n",
      "X_train: (5624, 1093) | X_test: (1406, 1093)\n",
      "y_train: (5624,) | y_test: (1406,)\n",
      "[RF/FFV_aug3D] val_MAE=0.007621  val_RMSE=0.017553  val_R2=0.6605\n",
      "[RF+3D/FFV]  val_MAE=0.007621  val_RMSE=0.017553  val_R2=0.6605\n",
      "\n",
      ">>> Training RF(+3D) for Tc\n",
      "Splits:\n",
      "X_train: (589, 1093) | X_test: (148, 1093)\n",
      "y_train: (589,) | y_test: (148,)\n",
      "[RF/Tc_aug3D] val_MAE=0.029937  val_RMSE=0.045036  val_R2=0.7313\n",
      "[RF+3D/Tc]  val_MAE=0.029937  val_RMSE=0.045036  val_R2=0.7313\n",
      "\n",
      ">>> Training RF(+3D) for Rg\n",
      "Splits:\n",
      "X_train: (491, 1093) | X_test: (123, 1093)\n",
      "y_train: (491,) | y_test: (123,)\n",
      "[RF/Rg_aug3D] val_MAE=1.648818  val_RMSE=2.493712  val_R2=0.7299\n",
      "[RF+3D/Rg]  val_MAE=1.648818  val_RMSE=2.493712  val_R2=0.7299\n",
      "\n",
      ">>> Training RF(+3D) for Tg\n",
      "Splits:\n",
      "X_train: (408, 1093) | X_test: (103, 1093)\n",
      "y_train: (408,) | y_test: (103,)\n",
      "[RF/Tg_aug3D] val_MAE=58.315801  val_RMSE=74.296699  val_R2=0.5846\n",
      "[RF+3D/Tg]  val_MAE=58.315801  val_RMSE=74.296699  val_R2=0.5846\n",
      "\n",
      ">>> Training RF(+3D) for Density\n",
      "Splits:\n",
      "X_train: (490, 1093) | X_test: (123, 1093)\n",
      "y_train: (490,) | y_test: (123,)\n",
      "[RF/Density_aug3D] val_MAE=0.037793  val_RMSE=0.070932  val_R2=0.7847\n",
      "[RF+3D/Density]  val_MAE=0.037793  val_RMSE=0.070932  val_R2=0.7847\n"
     ]
    }
   ],
   "source": [
    "# === helpers (uses the LMDB feature builders you already added) ===\n",
    "def train_rf_aug3d_for_target(\n",
    "    target_col: str,\n",
    "    rf_params: dict,\n",
    "    *,\n",
    "    train_csv_path: str,\n",
    "    lmdb_path: str,\n",
    "    save_dir: str = \"saved_models/rf_aug3d\",\n",
    "    tag_prefix: str = \"aug3D\",\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "):\n",
    "    \"\"\"Load rows with target, build X=[FP|3D], train RF via your train_eval_rf().\"\"\"\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    mask = ~df[target_col].isna()\n",
    "    ids_tr    = df.loc[mask, 'id'].astype(int).values\n",
    "    smiles_tr = df.loc[mask, 'SMILES'].astype(str).tolist()\n",
    "    y         = df.loc[mask, target_col].astype(float).values\n",
    "\n",
    "    X_aug = build_rf_features_from_lmdb(ids_tr, lmdb_path, smiles_tr)  # (N, 1024 + 69)\n",
    "\n",
    "    model, metrics, splits, path = train_eval_rf(\n",
    "        X_aug, y,\n",
    "        rf_params=rf_params,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=n_strat_bins,\n",
    "        save_dir=save_dir,\n",
    "        tag=f\"{target_col}_{tag_prefix}\"\n",
    "    )\n",
    "    return model, metrics, splits, path\n",
    "\n",
    "# === per-target configs (start with what worked; tweak later) ===\n",
    "rf_cfg_aug = {\n",
    "    \"FFV\":     {\"n_estimators\": 800, \"max_depth\": 30, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "    \"Tc\":      {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "    \"Rg\":      {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "    # reasonable first passes for the two GNN targets (just to A/B):\n",
    "    \"Tg\":      {\"n_estimators\": 600, \"max_depth\": 60, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "    \"Density\": {\"n_estimators\": 600, \"max_depth\": 40, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "}\n",
    "\n",
    "# === train all five with augmented features ===\n",
    "TRAIN_CSV = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "rf_models, rf_metrics, rf_splits, rf_paths = {}, {}, {}, {}\n",
    "\n",
    "for t in [\"FFV\", \"Tc\", \"Rg\", \"Tg\", \"Density\"]:\n",
    "    print(f\"\\n>>> Training RF(+3D) for {t}\")\n",
    "    m, met, sp, p = train_rf_aug3d_for_target(\n",
    "        t, rf_cfg_aug[t],\n",
    "        train_csv_path=TRAIN_CSV,\n",
    "        lmdb_path=TRAIN_LMDB,\n",
    "        save_dir=\"saved_models/rf_aug3d\",\n",
    "        tag_prefix=\"aug3D\",\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify_regression=True,\n",
    "        n_strat_bins=10,\n",
    "    )\n",
    "    rf_models[t], rf_metrics[t], rf_splits[t], rf_paths[t] = m, met, sp, p\n",
    "    print(f\"[RF+3D/{t}]  val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d0915ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df  = pd.read_csv(os.path.join(DATA_ROOT, 'test.csv'))\n",
    "# test_ids = test_df['id'].astype(int).values\n",
    "# test_smiles = test_df['SMILES'].astype(str).tolist()\n",
    "\n",
    "# X_test_aug = build_rf_features_from_lmdb(test_ids, TEST_LMDB, test_smiles)  # (M, 1093)\n",
    "\n",
    "# # Example: swap in RF(+3D) for FFV and Tc (and optionally Tg/Density/Rg if better)\n",
    "# rf_ffv = joblib.load(rf_paths[\"FFV\"])['model']\n",
    "# rf_tc  = joblib.load(rf_paths[\"Tc\"])['model']\n",
    "\n",
    "# pred_ffv = rf_ffv.predict(X_test_aug).astype(float)\n",
    "# pred_tc  = rf_tc.predict(X_test_aug).astype(float)\n",
    "\n",
    "# # If you want to try RF(+3D) for Tg/Density/Rg too:\n",
    "# # pred_tg = joblib.load(rf_paths[\"Tg\"])['model'].predict(X_test_aug).astype(float)\n",
    "# # pred_density = joblib.load(rf_paths[\"Density\"])['model'].predict(X_test_aug).astype(float)\n",
    "# # pred_rg = joblib.load(rf_paths[\"Rg\"])['model'].predict(X_test_aug).astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77f7ec",
   "metadata": {},
   "source": [
    "## ChemML GNN Model Results\n",
    "| Model Type             | Featurization        |   MAE |  RMSE |   R² | Notes             |\n",
    "|------------------------|----------------------|-------|-------|------|-------------------|\n",
    "| GNN (Tuned)            | tensorise_molecules Graph   | 0.302 | 0.411 | 0.900 | Best performance across all metrics   |\n",
    "| GNN (Untuned)          | tensorise_molecules Graph   | 0.400 | 0.519 | 0.841 | Good overall|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42db218",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Model Training\n",
    "\n",
    "Having explored different molecular graph representations and model architectures, I am now moving to training what is expected to be the best-performing model using the full dataset. The earlier GNN model was based on `tensorise_molecules` (ChemML) graphs and had strong performance with a **mean absolute error (MAE) around 0.30**. These graphs are based on RDKit's internal descriptors and do not reflect the original PCQM4Mv2 graph structure used in the Open Graph Benchmark (OGB). Therefore, I will shift focus to the `smiles2graph` representation provided by OGB, which aligns more directly with the benchmark's evaluation setup and top-performing models on the leaderboard.\n",
    "\n",
    "\n",
    "| Source                         | Atom/Bond Features                                                 | Format                                          | Customizable?     | Alignment with PCQM4Mv2?  |\n",
    "| ------------------------------ | ------------------------------------------------------------------ | ----------------------------------------------- | ----------------- | ---------------------- |\n",
    "| `tensorise_molecules` (ChemML) | RDKit-based descriptors (ex: atom number, degree, hybridization) | NumPy tensors (`X_atoms`, `X_bonds`, `X_edges`) | Limited           |  Not aligned          |\n",
    "| `smiles2graph` (OGB / PyG)     | Predefined categorical features from PCQM4Mv2                      | PyTorch Geometric `Data` objects                |  Highly flexible |  Matches OGB standard |\n",
    "\n",
    "By using `smiles2graph`, we:\n",
    "\n",
    "* Use OGB-standard graph construction and feature encoding for fair comparisons with leaderboard models\n",
    "* Include learnable AtomEncoder and BondEncoder embeddings from `ogb.graphproppred.mol_encoder`, which improve model expressiveness\n",
    "* Maintain compatibility with PyTorch Geometric, DGL, and OGB tools\n",
    "\n",
    "I will also concatenate GNN-derived embeddings with SMILES-based RDKit descriptors, feeding this hybrid representation into MLP head. This allows you to combine structural and cheminformatics perspectives for improved prediction accuracy. With this setup, I aim to improve upon the MAE of \\~0.30 achieved earlier and push closer toward state-of-the-art performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d599b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tg ids: (511,) Density ids: (613,)\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))  # keep 'id'!\n",
    "lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "if os.path.exists(lmdb_ids_path):\n",
    "    with open(lmdb_ids_path) as f:\n",
    "        kept_ids = set(int(x.strip()) for x in f if x.strip())\n",
    "else:\n",
    "    kept_ids = set(train_csv['id'].astype(int).tolist())\n",
    "\n",
    "def ids_for_task(task):\n",
    "    t = task2idx[task]\n",
    "    col = label_cols[t]\n",
    "    ids = train_csv.loc[~train_csv[col].isna(), 'id'].astype(int).tolist()\n",
    "    # only those that actually exist in LMDB\n",
    "    return np.array([i for i in ids if i in kept_ids], dtype=int)\n",
    "\n",
    "ids_tg  = ids_for_task(\"Tg\")\n",
    "ids_den = ids_for_task(\"Density\")\n",
    "ids_tc = ids_for_task(\"Tc\")\n",
    "ids_rg = ids_for_task(\"Rg\")\n",
    "ids_ffv = ids_for_task(\"FFV\")\n",
    "print(\"Tg ids:\", ids_tg.shape, \"Density ids:\", ids_den.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3efce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import torch, numpy as np\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def _get_rdkit_feats_from_record(rec):\n",
    "    arr = getattr(rec, \"rdkit_feats\", None)\n",
    "    if arr is None:\n",
    "        return torch.zeros(15, dtype=torch.float32)   # or 6 if that’s your build\n",
    "    v = torch.as_tensor(np.asarray(arr, np.float32).reshape(-1), dtype=torch.float32)\n",
    "    return v.unsqueeze(0)  # <<< IMPORTANT: (1, D) so batch -> (B, D)\n",
    "\n",
    "\n",
    "class LMDBtoPyGSingleTask(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ids,\n",
    "        lmdb_path,\n",
    "        target_index=None,\n",
    "        *,\n",
    "        use_mixed_edges: bool = True,      # <— enables 3 cat + 32 RBF continuous\n",
    "        include_extra_atom_feats: bool = True,  # <— attach per-atom extras\n",
    "    ):\n",
    "        self.base = LMDBDataset(ids, lmdb_path)\n",
    "        self.t = target_index\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        self.include_extra_atom_feats = include_extra_atom_feats\n",
    "\n",
    "    def __len__(self): return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.base[idx]\n",
    "\n",
    "        x  = torch.as_tensor(rec.x, dtype=torch.long)\n",
    "        ei = torch.as_tensor(rec.edge_index, dtype=torch.long)\n",
    "\n",
    "        ea = torch.as_tensor(rec.edge_attr)              # (E, 3 + 32)\n",
    "        if self.use_mixed_edges:\n",
    "            # keep all columns; EdgeEncoderMixed will split cat vs cont\n",
    "            edge_attr = ea.to(torch.float32)\n",
    "        else:\n",
    "            # categorical-only for vanilla BondEncoder\n",
    "            edge_attr = ea[:, :3].to(torch.long)\n",
    "\n",
    "        # rdkit globals: KEEP AS (1, D) so PyG collates to (B, D)\n",
    "        rdkit_feats = _get_rdkit_feats_from_record(rec)  # (1, D)\n",
    "        d = Data(x=x, edge_index=ei, edge_attr=edge_attr, rdkit_feats=rdkit_feats)\n",
    "\n",
    "        if self.include_extra_atom_feats and hasattr(rec, \"extra_atom_feats\"):\n",
    "            d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32)  # (N,5)\n",
    "\n",
    "        if hasattr(rec, \"has_xyz\"):\n",
    "            # collates to (B,1); handy as a gating/global indicator\n",
    "            hz = np.asarray(rec.has_xyz, np.uint8).reshape(-1)\n",
    "            d.has_xyz = torch.from_numpy(hz.astype(np.float32))\n",
    "\n",
    "        if (self.t is not None) and hasattr(rec, \"y\"):\n",
    "            yv = torch.as_tensor(rec.y, dtype=torch.float32).view(-1)\n",
    "            if self.t < yv.numel():\n",
    "                d.y = yv[self.t:self.t+1]  # (1,)\n",
    "\n",
    "        # geometry & extras from LMDB (if present)\n",
    "        if hasattr(rec, \"pos\"):              # (N,3) float\n",
    "            d.pos = torch.as_tensor(rec.pos, dtype=torch.float32)\n",
    "        if hasattr(rec, \"extra_atom_feats\"): # (N,5) float\n",
    "            d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32)\n",
    "        if hasattr(rec, \"has_xyz\"):          # (1,) bool/uint8\n",
    "            d.has_xyz = torch.as_tensor(rec.has_xyz, dtype=torch.float32)\n",
    "        # LMDBtoPyGSingleTask.__getitem__  (add this near the end, after you create Data d)\n",
    "        if hasattr(rec, \"dist\"):\n",
    "            # rec.dist is (L, L) (uint8) in your LMDB\n",
    "            d.hops = torch.as_tensor(rec.dist, dtype=torch.long).unsqueeze(0)  # (1, L, L)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "694612d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "def make_loaders_for_task(task, ids, *, batch_size=64, seed=42,\n",
    "                          use_mixed_edges=True, include_extra_atom_feats=True):\n",
    "    t = task2idx[task]\n",
    "    tr_ids, va_ids = train_test_split(ids, test_size=0.2, random_state=seed)\n",
    "    tr_ds = LMDBtoPyGSingleTask(tr_ids, TRAIN_LMDB, target_index=t,\n",
    "                                use_mixed_edges=use_mixed_edges,\n",
    "                                include_extra_atom_feats=include_extra_atom_feats)\n",
    "    va_ds = LMDBtoPyGSingleTask(va_ids, TRAIN_LMDB, target_index=t,\n",
    "                                use_mixed_edges=use_mixed_edges,\n",
    "                                include_extra_atom_feats=include_extra_atom_feats)\n",
    "    tr = GeoDataLoader(tr_ds, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "    va = GeoDataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    return tr, va\n",
    "\n",
    "\n",
    "# train_loader_tg,  val_loader_tg  = make_loaders_for_task(\"Tg\", ids_tg,  batch_size=32)\n",
    "# train_loader_den, val_loader_den = make_loaders_for_task(\"Density\", ids_den, batch_size=32)\n",
    "# train_loader_tc,  val_loader_tc  = make_loaders_for_task(\"Tc\", ids_tc,  batch_size=32)\n",
    "# train_loader_rg, val_loader_rg = make_loaders_for_task(\"Rg\", ids_rg, batch_size=32)\n",
    "# train_loader_ffv, val_loader_ffv = make_loaders_for_task(\"FFV\", ids_ffv, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983db98",
   "metadata": {},
   "source": [
    "## Step 5: Define the Hybrid GNN Model\n",
    "\n",
    "The final architecture uses both structural and cheminformatics data by combining GNN-learned graph embeddings with SMILES-derived RDKit descriptors. This Hybrid GNN model uses `smiles2graph` for graph construction and augments it with RDKit-based molecular features for improved prediction accuracy.\n",
    "\n",
    "### Model Components:\n",
    "\n",
    "* **AtomEncoder / BondEncoder**\n",
    "  Transforms categorical atom and bond features (provided by OGB) into learnable embeddings using the encoders from `ogb.graphproppred.mol_encoder`. These provide a strong foundation for expressive graph learning.\n",
    "\n",
    "* **GINEConv Layers (x2)**\n",
    "  I use two stacked GINEConv layers (Graph Isomorphism Network with Edge features). These layers perform neighborhood aggregation based on edge attributes, allowing the model to capture localized chemical environments.\n",
    "\n",
    "* **Global Mean Pooling**\n",
    "  After message passing, node level embeddings are aggregated into a fixed size graph level representation using `global_mean_pool`.\n",
    "\n",
    "* **Concatenation with RDKit Descriptors**\n",
    "  The pooled GNN embedding is concatenated with external RDKit descriptors, which capture global molecular properties not easily inferred from graph data alone.\n",
    "\n",
    "* **MLP Prediction Head**\n",
    "  A multilayer perceptron processes the combined feature vector with ReLU activations, dropout regularization, and linear layers to predict the HOMO–LUMO gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82dad355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = float(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        rand = keep + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        rand.floor_()  # 0/1\n",
    "        return x.div(keep) * rand\n",
    "\n",
    "\n",
    "def _act(name: str):\n",
    "    name = (name or \"ReLU\").lower()\n",
    "    if name == \"relu\": return nn.ReLU()\n",
    "    if name == \"gelu\": return nn.GELU()\n",
    "    if name in (\"swish\", \"silu\"): return nn.SiLU()\n",
    "    return nn.ReLU()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0946f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeEncoderMixed(nn.Module):\n",
    "    def __init__(self, emb_dim: int, cont_dim: int = 32, activation=\"ReLU\"):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        # OGB bond categorical widths: type(5), stereo(6), conjugation(2)\n",
    "        self.emb0 = nn.Embedding(5, emb_dim)\n",
    "        self.emb1 = nn.Embedding(6, emb_dim)\n",
    "        self.emb2 = nn.Embedding(2, emb_dim)\n",
    "        self.mlp_cont = nn.Sequential(\n",
    "            nn.Linear(cont_dim, emb_dim),\n",
    "            act,\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        # edge_attr: (E, 3+K)\n",
    "        cat = edge_attr[:, :3].long()\n",
    "        cont = edge_attr[:, 3:].float()\n",
    "        e_cat  = self.emb0(cat[:,0]) + self.emb1(cat[:,1]) + self.emb2(cat[:,2])\n",
    "        e_cont = self.mlp_cont(cont)\n",
    "        return e_cat + e_cont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5380b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtraAtomEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, activation=\"ReLU\"):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            act,\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, extra):\n",
    "        return self.proj(extra)  # (N, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8e379ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINEConv\n",
    "\n",
    "class GINEBlock_GNN(nn.Module):\n",
    "    def __init__(self, dim, activation=\"ReLU\", dropout=0.1, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.conv = GINEConv(nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            act,\n",
    "            nn.Linear(dim, dim),\n",
    "        ))\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dp1 = DropPath(drop_path)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, 2*dim),\n",
    "            act,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2*dim, dim),\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dp2 = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_emb):\n",
    "        # pre-norm transformer style\n",
    "        h = self.norm1(x)\n",
    "        h = self.conv(h, edge_index, edge_emb)\n",
    "        h = self.dropout1(h)\n",
    "        x = x + self.dp1(h)\n",
    "\n",
    "        h2 = self.norm2(x)\n",
    "        h2 = self.ffn(h2)\n",
    "        h2 = self.dropout2(h2)\n",
    "        x = x + self.dp2(h2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bef6fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool, global_max_pool, GlobalAttention\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "from torch import nn\n",
    "\n",
    "class HybridGNNv2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gnn_dim: int,\n",
    "        rdkit_dim: int,\n",
    "        hidden_dim: int,\n",
    "        *,\n",
    "        num_layers: int = 8,\n",
    "        activation: str = \"Swish\",\n",
    "        dropout: float = 0.2,\n",
    "        drop_path_rate: float = 0.1,\n",
    "        use_mixed_edges: bool = True,\n",
    "        cont_dim: int = 32,\n",
    "        use_extra_atom_feats: bool = True,\n",
    "        extra_atom_dim: int = 5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gnn_dim = gnn_dim\n",
    "        self.rdkit_dim = rdkit_dim\n",
    "        self.use_extra_atom_feats = use_extra_atom_feats\n",
    "\n",
    "        # encoders\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=gnn_dim)\n",
    "        if use_mixed_edges:\n",
    "            self.edge_encoder = EdgeEncoderMixed(emb_dim=gnn_dim, cont_dim=cont_dim, activation=activation)\n",
    "        else:\n",
    "            self.edge_encoder = BondEncoder(emb_dim=gnn_dim)\n",
    "\n",
    "        if use_extra_atom_feats:\n",
    "            self.extra_atom = ExtraAtomEncoder(in_dim=extra_atom_dim, out_dim=gnn_dim, activation=activation)\n",
    "            self.extra_gate = nn.Sequential(nn.Linear(2*gnn_dim, gnn_dim), _act(activation))\n",
    "\n",
    "        # backbone\n",
    "        dpr = [drop_path_rate * i / max(1, num_layers - 1) for i in range(num_layers)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GINEBlock_GNN(gnn_dim, activation=activation, dropout=dropout, drop_path=dpr[i])\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # pooling (concat of mean/max/attention)\n",
    "        self.att_pool = GlobalAttention(\n",
    "            gate_nn=nn.Sequential(\n",
    "                nn.Linear(gnn_dim, gnn_dim // 2),\n",
    "                _act(activation),\n",
    "                nn.Linear(gnn_dim // 2, 1),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        pooled_dim = 3 * gnn_dim  # mean + max + attention\n",
    "        # plus rdkit globals (+ optional has_xyz scalar)\n",
    "        self.with_has_xyz = True\n",
    "        head_in = pooled_dim + rdkit_dim + (1 if self.with_has_xyz else 0)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(head_in),\n",
    "            nn.Linear(head_in, hidden_dim),\n",
    "            _act(activation),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            _act(activation),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.atom_encoder(data.x)  # (N, D)\n",
    "\n",
    "        if self.use_extra_atom_feats and hasattr(data, \"extra_atom_feats\"):\n",
    "            xa = self.extra_atom(data.extra_atom_feats)  # (N, D)\n",
    "            x = self.extra_gate(torch.cat([x, xa], dim=1))\n",
    "\n",
    "        e = self.edge_encoder(data.edge_attr)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, data.edge_index, e)\n",
    "\n",
    "        # pool\n",
    "        mean = global_mean_pool(x, data.batch)\n",
    "        mmax = global_max_pool(x, data.batch)\n",
    "        attn = self.att_pool(x, data.batch)\n",
    "        g = torch.cat([mean, mmax, attn], dim=1)\n",
    "\n",
    "        rd = data.rdkit_feats.view(g.size(0), -1)\n",
    "        extras = [g, rd]\n",
    "\n",
    "        if self.with_has_xyz and hasattr(data, \"has_xyz\"):\n",
    "            # has_xyz collates to (B,1)\n",
    "            extras.append(data.has_xyz.view(-1, 1).float())\n",
    "\n",
    "        out = torch.cat(extras, dim=1)\n",
    "        return self.head(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc992041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numpy as np, torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, RMSprop\n",
    "from torch.amp import GradScaler, autocast\n",
    "from copy import deepcopy\n",
    "\n",
    "def train_hybrid_gnn_sota(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    *,\n",
    "    lr: float = 5e-4,\n",
    "    optimizer: str = \"AdamW\",\n",
    "    weight_decay: float = 1e-5,\n",
    "    epochs: int = 120,\n",
    "    warmup_epochs: int = 5,\n",
    "    patience: int = 15,\n",
    "    clip_norm: float = 1.0,\n",
    "    amp: bool = True,\n",
    "    loss_name: str = \"mse\",   # \"mse\" or \"huber\"\n",
    "    save_dir: str = \"saved_models/gnn\",\n",
    "    tag: str = \"model_sota\",\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    opt_name = optimizer.lower()\n",
    "    if opt_name == \"rmsprop\":\n",
    "        opt = RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.0)\n",
    "    else:\n",
    "        opt = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # cosine schedule w/ warmup\n",
    "    def lr_factor(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / max(1, warmup_epochs)\n",
    "        t = (epoch - warmup_epochs) / max(1, (epochs - warmup_epochs))\n",
    "        return 0.5 * (1 + math.cos(math.pi * t))\n",
    "    scaler = GradScaler(\"cuda\", enabled=amp)\n",
    "\n",
    "    def loss_fn(pred, target):\n",
    "        if loss_name.lower() == \"huber\":\n",
    "            return F.huber_loss(pred, target, delta=1.0)\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_once(loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        for b in loader:\n",
    "            b = b.to(device)\n",
    "            p = model(b)\n",
    "            preds.append(p.detach().cpu())\n",
    "            trues.append(b.y.view(-1,1).cpu())\n",
    "        preds = torch.cat(preds).numpy(); trues = torch.cat(trues).numpy()\n",
    "        mae = np.mean(np.abs(preds - trues))\n",
    "        rmse = float(np.sqrt(np.mean((preds - trues)**2)))\n",
    "        r2 = float(1 - np.sum((preds - trues)**2) / np.sum((trues - trues.mean())**2))\n",
    "        return mae, rmse, r2\n",
    "\n",
    "    best_mae = float(\"inf\")\n",
    "    best = None\n",
    "    best_path = os.path.join(save_dir, f\"{tag}.pt\")\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # schedule\n",
    "        for g in opt.param_groups:\n",
    "            g[\"lr\"] = lr * lr_factor(ep-1)\n",
    "\n",
    "        model.train()\n",
    "        total, count = 0.0, 0\n",
    "        for b in train_loader:\n",
    "            b = b.to(device)\n",
    "            with autocast(\"cuda\", enabled=amp):\n",
    "                pred = model(b)\n",
    "                loss = loss_fn(pred, b.y.view(-1,1))\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            if clip_norm is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "            scaler.step(opt); scaler.update()\n",
    "\n",
    "            total += loss.item() * b.num_graphs\n",
    "            count += b.num_graphs\n",
    "\n",
    "        tr_mse = total / max(1, count)\n",
    "        mae, rmse, r2 = eval_once(val_loader)\n",
    "        print(f\"Epoch {ep:03d} | tr_MSE {tr_mse:.5f} | val_MAE {mae:.5f} | val_RMSE {rmse:.5f} | R2 {r2:.4f}\")\n",
    "\n",
    "        if mae < best_mae - 1e-6:\n",
    "            best_mae = mae\n",
    "            best = deepcopy(model.state_dict())\n",
    "            torch.save(best, best_path)\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best is not None:\n",
    "        model.load_state_dict(best)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "    final_mae, final_rmse, final_r2 = eval_once(val_loader)\n",
    "    print(f\"[{tag}] Best Val — MAE {final_mae:.6f} | RMSE {final_rmse:.6f} | R2 {final_r2:.4f}\")\n",
    "    return model, best_path, {\"MAE\": final_mae, \"RMSE\": final_rmse, \"R2\": final_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0813a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdkit_dim = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | tr_MSE 22759.86803 | val_MAE 100.49586 | val_RMSE 131.00262 | R2 -0.7978\n",
      "Epoch 002 | tr_MSE 21813.04063 | val_MAE 87.93384 | val_RMSE 115.11349 | R2 -0.3881\n",
      "Epoch 003 | tr_MSE 15147.35281 | val_MAE 84.16945 | val_RMSE 99.33015 | R2 -0.0336\n",
      "Epoch 004 | tr_MSE 13345.48935 | val_MAE 71.59711 | val_RMSE 93.32719 | R2 0.0876\n",
      "Epoch 005 | tr_MSE 10381.04192 | val_MAE 68.64759 | val_RMSE 84.64999 | R2 0.2494\n",
      "Epoch 006 | tr_MSE 10883.44371 | val_MAE 69.98320 | val_RMSE 85.59296 | R2 0.2326\n",
      "Epoch 007 | tr_MSE 9134.36121 | val_MAE 183.53329 | val_RMSE 202.26414 | R2 -3.2856\n",
      "Epoch 008 | tr_MSE 20454.84035 | val_MAE 100.56407 | val_RMSE 130.58505 | R2 -0.7863\n",
      "Epoch 009 | tr_MSE 11870.74661 | val_MAE 65.34143 | val_RMSE 83.27809 | R2 0.2735\n",
      "Epoch 010 | tr_MSE 9277.04102 | val_MAE 67.25314 | val_RMSE 82.90101 | R2 0.2801\n",
      "Epoch 011 | tr_MSE 8974.78274 | val_MAE 63.24532 | val_RMSE 78.93472 | R2 0.3473\n",
      "Epoch 012 | tr_MSE 8616.08253 | val_MAE 58.61529 | val_RMSE 77.56824 | R2 0.3697\n",
      "Epoch 013 | tr_MSE 7449.01969 | val_MAE 57.02232 | val_RMSE 72.83905 | R2 0.4442\n",
      "Epoch 014 | tr_MSE 7428.55823 | val_MAE 56.25684 | val_RMSE 71.55058 | R2 0.4637\n",
      "Epoch 015 | tr_MSE 6495.64768 | val_MAE 69.97124 | val_RMSE 88.35696 | R2 0.1822\n",
      "Epoch 016 | tr_MSE 6876.69656 | val_MAE 56.17709 | val_RMSE 70.17194 | R2 0.4842\n",
      "Epoch 017 | tr_MSE 6222.33285 | val_MAE 55.75706 | val_RMSE 69.34523 | R2 0.4963\n",
      "Epoch 018 | tr_MSE 6145.92911 | val_MAE 59.54287 | val_RMSE 76.55832 | R2 0.3860\n",
      "Epoch 019 | tr_MSE 5830.31396 | val_MAE 58.19396 | val_RMSE 72.50477 | R2 0.4493\n",
      "Epoch 020 | tr_MSE 6435.37904 | val_MAE 61.10825 | val_RMSE 75.03188 | R2 0.4103\n",
      "Epoch 021 | tr_MSE 5651.65186 | val_MAE 55.50850 | val_RMSE 71.09595 | R2 0.4705\n",
      "Epoch 022 | tr_MSE 5640.64263 | val_MAE 55.99710 | val_RMSE 71.23466 | R2 0.4684\n",
      "Epoch 023 | tr_MSE 6015.42961 | val_MAE 56.07663 | val_RMSE 69.26608 | R2 0.4974\n",
      "Epoch 024 | tr_MSE 5937.90612 | val_MAE 54.72252 | val_RMSE 68.28975 | R2 0.5115\n",
      "Epoch 025 | tr_MSE 5426.28945 | val_MAE 54.62487 | val_RMSE 68.54112 | R2 0.5079\n",
      "Epoch 026 | tr_MSE 6008.79716 | val_MAE 55.24532 | val_RMSE 69.04531 | R2 0.5006\n",
      "Epoch 027 | tr_MSE 5610.65822 | val_MAE 57.73568 | val_RMSE 72.31562 | R2 0.4522\n",
      "Epoch 028 | tr_MSE 5616.75814 | val_MAE 56.57460 | val_RMSE 71.36943 | R2 0.4664\n",
      "Epoch 029 | tr_MSE 5594.34181 | val_MAE 63.70031 | val_RMSE 78.72525 | R2 0.3508\n",
      "Epoch 030 | tr_MSE 5958.39631 | val_MAE 54.68828 | val_RMSE 69.22715 | R2 0.4980\n",
      "Epoch 031 | tr_MSE 5509.58215 | val_MAE 55.86318 | val_RMSE 72.62028 | R2 0.4476\n",
      "Epoch 032 | tr_MSE 5564.34957 | val_MAE 54.93974 | val_RMSE 70.03263 | R2 0.4862\n",
      "Epoch 033 | tr_MSE 5508.43750 | val_MAE 53.80259 | val_RMSE 68.50764 | R2 0.5084\n",
      "Epoch 034 | tr_MSE 5343.98043 | val_MAE 57.73422 | val_RMSE 71.61655 | R2 0.4627\n",
      "Epoch 035 | tr_MSE 5262.20370 | val_MAE 59.86918 | val_RMSE 74.53135 | R2 0.4181\n",
      "Epoch 036 | tr_MSE 5493.10653 | val_MAE 54.56230 | val_RMSE 68.32796 | R2 0.5109\n",
      "Epoch 037 | tr_MSE 5578.90273 | val_MAE 54.85936 | val_RMSE 68.01398 | R2 0.5154\n",
      "Epoch 038 | tr_MSE 5429.12353 | val_MAE 54.70576 | val_RMSE 69.06800 | R2 0.5003\n",
      "Epoch 039 | tr_MSE 5306.72190 | val_MAE 54.64883 | val_RMSE 67.63229 | R2 0.5208\n",
      "Epoch 040 | tr_MSE 5529.39329 | val_MAE 54.82664 | val_RMSE 68.78290 | R2 0.5044\n",
      "Epoch 041 | tr_MSE 5322.75873 | val_MAE 57.49673 | val_RMSE 71.69588 | R2 0.4615\n",
      "Epoch 042 | tr_MSE 5643.68475 | val_MAE 55.08693 | val_RMSE 67.93677 | R2 0.5165\n",
      "Epoch 043 | tr_MSE 5501.59767 | val_MAE 57.88183 | val_RMSE 72.05923 | R2 0.4561\n",
      "Epoch 044 | tr_MSE 5132.35305 | val_MAE 62.01268 | val_RMSE 75.69537 | R2 0.3998\n",
      "Epoch 045 | tr_MSE 5511.07304 | val_MAE 57.44941 | val_RMSE 71.53115 | R2 0.4640\n",
      "Epoch 046 | tr_MSE 5402.48246 | val_MAE 60.44571 | val_RMSE 75.54538 | R2 0.4022\n",
      "Epoch 047 | tr_MSE 5260.91420 | val_MAE 54.48334 | val_RMSE 70.26241 | R2 0.4829\n"
     ]
    }
   ],
   "source": [
    "# Build loaders (now feeding mixed edges + extra atom feats)\n",
    "train_loader_tg,  val_loader_tg  = make_loaders_for_task(\"Tg\",      ids_tg,  batch_size=64,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "train_loader_den, val_loader_den = make_loaders_for_task(\"Density\", ids_den, batch_size=64,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "train_loader_rg,  val_loader_rg  = make_loaders_for_task(\"Rg\",      ids_rg,  batch_size=64,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "\n",
    "# Introspect dims from a real batch\n",
    "b_tg = next(iter(train_loader_tg))\n",
    "rd_dim = b_tg.rdkit_feats.shape[-1]           # 15 if you rebuilt with 15 globals\n",
    "print(\"rdkit_dim =\", rd_dim)\n",
    "\n",
    "# Tg (example baseline dims)\n",
    "model_tg = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "\n",
    "model_tg, ckpt_tg, metrics_tg = train_hybrid_gnn_sota(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=0.0005555079210176292, optimizer=\"RMSprop\", weight_decay=9.056299733554687e-06,\n",
    "    epochs=200, warmup_epochs=5, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gnn_tg_v2\", tag=\"hybridgnn_tg_v2\"\n",
    ")\n",
    "\n",
    "\n",
    "# Density (use your tuned dims if you like larger backbones)\n",
    "model_den = HybridGNNv2(\n",
    "    gnn_dim=1024, rdkit_dim=rd_dim, hidden_dim=384,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.1, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "model_den, ckpt_den, metrics_den = train_hybrid_gnn_sota(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=5.956024201538505e-04, optimizer=\"AdamW\", weight_decay=8.619671341229739e-06,\n",
    "    epochs=200, warmup_epochs=8, patience=30,\n",
    "    clip_norm=0.5, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gnn_density_v2\", tag=\"hybridgnn_density_v2\"\n",
    ")\n",
    "\n",
    "\n",
    "# Rg (your tuned gnn_dim + swish + RMSprop work fine here)\n",
    "model_rg = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "model_rg, ckpt_rg, metrics_rg = train_hybrid_gnn_sota(\n",
    "    model_rg, train_loader_rg, val_loader_rg,\n",
    "    lr=5.6e-4, optimizer=\"RMSprop\", weight_decay=9.0e-6,\n",
    "    epochs=120, warmup_epochs=6, patience=20,\n",
    "    clip_norm=0.5, amp=True, loss_name=\"huber\",  # Huber often helps Rg\n",
    "    save_dir=\"saved_models/gnn_rg_v2\", tag=\"hybridgnn_rg_v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5aba07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "\n",
    "# ---------- helpers ----------\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, p=0.0): super().__init__(); self.p=float(p)\n",
    "    def forward(self, x):\n",
    "        if self.p==0.0 or not self.training: return x\n",
    "        keep = 1.0 - self.p\n",
    "        shape = (x.size(0),) + (1,)*(x.ndim-1)\n",
    "        rnd = keep + torch.rand(shape, device=x.device, dtype=x.dtype)\n",
    "        rnd.floor_()\n",
    "        return x * rnd / keep\n",
    "\n",
    "def _act(name): \n",
    "    name=(name or \"relu\").lower()\n",
    "    return nn.GELU() if name==\"gelu\" else nn.SiLU() if name in (\"swish\",\"silu\") else nn.ReLU()\n",
    "\n",
    "# ---------- attention layer that accepts additive attn_bias ----------\n",
    "class MHALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps torch.nn.MultiheadAttention but allows a per-head additive bias:\n",
    "      attn_bias: (B, H, L, L) float, added to scaled dot-product before softmax.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def forward(self, x, key_padding_mask, attn_bias):\n",
    "        # x: (B,L,D), key_padding_mask: (B,L) True=PAD, attn_bias: (B,H,L,L)\n",
    "        B, L, D = x.shape\n",
    "        H = self.nhead\n",
    "        # PyTorch accepts attn_mask shaped (B*H, L, L) additive\n",
    "        mask_add = attn_bias.reshape(B*H, L, L)\n",
    "        out, _ = self.mha(\n",
    "            x, x, x,\n",
    "            key_padding_mask=key_padding_mask,  # True at pads\n",
    "            attn_mask=mask_add,\n",
    "            need_weights=False\n",
    "        )\n",
    "        return out\n",
    "\n",
    "# ---------- Transformer block (pre-norm, bias-aware attention, FFN, drop-path) ----------\n",
    "class TFBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, mlp_ratio=4.0, act=\"gelu\", dropout=0.1, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn  = MHALayer(d_model, nhead, dropout=dropout)\n",
    "        self.dp1   = DropPath(drop_path)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        hidden = int(d_model * mlp_ratio)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden), _act(act), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, d_model), nn.Dropout(dropout),\n",
    "        )\n",
    "        self.dp2   = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, x, key_padding_mask, attn_bias):\n",
    "        h = self.attn(self.norm1(x), key_padding_mask, attn_bias)\n",
    "        x = x + self.dp1(h)\n",
    "        h = self.ffn(self.norm2(x))\n",
    "        x = x + self.dp2(h)\n",
    "        return x\n",
    "\n",
    "# ---------- build geometry/graph attention bias ----------\n",
    "class AttnBiasBuilder(nn.Module):\n",
    "    def __init__(self, n_heads, rbf_k=32, dmin=0.0, dmax=10.0, beta=5.0, act=\"silu\"):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.register_buffer(\"centers\", torch.linspace(dmin, dmax, rbf_k).view(1,1,rbf_k))\n",
    "        self.beta = beta\n",
    "        self.geo_mlp = nn.Sequential(\n",
    "            nn.Linear(rbf_k, 2*n_heads), _act(act), nn.Linear(2*n_heads, n_heads)\n",
    "        )\n",
    "        self.adj_bias = nn.Parameter(torch.zeros(n_heads))\n",
    "        nn.init.normal_(self.adj_bias, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, pos, edge_index, batch, key_padding_mask):\n",
    "        \"\"\"\n",
    "        Build additive attention bias of shape (B, H, L0, L0) before CLS.\n",
    "        key_padding_mask: (B, L0)   True == PAD   (no CLS here)\n",
    "        Uses:\n",
    "        - self.n_heads (int)\n",
    "        - self.centers (K,)  float tensor for RBF centers\n",
    "        - self.beta    (float) scalar for RBF width\n",
    "        - self.geo_mlp : nn.Sequential(K -> H) maps RBF to per-head geometry bias\n",
    "        - self.adj_bias: nn.Parameter(H,) per-head adjacency bias scale\n",
    "        \"\"\"\n",
    "        # 1) dense adjacency  (B, L0, L0)\n",
    "        A = to_dense_adj(edge_index, batch=batch)          # (B, 1, L0, L0) or (B, L0, L0) depending on version\n",
    "        if A.dim() == 4:\n",
    "            A = A.squeeze(1)                               # -> (B, L0, L0)\n",
    "\n",
    "        B, L0, _ = A.shape\n",
    "        H = self.n_heads\n",
    "        device = A.device\n",
    "\n",
    "        # 2) geometry bias (zeros if pos is None)\n",
    "        if pos is not None:\n",
    "            pad_pos, _valid = to_dense_batch(pos, batch)   # (B, L0, 3)\n",
    "            # pairwise distances\n",
    "            diff = pad_pos.unsqueeze(2) - pad_pos.unsqueeze(1)         # (B, L0, L0, 3)\n",
    "            dist = torch.sqrt(torch.clamp((diff**2).sum(-1), min=0.0)) # (B, L0, L0)\n",
    "\n",
    "            centers = self.centers.to(dist.device)  # (K,)\n",
    "            rbf = torch.exp(-self.beta * (dist.unsqueeze(-1) - centers)**2)  # (B, L0, L0, K)\n",
    "            geo_bias = self.geo_mlp(rbf).permute(0, 3, 1, 2).contiguous()    # (B, H, L0, L0)\n",
    "            geo_bias = geo_bias.to(torch.float32)\n",
    "        else:\n",
    "            geo_bias = torch.zeros((B, H, L0, L0), device=device, dtype=torch.float32)\n",
    "\n",
    "        # 3) adjacency bias per head\n",
    "        #     A is (B, L0, L0) float; scale each head with learned scalar\n",
    "        adj_bias = self.adj_bias.view(1, H, 1, 1) * A.unsqueeze(1)     # (B, H, L0, L0)\n",
    "\n",
    "        # 4) PAD masking (rows from PAD, cols to PAD) with big negative\n",
    "        #    (we keep diagonal as-is here; model will final-fix diagonals if needed)\n",
    "        pad = key_padding_mask.to(torch.bool)                           # (B, L0)\n",
    "        big_neg = geo_bias.new_tensor(-1e4)\n",
    "\n",
    "        # rows FROM PAD\n",
    "        geo_bias = geo_bias.masked_fill(pad.view(B, 1, L0, 1), big_neg)\n",
    "        adj_bias = adj_bias.masked_fill(pad.view(B, 1, L0, 1), 0.0)\n",
    "        # cols TO PAD\n",
    "        geo_bias = geo_bias.masked_fill(pad.view(B, 1, 1, L0), big_neg)\n",
    "        adj_bias = adj_bias.masked_fill(pad.view(B, 1, 1, L0), 0.0)\n",
    "\n",
    "        # 5) return head-wise additive bias\n",
    "        return geo_bias + adj_bias                                     # (B, H, L0, L0)\n",
    "\n",
    "# ---------- the Graph Transformer ----------\n",
    "class GraphTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 8,\n",
    "        nlayers: int = 6,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        dropout: float = 0.1,\n",
    "        drop_path: float = 0.1,\n",
    "        atom_emb_dim: int = 256,          # kept equal to d_model\n",
    "        rdkit_dim: int = 15,              # from LMDB (6 or 15)\n",
    "        extra_atom_dim: int = 5,          # LMDB extra per-atom feats\n",
    "        activation: str = \"silu\",\n",
    "        use_extra_atom_feats: bool = True,\n",
    "        use_cls_token: bool = True,\n",
    "        use_has_xyz: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_extra_atom_feats = use_extra_atom_feats\n",
    "        self.use_cls_token = use_cls_token\n",
    "        self.use_has_xyz = use_has_xyz\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.atom_enc = AtomEncoder(emb_dim=atom_emb_dim)\n",
    "        if use_extra_atom_feats:\n",
    "            self.extra_proj = nn.Sequential(\n",
    "                nn.Linear(extra_atom_dim, d_model), _act(activation),\n",
    "                nn.Linear(d_model, d_model)\n",
    "            )\n",
    "            self.extra_gate = nn.Sequential(\n",
    "                nn.Linear(2*d_model, d_model), _act(activation)\n",
    "            )\n",
    "\n",
    "        # class token appended at end of each sequence\n",
    "        if use_cls_token:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, d_model))\n",
    "            nn.init.normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        # blocks with linearly increasing drop-path\n",
    "        dprs = [drop_path * i / max(1,(nlayers-1)) for i in range(nlayers)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TFBlock(d_model, nhead, mlp_ratio, activation, dropout, dprs[i]) for i in range(nlayers)\n",
    "        ])\n",
    "\n",
    "        self.bias_builder = AttnBiasBuilder(n_heads=nhead, rbf_k=32, dmin=0.0, dmax=10.0, beta=5.0, act=activation)\n",
    "\n",
    "        head_in = d_model + rdkit_dim + (1 if use_has_xyz else 0)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(head_in),\n",
    "            nn.Linear(head_in, 2*d_model), _act(activation), nn.Dropout(dropout),\n",
    "            nn.Linear(2*d_model, d_model//2), _act(activation), nn.Dropout(dropout),\n",
    "            nn.Linear(d_model//2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.atom_enc(data.x)\n",
    "        if self.use_extra_atom_feats and hasattr(data, \"extra_atom_feats\"):\n",
    "            xa = self.extra_proj(data.extra_atom_feats)\n",
    "            x  = self.extra_gate(torch.cat([x, xa], dim=1))\n",
    "\n",
    "        # Pack (no CLS yet)\n",
    "        x_pad, valid_mask = to_dense_batch(x, data.batch)  # (B,L0,D)\n",
    "        B, L0, D = x_pad.shape\n",
    "        key_padding_mask = ~valid_mask                     # (B,L0)  True=PAD\n",
    "\n",
    "        # Build bias on L0 tokens (no CLS)\n",
    "        attn_bias = self.bias_builder(\n",
    "            pos=data.pos if hasattr(data, \"pos\") else None,\n",
    "            edge_index=data.edge_index, batch=data.batch,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )  # (B,H,L0,L0)\n",
    "\n",
    "        # Append CLS and pad bias\n",
    "        if self.use_cls_token:\n",
    "            cls = self.cls_token.expand(B, 1, D)\n",
    "            x_pad = torch.cat([x_pad, cls], dim=1)                        # (B,L0+1,D)\n",
    "            key_padding_mask = torch.cat(\n",
    "                [key_padding_mask, torch.zeros(B,1, dtype=torch.bool, device=x_pad.device)],\n",
    "                dim=1\n",
    "            )                                                              # (B,L0+1)\n",
    "            # pad attn_bias by one row and one col (zeros) for CLS\n",
    "            attn_bias = F.pad(attn_bias, (0,1, 0,1))                       # (B,H,L0+1,L0+1)\n",
    "            L = L0 + 1\n",
    "        else:\n",
    "            L = L0\n",
    "\n",
    "        # Transformer\n",
    "        h = x_pad\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h, key_padding_mask[:, :L], attn_bias)\n",
    "\n",
    "        # Readout\n",
    "        if self.use_cls_token:\n",
    "            gvec = h[:, L-1, :]\n",
    "        else:\n",
    "            gvec = (h * valid_mask.unsqueeze(-1)).sum(1) / valid_mask.sum(1, keepdim=True)\n",
    "\n",
    "        rd = data.rdkit_feats.view(B, -1).float()\n",
    "        pieces = [gvec, rd]\n",
    "        if self.use_has_xyz and hasattr(data, \"has_xyz\"):\n",
    "            pieces.append(data.has_xyz.view(B,1).float())\n",
    "        out = torch.cat(pieces, dim=1)\n",
    "        return self.head(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e524006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loaders with mixed edges ON (so edge_attr keeps the 3 cat + 32 RBF),\n",
    "# and with extra_atom_feats so the model can use them.\n",
    "train_loader_tg,  val_loader_tg  = make_loaders_for_task(\"Tg\",      ids_tg,  batch_size=512,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "train_loader_den, val_loader_den = make_loaders_for_task(\"Density\", ids_den, batch_size=512,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "train_loader_rg,  val_loader_rg  = make_loaders_for_task(\"Rg\",      ids_rg,  batch_size=512,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bee54662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | tr_MSE 22793.58398 | val_MAE 100.59176 | val_RMSE 131.11130 | R2 -0.8007\n",
      "Epoch 002 | tr_MSE 22792.69141 | val_MAE 100.59176 | val_RMSE 131.11130 | R2 -0.8007\n",
      "Epoch 003 | tr_MSE 22792.95117 | val_MAE 100.59176 | val_RMSE 131.11130 | R2 -0.8007\n",
      "Epoch 004 | tr_MSE 22793.35938 | val_MAE 100.59176 | val_RMSE 131.11130 | R2 -0.8007\n",
      "Epoch 005 | tr_MSE 22791.58398 | val_MAE 100.59176 | val_RMSE 131.11130 | R2 -0.8007\n",
      "Epoch 006 | tr_MSE 22792.68750 | val_MAE 100.59176 | val_RMSE 131.11130 | R2 -0.8007\n",
      "Epoch 007 | tr_MSE 22793.06055 | val_MAE 100.59176 | val_RMSE 131.11130 | R2 -0.8007\n",
      "Epoch 008 | tr_MSE 22793.08398 | val_MAE 100.59176 | val_RMSE 131.11130 | R2 -0.8007\n",
      "Epoch 009 | tr_MSE 22793.26758 | val_MAE 100.41499 | val_RMSE 130.91141 | R2 -0.7953\n",
      "Epoch 010 | tr_MSE 22736.45117 | val_MAE 100.23349 | val_RMSE 130.70688 | R2 -0.7896\n",
      "Epoch 011 | tr_MSE 22677.57227 | val_MAE 100.05190 | val_RMSE 130.49948 | R2 -0.7840\n",
      "Epoch 012 | tr_MSE 22619.01953 | val_MAE 99.85517 | val_RMSE 130.27191 | R2 -0.7778\n",
      "Epoch 013 | tr_MSE 22554.13477 | val_MAE 99.63044 | val_RMSE 130.01273 | R2 -0.7707\n",
      "Epoch 014 | tr_MSE 22480.71094 | val_MAE 99.36794 | val_RMSE 129.71049 | R2 -0.7625\n",
      "Epoch 015 | tr_MSE 22398.67773 | val_MAE 99.06859 | val_RMSE 129.35384 | R2 -0.7528\n",
      "Epoch 016 | tr_MSE 22294.16211 | val_MAE 99.06859 | val_RMSE 129.35384 | R2 -0.7528\n",
      "Epoch 017 | tr_MSE 22296.00586 | val_MAE 98.71496 | val_RMSE 128.93422 | R2 -0.7414\n",
      "Epoch 018 | tr_MSE 22180.32031 | val_MAE 98.29794 | val_RMSE 128.44223 | R2 -0.7282\n",
      "Epoch 019 | tr_MSE 22033.69727 | val_MAE 97.81165 | val_RMSE 127.87263 | R2 -0.7129\n",
      "Epoch 020 | tr_MSE 21877.30469 | val_MAE 97.25332 | val_RMSE 127.22405 | R2 -0.6955\n",
      "Epoch 021 | tr_MSE 21688.40820 | val_MAE 97.25332 | val_RMSE 127.22405 | R2 -0.6955\n",
      "Epoch 022 | tr_MSE 21691.13477 | val_MAE 96.62647 | val_RMSE 126.50253 | R2 -0.6764\n",
      "Epoch 023 | tr_MSE 21479.48047 | val_MAE 95.95477 | val_RMSE 125.70831 | R2 -0.6554\n",
      "Epoch 024 | tr_MSE 21262.87305 | val_MAE 95.21817 | val_RMSE 124.84586 | R2 -0.6327\n",
      "Epoch 025 | tr_MSE 21011.40820 | val_MAE 94.46030 | val_RMSE 123.91916 | R2 -0.6086\n",
      "Epoch 026 | tr_MSE 20792.13281 | val_MAE 93.66547 | val_RMSE 122.93169 | R2 -0.5831\n",
      "Epoch 027 | tr_MSE 20471.87305 | val_MAE 93.66547 | val_RMSE 122.93169 | R2 -0.5831\n",
      "Epoch 028 | tr_MSE 20478.06250 | val_MAE 92.85808 | val_RMSE 121.89523 | R2 -0.5565\n",
      "Epoch 029 | tr_MSE 20154.07812 | val_MAE 92.01606 | val_RMSE 120.80565 | R2 -0.5288\n",
      "Epoch 030 | tr_MSE 19870.44336 | val_MAE 91.14574 | val_RMSE 119.66680 | R2 -0.5001\n",
      "Epoch 031 | tr_MSE 19530.94922 | val_MAE 90.26087 | val_RMSE 118.48280 | R2 -0.4706\n",
      "Epoch 032 | tr_MSE 19241.42773 | val_MAE 89.38516 | val_RMSE 117.25912 | R2 -0.4403\n",
      "Epoch 033 | tr_MSE 18897.42773 | val_MAE 88.48793 | val_RMSE 116.00227 | R2 -0.4096\n",
      "Epoch 034 | tr_MSE 18545.74414 | val_MAE 87.63712 | val_RMSE 114.71918 | R2 -0.3786\n",
      "Epoch 035 | tr_MSE 18163.02148 | val_MAE 86.85622 | val_RMSE 113.41810 | R2 -0.3475\n",
      "Epoch 036 | tr_MSE 17810.26562 | val_MAE 86.06375 | val_RMSE 112.10804 | R2 -0.3166\n",
      "Epoch 037 | tr_MSE 17511.27734 | val_MAE 85.24156 | val_RMSE 110.79824 | R2 -0.2860\n",
      "Epoch 038 | tr_MSE 17205.53906 | val_MAE 84.43288 | val_RMSE 109.49883 | R2 -0.2560\n",
      "Epoch 039 | tr_MSE 16851.68750 | val_MAE 84.43288 | val_RMSE 109.49883 | R2 -0.2560\n",
      "Epoch 040 | tr_MSE 16746.99609 | val_MAE 83.69238 | val_RMSE 108.23671 | R2 -0.2272\n",
      "Epoch 041 | tr_MSE 16513.71875 | val_MAE 83.00976 | val_RMSE 107.00541 | R2 -0.1994\n",
      "Epoch 042 | tr_MSE 16161.13672 | val_MAE 82.37608 | val_RMSE 105.81460 | R2 -0.1729\n",
      "Epoch 043 | tr_MSE 15886.10547 | val_MAE 81.76184 | val_RMSE 104.67362 | R2 -0.1477\n",
      "Epoch 044 | tr_MSE 15492.80566 | val_MAE 81.22473 | val_RMSE 103.59206 | R2 -0.1241\n",
      "Epoch 045 | tr_MSE 15392.91504 | val_MAE 80.71941 | val_RMSE 102.57962 | R2 -0.1023\n",
      "Epoch 046 | tr_MSE 14963.92676 | val_MAE 80.24644 | val_RMSE 101.64490 | R2 -0.0823\n",
      "Epoch 047 | tr_MSE 14678.28027 | val_MAE 79.76739 | val_RMSE 100.79617 | R2 -0.0643\n",
      "Epoch 048 | tr_MSE 14513.22852 | val_MAE 79.33456 | val_RMSE 100.04097 | R2 -0.0484\n",
      "Epoch 049 | tr_MSE 14163.24316 | val_MAE 79.01675 | val_RMSE 99.38602 | R2 -0.0347\n",
      "Epoch 050 | tr_MSE 14043.98047 | val_MAE 78.72131 | val_RMSE 98.83743 | R2 -0.0233\n",
      "Epoch 051 | tr_MSE 13917.08594 | val_MAE 78.54030 | val_RMSE 98.40045 | R2 -0.0143\n",
      "Epoch 052 | tr_MSE 13692.61328 | val_MAE 78.63909 | val_RMSE 98.07862 | R2 -0.0077\n",
      "Epoch 053 | tr_MSE 13551.55957 | val_MAE 78.84586 | val_RMSE 97.87463 | R2 -0.0035\n",
      "Epoch 054 | tr_MSE 13343.07812 | val_MAE 79.12658 | val_RMSE 97.78976 | R2 -0.0017\n",
      "Epoch 055 | tr_MSE 13236.86035 | val_MAE 79.60973 | val_RMSE 97.82356 | R2 -0.0024\n",
      "Epoch 056 | tr_MSE 13138.38281 | val_MAE 80.19465 | val_RMSE 97.97416 | R2 -0.0055\n",
      "Epoch 057 | tr_MSE 13162.04199 | val_MAE 80.83411 | val_RMSE 98.23660 | R2 -0.0109\n",
      "Epoch 058 | tr_MSE 13096.19922 | val_MAE 81.49133 | val_RMSE 98.59669 | R2 -0.0183\n",
      "Epoch 059 | tr_MSE 13056.94434 | val_MAE 82.07473 | val_RMSE 98.99224 | R2 -0.0265\n",
      "Epoch 060 | tr_MSE 13143.55762 | val_MAE 82.56753 | val_RMSE 99.36006 | R2 -0.0342\n",
      "Epoch 061 | tr_MSE 13065.23438 | val_MAE 82.99242 | val_RMSE 99.66802 | R2 -0.0406\n",
      "Epoch 062 | tr_MSE 13227.92285 | val_MAE 83.29190 | val_RMSE 99.89548 | R2 -0.0454\n",
      "Epoch 063 | tr_MSE 13300.40820 | val_MAE 83.46812 | val_RMSE 100.03519 | R2 -0.0483\n",
      "Epoch 064 | tr_MSE 13068.58105 | val_MAE 83.53548 | val_RMSE 100.08944 | R2 -0.0494\n",
      "Epoch 065 | tr_MSE 13130.20117 | val_MAE 83.50772 | val_RMSE 100.06605 | R2 -0.0489\n",
      "Epoch 066 | tr_MSE 13227.67090 | val_MAE 83.39731 | val_RMSE 99.97583 | R2 -0.0470\n",
      "Epoch 067 | tr_MSE 13166.73340 | val_MAE 83.21619 | val_RMSE 99.83163 | R2 -0.0440\n",
      "Epoch 068 | tr_MSE 12985.94922 | val_MAE 82.97431 | val_RMSE 99.64595 | R2 -0.0401\n",
      "Epoch 069 | tr_MSE 13096.16699 | val_MAE 82.67982 | val_RMSE 99.43056 | R2 -0.0356\n",
      "Epoch 070 | tr_MSE 13170.94043 | val_MAE 82.37238 | val_RMSE 99.19955 | R2 -0.0308\n",
      "Epoch 071 | tr_MSE 12943.60352 | val_MAE 82.05852 | val_RMSE 98.96470 | R2 -0.0260\n",
      "Early stopping.\n",
      "[graphtransformer_tg] Best Val — MAE 78.540298 | RMSE 98.400452 | R2 -0.0143\n",
      "Epoch 001 | tr_MSE 1.01801 | val_MAE 0.99521 | val_RMSE 1.00380 | R2 -56.3804\n",
      "Epoch 002 | tr_MSE 1.01668 | val_MAE 0.89436 | val_RMSE 0.90404 | R2 -45.5419\n",
      "Epoch 003 | tr_MSE 0.83029 | val_MAE 0.76570 | val_RMSE 0.77753 | R2 -33.4276\n",
      "Epoch 004 | tr_MSE 0.61986 | val_MAE 0.59520 | val_RMSE 0.61157 | R2 -20.2994\n",
      "Epoch 005 | tr_MSE 0.38381 | val_MAE 0.37279 | val_RMSE 0.40233 | R2 -8.2178\n",
      "Epoch 006 | tr_MSE 0.17627 | val_MAE 0.15550 | val_RMSE 0.19053 | R2 -1.0672\n",
      "Epoch 007 | tr_MSE 0.05086 | val_MAE 0.28298 | val_RMSE 0.33326 | R2 -5.3247\n",
      "Epoch 008 | tr_MSE 0.12873 | val_MAE 0.42545 | val_RMSE 0.45798 | R2 -10.9445\n",
      "Epoch 009 | tr_MSE 0.21144 | val_MAE 0.40792 | val_RMSE 0.42996 | R2 -9.5273\n",
      "Epoch 010 | tr_MSE 0.19145 | val_MAE 0.28903 | val_RMSE 0.31007 | R2 -4.4750\n",
      "Epoch 011 | tr_MSE 0.10774 | val_MAE 0.16099 | val_RMSE 0.19055 | R2 -1.0676\n",
      "Epoch 012 | tr_MSE 0.04712 | val_MAE 0.17421 | val_RMSE 0.22551 | R2 -1.8961\n",
      "Epoch 013 | tr_MSE 0.05874 | val_MAE 0.16938 | val_RMSE 0.22094 | R2 -1.7798\n",
      "Epoch 014 | tr_MSE 0.05438 | val_MAE 0.16659 | val_RMSE 0.19589 | R2 -1.1852\n",
      "Epoch 015 | tr_MSE 0.04600 | val_MAE 0.11902 | val_RMSE 0.15102 | R2 -0.2987\n",
      "Epoch 016 | tr_MSE 0.03252 | val_MAE 0.11610 | val_RMSE 0.15047 | R2 -0.2894\n",
      "Epoch 017 | tr_MSE 0.02923 | val_MAE 0.10808 | val_RMSE 0.13842 | R2 -0.0911\n",
      "Epoch 018 | tr_MSE 0.02645 | val_MAE 0.10502 | val_RMSE 0.12920 | R2 0.0494\n",
      "Epoch 019 | tr_MSE 0.02514 | val_MAE 0.10491 | val_RMSE 0.12772 | R2 0.0710\n",
      "Epoch 020 | tr_MSE 0.02520 | val_MAE 0.10358 | val_RMSE 0.12665 | R2 0.0865\n",
      "Epoch 021 | tr_MSE 0.02400 | val_MAE 0.09961 | val_RMSE 0.12421 | R2 0.1214\n",
      "Epoch 022 | tr_MSE 0.02399 | val_MAE 0.09649 | val_RMSE 0.12348 | R2 0.1317\n",
      "Epoch 023 | tr_MSE 0.02232 | val_MAE 0.09462 | val_RMSE 0.12447 | R2 0.1177\n",
      "Epoch 024 | tr_MSE 0.02353 | val_MAE 0.09351 | val_RMSE 0.12475 | R2 0.1138\n",
      "Epoch 025 | tr_MSE 0.02372 | val_MAE 0.09312 | val_RMSE 0.12340 | R2 0.1328\n",
      "Epoch 026 | tr_MSE 0.02355 | val_MAE 0.09373 | val_RMSE 0.12150 | R2 0.1594\n",
      "Epoch 027 | tr_MSE 0.02320 | val_MAE 0.09560 | val_RMSE 0.12134 | R2 0.1615\n",
      "Epoch 028 | tr_MSE 0.02240 | val_MAE 0.09704 | val_RMSE 0.12187 | R2 0.1542\n",
      "Epoch 029 | tr_MSE 0.02414 | val_MAE 0.09517 | val_RMSE 0.12019 | R2 0.1774\n",
      "Epoch 030 | tr_MSE 0.02144 | val_MAE 0.09181 | val_RMSE 0.11810 | R2 0.2057\n",
      "Epoch 031 | tr_MSE 0.02188 | val_MAE 0.08874 | val_RMSE 0.11753 | R2 0.2134\n",
      "Epoch 032 | tr_MSE 0.02212 | val_MAE 0.08727 | val_RMSE 0.11722 | R2 0.2176\n",
      "Epoch 033 | tr_MSE 0.02304 | val_MAE 0.08705 | val_RMSE 0.11541 | R2 0.2415\n",
      "Epoch 034 | tr_MSE 0.02022 | val_MAE 0.08835 | val_RMSE 0.11383 | R2 0.2621\n",
      "Epoch 035 | tr_MSE 0.02177 | val_MAE 0.09009 | val_RMSE 0.11377 | R2 0.2628\n",
      "Epoch 036 | tr_MSE 0.01997 | val_MAE 0.08979 | val_RMSE 0.11322 | R2 0.2700\n",
      "Epoch 037 | tr_MSE 0.02074 | val_MAE 0.08734 | val_RMSE 0.11163 | R2 0.2904\n",
      "Epoch 038 | tr_MSE 0.02015 | val_MAE 0.08489 | val_RMSE 0.11065 | R2 0.3028\n",
      "Epoch 039 | tr_MSE 0.01981 | val_MAE 0.08375 | val_RMSE 0.11032 | R2 0.3069\n",
      "Epoch 040 | tr_MSE 0.01862 | val_MAE 0.08329 | val_RMSE 0.10943 | R2 0.3181\n",
      "Epoch 041 | tr_MSE 0.01948 | val_MAE 0.08462 | val_RMSE 0.10879 | R2 0.3260\n",
      "Epoch 042 | tr_MSE 0.01904 | val_MAE 0.08709 | val_RMSE 0.10972 | R2 0.3144\n",
      "Epoch 043 | tr_MSE 0.01985 | val_MAE 0.08596 | val_RMSE 0.10871 | R2 0.3270\n",
      "Epoch 044 | tr_MSE 0.01872 | val_MAE 0.08171 | val_RMSE 0.10635 | R2 0.3559\n",
      "Epoch 045 | tr_MSE 0.01872 | val_MAE 0.07814 | val_RMSE 0.10622 | R2 0.3574\n",
      "Epoch 046 | tr_MSE 0.01714 | val_MAE 0.07688 | val_RMSE 0.10641 | R2 0.3552\n",
      "Epoch 047 | tr_MSE 0.01760 | val_MAE 0.07723 | val_RMSE 0.10436 | R2 0.3798\n",
      "Epoch 048 | tr_MSE 0.01749 | val_MAE 0.08182 | val_RMSE 0.10509 | R2 0.3711\n",
      "Epoch 049 | tr_MSE 0.01693 | val_MAE 0.08434 | val_RMSE 0.10703 | R2 0.3476\n",
      "Epoch 050 | tr_MSE 0.01834 | val_MAE 0.08170 | val_RMSE 0.10490 | R2 0.3733\n",
      "Epoch 051 | tr_MSE 0.01600 | val_MAE 0.07577 | val_RMSE 0.10212 | R2 0.4061\n",
      "Epoch 052 | tr_MSE 0.01615 | val_MAE 0.07299 | val_RMSE 0.10310 | R2 0.3947\n",
      "Epoch 053 | tr_MSE 0.01684 | val_MAE 0.07247 | val_RMSE 0.10265 | R2 0.4000\n",
      "Epoch 054 | tr_MSE 0.01708 | val_MAE 0.07407 | val_RMSE 0.10068 | R2 0.4228\n",
      "Epoch 055 | tr_MSE 0.01585 | val_MAE 0.07877 | val_RMSE 0.10267 | R2 0.3998\n",
      "Epoch 056 | tr_MSE 0.01727 | val_MAE 0.08137 | val_RMSE 0.10479 | R2 0.3747\n",
      "Epoch 057 | tr_MSE 0.01607 | val_MAE 0.07740 | val_RMSE 0.10155 | R2 0.4127\n",
      "Epoch 058 | tr_MSE 0.01579 | val_MAE 0.07220 | val_RMSE 0.09851 | R2 0.4473\n",
      "Epoch 059 | tr_MSE 0.01528 | val_MAE 0.06933 | val_RMSE 0.09831 | R2 0.4497\n",
      "Epoch 060 | tr_MSE 0.01453 | val_MAE 0.06873 | val_RMSE 0.09796 | R2 0.4536\n",
      "Epoch 061 | tr_MSE 0.01541 | val_MAE 0.06967 | val_RMSE 0.09692 | R2 0.4650\n",
      "Epoch 062 | tr_MSE 0.01352 | val_MAE 0.07208 | val_RMSE 0.09776 | R2 0.4558\n",
      "Epoch 063 | tr_MSE 0.01351 | val_MAE 0.07269 | val_RMSE 0.09815 | R2 0.4514\n",
      "Epoch 064 | tr_MSE 0.01471 | val_MAE 0.07005 | val_RMSE 0.09630 | R2 0.4718\n",
      "Epoch 065 | tr_MSE 0.01284 | val_MAE 0.06579 | val_RMSE 0.09456 | R2 0.4908\n",
      "Epoch 066 | tr_MSE 0.01320 | val_MAE 0.06434 | val_RMSE 0.09427 | R2 0.4939\n",
      "Epoch 067 | tr_MSE 0.01261 | val_MAE 0.06479 | val_RMSE 0.09329 | R2 0.5044\n",
      "Epoch 068 | tr_MSE 0.01500 | val_MAE 0.06813 | val_RMSE 0.09471 | R2 0.4892\n",
      "Epoch 069 | tr_MSE 0.01416 | val_MAE 0.06822 | val_RMSE 0.09471 | R2 0.4892\n",
      "Epoch 070 | tr_MSE 0.01322 | val_MAE 0.06243 | val_RMSE 0.09135 | R2 0.5248\n",
      "Epoch 071 | tr_MSE 0.01191 | val_MAE 0.05980 | val_RMSE 0.09136 | R2 0.5247\n",
      "Epoch 072 | tr_MSE 0.01365 | val_MAE 0.05868 | val_RMSE 0.09081 | R2 0.5304\n",
      "Epoch 073 | tr_MSE 0.01450 | val_MAE 0.05996 | val_RMSE 0.08950 | R2 0.5438\n",
      "Epoch 074 | tr_MSE 0.01254 | val_MAE 0.06503 | val_RMSE 0.09279 | R2 0.5096\n",
      "Epoch 075 | tr_MSE 0.01397 | val_MAE 0.06078 | val_RMSE 0.09002 | R2 0.5385\n",
      "Epoch 076 | tr_MSE 0.01299 | val_MAE 0.05579 | val_RMSE 0.08840 | R2 0.5550\n",
      "Epoch 077 | tr_MSE 0.01172 | val_MAE 0.05723 | val_RMSE 0.08889 | R2 0.5500\n",
      "Epoch 078 | tr_MSE 0.01085 | val_MAE 0.05783 | val_RMSE 0.08821 | R2 0.5569\n",
      "Epoch 079 | tr_MSE 0.01309 | val_MAE 0.06389 | val_RMSE 0.09334 | R2 0.5039\n",
      "Epoch 080 | tr_MSE 0.01139 | val_MAE 0.06321 | val_RMSE 0.09252 | R2 0.5126\n",
      "Epoch 081 | tr_MSE 0.01133 | val_MAE 0.05413 | val_RMSE 0.08650 | R2 0.5739\n",
      "Epoch 082 | tr_MSE 0.01105 | val_MAE 0.05503 | val_RMSE 0.08760 | R2 0.5630\n",
      "Epoch 083 | tr_MSE 0.01248 | val_MAE 0.05288 | val_RMSE 0.08536 | R2 0.5850\n",
      "Epoch 084 | tr_MSE 0.01201 | val_MAE 0.05628 | val_RMSE 0.08745 | R2 0.5645\n",
      "Epoch 085 | tr_MSE 0.01069 | val_MAE 0.05916 | val_RMSE 0.09013 | R2 0.5374\n",
      "Epoch 086 | tr_MSE 0.01046 | val_MAE 0.05668 | val_RMSE 0.08762 | R2 0.5628\n",
      "Epoch 087 | tr_MSE 0.01105 | val_MAE 0.05437 | val_RMSE 0.08445 | R2 0.5939\n",
      "Epoch 088 | tr_MSE 0.00978 | val_MAE 0.05223 | val_RMSE 0.08260 | R2 0.6115\n",
      "Epoch 089 | tr_MSE 0.00969 | val_MAE 0.05115 | val_RMSE 0.08335 | R2 0.6044\n",
      "Epoch 090 | tr_MSE 0.00968 | val_MAE 0.05114 | val_RMSE 0.08385 | R2 0.5996\n",
      "Epoch 091 | tr_MSE 0.01004 | val_MAE 0.05314 | val_RMSE 0.08433 | R2 0.5950\n",
      "Epoch 092 | tr_MSE 0.00963 | val_MAE 0.05359 | val_RMSE 0.08528 | R2 0.5858\n",
      "Epoch 093 | tr_MSE 0.00793 | val_MAE 0.05504 | val_RMSE 0.08741 | R2 0.5649\n",
      "Epoch 094 | tr_MSE 0.00966 | val_MAE 0.05741 | val_RMSE 0.09085 | R2 0.5299\n",
      "Epoch 095 | tr_MSE 0.00935 | val_MAE 0.05712 | val_RMSE 0.09001 | R2 0.5386\n",
      "Epoch 096 | tr_MSE 0.00921 | val_MAE 0.05598 | val_RMSE 0.08742 | R2 0.5648\n",
      "Epoch 097 | tr_MSE 0.00918 | val_MAE 0.05612 | val_RMSE 0.08803 | R2 0.5587\n",
      "Epoch 098 | tr_MSE 0.00877 | val_MAE 0.05557 | val_RMSE 0.08718 | R2 0.5672\n",
      "Epoch 099 | tr_MSE 0.00841 | val_MAE 0.05541 | val_RMSE 0.08698 | R2 0.5691\n",
      "Epoch 100 | tr_MSE 0.00853 | val_MAE 0.05439 | val_RMSE 0.08542 | R2 0.5844\n",
      "Epoch 101 | tr_MSE 0.00864 | val_MAE 0.05282 | val_RMSE 0.08467 | R2 0.5918\n",
      "Epoch 102 | tr_MSE 0.00917 | val_MAE 0.05246 | val_RMSE 0.08565 | R2 0.5823\n",
      "Epoch 103 | tr_MSE 0.00794 | val_MAE 0.05276 | val_RMSE 0.08326 | R2 0.6052\n",
      "Epoch 104 | tr_MSE 0.00864 | val_MAE 0.05131 | val_RMSE 0.08258 | R2 0.6116\n",
      "Epoch 105 | tr_MSE 0.00868 | val_MAE 0.05543 | val_RMSE 0.09037 | R2 0.5349\n",
      "Epoch 106 | tr_MSE 0.01003 | val_MAE 0.05408 | val_RMSE 0.08694 | R2 0.5696\n",
      "Epoch 107 | tr_MSE 0.00757 | val_MAE 0.05441 | val_RMSE 0.08447 | R2 0.5936\n",
      "Epoch 108 | tr_MSE 0.00701 | val_MAE 0.05507 | val_RMSE 0.08479 | R2 0.5906\n",
      "Epoch 109 | tr_MSE 0.00737 | val_MAE 0.05571 | val_RMSE 0.08782 | R2 0.5608\n",
      "Epoch 110 | tr_MSE 0.00812 | val_MAE 0.05752 | val_RMSE 0.09145 | R2 0.5237\n",
      "Epoch 111 | tr_MSE 0.00717 | val_MAE 0.05663 | val_RMSE 0.09023 | R2 0.5364\n",
      "Epoch 112 | tr_MSE 0.00830 | val_MAE 0.05356 | val_RMSE 0.08478 | R2 0.5907\n",
      "Epoch 113 | tr_MSE 0.00781 | val_MAE 0.05333 | val_RMSE 0.08558 | R2 0.5829\n",
      "Epoch 114 | tr_MSE 0.00855 | val_MAE 0.05219 | val_RMSE 0.08294 | R2 0.6083\n",
      "Epoch 115 | tr_MSE 0.00779 | val_MAE 0.05124 | val_RMSE 0.08318 | R2 0.6060\n",
      "Epoch 116 | tr_MSE 0.00764 | val_MAE 0.05658 | val_RMSE 0.09199 | R2 0.5181\n",
      "Epoch 117 | tr_MSE 0.00860 | val_MAE 0.05095 | val_RMSE 0.08256 | R2 0.6118\n",
      "Epoch 118 | tr_MSE 0.00779 | val_MAE 0.05182 | val_RMSE 0.08299 | R2 0.6078\n",
      "Epoch 119 | tr_MSE 0.00822 | val_MAE 0.05198 | val_RMSE 0.08581 | R2 0.5807\n",
      "Epoch 120 | tr_MSE 0.00727 | val_MAE 0.05474 | val_RMSE 0.08938 | R2 0.5451\n",
      "Epoch 121 | tr_MSE 0.00772 | val_MAE 0.05360 | val_RMSE 0.08721 | R2 0.5669\n",
      "Epoch 122 | tr_MSE 0.00759 | val_MAE 0.05204 | val_RMSE 0.08503 | R2 0.5883\n",
      "Epoch 123 | tr_MSE 0.00779 | val_MAE 0.05202 | val_RMSE 0.08487 | R2 0.5898\n",
      "Epoch 124 | tr_MSE 0.00755 | val_MAE 0.05167 | val_RMSE 0.08458 | R2 0.5926\n",
      "Epoch 125 | tr_MSE 0.00707 | val_MAE 0.05183 | val_RMSE 0.08493 | R2 0.5893\n",
      "Epoch 126 | tr_MSE 0.00749 | val_MAE 0.05096 | val_RMSE 0.08404 | R2 0.5978\n",
      "Epoch 127 | tr_MSE 0.00676 | val_MAE 0.05067 | val_RMSE 0.08401 | R2 0.5981\n",
      "Epoch 128 | tr_MSE 0.00745 | val_MAE 0.04955 | val_RMSE 0.08288 | R2 0.6088\n",
      "Epoch 129 | tr_MSE 0.00704 | val_MAE 0.04894 | val_RMSE 0.08205 | R2 0.6166\n",
      "Epoch 130 | tr_MSE 0.00740 | val_MAE 0.04957 | val_RMSE 0.08225 | R2 0.6148\n",
      "Epoch 131 | tr_MSE 0.00694 | val_MAE 0.05079 | val_RMSE 0.08389 | R2 0.5992\n",
      "Epoch 132 | tr_MSE 0.00706 | val_MAE 0.05076 | val_RMSE 0.08457 | R2 0.5928\n",
      "Epoch 133 | tr_MSE 0.00622 | val_MAE 0.04851 | val_RMSE 0.08100 | R2 0.6264\n",
      "Epoch 134 | tr_MSE 0.00670 | val_MAE 0.04885 | val_RMSE 0.08025 | R2 0.6332\n",
      "Epoch 135 | tr_MSE 0.00716 | val_MAE 0.04877 | val_RMSE 0.08152 | R2 0.6216\n",
      "Epoch 136 | tr_MSE 0.00719 | val_MAE 0.05402 | val_RMSE 0.08927 | R2 0.5461\n",
      "Epoch 137 | tr_MSE 0.00795 | val_MAE 0.05133 | val_RMSE 0.08517 | R2 0.5869\n",
      "Epoch 138 | tr_MSE 0.00738 | val_MAE 0.05011 | val_RMSE 0.08097 | R2 0.6266\n",
      "Epoch 139 | tr_MSE 0.00778 | val_MAE 0.05030 | val_RMSE 0.08134 | R2 0.6232\n",
      "Epoch 140 | tr_MSE 0.00745 | val_MAE 0.05207 | val_RMSE 0.08620 | R2 0.5768\n",
      "Epoch 141 | tr_MSE 0.00691 | val_MAE 0.05314 | val_RMSE 0.08782 | R2 0.5608\n",
      "Epoch 142 | tr_MSE 0.00737 | val_MAE 0.04864 | val_RMSE 0.08142 | R2 0.6225\n",
      "Epoch 143 | tr_MSE 0.00786 | val_MAE 0.04744 | val_RMSE 0.07969 | R2 0.6384\n",
      "Epoch 144 | tr_MSE 0.00728 | val_MAE 0.04687 | val_RMSE 0.07961 | R2 0.6391\n",
      "Epoch 145 | tr_MSE 0.00750 | val_MAE 0.04708 | val_RMSE 0.08083 | R2 0.6279\n",
      "Epoch 146 | tr_MSE 0.00764 | val_MAE 0.04862 | val_RMSE 0.08367 | R2 0.6013\n",
      "Epoch 147 | tr_MSE 0.00735 | val_MAE 0.04779 | val_RMSE 0.08265 | R2 0.6110\n",
      "Epoch 148 | tr_MSE 0.00712 | val_MAE 0.04618 | val_RMSE 0.08010 | R2 0.6346\n",
      "Epoch 149 | tr_MSE 0.00713 | val_MAE 0.04725 | val_RMSE 0.08041 | R2 0.6318\n",
      "Epoch 150 | tr_MSE 0.00793 | val_MAE 0.04826 | val_RMSE 0.08126 | R2 0.6240\n",
      "Epoch 151 | tr_MSE 0.00711 | val_MAE 0.04936 | val_RMSE 0.08311 | R2 0.6067\n",
      "Epoch 152 | tr_MSE 0.00620 | val_MAE 0.05189 | val_RMSE 0.08665 | R2 0.5724\n",
      "Epoch 153 | tr_MSE 0.00588 | val_MAE 0.05212 | val_RMSE 0.08675 | R2 0.5715\n",
      "Epoch 154 | tr_MSE 0.00682 | val_MAE 0.04969 | val_RMSE 0.08332 | R2 0.6046\n",
      "Epoch 155 | tr_MSE 0.00607 | val_MAE 0.04786 | val_RMSE 0.08114 | R2 0.6251\n",
      "Epoch 156 | tr_MSE 0.00650 | val_MAE 0.04712 | val_RMSE 0.08009 | R2 0.6347\n",
      "Epoch 157 | tr_MSE 0.00716 | val_MAE 0.04716 | val_RMSE 0.07994 | R2 0.6361\n",
      "Epoch 158 | tr_MSE 0.00706 | val_MAE 0.04753 | val_RMSE 0.08054 | R2 0.6306\n",
      "Epoch 159 | tr_MSE 0.00713 | val_MAE 0.04838 | val_RMSE 0.08155 | R2 0.6213\n",
      "Epoch 160 | tr_MSE 0.00672 | val_MAE 0.04955 | val_RMSE 0.08305 | R2 0.6072\n",
      "Epoch 161 | tr_MSE 0.00705 | val_MAE 0.05029 | val_RMSE 0.08394 | R2 0.5988\n",
      "Epoch 162 | tr_MSE 0.00755 | val_MAE 0.04977 | val_RMSE 0.08326 | R2 0.6053\n",
      "Epoch 163 | tr_MSE 0.00689 | val_MAE 0.04877 | val_RMSE 0.08199 | R2 0.6172\n",
      "Epoch 164 | tr_MSE 0.00654 | val_MAE 0.04867 | val_RMSE 0.08173 | R2 0.6196\n",
      "Epoch 165 | tr_MSE 0.00749 | val_MAE 0.04891 | val_RMSE 0.08218 | R2 0.6154\n",
      "Epoch 166 | tr_MSE 0.00760 | val_MAE 0.04937 | val_RMSE 0.08291 | R2 0.6086\n",
      "Epoch 167 | tr_MSE 0.00647 | val_MAE 0.04917 | val_RMSE 0.08267 | R2 0.6108\n",
      "Epoch 168 | tr_MSE 0.00602 | val_MAE 0.04849 | val_RMSE 0.08198 | R2 0.6173\n",
      "Epoch 169 | tr_MSE 0.00706 | val_MAE 0.04793 | val_RMSE 0.08134 | R2 0.6232\n",
      "Epoch 170 | tr_MSE 0.00624 | val_MAE 0.04759 | val_RMSE 0.08112 | R2 0.6253\n",
      "Epoch 171 | tr_MSE 0.00584 | val_MAE 0.04769 | val_RMSE 0.08142 | R2 0.6224\n",
      "Epoch 172 | tr_MSE 0.00642 | val_MAE 0.04805 | val_RMSE 0.08191 | R2 0.6179\n",
      "Epoch 173 | tr_MSE 0.00663 | val_MAE 0.04790 | val_RMSE 0.08189 | R2 0.6181\n",
      "Epoch 174 | tr_MSE 0.00731 | val_MAE 0.04739 | val_RMSE 0.08149 | R2 0.6218\n",
      "Epoch 175 | tr_MSE 0.00663 | val_MAE 0.04685 | val_RMSE 0.08096 | R2 0.6267\n",
      "Epoch 176 | tr_MSE 0.00660 | val_MAE 0.04639 | val_RMSE 0.08043 | R2 0.6316\n",
      "Epoch 177 | tr_MSE 0.00675 | val_MAE 0.04623 | val_RMSE 0.08015 | R2 0.6342\n",
      "Epoch 178 | tr_MSE 0.00772 | val_MAE 0.04637 | val_RMSE 0.08027 | R2 0.6331\n",
      "Early stopping.\n",
      "[graphtransformer_density] Best Val — MAE 0.046177 | RMSE 0.080099 | R2 0.6346\n",
      "Epoch 001 | tr_MSE 15.75148 | val_MAE 16.61267 | val_RMSE 17.26240 | R2 -12.5397\n",
      "Epoch 002 | tr_MSE 15.75371 | val_MAE 16.45199 | val_RMSE 17.10876 | R2 -12.2997\n",
      "Epoch 003 | tr_MSE 15.59808 | val_MAE 16.24255 | val_RMSE 16.90845 | R2 -11.9901\n",
      "Epoch 004 | tr_MSE 15.39544 | val_MAE 15.95934 | val_RMSE 16.63828 | R2 -11.5783\n",
      "Epoch 005 | tr_MSE 15.12194 | val_MAE 15.58032 | val_RMSE 16.27849 | R2 -11.0402\n",
      "Epoch 006 | tr_MSE 14.75233 | val_MAE 15.06893 | val_RMSE 15.79637 | R2 -10.3376\n",
      "Epoch 007 | tr_MSE 14.24809 | val_MAE 14.47358 | val_RMSE 15.23946 | R2 -9.5522\n",
      "Epoch 008 | tr_MSE 13.66995 | val_MAE 13.77132 | val_RMSE 14.58779 | R2 -8.6691\n",
      "Epoch 009 | tr_MSE 13.01509 | val_MAE 12.94315 | val_RMSE 13.82548 | R2 -7.6849\n",
      "Epoch 010 | tr_MSE 12.18049 | val_MAE 11.97500 | val_RMSE 12.94218 | R2 -6.6106\n",
      "Epoch 011 | tr_MSE 11.25623 | val_MAE 11.97500 | val_RMSE 12.94218 | R2 -6.6106\n",
      "Epoch 012 | tr_MSE 11.23891 | val_MAE 10.85793 | val_RMSE 11.93443 | R2 -5.4715\n",
      "Epoch 013 | tr_MSE 10.14519 | val_MAE 9.58584 | val_RMSE 10.80596 | R2 -4.3056\n",
      "Epoch 014 | tr_MSE 8.86892 | val_MAE 8.15466 | val_RMSE 9.57112 | R2 -3.1623\n",
      "Epoch 015 | tr_MSE 7.46290 | val_MAE 8.15466 | val_RMSE 9.57112 | R2 -3.1623\n",
      "Epoch 016 | tr_MSE 7.40136 | val_MAE 6.57609 | val_RMSE 8.26433 | R2 -2.1033\n",
      "Epoch 017 | tr_MSE 5.98862 | val_MAE 5.12391 | val_RMSE 6.95326 | R2 -1.1968\n",
      "Epoch 018 | tr_MSE 4.65209 | val_MAE 4.29475 | val_RMSE 5.78476 | R2 -0.5205\n",
      "Epoch 019 | tr_MSE 3.86617 | val_MAE 4.23073 | val_RMSE 5.05682 | R2 -0.1619\n",
      "Epoch 020 | tr_MSE 3.72589 | val_MAE 4.40814 | val_RMSE 5.00803 | R2 -0.1396\n",
      "Epoch 021 | tr_MSE 4.05836 | val_MAE 4.58788 | val_RMSE 5.24840 | R2 -0.2516\n",
      "Epoch 022 | tr_MSE 4.35917 | val_MAE 4.69711 | val_RMSE 5.47790 | R2 -0.3634\n",
      "Epoch 023 | tr_MSE 4.36127 | val_MAE 4.74097 | val_RMSE 5.56308 | R2 -0.4062\n",
      "Epoch 024 | tr_MSE 4.42415 | val_MAE 4.66908 | val_RMSE 5.47712 | R2 -0.3630\n",
      "Epoch 025 | tr_MSE 4.47060 | val_MAE 4.49600 | val_RMSE 5.25121 | R2 -0.2529\n",
      "Epoch 026 | tr_MSE 4.15110 | val_MAE 4.27685 | val_RMSE 4.95771 | R2 -0.1168\n",
      "Epoch 027 | tr_MSE 3.92222 | val_MAE 4.04255 | val_RMSE 4.70696 | R2 -0.0067\n",
      "Epoch 028 | tr_MSE 3.75449 | val_MAE 3.83652 | val_RMSE 4.63967 | R2 0.0219\n",
      "Epoch 029 | tr_MSE 3.48114 | val_MAE 3.73865 | val_RMSE 4.88200 | R2 -0.0829\n",
      "Epoch 030 | tr_MSE 3.45098 | val_MAE 3.82577 | val_RMSE 5.24477 | R2 -0.2499\n",
      "Epoch 031 | tr_MSE 3.42921 | val_MAE 3.95839 | val_RMSE 5.48783 | R2 -0.3684\n",
      "Epoch 032 | tr_MSE 3.71143 | val_MAE 3.99781 | val_RMSE 5.54971 | R2 -0.3994\n",
      "Epoch 033 | tr_MSE 3.67322 | val_MAE 3.92669 | val_RMSE 5.44141 | R2 -0.3453\n",
      "Epoch 034 | tr_MSE 3.51756 | val_MAE 3.80290 | val_RMSE 5.21117 | R2 -0.2339\n",
      "Epoch 035 | tr_MSE 3.39441 | val_MAE 3.71932 | val_RMSE 4.93037 | R2 -0.1045\n",
      "Epoch 036 | tr_MSE 3.40525 | val_MAE 3.73045 | val_RMSE 4.69451 | R2 -0.0014\n",
      "Epoch 037 | tr_MSE 3.40176 | val_MAE 3.76651 | val_RMSE 4.61991 | R2 0.0302\n",
      "Epoch 038 | tr_MSE 3.42779 | val_MAE 3.78021 | val_RMSE 4.60427 | R2 0.0368\n",
      "Epoch 039 | tr_MSE 3.41615 | val_MAE 3.76206 | val_RMSE 4.61300 | R2 0.0331\n",
      "Epoch 040 | tr_MSE 3.49379 | val_MAE 3.72599 | val_RMSE 4.66183 | R2 0.0125\n",
      "Epoch 041 | tr_MSE 3.40560 | val_MAE 3.69343 | val_RMSE 4.77862 | R2 -0.0376\n",
      "Epoch 042 | tr_MSE 3.42066 | val_MAE 3.69047 | val_RMSE 4.87985 | R2 -0.0820\n",
      "Epoch 043 | tr_MSE 3.37272 | val_MAE 3.69203 | val_RMSE 4.91867 | R2 -0.0993\n",
      "Epoch 044 | tr_MSE 3.48677 | val_MAE 3.68077 | val_RMSE 4.87976 | R2 -0.0819\n",
      "Epoch 045 | tr_MSE 3.40192 | val_MAE 3.66877 | val_RMSE 4.78820 | R2 -0.0417\n",
      "Epoch 046 | tr_MSE 3.35500 | val_MAE 3.67741 | val_RMSE 4.68279 | R2 0.0036\n",
      "Epoch 047 | tr_MSE 3.41035 | val_MAE 3.67646 | val_RMSE 4.65906 | R2 0.0137\n",
      "Epoch 048 | tr_MSE 3.23229 | val_MAE 3.65641 | val_RMSE 4.69415 | R2 -0.0012\n",
      "Epoch 049 | tr_MSE 3.40930 | val_MAE 3.64003 | val_RMSE 4.73392 | R2 -0.0182\n",
      "Epoch 050 | tr_MSE 3.33150 | val_MAE 3.62979 | val_RMSE 4.73352 | R2 -0.0181\n",
      "Epoch 051 | tr_MSE 3.41808 | val_MAE 3.62081 | val_RMSE 4.70834 | R2 -0.0073\n",
      "Epoch 052 | tr_MSE 3.29663 | val_MAE 3.60991 | val_RMSE 4.68720 | R2 0.0018\n",
      "Epoch 053 | tr_MSE 3.35426 | val_MAE 3.59372 | val_RMSE 4.70469 | R2 -0.0057\n",
      "Epoch 054 | tr_MSE 3.32099 | val_MAE 3.58437 | val_RMSE 4.81227 | R2 -0.0522\n",
      "Epoch 055 | tr_MSE 3.22445 | val_MAE 3.57732 | val_RMSE 4.84382 | R2 -0.0661\n",
      "Epoch 056 | tr_MSE 3.27896 | val_MAE 3.55699 | val_RMSE 4.79049 | R2 -0.0427\n",
      "Epoch 057 | tr_MSE 3.33242 | val_MAE 3.54779 | val_RMSE 4.67494 | R2 0.0070\n",
      "Epoch 058 | tr_MSE 3.25589 | val_MAE 3.54495 | val_RMSE 4.61098 | R2 0.0340\n",
      "Epoch 059 | tr_MSE 3.22617 | val_MAE 3.52449 | val_RMSE 4.63244 | R2 0.0250\n",
      "Epoch 060 | tr_MSE 3.20990 | val_MAE 3.50344 | val_RMSE 4.73922 | R2 -0.0205\n",
      "Epoch 061 | tr_MSE 3.25987 | val_MAE 3.49962 | val_RMSE 4.78719 | R2 -0.0413\n",
      "Epoch 062 | tr_MSE 3.27039 | val_MAE 3.47931 | val_RMSE 4.74171 | R2 -0.0216\n",
      "Epoch 063 | tr_MSE 3.23650 | val_MAE 3.45990 | val_RMSE 4.63557 | R2 0.0236\n",
      "Epoch 064 | tr_MSE 3.18150 | val_MAE 3.46697 | val_RMSE 4.55630 | R2 0.0567\n",
      "Epoch 065 | tr_MSE 3.16659 | val_MAE 3.45007 | val_RMSE 4.57089 | R2 0.0507\n",
      "Epoch 066 | tr_MSE 3.22904 | val_MAE 3.41985 | val_RMSE 4.69492 | R2 -0.0015\n",
      "Epoch 067 | tr_MSE 3.14084 | val_MAE 3.46280 | val_RMSE 4.83422 | R2 -0.0618\n",
      "Epoch 068 | tr_MSE 3.30126 | val_MAE 3.45979 | val_RMSE 4.84113 | R2 -0.0649\n",
      "Epoch 069 | tr_MSE 3.18357 | val_MAE 3.41401 | val_RMSE 4.73117 | R2 -0.0171\n",
      "Epoch 070 | tr_MSE 3.15903 | val_MAE 3.40766 | val_RMSE 4.58923 | R2 0.0431\n",
      "Epoch 071 | tr_MSE 3.19107 | val_MAE 3.40999 | val_RMSE 4.56521 | R2 0.0531\n",
      "Epoch 072 | tr_MSE 3.17993 | val_MAE 3.39222 | val_RMSE 4.60450 | R2 0.0367\n",
      "Epoch 073 | tr_MSE 3.16153 | val_MAE 3.40462 | val_RMSE 4.74770 | R2 -0.0242\n",
      "Epoch 074 | tr_MSE 3.09913 | val_MAE 3.44731 | val_RMSE 4.84673 | R2 -0.0673\n",
      "Epoch 075 | tr_MSE 3.21136 | val_MAE 3.42090 | val_RMSE 4.80739 | R2 -0.0501\n",
      "Epoch 076 | tr_MSE 3.11536 | val_MAE 3.37414 | val_RMSE 4.70940 | R2 -0.0077\n",
      "Epoch 077 | tr_MSE 3.12572 | val_MAE 3.36498 | val_RMSE 4.71145 | R2 -0.0086\n",
      "Epoch 078 | tr_MSE 3.09928 | val_MAE 3.34841 | val_RMSE 4.73268 | R2 -0.0177\n",
      "Epoch 079 | tr_MSE 3.02608 | val_MAE 3.38406 | val_RMSE 4.77839 | R2 -0.0375\n",
      "Epoch 080 | tr_MSE 3.01978 | val_MAE 3.35142 | val_RMSE 4.71558 | R2 -0.0104\n",
      "Epoch 081 | tr_MSE 3.05750 | val_MAE 3.35254 | val_RMSE 4.71735 | R2 -0.0111\n",
      "Epoch 082 | tr_MSE 2.97448 | val_MAE 14.91366 | val_RMSE 18.38459 | R2 -14.3572\n",
      "Epoch 083 | tr_MSE 13.44619 | val_MAE 14.66368 | val_RMSE 18.02879 | R2 -13.7686\n",
      "Epoch 084 | tr_MSE 13.03808 | val_MAE 13.84385 | val_RMSE 16.95279 | R2 -12.0583\n",
      "Epoch 085 | tr_MSE 12.26536 | val_MAE 12.99397 | val_RMSE 15.84883 | R2 -10.4130\n",
      "Epoch 086 | tr_MSE 11.48992 | val_MAE 12.23989 | val_RMSE 14.88237 | R2 -9.0635\n",
      "Epoch 087 | tr_MSE 10.87987 | val_MAE 11.65055 | val_RMSE 14.14440 | R2 -8.0902\n",
      "Epoch 088 | tr_MSE 10.42491 | val_MAE 11.23176 | val_RMSE 13.63241 | R2 -7.4440\n",
      "Epoch 089 | tr_MSE 10.02838 | val_MAE 10.90118 | val_RMSE 13.23937 | R2 -6.9642\n",
      "Epoch 090 | tr_MSE 9.69043 | val_MAE 10.52722 | val_RMSE 12.76016 | R2 -6.3980\n",
      "Epoch 091 | tr_MSE 9.41542 | val_MAE 9.78647 | val_RMSE 11.81564 | R2 -5.3434\n",
      "Epoch 092 | tr_MSE 8.51438 | val_MAE 7.77800 | val_RMSE 9.34233 | R2 -2.9657\n",
      "Epoch 093 | tr_MSE 6.40278 | val_MAE 4.82266 | val_RMSE 6.31756 | R2 -0.8134\n",
      "Epoch 094 | tr_MSE 4.14539 | val_MAE 4.73620 | val_RMSE 6.13714 | R2 -0.7113\n",
      "Epoch 095 | tr_MSE 3.50644 | val_MAE 3.67187 | val_RMSE 5.23839 | R2 -0.2468\n",
      "Epoch 096 | tr_MSE 3.28422 | val_MAE 3.56667 | val_RMSE 5.17981 | R2 -0.2191\n",
      "Epoch 097 | tr_MSE 3.14534 | val_MAE 3.39787 | val_RMSE 4.94758 | R2 -0.1122\n",
      "Epoch 098 | tr_MSE 3.04004 | val_MAE 3.29664 | val_RMSE 4.71397 | R2 -0.0097\n",
      "Epoch 099 | tr_MSE 2.96403 | val_MAE 3.33141 | val_RMSE 4.66246 | R2 0.0123\n",
      "Epoch 100 | tr_MSE 3.07715 | val_MAE 3.34355 | val_RMSE 4.65227 | R2 0.0166\n",
      "Epoch 101 | tr_MSE 3.04989 | val_MAE 3.30069 | val_RMSE 4.65578 | R2 0.0151\n",
      "Epoch 102 | tr_MSE 3.00414 | val_MAE 3.25322 | val_RMSE 4.70377 | R2 -0.0053\n",
      "Epoch 103 | tr_MSE 2.96254 | val_MAE 3.27868 | val_RMSE 4.81395 | R2 -0.0530\n",
      "Epoch 104 | tr_MSE 3.01763 | val_MAE 3.24992 | val_RMSE 4.77821 | R2 -0.0374\n",
      "Epoch 105 | tr_MSE 2.87238 | val_MAE 3.15746 | val_RMSE 4.63015 | R2 0.0259\n",
      "Epoch 106 | tr_MSE 2.92903 | val_MAE 3.08283 | val_RMSE 4.51996 | R2 0.0717\n",
      "Epoch 107 | tr_MSE 2.80008 | val_MAE 3.02826 | val_RMSE 4.44368 | R2 0.1028\n",
      "Epoch 108 | tr_MSE 2.87326 | val_MAE 2.99390 | val_RMSE 4.41763 | R2 0.1133\n",
      "Epoch 109 | tr_MSE 2.78618 | val_MAE 2.98332 | val_RMSE 4.44073 | R2 0.1040\n",
      "Epoch 110 | tr_MSE 2.69746 | val_MAE 2.95021 | val_RMSE 4.42873 | R2 0.1088\n",
      "Epoch 111 | tr_MSE 2.71184 | val_MAE 2.89030 | val_RMSE 4.35808 | R2 0.1370\n",
      "Epoch 112 | tr_MSE 2.64602 | val_MAE 2.81290 | val_RMSE 4.25918 | R2 0.1758\n",
      "Epoch 113 | tr_MSE 2.63151 | val_MAE 2.76644 | val_RMSE 4.22006 | R2 0.1908\n",
      "Epoch 114 | tr_MSE 2.74914 | val_MAE 2.74230 | val_RMSE 4.22132 | R2 0.1903\n",
      "Epoch 115 | tr_MSE 2.55746 | val_MAE 2.72938 | val_RMSE 4.23246 | R2 0.1861\n",
      "Epoch 116 | tr_MSE 2.54898 | val_MAE 2.70984 | val_RMSE 4.22142 | R2 0.1903\n",
      "Epoch 117 | tr_MSE 2.48779 | val_MAE 2.69571 | val_RMSE 4.18500 | R2 0.2042\n",
      "Epoch 118 | tr_MSE 2.53578 | val_MAE 2.73541 | val_RMSE 4.21808 | R2 0.1916\n",
      "Epoch 119 | tr_MSE 2.59070 | val_MAE 2.79037 | val_RMSE 4.29089 | R2 0.1634\n",
      "Epoch 120 | tr_MSE 2.46823 | val_MAE 2.78147 | val_RMSE 4.27202 | R2 0.1708\n",
      "Epoch 121 | tr_MSE 2.57038 | val_MAE 2.75107 | val_RMSE 4.21852 | R2 0.1914\n",
      "Epoch 122 | tr_MSE 2.56797 | val_MAE 2.68893 | val_RMSE 4.16711 | R2 0.2110\n",
      "Epoch 123 | tr_MSE 2.52549 | val_MAE 2.66680 | val_RMSE 4.13658 | R2 0.2225\n",
      "Epoch 124 | tr_MSE 2.55023 | val_MAE 2.65762 | val_RMSE 4.13890 | R2 0.2216\n",
      "Epoch 125 | tr_MSE 2.52062 | val_MAE 2.65376 | val_RMSE 4.16326 | R2 0.2125\n",
      "Epoch 126 | tr_MSE 2.52271 | val_MAE 2.65830 | val_RMSE 4.18301 | R2 0.2050\n",
      "Epoch 127 | tr_MSE 2.47340 | val_MAE 2.65256 | val_RMSE 4.17983 | R2 0.2062\n",
      "Epoch 128 | tr_MSE 2.43148 | val_MAE 2.64186 | val_RMSE 4.15533 | R2 0.2155\n",
      "Epoch 129 | tr_MSE 2.52880 | val_MAE 2.63919 | val_RMSE 4.13114 | R2 0.2246\n",
      "Epoch 130 | tr_MSE 2.51901 | val_MAE 2.63920 | val_RMSE 4.13111 | R2 0.2246\n",
      "Epoch 131 | tr_MSE 2.49350 | val_MAE 2.63429 | val_RMSE 4.12725 | R2 0.2260\n",
      "Epoch 132 | tr_MSE 2.47429 | val_MAE 2.61504 | val_RMSE 4.10604 | R2 0.2340\n",
      "Epoch 133 | tr_MSE 2.48541 | val_MAE 2.59419 | val_RMSE 4.08347 | R2 0.2424\n",
      "Epoch 134 | tr_MSE 2.49811 | val_MAE 2.59535 | val_RMSE 4.10083 | R2 0.2359\n",
      "Epoch 135 | tr_MSE 2.50993 | val_MAE 2.60045 | val_RMSE 4.10097 | R2 0.2359\n",
      "Epoch 136 | tr_MSE 2.39123 | val_MAE 2.61090 | val_RMSE 4.11017 | R2 0.2324\n",
      "Epoch 137 | tr_MSE 2.45038 | val_MAE 2.60782 | val_RMSE 4.10798 | R2 0.2332\n",
      "Epoch 138 | tr_MSE 2.44968 | val_MAE 2.59670 | val_RMSE 4.10033 | R2 0.2361\n",
      "Epoch 139 | tr_MSE 2.41738 | val_MAE 2.59162 | val_RMSE 4.10183 | R2 0.2355\n",
      "Epoch 140 | tr_MSE 2.40990 | val_MAE 2.58372 | val_RMSE 4.08168 | R2 0.2430\n",
      "Epoch 141 | tr_MSE 2.37453 | val_MAE 2.58055 | val_RMSE 4.05880 | R2 0.2515\n",
      "Epoch 142 | tr_MSE 2.31532 | val_MAE 2.56271 | val_RMSE 4.03572 | R2 0.2600\n",
      "Epoch 143 | tr_MSE 2.36113 | val_MAE 2.54608 | val_RMSE 4.01532 | R2 0.2674\n",
      "Epoch 144 | tr_MSE 2.45995 | val_MAE 2.53094 | val_RMSE 3.99316 | R2 0.2755\n",
      "Epoch 145 | tr_MSE 2.36301 | val_MAE 2.51429 | val_RMSE 3.97986 | R2 0.2803\n",
      "Epoch 146 | tr_MSE 2.29343 | val_MAE 2.48999 | val_RMSE 3.93929 | R2 0.2949\n",
      "Epoch 147 | tr_MSE 2.34133 | val_MAE 2.44355 | val_RMSE 3.85644 | R2 0.3243\n",
      "Epoch 148 | tr_MSE 2.19303 | val_MAE 2.40480 | val_RMSE 3.81121 | R2 0.3400\n",
      "Epoch 149 | tr_MSE 2.25594 | val_MAE 2.41820 | val_RMSE 3.79624 | R2 0.3452\n",
      "Epoch 150 | tr_MSE 2.25831 | val_MAE 2.41379 | val_RMSE 3.80515 | R2 0.3421\n",
      "Epoch 151 | tr_MSE 2.26165 | val_MAE 2.41227 | val_RMSE 3.81267 | R2 0.3395\n",
      "Epoch 152 | tr_MSE 2.28348 | val_MAE 2.40601 | val_RMSE 3.80675 | R2 0.3416\n",
      "Epoch 153 | tr_MSE 2.21554 | val_MAE 2.39494 | val_RMSE 3.78014 | R2 0.3507\n",
      "Epoch 154 | tr_MSE 2.16818 | val_MAE 2.39373 | val_RMSE 3.75842 | R2 0.3582\n",
      "Epoch 155 | tr_MSE 2.20896 | val_MAE 2.39388 | val_RMSE 3.70328 | R2 0.3769\n",
      "Epoch 156 | tr_MSE 2.21008 | val_MAE 2.36865 | val_RMSE 3.68216 | R2 0.3840\n",
      "Epoch 157 | tr_MSE 2.15693 | val_MAE 2.38542 | val_RMSE 3.67737 | R2 0.3856\n",
      "Epoch 158 | tr_MSE 2.34556 | val_MAE 2.39682 | val_RMSE 3.67917 | R2 0.3850\n",
      "Epoch 159 | tr_MSE 2.25250 | val_MAE 2.39549 | val_RMSE 3.66283 | R2 0.3904\n",
      "Epoch 160 | tr_MSE 2.14052 | val_MAE 2.36928 | val_RMSE 3.63473 | R2 0.3997\n",
      "Epoch 161 | tr_MSE 2.17699 | val_MAE 2.35350 | val_RMSE 3.61084 | R2 0.4076\n",
      "Epoch 162 | tr_MSE 2.17721 | val_MAE 2.35204 | val_RMSE 3.62198 | R2 0.4039\n",
      "Epoch 163 | tr_MSE 2.06853 | val_MAE 2.34907 | val_RMSE 3.60769 | R2 0.4086\n",
      "Epoch 164 | tr_MSE 2.07975 | val_MAE 2.35069 | val_RMSE 3.59519 | R2 0.4127\n",
      "Epoch 165 | tr_MSE 2.18138 | val_MAE 2.34583 | val_RMSE 3.58218 | R2 0.4170\n",
      "Epoch 166 | tr_MSE 2.15326 | val_MAE 2.34523 | val_RMSE 3.57237 | R2 0.4201\n",
      "Epoch 167 | tr_MSE 2.09048 | val_MAE 2.33387 | val_RMSE 3.55448 | R2 0.4259\n",
      "Epoch 168 | tr_MSE 2.19329 | val_MAE 2.33613 | val_RMSE 3.55139 | R2 0.4269\n",
      "Epoch 169 | tr_MSE 2.06847 | val_MAE 2.33460 | val_RMSE 3.54722 | R2 0.4283\n",
      "Epoch 170 | tr_MSE 2.09602 | val_MAE 2.32910 | val_RMSE 3.53899 | R2 0.4309\n",
      "Epoch 171 | tr_MSE 2.13183 | val_MAE 2.32358 | val_RMSE 3.53324 | R2 0.4328\n",
      "Epoch 172 | tr_MSE 2.08501 | val_MAE 2.31956 | val_RMSE 3.52982 | R2 0.4339\n",
      "Epoch 173 | tr_MSE 2.01863 | val_MAE 2.31586 | val_RMSE 3.52802 | R2 0.4345\n",
      "Epoch 174 | tr_MSE 2.04760 | val_MAE 2.31173 | val_RMSE 3.52826 | R2 0.4344\n",
      "Epoch 175 | tr_MSE 2.09778 | val_MAE 2.31205 | val_RMSE 3.53587 | R2 0.4319\n",
      "Epoch 176 | tr_MSE 2.11946 | val_MAE 2.31736 | val_RMSE 3.54431 | R2 0.4292\n",
      "Epoch 177 | tr_MSE 2.01547 | val_MAE 2.31681 | val_RMSE 3.54597 | R2 0.4287\n",
      "Epoch 178 | tr_MSE 2.08778 | val_MAE 2.30894 | val_RMSE 3.54404 | R2 0.4293\n",
      "Epoch 179 | tr_MSE 2.18283 | val_MAE 2.30411 | val_RMSE 3.54635 | R2 0.4286\n",
      "Epoch 180 | tr_MSE 2.02194 | val_MAE 2.30133 | val_RMSE 3.54832 | R2 0.4279\n",
      "Epoch 181 | tr_MSE 2.06117 | val_MAE 2.29858 | val_RMSE 3.54698 | R2 0.4284\n",
      "Epoch 182 | tr_MSE 2.05144 | val_MAE 2.29607 | val_RMSE 3.54325 | R2 0.4296\n",
      "Epoch 183 | tr_MSE 2.04577 | val_MAE 2.29470 | val_RMSE 3.53950 | R2 0.4308\n",
      "Epoch 184 | tr_MSE 2.14663 | val_MAE 2.29572 | val_RMSE 3.53734 | R2 0.4315\n",
      "Epoch 185 | tr_MSE 2.10767 | val_MAE 2.29615 | val_RMSE 3.53371 | R2 0.4326\n",
      "Epoch 186 | tr_MSE 2.13660 | val_MAE 2.29627 | val_RMSE 3.52978 | R2 0.4339\n",
      "Epoch 187 | tr_MSE 2.15927 | val_MAE 2.29556 | val_RMSE 3.52606 | R2 0.4351\n",
      "Epoch 188 | tr_MSE 2.14690 | val_MAE 2.29459 | val_RMSE 3.52309 | R2 0.4360\n",
      "Epoch 189 | tr_MSE 2.04311 | val_MAE 2.29384 | val_RMSE 3.52028 | R2 0.4369\n",
      "Epoch 190 | tr_MSE 2.14341 | val_MAE 2.29318 | val_RMSE 3.51753 | R2 0.4378\n",
      "Epoch 191 | tr_MSE 2.09878 | val_MAE 2.29280 | val_RMSE 3.51532 | R2 0.4385\n",
      "Epoch 192 | tr_MSE 2.11214 | val_MAE 2.29248 | val_RMSE 3.51315 | R2 0.4392\n",
      "Epoch 193 | tr_MSE 2.08162 | val_MAE 2.29209 | val_RMSE 3.51121 | R2 0.4398\n",
      "Epoch 194 | tr_MSE 2.06196 | val_MAE 2.29170 | val_RMSE 3.50961 | R2 0.4403\n",
      "Epoch 195 | tr_MSE 2.05348 | val_MAE 2.29143 | val_RMSE 3.50855 | R2 0.4407\n",
      "Epoch 196 | tr_MSE 2.11394 | val_MAE 2.29128 | val_RMSE 3.50784 | R2 0.4409\n",
      "Epoch 197 | tr_MSE 2.01351 | val_MAE 2.29118 | val_RMSE 3.50738 | R2 0.4411\n",
      "Epoch 198 | tr_MSE 2.08560 | val_MAE 2.29110 | val_RMSE 3.50714 | R2 0.4411\n",
      "Epoch 199 | tr_MSE 2.08636 | val_MAE 2.29107 | val_RMSE 3.50704 | R2 0.4412\n",
      "Epoch 200 | tr_MSE 2.00887 | val_MAE 2.29106 | val_RMSE 3.50702 | R2 0.4412\n",
      "[gt_rg] Best Val — MAE 2.291059 | RMSE 3.507023 | R2 0.4412\n"
     ]
    }
   ],
   "source": [
    "# infer rdkit_dim from a batch (6 or 15 depending on your LMDB)\n",
    "b = next(iter(train_loader_tg))\n",
    "rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "model_tg = GraphTransformer(d_model=256, nhead=8, nlayers=6,\n",
    "                            dropout=0.2, drop_path=0.1,\n",
    "                            rdkit_dim=rd_dim, use_extra_atom_feats=True,\n",
    "                            activation=\"silu\").to(b.x.device)\n",
    "\n",
    "model_tg, ckpt_tg, metrics_tg = train_hybrid_gnn_sota(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=120, warmup_epochs=5, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_tg\", tag=\"graphtransformer_tg\"\n",
    ")\n",
    "\n",
    "\n",
    "# Density (use your tuned dims if you like larger backbones)\n",
    "model_den = GraphTransformer(d_model=256,nhead=8, nlayers=6,\n",
    "                             dropout=0.2, drop_path=0.1,\n",
    "                             rdkit_dim=rd_dim, use_extra_atom_feats=True,\n",
    "                             activation=\"silu\").to(b.x.device)\n",
    "\n",
    "model_den, ckpt_den, metrics_den = train_hybrid_gnn_sota(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=8, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_density\", tag=\"graphtransformer_density\"\n",
    ")\n",
    "\n",
    "\n",
    "# Rg (your tuned gnn_dim + swish + RMSprop work fine here)\n",
    "model_rg = GraphTransformer(d_model=256, nhead=8, nlayers=6,\n",
    "                            dropout=0.2, drop_path=0.1,\n",
    "                            rdkit_dim=rd_dim, use_extra_atom_feats=True,\n",
    "                            activation=\"silu\").to(b.x.device)\n",
    "\n",
    "model_rg, ckpt_rg, metrics_rg = train_hybrid_gnn_sota(\n",
    "    model_rg, train_loader_rg, val_loader_rg,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=6, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"huber\",  # Huber often helps Rg\n",
    "    save_dir=\"saved_models/gt_rg\", tag=\"gt_rg\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9449bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "def _act(name: str):\n",
    "    name = (name or \"relu\").lower()\n",
    "    if name == \"gelu\": return nn.GELU()\n",
    "    if name in (\"silu\", \"swish\"): return nn.SiLU()\n",
    "    return nn.ReLU()\n",
    "\n",
    "\n",
    "class AttnBiasFull(nn.Module):\n",
    "    \"\"\"\n",
    "    Produces additive per-head attention bias of shape (B, H, L0, L0)\n",
    "    from geometry (xyz), adjacency, SPD buckets, and categorical edge types.\n",
    "\n",
    "    Accepts both old arg names (use_geo/use_adj_const/spd_max/rbf_K) and\n",
    "    new ones (use_geo_bias/use_adj_bias/spd_buckets/rbf_k/edge_cats).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_heads: int,\n",
    "        *,\n",
    "        # old names\n",
    "        use_geo: bool = None, use_adj_const: bool = None, use_spd: bool = True,\n",
    "        spd_max: int = None, rbf_K: int = None,\n",
    "        # new alias names\n",
    "        use_geo_bias: bool = None, use_adj_bias: bool = None,\n",
    "        spd_buckets: int = None, rbf_k: int = None,\n",
    "        edge_cats: tuple = (5, 6, 2),\n",
    "        use_edge_bias: bool = True,\n",
    "        # shared\n",
    "        rbf_beta: float = 5.0, activation: str = \"relu\",\n",
    "        edge_cont_dim: int = 32,  # (kept for compatibility; not used here)\n",
    "        use_headnorm: bool = True,\n",
    "        bound_scale: float = 0.1,   # tanh scale for gentle bounding\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_heads = int(n_heads)\n",
    "        self.bound_scale = float(bound_scale)\n",
    "        self.use_headnorm = bool(use_headnorm)\n",
    "\n",
    "        # ---- resolve aliases / defaults ----\n",
    "        def pick(*vals, default):\n",
    "            for v in vals:\n",
    "                if v is not None:\n",
    "                    return v\n",
    "            return default\n",
    "\n",
    "        self.use_geo = bool(pick(use_geo, use_geo_bias, default=True))\n",
    "        self.use_adj_const = bool(pick(use_adj_const, use_adj_bias, default=True))\n",
    "\n",
    "        # SPD: if spd_buckets given, use exactly that; else spd_max + 2 (0..spd_max + catch-all)\n",
    "        if spd_buckets is not None:\n",
    "            self.spd_buckets = int(spd_buckets)\n",
    "        else:\n",
    "            smax = 5 if spd_max is None else int(spd_max)\n",
    "            self.spd_buckets = smax + 2  # 0..smax + 1(>=)\n",
    "\n",
    "        K = int(pick(rbf_K, rbf_k, default=16))\n",
    "        self.rbf_beta = float(rbf_beta)\n",
    "\n",
    "        # ---- geometry → per-head bias ----\n",
    "        if self.use_geo:\n",
    "            centers = torch.linspace(0.0, 10.0, K)\n",
    "            self.register_buffer(\"centers\", centers, persistent=False)\n",
    "            self.geo_mlp = nn.Sequential(\n",
    "                nn.Linear(K, self.n_heads),  # simple per-head projection\n",
    "            )\n",
    "\n",
    "        # ---- adjacency constant per head ----\n",
    "        if self.use_adj_const:\n",
    "            self.adj_bias = nn.Parameter(torch.zeros(self.n_heads))\n",
    "\n",
    "        # ---- SPD buckets → per-head bias ----\n",
    "        self.use_spd = bool(use_spd)\n",
    "        if self.use_spd:\n",
    "            self.spd_emb = nn.Embedding(self.spd_buckets, self.n_heads)\n",
    "\n",
    "        # ---- edge categorical bias (configurable widths) ----\n",
    "        t, s, c = edge_cats\n",
    "        self.use_edge_bias = bool(use_edge_bias)\n",
    "        if self.use_edge_bias:\n",
    "            self.edge_emb0 = nn.Embedding(int(t), self.n_heads)\n",
    "            self.edge_emb1 = nn.Embedding(int(s), self.n_heads)\n",
    "            self.edge_emb2 = nn.Embedding(int(c), self.n_heads)\n",
    "        else:\n",
    "            self.edge_emb0 = self.edge_emb1 = self.edge_emb2 = None\n",
    "\n",
    "        # ---- per-component learnable scalers ----\n",
    "        self.alpha_geo  = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_spd  = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_adj  = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_edge = nn.Parameter(torch.tensor(0.2))\n",
    "\n",
    "        # ---- simple head-wise LayerNorms (normalize across H) ----\n",
    "        if self.use_headnorm:\n",
    "            self.ln_geo  = nn.LayerNorm(self.n_heads)\n",
    "            self.ln_spd  = nn.LayerNorm(self.n_heads)\n",
    "            self.ln_edge = nn.LayerNorm(self.n_heads)\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def _apply_ln_heads(self, t: torch.Tensor, ln: nn.LayerNorm) -> torch.Tensor:\n",
    "        \"\"\"Apply LayerNorm across heads for a (B,H,L,L) tensor.\"\"\"\n",
    "        # (B,H,L,L) -> (B,L,L,H) -> LN(H) -> (B,H,L,L)\n",
    "        t = t.permute(0, 2, 3, 1)\n",
    "        t = ln(t)\n",
    "        t = t.permute(0, 3, 1, 2).contiguous()\n",
    "        return t\n",
    "\n",
    "    def _bound(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Bound magnitudes to avoid dominating softmax; keeps gradients smooth.\"\"\"\n",
    "        return self.bound_scale * torch.tanh(t)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _spd_bias(self, hops: torch.Tensor, valid_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        hops: (B, MAX_NODES, MAX_NODES) or (B, L0, L0) shortest-path distances (uint8/long)\n",
    "        valid_mask: (B, L0, L0) bool, True where both tokens are real (not PAD)\n",
    "        returns: (B, H, L0, L0) additive per-head bias\n",
    "        \"\"\"\n",
    "        if hops.dim() == 2:  # (L,L) -> (1,L,L)\n",
    "            hops = hops.unsqueeze(0)\n",
    "\n",
    "        B, L0, _ = valid_mask.shape\n",
    "\n",
    "        # align SPD to current L0 (top-left block)\n",
    "        if hops.size(1) != L0 or hops.size(2) != L0:\n",
    "            hops = hops[:, :L0, :L0]\n",
    "\n",
    "        # bucketize SPD: last bucket = catch-all (>= last)\n",
    "        last = self.spd_buckets - 1\n",
    "        raw = hops.to(valid_mask.device).long().clamp_min_(0)\n",
    "        catch_all = raw >= last\n",
    "        raw = raw.clamp_max(last - 1)\n",
    "        bucket = torch.where(catch_all, raw.new_full(raw.shape, last), raw)\n",
    "\n",
    "        # wipe invalid pairs\n",
    "        bucket = torch.where(valid_mask, bucket, torch.zeros_like(bucket))\n",
    "\n",
    "        emb = self.spd_emb(bucket)              # (B, L0, L0, H)\n",
    "        return emb.permute(0, 3, 1, 2).contiguous()  # (B, H, L0, L0)\n",
    "\n",
    "    def _edge_bias(self, edge_index, edge_attr, batch, L0, ptr=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-head additive bias from categorical bond attributes.\n",
    "        Returns: (B, H, L0, L0)\n",
    "        \"\"\"\n",
    "        u, v = edge_index\n",
    "        be   = batch[u]  # graph id per edge\n",
    "\n",
    "        if ptr is None:\n",
    "            B = int(batch.max().item()) + 1\n",
    "            counts = torch.bincount(batch, minlength=B)\n",
    "            ptr = torch.zeros(B + 1, dtype=torch.long, device=batch.device)\n",
    "            ptr[1:] = torch.cumsum(counts, dim=0)\n",
    "        B = int(ptr.numel() - 1)\n",
    "\n",
    "        start = ptr[be]\n",
    "        u_loc = (u - start).long()\n",
    "        v_loc = (v - start).long()\n",
    "\n",
    "        cat = edge_attr[:, :3].long()\n",
    "        eh  = ( self.edge_emb0(cat[:, 0])\n",
    "              + self.edge_emb1(cat[:, 1])\n",
    "              + self.edge_emb2(cat[:, 2]) )  # (E,H)\n",
    "\n",
    "        H = self.n_heads\n",
    "        eb = torch.zeros((B, H, L0, L0), device=edge_attr.device, dtype=torch.float32)\n",
    "        for b in range(B):\n",
    "            m = (be == b)\n",
    "            if not torch.any(m):\n",
    "                continue\n",
    "            eb[b, :, u_loc[m], v_loc[m]] += eh[m].T\n",
    "        return eb\n",
    "\n",
    "    # ---------- forward ----------\n",
    "    def forward(self, pos, edge_index, edge_attr, batch, key_padding_mask, hops=None, ptr=None):\n",
    "        \"\"\"\n",
    "        Returns (B, H, L0, L0) additive bias. PAD rows/cols are filled with large negative.\n",
    "        \"\"\"\n",
    "        A = to_dense_adj(edge_index, batch=batch).squeeze(1)  # (B,L0,L0)\n",
    "        B, L0, _ = A.shape\n",
    "        H = self.n_heads\n",
    "        device = A.device\n",
    "\n",
    "        valid = ~key_padding_mask                             # (B,L0)\n",
    "        valid2d = valid.unsqueeze(2) & valid.unsqueeze(1)     # (B,L0,L0)\n",
    "\n",
    "        # geometry\n",
    "        if self.use_geo and (pos is not None):\n",
    "            pad_pos, _ = to_dense_batch(pos, batch)           # (B,L0,3)\n",
    "            diff = pad_pos.unsqueeze(2) - pad_pos.unsqueeze(1)      # (B,L0,L0,3)\n",
    "            dist = torch.sqrt(torch.clamp((diff**2).sum(-1), min=0.0))  # (B,L0,L0)\n",
    "            centers = self.centers.to(dist.device)\n",
    "            rbf = torch.exp(-self.rbf_beta * (dist.unsqueeze(-1) - centers)**2)\n",
    "            geo = self.geo_mlp(rbf).permute(0, 3, 1, 2).contiguous()    # (B,H,L0,L0)\n",
    "        else:\n",
    "            geo = torch.zeros((B, H, L0, L0), device=device)\n",
    "\n",
    "        # adjacency constant per head\n",
    "        if self.use_adj_const:\n",
    "            adj = A.unsqueeze(1) * self.adj_bias.view(1, H, 1, 1)       # (B,H,L0,L0)\n",
    "        else:\n",
    "            adj = torch.zeros_like(geo)\n",
    "\n",
    "        # SPD\n",
    "        if self.use_spd and (hops is not None):\n",
    "            spd = self._spd_bias(hops, valid2d)                          # (B,H,L0,L0)\n",
    "        else:\n",
    "            spd = torch.zeros_like(geo)\n",
    "\n",
    "        # edge categorical\n",
    "        if self.use_edge_bias and (edge_attr is not None):\n",
    "            edg = self._edge_bias(edge_index, edge_attr, batch, L0, ptr) # (B,H,L0,L0)\n",
    "        else:\n",
    "            edg = torch.zeros_like(geo)\n",
    "\n",
    "        # ---- normalize & bound each component, then scale ----\n",
    "        if self.use_headnorm:\n",
    "            if self.use_geo:  geo = self._apply_ln_heads(geo,  self.ln_geo)\n",
    "            if self.use_spd:  spd = self._apply_ln_heads(spd,  self.ln_spd)\n",
    "            if self.use_edge_bias: edg = self._apply_ln_heads(edg, self.ln_edge)\n",
    "\n",
    "        # gently bound to keep attention stable\n",
    "        if self.use_geo:       geo = self._bound(geo)\n",
    "        if self.use_spd:       spd = self._bound(spd)\n",
    "        if self.use_edge_bias: edg = self._bound(edg)\n",
    "        # typically don't bound adj; it’s already a small learned scalar per head\n",
    "\n",
    "        bias = (self.alpha_geo  * geo\n",
    "              + self.alpha_spd  * spd\n",
    "              + self.alpha_adj  * adj\n",
    "              + self.alpha_edge * edg)\n",
    "\n",
    "        # mask PAD rows/cols; keep diagonal 0 for valid tokens\n",
    "        pad = key_padding_mask\n",
    "        big_neg = torch.tensor(-1e4, device=bias.device, dtype=bias.dtype)\n",
    "        bias = bias.masked_fill(pad.view(B, 1, L0, 1), big_neg)\n",
    "        bias = bias.masked_fill(pad.view(B, 1, 1, L0), big_neg)\n",
    "        I = torch.eye(L0, device=device, dtype=torch.bool).view(1, 1, L0, L0)\n",
    "        bias = torch.where(I, bias.new_zeros(()), bias)\n",
    "\n",
    "        return bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026345f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import GINEConv\n",
    "\n",
    "# class GINEBlock(nn.Module):\n",
    "#     def __init__(self, dim, activation=\"silu\", dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         act = _act(activation)\n",
    "#         self.norm1 = nn.LayerNorm(dim)\n",
    "#         self.conv = GINEConv(nn.Sequential(\n",
    "#             nn.Linear(dim, dim), act, nn.Linear(dim, dim)\n",
    "#         ))\n",
    "#         self.drop1 = nn.Dropout(dropout)\n",
    "#         self.norm2 = nn.LayerNorm(dim)\n",
    "#         self.ffn = nn.Sequential(nn.Linear(dim, 2*dim), act, nn.Dropout(dropout), nn.Linear(2*dim, dim))\n",
    "#         self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x, edge_index, edge_emb):\n",
    "#         h = self.conv(self.norm1(x), edge_index, edge_emb)\n",
    "#         x = x + self.drop1(h)\n",
    "#         x = x + self.drop2(self.ffn(self.norm2(x)))\n",
    "#         return x\n",
    "\n",
    "# class EdgeEncoderMixed(nn.Module):\n",
    "#     def __init__(self, emb_dim: int, cont_dim: int = 32, activation=\"silu\"):\n",
    "#         super().__init__()\n",
    "#         act = _act(activation)\n",
    "#         self.emb0 = nn.Embedding(5, emb_dim)\n",
    "#         self.emb1 = nn.Embedding(6, emb_dim)\n",
    "#         self.emb2 = nn.Embedding(2, emb_dim)\n",
    "#         self.mlp_cont = nn.Sequential(\n",
    "#             nn.Linear(cont_dim, emb_dim),\n",
    "#             act,\n",
    "#             nn.Linear(emb_dim, emb_dim),\n",
    "#             nn.LayerNorm(emb_dim),       # <<< add\n",
    "#         )\n",
    "\n",
    "#     def forward(self, edge_attr):\n",
    "#         cat  = edge_attr[:, :3].long()\n",
    "#         cont = edge_attr[:, 3:].float()\n",
    "#         e_cat  = self.emb0(cat[:,0]) + self.emb1(cat[:,1]) + self.emb2(cat[:,2])\n",
    "#         e_cont = self.mlp_cont(cont)\n",
    "#         return e_cat + 0.5 * e_cont     # <<< gentle scale on cont branch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f224f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class GraphTransformerGPS(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 8,\n",
    "        nlayers: int = 6,\n",
    "        dropout: float = 0.2,\n",
    "        drop_path: float = 0.0,   # (kept for extensibility)\n",
    "        activation: str = \"silu\",\n",
    "        rdkit_dim: int = 15,\n",
    "        use_extra_atom_feats: bool = True,\n",
    "        extra_atom_dim: int = 5,\n",
    "        # local GNN (GPS) settings\n",
    "        local_layers: int = 2,\n",
    "        use_mixed_edges: bool = True,\n",
    "        cont_dim: int = 32,\n",
    "        # bias knobs\n",
    "        use_geo_bias: bool = True,\n",
    "        use_spd_bias: bool = True,\n",
    "        spd_max: int = 5,\n",
    "        use_adj_const: bool = True,\n",
    "        use_edge_bias: bool = True,\n",
    "        # readout\n",
    "        use_cls: bool = True,\n",
    "        use_has_xyz: bool = True,\n",
    "        head_hidden: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead   = nhead\n",
    "        self.use_cls = use_cls\n",
    "        self.use_has_xyz = use_has_xyz\n",
    "        self.use_extra_atom_feats = use_extra_atom_feats\n",
    "        # inside GraphTransformerGPS.__init__(...)\n",
    "        self.bias_builder = AttnBiasFull(\n",
    "            n_heads=nhead,\n",
    "            rbf_k=32,\n",
    "            rbf_beta=5.0,\n",
    "            use_geo_bias=use_geo_bias,          # was use_geo\n",
    "            use_adj_bias=use_adj_const,         # was use_adj_const (name matches here)\n",
    "            use_spd=use_spd_bias,               # was use_spd\n",
    "            spd_buckets=(spd_max + 1),          # was spd_max; +1 gives the \">= spd_max\" bucket\n",
    "            use_edge_bias=use_edge_bias,\n",
    "            edge_cats=(5, 6, 2),\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "\n",
    "        act = _act(activation)\n",
    "\n",
    "        # encoders\n",
    "        self.atom_enc = AtomEncoder(emb_dim=d_model)\n",
    "        if use_extra_atom_feats:\n",
    "            self.extra_proj = nn.Sequential(nn.Linear(extra_atom_dim, d_model), act, nn.Linear(d_model, d_model))\n",
    "            self.extra_gate = nn.Sequential(nn.Linear(2*d_model, d_model), act)\n",
    "\n",
    "        # local GNN stack\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        if use_mixed_edges:\n",
    "            self.edge_enc = EdgeEncoderMixed(d_model, cont_dim=cont_dim, activation=activation)\n",
    "        else:\n",
    "            from ogb.graphproppred.mol_encoder import BondEncoder\n",
    "            self.edge_enc = BondEncoder(emb_dim=d_model)\n",
    "        self.local_blocks = nn.ModuleList([GINEBlock(d_model, activation=activation, dropout=dropout) \n",
    "                                           for _ in range(local_layers)])\n",
    "\n",
    "        # transformer stack (PyTorch encoder)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=4*d_model,\n",
    "                                               dropout=dropout, activation=activation, batch_first=True, norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        # readout: concat mean + max + (optional) CLS + attention pool\n",
    "        self.gate_pool = nn.Sequential(nn.Linear(d_model, d_model//2), act, nn.Linear(d_model//2, 1))\n",
    "        # features: mean(d), max(d), attn(d) = 3d, (+cls d) optional, + rdkit, + has_xyz\n",
    "        pooled_dim = 3*d_model + (d_model if use_cls else 0)\n",
    "        head_in = pooled_dim + rdkit_dim + (1 if use_has_xyz else 0)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(head_in),\n",
    "            nn.Linear(head_in, head_hidden), act, nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden, head_hidden//2), act, nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden//2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # 1) atom encoding + optional per-atom extras\n",
    "        x = self.atom_enc(data.x)  # (N,D)\n",
    "        if getattr(self, \"use_extra_atom_feats\", False) and hasattr(data, \"extra_atom_feats\"):\n",
    "            xa = self.extra_proj(data.extra_atom_feats.float())          # (N,D)\n",
    "            x  = self.extra_gate(torch.cat([x, xa], dim=1))              # (N,D)\n",
    "\n",
    "        # 2) local GNN over sparse graph\n",
    "        e = self.edge_enc(data.edge_attr)\n",
    "        for blk in self.local_blocks:\n",
    "            x = blk(x, data.edge_index, e)                               # (N,D)\n",
    "\n",
    "        # 3) pack to dense (no CLS yet)\n",
    "        x_pad, valid = to_dense_batch(x, data.batch)                     # (B,L0,D)\n",
    "        B, L0, D = x_pad.shape\n",
    "        key_padding = ~valid                                             # (B,L0) True == PAD\n",
    "\n",
    "        # 4) head-wise attention bias on L0 tokens (B,H,L0,L0), pre-CLS\n",
    "        #    Your AttnBiasFull typically supports SPD, geometry, adjacency, edges\n",
    "        hops = getattr(data, \"hops\", None)                               # (B,MAX_NODES,MAX_NODES) or None\n",
    "        ptr = getattr(data, \"ptr\", None)\n",
    "        attn_bias = self.bias_builder(\n",
    "            pos=(data.pos if hasattr(data, \"pos\") else None),\n",
    "            edge_index=data.edge_index,\n",
    "            edge_attr=(data.edge_attr if hasattr(data, \"edge_attr\") else None),\n",
    "            batch=data.batch,\n",
    "            key_padding_mask=key_padding,   # (B,L0), True=PAD\n",
    "            hops=getattr(data, \"hops\", None),\n",
    "            ptr=ptr\n",
    "        )  # (B,H,L0,L0)                                                # (B,H,L0,L0)\n",
    "\n",
    "        # 5) finalize bias (mask PAD rows/cols, keep diagonal 0), then optionally append CLS\n",
    "        B, H, L = attn_bias.shape[0], attn_bias.shape[1], attn_bias.shape[-1]\n",
    "        pad = key_padding                                                 # (B,L)\n",
    "        huge = attn_bias.new_tensor(-1e4)\n",
    "\n",
    "        # rows FROM PAD, cols TO PAD\n",
    "        attn_bias = attn_bias.masked_fill(pad.view(B, 1, L, 1), huge)\n",
    "        attn_bias = attn_bias.masked_fill(pad.view(B, 1, 1, L), huge)\n",
    "\n",
    "        # keep diagonal = 0 on valid tokens\n",
    "        I = torch.eye(L, device=attn_bias.device, dtype=torch.bool).view(1, 1, L, L)\n",
    "        attn_bias = torch.where(I, attn_bias.new_zeros(()), attn_bias)\n",
    "\n",
    "        # (optional) append CLS token at the end\n",
    "        if getattr(self, \"use_cls\", False):\n",
    "            # append CLS embedding\n",
    "            cls = self.cls_token.expand(B, 1, D)                         # (B,1,D)\n",
    "            x_pad = torch.cat([x_pad, cls], dim=1)                       # (B,L+1,D)\n",
    "\n",
    "            # extend key_padding: CLS is always valid (False)\n",
    "            key_padding = torch.cat(\n",
    "                [key_padding, torch.zeros(B, 1, dtype=torch.bool, device=x_pad.device)],\n",
    "                dim=1\n",
    "            )                                                             # (B,L+1)\n",
    "\n",
    "            # pad bias by one row/col with zeros for CLS -> (B,H,L+1,L+1)\n",
    "            attn_bias = F.pad(attn_bias, (0, 1, 0, 1), value=0.0)\n",
    "            L = L + 1\n",
    "\n",
    "        # 6) transformer encoder with 3D additive mask (B*H,L,L)\n",
    "        attn_mask_3d = attn_bias.reshape(B * H, L, L).to(x_pad.dtype)\n",
    "        h = self.encoder(                                                # returns (B,L,D) when batch_first=True\n",
    "            x_pad,\n",
    "            mask=attn_mask_3d,\n",
    "            src_key_padding_mask=key_padding\n",
    "        )\n",
    "\n",
    "        # 7) pooling (mean + max + gated attention), plus optional CLS; then RDKit/has_xyz and head\n",
    "        # exclude CLS from token pools\n",
    "        h_tok = h[:, :L0, :]                                             # (B,L0,D)\n",
    "        mask_f = valid.float()                                           # (B,L0)\n",
    "\n",
    "        mean = (h_tok * mask_f.unsqueeze(-1)).sum(1) / (mask_f.sum(1, keepdim=True) + 1e-8)  # (B,D)\n",
    "        mmax, _ = (h_tok + (1.0 - mask_f.unsqueeze(-1)) * (-1e4)).max(dim=1)                 # (B,D)\n",
    "\n",
    "        gate_logits = self.gate_pool(h_tok).squeeze(-1)                  # (B,L0)\n",
    "        gate = torch.softmax(gate_logits.masked_fill(~valid, -1e4), dim=1)\n",
    "        attn_pool = (h_tok * gate.unsqueeze(-1)).sum(1)                  # (B,D)\n",
    "\n",
    "        parts = [mean, mmax, attn_pool]\n",
    "\n",
    "        if getattr(self, \"use_cls\", False):\n",
    "            parts.append(h[:, L-1, :])                                   # CLS vector (B,D)\n",
    "\n",
    "        # RDKit globals\n",
    "        rd = data.rdkit_feats.view(B, -1).float()                        # (B, rdkit_dim)\n",
    "        parts.append(rd)\n",
    "\n",
    "        # optional has_xyz scalar if present\n",
    "        if getattr(self, \"use_has_xyz\", False) and hasattr(data, \"has_xyz\"):\n",
    "            parts.append(data.has_xyz.view(B, 1).float())\n",
    "\n",
    "        out = torch.cat(parts, dim=1)\n",
    "        return self.head(out)                                            # (B,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9de38ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | tr_MSE 22925.54102 | val_MAE 103.70203 | val_RMSE 140.37576 | R2 -0.7455\n",
      "Epoch 002 | tr_MSE 22925.47852 | val_MAE 103.70203 | val_RMSE 140.37576 | R2 -0.7455\n",
      "Epoch 003 | tr_MSE 22925.38867 | val_MAE 103.70203 | val_RMSE 140.37576 | R2 -0.7455\n",
      "Epoch 004 | tr_MSE 22925.17969 | val_MAE 103.70203 | val_RMSE 140.37576 | R2 -0.7455\n",
      "Epoch 005 | tr_MSE 22925.62695 | val_MAE 103.70203 | val_RMSE 140.37576 | R2 -0.7455\n",
      "Epoch 006 | tr_MSE 22925.28125 | val_MAE 103.70203 | val_RMSE 140.37576 | R2 -0.7455\n",
      "Epoch 007 | tr_MSE 22925.49609 | val_MAE 103.70203 | val_RMSE 140.37576 | R2 -0.7455\n",
      "Epoch 008 | tr_MSE 22925.65820 | val_MAE 103.70203 | val_RMSE 140.37576 | R2 -0.7455\n",
      "Epoch 009 | tr_MSE 22925.25391 | val_MAE 103.35062 | val_RMSE 140.03253 | R2 -0.7370\n",
      "Epoch 010 | tr_MSE 22817.98047 | val_MAE 102.86918 | val_RMSE 139.55515 | R2 -0.7252\n",
      "Epoch 011 | tr_MSE 22670.12109 | val_MAE 102.23938 | val_RMSE 138.92581 | R2 -0.7097\n",
      "Epoch 012 | tr_MSE 22477.48047 | val_MAE 102.23938 | val_RMSE 138.92581 | R2 -0.7097\n",
      "Epoch 013 | tr_MSE 22481.19727 | val_MAE 101.36304 | val_RMSE 138.02545 | R2 -0.6876\n",
      "Epoch 014 | tr_MSE 22207.52344 | val_MAE 101.36304 | val_RMSE 138.02545 | R2 -0.6876\n",
      "Epoch 015 | tr_MSE 22209.09766 | val_MAE 100.20683 | val_RMSE 136.79640 | R2 -0.6577\n",
      "Epoch 016 | tr_MSE 21839.61328 | val_MAE 98.73014 | val_RMSE 135.22623 | R2 -0.6198\n",
      "Epoch 017 | tr_MSE 21367.52539 | val_MAE 97.02119 | val_RMSE 133.32976 | R2 -0.5747\n",
      "Epoch 018 | tr_MSE 20812.10352 | val_MAE 97.02119 | val_RMSE 133.32976 | R2 -0.5747\n",
      "Epoch 019 | tr_MSE 20808.93750 | val_MAE 95.15020 | val_RMSE 131.13521 | R2 -0.5233\n",
      "Epoch 020 | tr_MSE 20164.50977 | val_MAE 93.24226 | val_RMSE 128.64632 | R2 -0.4660\n",
      "Epoch 021 | tr_MSE 19438.12305 | val_MAE 91.23161 | val_RMSE 125.89793 | R2 -0.4040\n",
      "Epoch 022 | tr_MSE 18627.92969 | val_MAE 89.12268 | val_RMSE 122.94543 | R2 -0.3390\n",
      "Epoch 023 | tr_MSE 17760.97461 | val_MAE 86.94582 | val_RMSE 119.87305 | R2 -0.2729\n",
      "Epoch 024 | tr_MSE 16948.57617 | val_MAE 85.00904 | val_RMSE 116.79353 | R2 -0.2083\n",
      "Epoch 025 | tr_MSE 16068.88574 | val_MAE 83.27553 | val_RMSE 113.83878 | R2 -0.1480\n",
      "Epoch 026 | tr_MSE 15252.36426 | val_MAE 81.71368 | val_RMSE 111.15374 | R2 -0.0944\n",
      "Epoch 027 | tr_MSE 14524.29004 | val_MAE 80.68275 | val_RMSE 108.90159 | R2 -0.0505\n",
      "Epoch 028 | tr_MSE 13837.21387 | val_MAE 80.54211 | val_RMSE 107.26315 | R2 -0.0192\n",
      "Epoch 029 | tr_MSE 13279.56445 | val_MAE 81.86011 | val_RMSE 106.42650 | R2 -0.0033\n",
      "Epoch 030 | tr_MSE 12866.66992 | val_MAE 84.06229 | val_RMSE 106.56810 | R2 -0.0060\n",
      "Epoch 031 | tr_MSE 12630.60547 | val_MAE 87.14660 | val_RMSE 107.78180 | R2 -0.0290\n",
      "Epoch 032 | tr_MSE 12702.89453 | val_MAE 90.02894 | val_RMSE 109.48213 | R2 -0.0618\n",
      "Epoch 033 | tr_MSE 12702.46777 | val_MAE 92.18420 | val_RMSE 111.07887 | R2 -0.0930\n",
      "Epoch 034 | tr_MSE 13093.08789 | val_MAE 93.43378 | val_RMSE 112.14225 | R2 -0.1140\n",
      "Epoch 035 | tr_MSE 13362.61133 | val_MAE 93.43377 | val_RMSE 112.14225 | R2 -0.1140\n",
      "Epoch 036 | tr_MSE 13373.95312 | val_MAE 93.78806 | val_RMSE 112.44722 | R2 -0.1201\n",
      "Epoch 037 | tr_MSE 13219.52637 | val_MAE 93.27193 | val_RMSE 111.99472 | R2 -0.1111\n",
      "Epoch 038 | tr_MSE 13149.06348 | val_MAE 92.01593 | val_RMSE 110.92865 | R2 -0.0900\n",
      "Epoch 039 | tr_MSE 13126.79004 | val_MAE 90.13056 | val_RMSE 109.51572 | R2 -0.0624\n",
      "Epoch 040 | tr_MSE 12890.66309 | val_MAE 87.82042 | val_RMSE 108.04854 | R2 -0.0341\n",
      "Epoch 041 | tr_MSE 12646.03809 | val_MAE 85.18188 | val_RMSE 106.82582 | R2 -0.0109\n",
      "Epoch 042 | tr_MSE 12501.97363 | val_MAE 83.01067 | val_RMSE 106.19209 | R2 0.0011\n",
      "Epoch 043 | tr_MSE 12605.75781 | val_MAE 81.92037 | val_RMSE 106.17834 | R2 0.0013\n",
      "Early stopping.\n",
      "[graphtransformer_tg_spd] Best Val — MAE 80.542114 | RMSE 107.263145 | R2 -0.0192\n",
      "Epoch 001 | tr_MSE 0.96115 | val_MAE 0.97492 | val_RMSE 0.98404 | R2 -54.1433\n",
      "Epoch 002 | tr_MSE 0.96717 | val_MAE 0.80153 | val_RMSE 0.81329 | R2 -36.6671\n",
      "Epoch 003 | tr_MSE 0.66756 | val_MAE 0.59354 | val_RMSE 0.61243 | R2 -20.3588\n",
      "Epoch 004 | tr_MSE 0.37116 | val_MAE 0.31183 | val_RMSE 0.35282 | R2 -6.0890\n",
      "Epoch 005 | tr_MSE 0.12951 | val_MAE 0.20292 | val_RMSE 0.25674 | R2 -2.7537\n",
      "Epoch 006 | tr_MSE 0.08157 | val_MAE 0.22746 | val_RMSE 0.27131 | R2 -3.1917\n",
      "Epoch 007 | tr_MSE 0.09901 | val_MAE 0.11116 | val_RMSE 0.13785 | R2 -0.0821\n",
      "Epoch 008 | tr_MSE 0.03438 | val_MAE 0.25521 | val_RMSE 0.31312 | R2 -4.5832\n",
      "Epoch 009 | tr_MSE 0.09809 | val_MAE 0.21123 | val_RMSE 0.27062 | R2 -3.1704\n",
      "Epoch 010 | tr_MSE 0.07679 | val_MAE 0.11871 | val_RMSE 0.14443 | R2 -0.1879\n",
      "Epoch 011 | tr_MSE 0.03256 | val_MAE 0.17302 | val_RMSE 0.19636 | R2 -1.1956\n",
      "Epoch 012 | tr_MSE 0.05035 | val_MAE 0.13533 | val_RMSE 0.16067 | R2 -0.4700\n",
      "Epoch 013 | tr_MSE 0.03576 | val_MAE 0.12263 | val_RMSE 0.16684 | R2 -0.5851\n",
      "Epoch 014 | tr_MSE 0.03318 | val_MAE 0.14019 | val_RMSE 0.18997 | R2 -1.0551\n",
      "Epoch 015 | tr_MSE 0.04084 | val_MAE 0.10628 | val_RMSE 0.14313 | R2 -0.1666\n",
      "Epoch 016 | tr_MSE 0.02593 | val_MAE 0.11065 | val_RMSE 0.13351 | R2 -0.0151\n",
      "Epoch 017 | tr_MSE 0.02574 | val_MAE 0.12211 | val_RMSE 0.14604 | R2 -0.2146\n",
      "Epoch 018 | tr_MSE 0.03108 | val_MAE 0.10704 | val_RMSE 0.13009 | R2 0.0362\n",
      "Epoch 019 | tr_MSE 0.02551 | val_MAE 0.10115 | val_RMSE 0.13575 | R2 -0.0495\n",
      "Epoch 020 | tr_MSE 0.02314 | val_MAE 0.11006 | val_RMSE 0.15289 | R2 -0.3312\n",
      "Epoch 021 | tr_MSE 0.02871 | val_MAE 0.10295 | val_RMSE 0.14173 | R2 -0.1440\n",
      "Epoch 022 | tr_MSE 0.02655 | val_MAE 0.09689 | val_RMSE 0.12445 | R2 0.1180\n",
      "Epoch 023 | tr_MSE 0.02366 | val_MAE 0.10748 | val_RMSE 0.13120 | R2 0.0197\n",
      "Epoch 024 | tr_MSE 0.02607 | val_MAE 0.10708 | val_RMSE 0.13077 | R2 0.0262\n",
      "Epoch 025 | tr_MSE 0.02475 | val_MAE 0.09542 | val_RMSE 0.12138 | R2 0.1611\n",
      "Epoch 026 | tr_MSE 0.02308 | val_MAE 0.09402 | val_RMSE 0.12823 | R2 0.0636\n",
      "Epoch 027 | tr_MSE 0.02323 | val_MAE 0.09593 | val_RMSE 0.13443 | R2 -0.0292\n",
      "Epoch 028 | tr_MSE 0.02438 | val_MAE 0.09060 | val_RMSE 0.12338 | R2 0.1331\n",
      "Epoch 029 | tr_MSE 0.02062 | val_MAE 0.09235 | val_RMSE 0.11641 | R2 0.2283\n",
      "Epoch 030 | tr_MSE 0.02041 | val_MAE 0.09911 | val_RMSE 0.12119 | R2 0.1637\n",
      "Epoch 031 | tr_MSE 0.02294 | val_MAE 0.09002 | val_RMSE 0.11359 | R2 0.2652\n",
      "Epoch 032 | tr_MSE 0.01877 | val_MAE 0.08265 | val_RMSE 0.11644 | R2 0.2278\n",
      "Epoch 033 | tr_MSE 0.02025 | val_MAE 0.07906 | val_RMSE 0.11282 | R2 0.2751\n",
      "Epoch 034 | tr_MSE 0.01907 | val_MAE 0.08236 | val_RMSE 0.10673 | R2 0.3513\n",
      "Epoch 035 | tr_MSE 0.01725 | val_MAE 0.08708 | val_RMSE 0.10983 | R2 0.3131\n",
      "Epoch 036 | tr_MSE 0.01756 | val_MAE 0.07657 | val_RMSE 0.10266 | R2 0.3998\n",
      "Epoch 037 | tr_MSE 0.01589 | val_MAE 0.07100 | val_RMSE 0.10185 | R2 0.4093\n",
      "Epoch 038 | tr_MSE 0.01671 | val_MAE 0.07007 | val_RMSE 0.09803 | R2 0.4528\n",
      "Epoch 039 | tr_MSE 0.01538 | val_MAE 0.08665 | val_RMSE 0.11073 | R2 0.3017\n",
      "Epoch 040 | tr_MSE 0.01589 | val_MAE 0.07344 | val_RMSE 0.09972 | R2 0.4337\n",
      "Epoch 041 | tr_MSE 0.01322 | val_MAE 0.06241 | val_RMSE 0.09234 | R2 0.5144\n",
      "Epoch 042 | tr_MSE 0.01365 | val_MAE 0.08632 | val_RMSE 0.11206 | R2 0.2848\n",
      "Epoch 043 | tr_MSE 0.01230 | val_MAE 0.08611 | val_RMSE 0.11243 | R2 0.2802\n",
      "Epoch 044 | tr_MSE 0.01233 | val_MAE 0.06616 | val_RMSE 0.09430 | R2 0.4936\n",
      "Epoch 045 | tr_MSE 0.01170 | val_MAE 0.09222 | val_RMSE 0.11769 | R2 0.2113\n",
      "Epoch 046 | tr_MSE 0.01114 | val_MAE 0.09666 | val_RMSE 0.12103 | R2 0.1658\n",
      "Epoch 047 | tr_MSE 0.01022 | val_MAE 0.08884 | val_RMSE 0.11398 | R2 0.2602\n",
      "Epoch 048 | tr_MSE 0.00995 | val_MAE 0.11659 | val_RMSE 0.14094 | R2 -0.1311\n",
      "Epoch 049 | tr_MSE 0.00919 | val_MAE 0.10134 | val_RMSE 0.12810 | R2 0.0656\n",
      "Epoch 050 | tr_MSE 0.00881 | val_MAE 0.19268 | val_RMSE 0.21675 | R2 -1.6753\n",
      "Epoch 051 | tr_MSE 0.01205 | val_MAE 0.05782 | val_RMSE 0.08610 | R2 0.5778\n",
      "Epoch 052 | tr_MSE 0.01852 | val_MAE 0.08664 | val_RMSE 0.11368 | R2 0.2640\n",
      "Epoch 053 | tr_MSE 0.00827 | val_MAE 0.15155 | val_RMSE 0.17444 | R2 -0.7329\n",
      "Epoch 054 | tr_MSE 0.01663 | val_MAE 0.09902 | val_RMSE 0.12641 | R2 0.0900\n",
      "Epoch 055 | tr_MSE 0.00961 | val_MAE 0.06333 | val_RMSE 0.09543 | R2 0.4814\n",
      "Epoch 056 | tr_MSE 0.01051 | val_MAE 0.06366 | val_RMSE 0.10274 | R2 0.3989\n",
      "Epoch 057 | tr_MSE 0.01068 | val_MAE 0.12767 | val_RMSE 0.16662 | R2 -0.5810\n",
      "Epoch 058 | tr_MSE 0.00930 | val_MAE 0.14119 | val_RMSE 0.17364 | R2 -0.7170\n",
      "Epoch 059 | tr_MSE 0.00867 | val_MAE 0.08125 | val_RMSE 0.11223 | R2 0.2828\n",
      "Epoch 060 | tr_MSE 0.00738 | val_MAE 0.06605 | val_RMSE 0.09325 | R2 0.5048\n",
      "Epoch 061 | tr_MSE 0.00839 | val_MAE 0.07059 | val_RMSE 0.09689 | R2 0.4654\n",
      "Epoch 062 | tr_MSE 0.00798 | val_MAE 0.09068 | val_RMSE 0.11715 | R2 0.2184\n",
      "Epoch 063 | tr_MSE 0.00853 | val_MAE 0.11045 | val_RMSE 0.13764 | R2 -0.0788\n",
      "Epoch 064 | tr_MSE 0.00801 | val_MAE 0.10576 | val_RMSE 0.13694 | R2 -0.0679\n",
      "Epoch 065 | tr_MSE 0.00707 | val_MAE 0.08961 | val_RMSE 0.12916 | R2 0.0500\n",
      "Epoch 066 | tr_MSE 0.00747 | val_MAE 0.10065 | val_RMSE 0.14819 | R2 -0.2505\n",
      "Epoch 067 | tr_MSE 0.00740 | val_MAE 0.13840 | val_RMSE 0.18704 | R2 -0.9922\n",
      "Epoch 068 | tr_MSE 0.00634 | val_MAE 0.15572 | val_RMSE 0.20181 | R2 -1.3193\n",
      "Epoch 069 | tr_MSE 0.00720 | val_MAE 0.13313 | val_RMSE 0.17534 | R2 -0.7507\n",
      "Epoch 070 | tr_MSE 0.00578 | val_MAE 0.11293 | val_RMSE 0.15083 | R2 -0.2954\n",
      "Epoch 071 | tr_MSE 0.00630 | val_MAE 0.11407 | val_RMSE 0.14772 | R2 -0.2426\n",
      "Epoch 072 | tr_MSE 0.00566 | val_MAE 0.13048 | val_RMSE 0.16058 | R2 -0.4685\n",
      "Epoch 073 | tr_MSE 0.00569 | val_MAE 0.14413 | val_RMSE 0.17292 | R2 -0.7028\n",
      "Epoch 074 | tr_MSE 0.00557 | val_MAE 0.13802 | val_RMSE 0.16853 | R2 -0.6174\n",
      "Epoch 075 | tr_MSE 0.00558 | val_MAE 0.13370 | val_RMSE 0.16757 | R2 -0.5990\n",
      "Epoch 076 | tr_MSE 0.00571 | val_MAE 0.15406 | val_RMSE 0.19003 | R2 -1.0564\n",
      "Epoch 077 | tr_MSE 0.00516 | val_MAE 0.18308 | val_RMSE 0.21993 | R2 -1.7546\n",
      "Epoch 078 | tr_MSE 0.00500 | val_MAE 0.17544 | val_RMSE 0.21230 | R2 -1.5666\n",
      "Epoch 079 | tr_MSE 0.00556 | val_MAE 0.15980 | val_RMSE 0.19679 | R2 -1.2054\n",
      "Epoch 080 | tr_MSE 0.00553 | val_MAE 0.17554 | val_RMSE 0.21037 | R2 -1.5201\n",
      "Epoch 081 | tr_MSE 0.00589 | val_MAE 0.19864 | val_RMSE 0.23120 | R2 -2.0439\n",
      "Early stopping.\n",
      "[graphtransformer_den_spd] Best Val — MAE 0.057824 | RMSE 0.086103 | R2 0.5778\n",
      "Epoch 001 | tr_MSE 288.02106 | val_MAE 16.69804 | val_RMSE 17.34621 | R2 -12.6714\n",
      "Epoch 002 | tr_MSE 287.93884 | val_MAE 16.69804 | val_RMSE 17.34621 | R2 -12.6714\n",
      "Epoch 003 | tr_MSE 288.10129 | val_MAE 16.69804 | val_RMSE 17.34621 | R2 -12.6714\n",
      "Epoch 004 | tr_MSE 288.13690 | val_MAE 16.69804 | val_RMSE 17.34621 | R2 -12.6714\n",
      "Epoch 005 | tr_MSE 288.11478 | val_MAE 16.69804 | val_RMSE 17.34621 | R2 -12.6714\n",
      "Epoch 006 | tr_MSE 288.08160 | val_MAE 16.69804 | val_RMSE 17.34621 | R2 -12.6714\n",
      "Epoch 007 | tr_MSE 287.97971 | val_MAE 16.11971 | val_RMSE 16.80033 | R2 -11.8245\n",
      "Epoch 008 | tr_MSE 269.88989 | val_MAE 15.28573 | val_RMSE 16.02038 | R2 -10.6614\n",
      "Epoch 009 | tr_MSE 245.31140 | val_MAE 14.03313 | val_RMSE 14.86217 | R2 -9.0362\n",
      "Epoch 010 | tr_MSE 211.03352 | val_MAE 12.11286 | val_RMSE 13.11233 | R2 -6.8120\n",
      "Epoch 011 | tr_MSE 163.82678 | val_MAE 9.56714 | val_RMSE 10.86062 | R2 -4.3594\n",
      "Epoch 012 | tr_MSE 112.14138 | val_MAE 6.37935 | val_RMSE 8.22685 | R2 -2.0752\n",
      "Epoch 013 | tr_MSE 63.02776 | val_MAE 4.52863 | val_RMSE 5.83462 | R2 -0.5468\n",
      "Epoch 014 | tr_MSE 32.27089 | val_MAE 5.23090 | val_RMSE 5.89095 | R2 -0.5768\n",
      "Epoch 015 | tr_MSE 35.46404 | val_MAE 6.71754 | val_RMSE 7.86231 | R2 -1.8087\n",
      "Epoch 016 | tr_MSE 65.40265 | val_MAE 6.71754 | val_RMSE 7.86231 | R2 -1.8087\n",
      "Epoch 017 | tr_MSE 64.66169 | val_MAE 8.14462 | val_RMSE 9.33623 | R2 -2.9605\n",
      "Epoch 018 | tr_MSE 92.72720 | val_MAE 8.14462 | val_RMSE 9.33623 | R2 -2.9605\n",
      "Epoch 019 | tr_MSE 94.92559 | val_MAE 8.60495 | val_RMSE 9.73792 | R2 -3.3086\n",
      "Epoch 020 | tr_MSE 101.10811 | val_MAE 7.97627 | val_RMSE 9.05800 | R2 -2.7279\n",
      "Epoch 021 | tr_MSE 88.85438 | val_MAE 6.46119 | val_RMSE 7.52540 | R2 -1.5731\n",
      "Epoch 022 | tr_MSE 61.64120 | val_MAE 4.82555 | val_RMSE 5.65355 | R2 -0.4523\n",
      "Epoch 023 | tr_MSE 35.08192 | val_MAE 3.93832 | val_RMSE 4.71372 | R2 -0.0096\n",
      "Epoch 024 | tr_MSE 21.42058 | val_MAE 4.58569 | val_RMSE 6.24692 | R2 -0.7731\n",
      "Epoch 025 | tr_MSE 33.96732 | val_MAE 5.93915 | val_RMSE 7.50621 | R2 -1.5600\n",
      "Epoch 026 | tr_MSE 49.29431 | val_MAE 6.15448 | val_RMSE 7.67971 | R2 -1.6798\n",
      "Epoch 027 | tr_MSE 51.48702 | val_MAE 5.36601 | val_RMSE 7.02517 | R2 -1.2424\n",
      "Epoch 028 | tr_MSE 43.26491 | val_MAE 4.28161 | val_RMSE 5.90030 | R2 -0.5818\n",
      "Epoch 029 | tr_MSE 30.26169 | val_MAE 3.79168 | val_RMSE 4.85009 | R2 -0.0688\n",
      "Epoch 030 | tr_MSE 21.98730 | val_MAE 4.18608 | val_RMSE 4.83124 | R2 -0.0605\n",
      "Epoch 031 | tr_MSE 24.47626 | val_MAE 4.70779 | val_RMSE 5.52983 | R2 -0.3894\n",
      "Epoch 032 | tr_MSE 34.05347 | val_MAE 5.07968 | val_RMSE 5.99112 | R2 -0.6309\n",
      "Epoch 033 | tr_MSE 39.61243 | val_MAE 5.08994 | val_RMSE 6.00247 | R2 -0.6371\n",
      "Epoch 034 | tr_MSE 40.54022 | val_MAE 4.77796 | val_RMSE 5.61878 | R2 -0.4345\n",
      "Epoch 035 | tr_MSE 35.27516 | val_MAE 4.33438 | val_RMSE 5.03531 | R2 -0.1520\n",
      "Epoch 036 | tr_MSE 27.49022 | val_MAE 3.94203 | val_RMSE 4.63252 | R2 0.0249\n",
      "Epoch 037 | tr_MSE 22.07623 | val_MAE 3.76118 | val_RMSE 4.94274 | R2 -0.1100\n",
      "Epoch 038 | tr_MSE 22.45922 | val_MAE 3.93245 | val_RMSE 5.43653 | R2 -0.3429\n",
      "Epoch 039 | tr_MSE 26.53628 | val_MAE 4.04520 | val_RMSE 5.61044 | R2 -0.4302\n",
      "Epoch 040 | tr_MSE 28.28954 | val_MAE 3.93207 | val_RMSE 5.43550 | R2 -0.3424\n",
      "Epoch 041 | tr_MSE 25.67980 | val_MAE 3.78219 | val_RMSE 5.04684 | R2 -0.1573\n",
      "Epoch 042 | tr_MSE 23.20874 | val_MAE 3.78690 | val_RMSE 4.67867 | R2 0.0054\n",
      "Epoch 043 | tr_MSE 21.69350 | val_MAE 4.03397 | val_RMSE 4.68163 | R2 0.0041\n",
      "Epoch 044 | tr_MSE 22.41315 | val_MAE 4.24032 | val_RMSE 4.91640 | R2 -0.0982\n",
      "Epoch 045 | tr_MSE 25.54186 | val_MAE 4.31945 | val_RMSE 5.02290 | R2 -0.1463\n",
      "Epoch 046 | tr_MSE 27.46516 | val_MAE 4.24633 | val_RMSE 4.92729 | R2 -0.1031\n",
      "Epoch 047 | tr_MSE 25.85182 | val_MAE 4.07226 | val_RMSE 4.71527 | R2 -0.0102\n",
      "Epoch 048 | tr_MSE 22.52451 | val_MAE 3.84953 | val_RMSE 4.59027 | R2 0.0426\n",
      "Epoch 049 | tr_MSE 20.85786 | val_MAE 3.72839 | val_RMSE 4.82860 | R2 -0.0594\n",
      "Epoch 050 | tr_MSE 20.96448 | val_MAE 3.78921 | val_RMSE 5.13629 | R2 -0.1987\n",
      "Epoch 051 | tr_MSE 24.42919 | val_MAE 3.82183 | val_RMSE 5.24561 | R2 -0.2502\n",
      "Epoch 052 | tr_MSE 24.98242 | val_MAE 3.77830 | val_RMSE 5.12131 | R2 -0.1917\n",
      "Epoch 053 | tr_MSE 23.53674 | val_MAE 3.70821 | val_RMSE 4.84340 | R2 -0.0659\n",
      "Epoch 054 | tr_MSE 21.70641 | val_MAE 3.75639 | val_RMSE 4.58949 | R2 0.0430\n",
      "Epoch 055 | tr_MSE 19.86410 | val_MAE 3.87743 | val_RMSE 4.55857 | R2 0.0558\n",
      "Epoch 056 | tr_MSE 20.96613 | val_MAE 3.93538 | val_RMSE 4.58907 | R2 0.0431\n",
      "Epoch 057 | tr_MSE 22.24224 | val_MAE 3.88383 | val_RMSE 4.54693 | R2 0.0606\n",
      "Epoch 058 | tr_MSE 21.35851 | val_MAE 3.74518 | val_RMSE 4.48491 | R2 0.0861\n",
      "Epoch 059 | tr_MSE 19.32211 | val_MAE 3.62428 | val_RMSE 4.60667 | R2 0.0358\n",
      "Epoch 060 | tr_MSE 19.36121 | val_MAE 3.60253 | val_RMSE 4.76115 | R2 -0.0300\n",
      "Epoch 061 | tr_MSE 21.13775 | val_MAE 3.58559 | val_RMSE 4.75799 | R2 -0.0286\n",
      "Epoch 062 | tr_MSE 20.44175 | val_MAE 3.55525 | val_RMSE 4.59294 | R2 0.0415\n",
      "Epoch 063 | tr_MSE 19.51413 | val_MAE 3.59370 | val_RMSE 4.39936 | R2 0.1206\n",
      "Epoch 064 | tr_MSE 18.88621 | val_MAE 3.56525 | val_RMSE 4.32700 | R2 0.1493\n",
      "Epoch 065 | tr_MSE 18.30951 | val_MAE 3.45329 | val_RMSE 4.30920 | R2 0.1563\n",
      "Epoch 066 | tr_MSE 18.22085 | val_MAE 3.36136 | val_RMSE 4.27027 | R2 0.1715\n",
      "Epoch 067 | tr_MSE 16.86438 | val_MAE 3.28380 | val_RMSE 4.14946 | R2 0.2177\n",
      "Epoch 068 | tr_MSE 16.69736 | val_MAE 3.18449 | val_RMSE 4.00493 | R2 0.2712\n",
      "Epoch 069 | tr_MSE 16.33460 | val_MAE 3.01597 | val_RMSE 4.02686 | R2 0.2632\n",
      "Epoch 070 | tr_MSE 14.90195 | val_MAE 2.92334 | val_RMSE 3.94802 | R2 0.2918\n",
      "Epoch 071 | tr_MSE 14.93201 | val_MAE 2.76191 | val_RMSE 3.67483 | R2 0.3864\n",
      "Epoch 072 | tr_MSE 13.66599 | val_MAE 2.68503 | val_RMSE 3.60190 | R2 0.4105\n",
      "Epoch 073 | tr_MSE 13.75149 | val_MAE 2.74824 | val_RMSE 3.51709 | R2 0.4380\n",
      "Epoch 074 | tr_MSE 14.34142 | val_MAE 2.66252 | val_RMSE 3.52237 | R2 0.4363\n",
      "Epoch 075 | tr_MSE 14.11068 | val_MAE 2.78687 | val_RMSE 3.80346 | R2 0.3427\n",
      "Epoch 076 | tr_MSE 15.50491 | val_MAE 2.61662 | val_RMSE 3.56021 | R2 0.4241\n",
      "Epoch 077 | tr_MSE 14.01262 | val_MAE 3.28365 | val_RMSE 4.08348 | R2 0.2424\n",
      "Epoch 078 | tr_MSE 19.03446 | val_MAE 4.04404 | val_RMSE 4.80569 | R2 -0.0493\n",
      "Epoch 079 | tr_MSE 25.13451 | val_MAE 3.93577 | val_RMSE 4.69564 | R2 -0.0018\n",
      "Epoch 080 | tr_MSE 24.92685 | val_MAE 3.18556 | val_RMSE 3.96415 | R2 0.2860\n",
      "Epoch 081 | tr_MSE 17.63982 | val_MAE 2.57262 | val_RMSE 3.42695 | R2 0.4664\n",
      "Epoch 082 | tr_MSE 12.70157 | val_MAE 3.00117 | val_RMSE 4.13360 | R2 0.2236\n",
      "Epoch 083 | tr_MSE 16.55058 | val_MAE 3.42376 | val_RMSE 4.56119 | R2 0.0547\n",
      "Epoch 084 | tr_MSE 20.57142 | val_MAE 3.16122 | val_RMSE 4.29101 | R2 0.1634\n",
      "Epoch 085 | tr_MSE 17.78404 | val_MAE 2.65720 | val_RMSE 3.68949 | R2 0.3815\n",
      "Epoch 086 | tr_MSE 13.44664 | val_MAE 2.78789 | val_RMSE 3.58617 | R2 0.4157\n",
      "Epoch 087 | tr_MSE 13.70597 | val_MAE 3.13132 | val_RMSE 3.87665 | R2 0.3172\n",
      "Epoch 088 | tr_MSE 16.41222 | val_MAE 3.17491 | val_RMSE 3.91287 | R2 0.3043\n",
      "Epoch 089 | tr_MSE 16.12728 | val_MAE 2.93750 | val_RMSE 3.67911 | R2 0.3850\n",
      "Epoch 090 | tr_MSE 14.12535 | val_MAE 2.62703 | val_RMSE 3.55282 | R2 0.4265\n",
      "Epoch 091 | tr_MSE 12.64483 | val_MAE 2.64847 | val_RMSE 3.71495 | R2 0.3729\n",
      "Epoch 092 | tr_MSE 13.43907 | val_MAE 2.64959 | val_RMSE 3.75459 | R2 0.3595\n",
      "Epoch 093 | tr_MSE 14.24518 | val_MAE 2.53701 | val_RMSE 3.58962 | R2 0.4145\n",
      "Epoch 094 | tr_MSE 12.98556 | val_MAE 2.60916 | val_RMSE 3.48878 | R2 0.4470\n",
      "Epoch 095 | tr_MSE 13.16232 | val_MAE 2.65418 | val_RMSE 3.49165 | R2 0.4461\n",
      "Epoch 096 | tr_MSE 13.03982 | val_MAE 2.51372 | val_RMSE 3.42237 | R2 0.4678\n",
      "Epoch 097 | tr_MSE 11.85149 | val_MAE 2.56669 | val_RMSE 3.59462 | R2 0.4129\n",
      "Epoch 098 | tr_MSE 12.76190 | val_MAE 2.67829 | val_RMSE 3.72156 | R2 0.3707\n",
      "Epoch 099 | tr_MSE 13.76585 | val_MAE 2.58205 | val_RMSE 3.60810 | R2 0.4085\n",
      "Epoch 100 | tr_MSE 13.38844 | val_MAE 2.42944 | val_RMSE 3.40609 | R2 0.4729\n",
      "Epoch 101 | tr_MSE 12.05155 | val_MAE 2.55211 | val_RMSE 3.41314 | R2 0.4707\n",
      "Epoch 102 | tr_MSE 12.03002 | val_MAE 2.60415 | val_RMSE 3.49582 | R2 0.4447\n",
      "Epoch 103 | tr_MSE 12.27508 | val_MAE 2.36817 | val_RMSE 3.34221 | R2 0.4925\n",
      "Epoch 104 | tr_MSE 11.68460 | val_MAE 2.32280 | val_RMSE 3.46431 | R2 0.4547\n",
      "Epoch 105 | tr_MSE 12.90421 | val_MAE 2.37363 | val_RMSE 3.54043 | R2 0.4305\n",
      "Epoch 106 | tr_MSE 13.09648 | val_MAE 2.27719 | val_RMSE 3.39742 | R2 0.4756\n",
      "Epoch 107 | tr_MSE 12.84272 | val_MAE 2.36617 | val_RMSE 3.31765 | R2 0.4999\n",
      "Epoch 108 | tr_MSE 12.17471 | val_MAE 2.49188 | val_RMSE 3.37512 | R2 0.4824\n",
      "Epoch 109 | tr_MSE 12.59734 | val_MAE 2.43378 | val_RMSE 3.34283 | R2 0.4923\n",
      "Epoch 110 | tr_MSE 12.08298 | val_MAE 2.34372 | val_RMSE 3.36450 | R2 0.4857\n",
      "Epoch 111 | tr_MSE 11.74661 | val_MAE 2.36405 | val_RMSE 3.39963 | R2 0.4749\n",
      "Epoch 112 | tr_MSE 11.94960 | val_MAE 2.38969 | val_RMSE 3.34845 | R2 0.4906\n",
      "Epoch 113 | tr_MSE 11.53044 | val_MAE 2.59556 | val_RMSE 3.45051 | R2 0.4590\n",
      "Epoch 114 | tr_MSE 11.76025 | val_MAE 2.66305 | val_RMSE 3.49487 | R2 0.4450\n",
      "Epoch 115 | tr_MSE 12.23920 | val_MAE 2.49446 | val_RMSE 3.33430 | R2 0.4949\n",
      "Epoch 116 | tr_MSE 11.36473 | val_MAE 2.34538 | val_RMSE 3.27228 | R2 0.5135\n",
      "Epoch 117 | tr_MSE 11.47789 | val_MAE 2.35985 | val_RMSE 3.31992 | R2 0.4992\n",
      "Epoch 118 | tr_MSE 11.52656 | val_MAE 2.32386 | val_RMSE 3.27693 | R2 0.5121\n",
      "Epoch 119 | tr_MSE 11.49987 | val_MAE 2.47398 | val_RMSE 3.35361 | R2 0.4890\n",
      "Epoch 120 | tr_MSE 11.58410 | val_MAE 2.53054 | val_RMSE 3.39806 | R2 0.4754\n",
      "Epoch 121 | tr_MSE 11.90644 | val_MAE 2.39837 | val_RMSE 3.29460 | R2 0.5068\n",
      "Epoch 122 | tr_MSE 10.75798 | val_MAE 2.32006 | val_RMSE 3.31888 | R2 0.4995\n",
      "Epoch 123 | tr_MSE 11.45010 | val_MAE 2.35994 | val_RMSE 3.45738 | R2 0.4569\n",
      "Epoch 124 | tr_MSE 12.24461 | val_MAE 2.33140 | val_RMSE 3.37568 | R2 0.4822\n",
      "Epoch 125 | tr_MSE 11.34270 | val_MAE 2.43773 | val_RMSE 3.35620 | R2 0.4882\n",
      "Epoch 126 | tr_MSE 10.78341 | val_MAE 2.55468 | val_RMSE 3.44585 | R2 0.4605\n",
      "Epoch 127 | tr_MSE 11.52567 | val_MAE 2.50531 | val_RMSE 3.40518 | R2 0.4732\n",
      "Epoch 128 | tr_MSE 11.46763 | val_MAE 2.35263 | val_RMSE 3.30389 | R2 0.5040\n",
      "Epoch 129 | tr_MSE 11.14613 | val_MAE 2.32755 | val_RMSE 3.29378 | R2 0.5071\n",
      "Epoch 130 | tr_MSE 10.89734 | val_MAE 2.39920 | val_RMSE 3.31606 | R2 0.5004\n",
      "Epoch 131 | tr_MSE 10.38144 | val_MAE 2.37118 | val_RMSE 3.29573 | R2 0.5065\n",
      "Epoch 132 | tr_MSE 10.99095 | val_MAE 2.31190 | val_RMSE 3.28093 | R2 0.5109\n",
      "Epoch 133 | tr_MSE 10.91341 | val_MAE 2.33120 | val_RMSE 3.27590 | R2 0.5124\n",
      "Epoch 134 | tr_MSE 10.35474 | val_MAE 2.48176 | val_RMSE 3.36319 | R2 0.4861\n",
      "Epoch 135 | tr_MSE 11.14355 | val_MAE 2.50735 | val_RMSE 3.38534 | R2 0.4793\n",
      "Epoch 136 | tr_MSE 10.84808 | val_MAE 2.39275 | val_RMSE 3.29508 | R2 0.5067\n",
      "Early stopping.\n",
      "[graphtransformer_rg_spd] Best Val — MAE 2.277192 | RMSE 3.397420 | R2 0.4756\n"
     ]
    }
   ],
   "source": [
    "# introspect dims\n",
    "b = next(iter(train_loader_tg))\n",
    "rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "model_tg = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=6, dropout=0.1,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=False, spd_max=5,\n",
    "    use_adj_const=True, use_edge_bias=True,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_tg, ckpt_tg, met_tg = train_hybrid_gnn_sota(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=100, warmup_epochs=5, patience=15,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_tg_spd\", tag=\"graphtransformer_tg_spd\"\n",
    ")\n",
    "\n",
    "\n",
    "model_den = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=6, dropout=0.2,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=True, use_edge_bias=True,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_den, ckpt_den, met_den = train_hybrid_gnn_sota(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=6e-4, optimizer=\"Adam\", weight_decay=1e-5,\n",
    "    epochs=300, warmup_epochs=10, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_den_spd\", tag=\"graphtransformer_den_spd\"\n",
    ")\n",
    "\n",
    "\n",
    "model_rg = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=6, dropout=0.1,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=True, use_edge_bias=True,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_rg, ckpt_rg, met_rg = train_hybrid_gnn_sota(\n",
    "    model_rg, train_loader_rg, val_loader_rg,\n",
    "    lr=6e-4, optimizer=\"Adam\", weight_decay=1e-5,\n",
    "    epochs=300, warmup_epochs=10, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_rg_spd\", tag=\"graphtransformer_rg_spd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b7e9cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tg_cfg = {'gnn_dim': 256, 'hidden_dim': 512, 'dropout_rate': 0.34404144200017467, \n",
    "#           'lr': 0.0005555079210176292, 'activation': 'Swish', 'optimizer': 'RMSprop', \n",
    "#           'weight_decay': 9.056299733554687e-06}\n",
    "\n",
    "# model_tg = HybridGNN(\n",
    "#     gnn_dim=tg_cfg['gnn_dim'],\n",
    "#     rdkit_dim=15,\n",
    "#     hidden_dim=tg_cfg['hidden_dim'],\n",
    "#     dropout_rate=tg_cfg['dropout_rate'],\n",
    "#     activation=tg_cfg['activation']\n",
    "# )\n",
    "\n",
    "# model_tg, ckpt_tg, metrics_tg = train_hybrid_gnn(\n",
    "#     model_tg, train_loader_tg, val_loader_tg,\n",
    "#     lr=tg_cfg['lr'], optimizer=tg_cfg['optimizer'],\n",
    "#     weight_decay=tg_cfg['weight_decay'],\n",
    "#     epochs=120, patience=15,  \n",
    "#     save_dir=\"saved_models/gnn_tg\", tag=\"hybridgnn_tg\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# den_cfg = {'gnn_dim': 1024, 'hidden_dim': 384, 'dropout_rate': 0.3735260731607324,\n",
    "#            'lr': 5.956024201538505e-04, 'activation': 'Swish', 'optimizer': 'AdamW',\n",
    "#            'weight_decay': 8.619671341229739e-06}\n",
    "\n",
    "# model_den = HybridGNN(\n",
    "#     gnn_dim=den_cfg['gnn_dim'],\n",
    "#     rdkit_dim=15,\n",
    "#     hidden_dim=den_cfg['hidden_dim'],\n",
    "#     dropout_rate=den_cfg['dropout_rate'],\n",
    "#     activation=den_cfg['activation']\n",
    "# )\n",
    "# model_den, ckpt_den, metrics_den = train_hybrid_gnn(\n",
    "#     model_den, train_loader_den, val_loader_den,\n",
    "#     lr=den_cfg['lr'], optimizer=den_cfg['optimizer'],\n",
    "#     weight_decay=den_cfg['weight_decay'],\n",
    "#     epochs=120, patience=15,  \n",
    "#     save_dir=\"saved_models/gnn_density\", tag=\"hybridgnn_density\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rg_cfg = {'gnn_dim': 1024, 'hidden_dim': 384, 'dropout_rate': 0.3735260731607324,\n",
    "#            'lr': 5.956024201538505e-04, 'activation': 'Swish', 'optimizer': 'AdamW',\n",
    "#            'weight_decay': 8.619671341229739e-06}\n",
    "\n",
    "# model_rg = HybridGNN(\n",
    "#     gnn_dim=rg_cfg['gnn_dim'],\n",
    "#     rdkit_dim=15,\n",
    "#     hidden_dim=rg_cfg['hidden_dim'],\n",
    "#     dropout_rate=rg_cfg['dropout_rate'],\n",
    "#     activation=rg_cfg['activation']\n",
    "# )\n",
    "\n",
    "# model_rg, ckpt_rg, metrics_rg = train_hybrid_gnn(\n",
    "#     model_rg, train_loader_rg, val_loader_rg,\n",
    "#     lr=rg_cfg['lr'], optimizer=rg_cfg['optimizer'],\n",
    "#     weight_decay=rg_cfg['weight_decay'],\n",
    "#     epochs=120, patience=15,  \n",
    "#     save_dir=\"saved_models/gnn_rg\", tag=\"hybridgnn_rg\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffv_cfg = {'gnn_dim': 256, 'hidden_dim': 512, 'dropout_rate': 0.34404144200017467, \n",
    "#           'lr': 0.0005555079210176292, 'activation': 'Swish', 'optimizer': 'RMSprop', \n",
    "#           'weight_decay': 9.056299733554687e-06}\n",
    "\n",
    "# model_ffv = HybridGNN(\n",
    "#     gnn_dim=ffv_cfg['gnn_dim'],\n",
    "#     rdkit_dim=15,\n",
    "#     hidden_dim=ffv_cfg['hidden_dim'],\n",
    "#     dropout_rate=ffv_cfg['dropout_rate'],\n",
    "#     activation=ffv_cfg['activation']\n",
    "# )\n",
    "\n",
    "# model_ffv, ckpt_ffv, metrics_ffv = train_hybrid_gnn(\n",
    "#     model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "#     lr=ffv_cfg['lr'], optimizer=ffv_cfg['optimizer'],\n",
    "#     weight_decay=ffv_cfg['weight_decay'],\n",
    "#     epochs=120, patience=15,  \n",
    "#     save_dir=\"saved_models/gnn_ffv\", tag=\"hybridgnn_ffv\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2606b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Final submission: RF(FFV,Tc) + GNN(Tg,Density,Rg) using LMDB =====\n",
    "import os, numpy as np, pandas as pd, joblib, torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors as rdmd, DataStructs\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "\n",
    "# ---- 0) Load test ids & smiles\n",
    "sample   = pd.read_csv(os.path.join(DATA_ROOT, 'sample_submission.csv'))\n",
    "test_df  = pd.read_csv(os.path.join(DATA_ROOT, 'test.csv'))\n",
    "test_ids = test_df['id'].astype(sample['id'].dtype).values\n",
    "test_smiles = test_df['SMILES'].astype(str).tolist()\n",
    "\n",
    "#  1) Morgan FPs for RF models (FFV, Tc)\n",
    "def morgan_bits_from_smiles(smiles_list, n_bits=1024, radius=3):\n",
    "    X = np.zeros((len(smiles_list), n_bits), dtype=np.uint8)\n",
    "    for i, s in enumerate(smiles_list):\n",
    "        arr = np.zeros((n_bits,), dtype=np.uint8)\n",
    "        mol = Chem.MolFromSmiles(s)  # no canonicalization; if parse fails, stays zeros\n",
    "        if mol is not None:\n",
    "            fp = rdmd.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=n_bits)\n",
    "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        X[i] = arr\n",
    "    return X\n",
    "\n",
    "X_test_fp = morgan_bits_from_smiles(test_smiles, n_bits=1024, radius=3)\n",
    "\n",
    "# Load trained RFs\n",
    "rf_ffv = joblib.load(p_ffv)['model']\n",
    "rf_tc  = joblib.load(p_tc)['model']\n",
    "\n",
    "pred_ffv = rf_ffv.predict(X_test_fp).astype(float)\n",
    "pred_tc  = rf_tc.predict(X_test_fp).astype(float)\n",
    "\n",
    "# ---- 2) GNN predictions (Tg, Density, Rg) via LMDB test loader\n",
    "test_ds = LMDBtoPyGSingleTask(test_ids, TEST_LMDB, target_index=None)\n",
    "test_loader = GeoDataLoader(test_ds, batch_size=128, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_gnn(model, loader, device):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    for b in loader:\n",
    "        b = b.to(device)\n",
    "        p = model(b).view(-1).cpu().numpy()\n",
    "        outs.append(p)\n",
    "    return np.concatenate(outs, axis=0) if outs else np.zeros((0,), dtype=float)\n",
    "\n",
    "\n",
    "# Tg\n",
    "m_tg = HybridGNN(gnn_dim=256, rdkit_dim=15, hidden_dim=512,\n",
    "                 dropout_rate=0.34404144200017467, activation=\"Swish\").to(device)\n",
    "m_tg.load_state_dict(torch.load(ckpt_tg, map_location=device))\n",
    "pred_tg = predict_gnn(m_tg, test_loader, device)\n",
    "\n",
    "# Density\n",
    "den_cfg = {'gnn_dim': 1024, 'hidden_dim': 384, 'dropout_rate': 0.3735260731607324,\n",
    "           'lr': 5.956024201538505e-04, 'activation': 'Swish', 'optimizer': 'AdamW',\n",
    "           'weight_decay': 8.619671341229739e-06}\n",
    "m_den = HybridGNN(gnn_dim=den_cfg['gnn_dim'], rdkit_dim=15,\n",
    "                  hidden_dim=den_cfg['hidden_dim'], dropout_rate=den_cfg['dropout_rate'],\n",
    "                  activation=den_cfg['activation']).to(device)\n",
    "m_den.load_state_dict(torch.load(ckpt_den, map_location=device))\n",
    "pred_density = predict_gnn(m_den, test_loader, device)\n",
    "\n",
    "# Rg (your tuned GNN)\n",
    "m_rg = HybridGNN(gnn_dim=rg_cfg['gnn_dim'], rdkit_dim=15,\n",
    "                 hidden_dim=rg_cfg['hidden_dim'], dropout_rate=rg_cfg['dropout_rate'],\n",
    "                 activation=rg_cfg['activation']).to(device)\n",
    "m_rg.load_state_dict(torch.load(ckpt_rg, map_location=device))\n",
    "pred_rg = predict_gnn(m_rg, test_loader, device)\n",
    "\n",
    "# ---- 3) Safety + assemble submission\n",
    "pred_tg      = np.nan_to_num(pred_tg)\n",
    "pred_density = np.nan_to_num(pred_density)\n",
    "pred_ffv     = np.nan_to_num(pred_ffv)\n",
    "pred_tc      = np.nan_to_num(pred_tc)\n",
    "pred_rg      = np.nan_to_num(pred_rg)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Tg': pred_tg,\n",
    "    'FFV': pred_ffv,\n",
    "    'Tc': pred_tc,\n",
    "    'Density': pred_density,\n",
    "    'Rg': pred_rg,\n",
    "})\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "print('submission.csv written:', sub.shape)\n",
    "sub.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673460",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656cce6e",
   "metadata": {},
   "source": [
    "## Model Performance Summary\n",
    "\n",
    "All baseline models were initially trained and evaluated on a 5,000 molecule subset of the full dataset. Below is a comparison of results across different featurization strategies and model types:\n",
    "\n",
    "### 2D Baseline Models\n",
    "\n",
    "| Model Type    | Featurization      | MAE   | RMSE  | R²    | Notes                                 |\n",
    "| ------------- | ------------------ | ----- | ----- | ----- | ------------------------------------- |\n",
    "| MLP (Tuned)   | RDKit Fingerprints | 0.426 | 0.574 | 0.798 | Strong performance across all metrics |\n",
    "| KRR (Tuned)   | RDKit Fingerprints | 0.454 | 0.593 | 0.784 | Good overall, slightly lower R²       |\n",
    "| RF (Tuned)    | RDKit Fingerprints | 0.423 | 0.583 | 0.791 | Best MAE, very competitive overall    |\n",
    "| MLP (Tuned)   | Coulomb Matrix     | 0.636 | 0.819 | 0.588 | Significantly weaker performance      |\n",
    "| MLP (Untuned) | RDKit Fingerprints | 0.467 | 0.609 | 0.772 | Solid untuned baseline                |\n",
    "| KRR (Untuned) | RDKit Fingerprints | 0.519 | 0.668 | 0.726 | Notable drop from tuned version       |\n",
    "| RF (Untuned)  | RDKit Fingerprints | 0.426 | 0.587 | 0.788 | Surprisingly close to tuned RF        |\n",
    "| MLP (Untuned) | Coulomb Matrix     | 0.663 | 0.847 | 0.559 | Consistently underperforms            |\n",
    "\n",
    "### Graph Neural Network Models (ChemML)\n",
    "\n",
    "| Model Type    | Featurization               | MAE   | RMSE  | R²    | Notes                                |\n",
    "| ------------- | --------------------------- | ----- | ----- | ----- | ------------------------------------ |\n",
    "| GNN (Tuned)   | `tensorise_molecules` Graph | 0.302 | 0.411 | 0.900 | Best results from ChemML experiments |\n",
    "| GNN (Untuned) | `tensorise_molecules` Graph | 0.400 | 0.519 | 0.841 | Strong but less optimized            |\n",
    "\n",
    "### Final Hybrid GNN Model Trained on Full Dataset (OGB-Compatible)\n",
    "\n",
    "| Model Type           | Featurization                          | MAE   | RMSE  | R²    | Notes                              |\n",
    "| -------------------- | -------------------------------------- | ----- | ----- | ----- | ---------------------------------- |\n",
    "| Hybrid GNN (Tuned)   | OGB `smiles2graph` + RDKit descriptors | 0.159 | 0.234 | 0.965 | State-of-the-art level performance |\n",
    "| Hybrid GNN (Untuned) | OGB `smiles2graph` + RDKit descriptors | 0.223 | 0.308 | 0.939 | Still very strong pre-tuning       |\n",
    "\n",
    "---\n",
    "\n",
    "## Model Error Analysis\n",
    "\n",
    "I performed qualitative evaluation by comparing predicted vs. true HOMO–LUMO gaps for both randomly selected and poorly predicted molecules. The worst performing molecules often showed rare or complex structures likely underrepresented in the training set. This highlights the importance of structural diversity and potentially more expressive 3D information to improve generalization.\n",
    "\n",
    "## Next Steps: Integrating 3D Molecular Information\n",
    "\n",
    "To push performance even further and overcome limitations of 2D graphs and hand crafted descriptors, my next step will involve:\n",
    "\n",
    "* Using **3D molecular geometries** \n",
    "* Incorporating **interatomic distances**, angles, and **spatial encoding** (SchNet, DimeNet, or SE(3)-equivariant models)\n",
    "* Comparing results against the current best MAE (\\~0.159)\n",
    "\n",
    "This direction aligns with trends in molecular property prediction where 3D aware models often outperform purely 2D approaches, especially for quantum properties like HOMO–LUMO gaps.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
