{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61979795",
   "metadata": {},
   "source": [
    "# Polymer Property Predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ace_tools_open as tools\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import pickle\n",
    "import joblib\n",
    "import os \n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Module, Sequential, Dropout\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# OGB dataset \n",
    "from ogb.lsc import PygPCQM4Mv2Dataset, PCQM4Mv2Dataset\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# RDKit\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "\n",
    "# ChemML\n",
    "from chemml.chem import Molecule, RDKitFingerprint, CoulombMatrix, tensorise_molecules\n",
    "from chemml.models import MLP, NeuralGraphHidden, NeuralGraphOutput\n",
    "from chemml.utils import regression_metrics\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589db70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Built with CUDA: True\n",
      "CUDA available: True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device: /physical_device:GPU:0\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: data\n",
      "LMDB directory: data\\processed_chunks\n",
      "Train LMDB: data\\processed_chunks\\polymer_train3d_dist.lmdb\n",
      "Test LMDB: data\\processed_chunks\\polymer_test3d_dist.lmdb\n",
      "LMDBs already exist.\n"
     ]
    }
   ],
   "source": [
    "# Paths - Fixed for Kaggle environment\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  # Writable directory\n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/best_gnn_transformer_hybrid.pt'\n",
    "else:\n",
    "    DATA_ROOT = 'data'\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = 'best_gnn_transformer_hybrid.pt'\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"LMDB directory: {CHUNK_DIR}\")\n",
    "print(f\"Train LMDB: {TRAIN_LMDB}\")\n",
    "print(f\"Test LMDB: {TEST_LMDB}\")\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    # Run the LMDB builders\n",
    "    !python build_polymer_lmdb_fixed.py train\n",
    "    !python build_polymer_lmdb_fixed.py test\n",
    "    print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c34b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDB contains 79,730 train graphs (augmented key_ids)\n",
      "Global pools -> train_pool=71,750  val_pool=7,980\n",
      "     Tg:    511 parents with labels (pre-intersection-by-task)\n",
      "    FFV:   7030 parents with labels (pre-intersection-by-task)\n",
      "     Tc:    737 parents with labels (pre-intersection-by-task)\n",
      "Density:    613 parents with labels (pre-intersection-by-task)\n",
      "     Rg:    614 parents with labels (pre-intersection-by-task)\n"
     ]
    }
   ],
   "source": [
    "# LMDB+CSV wiring (augmented-aware)\n",
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "# --- constants / mapping\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "AUG_KEY_MULT = 1000  # must match the builder\n",
    "\n",
    "# paths assumed defined earlier:\n",
    "# DATA_ROOT, TRAIN_LMDB\n",
    "\n",
    "# 1) Read training labels (CSV = ground truth, parent ids)\n",
    "train_path = os.path.join(DATA_ROOT, 'train.csv')\n",
    "train_df   = pd.read_csv(train_path)\n",
    "assert {'id','SMILES'}.issubset(train_df.columns), \"train.csv must have id and SMILES\"\n",
    "train_df['id'] = train_df['id'].astype(int)\n",
    "\n",
    "# 2) LMDB ids (these are *augmented key_ids*)\n",
    "def read_lmdb_ids(lmdb_path: str) -> np.ndarray:\n",
    "    ids_txt = lmdb_path + \".ids.txt\"\n",
    "    if not os.path.exists(ids_txt):\n",
    "        raise FileNotFoundError(f\"Missing {ids_txt}. Rebuild LMDB or confirm paths.\")\n",
    "    ids = np.loadtxt(ids_txt, dtype=np.int64)\n",
    "    if ids.ndim == 0:\n",
    "        ids = ids.reshape(1)\n",
    "    return ids\n",
    "\n",
    "lmdb_ids = read_lmdb_ids(TRAIN_LMDB)\n",
    "print(f\"LMDB contains {len(lmdb_ids):,} train graphs (augmented key_ids)\")\n",
    "\n",
    "# 3) parent map (preferred) or derive from key_ids\n",
    "pmap_path = TRAIN_LMDB + \".parent_map.tsv\"\n",
    "if os.path.exists(pmap_path):\n",
    "    pmap = pd.read_csv(pmap_path, sep=\"\\t\")  # cols: key_id, parent_id, aug_idx, seed\n",
    "    pmap['key_id'] = pmap['key_id'].astype(np.int64)\n",
    "    pmap['parent_id'] = pmap['parent_id'].astype(np.int64)\n",
    "else:\n",
    "    # Fallback: derive parents from integer division\n",
    "    pmap = pd.DataFrame({\n",
    "        'key_id': lmdb_ids.astype(np.int64),\n",
    "        'parent_id': (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "    })\n",
    "\n",
    "# 4) parents that actually exist in LMDB\n",
    "parents_in_lmdb = np.sort(pmap['parent_id'].unique().astype(np.int64))\n",
    "\n",
    "# 5) helper: parent ids that have a label for a given task\n",
    "def parents_with_label(task: str) -> np.ndarray:\n",
    "    col = task\n",
    "    have_label = train_df.loc[~train_df[col].isna(), 'id'].astype(int).values\n",
    "    keep = np.intersect1d(have_label, parents_in_lmdb, assume_unique=False)\n",
    "    return keep\n",
    "\n",
    "# 6) Make a parent-level global split once (reused for each task)\n",
    "rng = np.random.default_rng(123)\n",
    "perm = rng.permutation(len(parents_in_lmdb))\n",
    "split = int(0.9 * len(parents_in_lmdb))\n",
    "parents_train = parents_in_lmdb[perm[:split]]\n",
    "parents_val   = parents_in_lmdb[perm[split:]]\n",
    "\n",
    "# Map parent split to augmented key_ids for the GNN (we'll use later)\n",
    "train_pool_key_ids = pmap.loc[pmap.parent_id.isin(parents_train), 'key_id'].astype(np.int64).values\n",
    "val_pool_key_ids   = pmap.loc[pmap.parent_id.isin(parents_val),   'key_id'].astype(np.int64).values\n",
    "\n",
    "print(f\"Global pools -> train_pool={len(train_pool_key_ids):,}  val_pool={len(val_pool_key_ids):,}\")\n",
    "\n",
    "# 7) Quick sanity: available *parent* counts per task\n",
    "for t in label_cols:\n",
    "    n = len(parents_with_label(t))\n",
    "    print(f\"{t:>7}: {n:6d} parents with labels (pre-intersection-by-task)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1d93173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the same notebook where you built parent splits\n",
    "HOMO_CSV = os.path.join(DATA_ROOT, \"homolumo_parent.csv\")\n",
    "\n",
    "homo = pd.read_csv(HOMO_CSV)                    # cols: parent_id, gap_pred, h_embed_*\n",
    "homo = homo.drop_duplicates(\"parent_id\").set_index(\"parent_id\").sort_index()\n",
    "\n",
    "# convenience: list of embedding columns (if you want them)\n",
    "embed_cols = [c for c in homo.columns if c.startswith(\"h_embed_\")]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "def append_homolumo_features(\n",
    "    X_base: np.ndarray,\n",
    "    parents_vec: np.ndarray,\n",
    "    homo_df: pd.DataFrame,\n",
    "    *,\n",
    "    use_gap: bool = True,\n",
    "    use_embed: bool = False,\n",
    ") -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Append HOMO–LUMO features to X_base using parent_ids in parents_vec.\n",
    "    Toggles:\n",
    "      - use_gap:    adds 1 col 'gap_pred'\n",
    "      - use_embed:  adds all 'h_embed_*' cols\n",
    "    \"\"\"\n",
    "    cols: List[str] = []\n",
    "    blocks: List[np.ndarray] = []\n",
    "\n",
    "    if use_gap:\n",
    "        g = homo_df.reindex(parents_vec)[\"gap_pred\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "        blocks.append(g)\n",
    "        cols.append(\"gap_pred\")\n",
    "\n",
    "    if use_embed:\n",
    "        embed_cols = [c for c in homo_df.columns if c.startswith(\"h_embed_\")]\n",
    "        if embed_cols:\n",
    "            E = homo_df.reindex(parents_vec)[embed_cols].to_numpy(dtype=np.float32)\n",
    "            blocks.append(E)\n",
    "            cols.extend(embed_cols)\n",
    "\n",
    "    if blocks:\n",
    "        H = np.hstack(blocks)\n",
    "        H = np.nan_to_num(H, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        X_aug = np.hstack([X_base.astype(np.float32), H]).astype(np.float32)\n",
    "        return X_aug, cols\n",
    "    else:\n",
    "        return X_base.astype(np.float32), cols\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3c3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def _safe_numpy(x, default_shape=None, dtype=np.float32):\n",
    "    try:\n",
    "        return torch.as_tensor(x).detach().cpu().numpy().astype(dtype)\n",
    "    except Exception:\n",
    "        if default_shape is None:\n",
    "            return np.array([], dtype=dtype)\n",
    "        return np.zeros(default_shape, dtype=dtype)\n",
    "\n",
    "def geom_features_from_rec(rec, rdkit_dim_expected=15, rbf_K=32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a fixed-length vector from a single LMDB record:\n",
    "      [rdkit(15), n_atoms, n_bonds, deg_mean, deg_max, has_xyz,\n",
    "       eig3(3), bbox_extents(3), radius_stats(3), hop_hist(3), extra_atom_mean(5),\n",
    "       edge_rbf_mean(32)]\n",
    "    ~ total len = 15 + 5 + 3 + 3 + 3 + 3 + 5 + 32 = 69\n",
    "    \"\"\"\n",
    "    # 15 RDKit descriptors stored in LMDB (your rebuilt version)\n",
    "    rd = getattr(rec, \"rdkit_feats\", None)\n",
    "    rd = _safe_numpy(rd, default_shape=(1, rdkit_dim_expected)).reshape(-1)\n",
    "    if rd.size != rdkit_dim_expected:\n",
    "        rd = np.zeros((rdkit_dim_expected,), dtype=np.float32)\n",
    "\n",
    "    # basic graph sizes & degree\n",
    "    x = torch.as_tensor(rec.x)             # [N, ...]\n",
    "    ei = torch.as_tensor(rec.edge_index)   # [2, E]\n",
    "    n = x.shape[0]\n",
    "    e = ei.shape[1] if ei.ndim == 2 else 0\n",
    "    deg = torch.bincount(ei[0], minlength=n) if e > 0 else torch.zeros(n, dtype=torch.long)\n",
    "    deg_mean = deg.float().mean().item() if n > 0 else 0.0\n",
    "    deg_max  = deg.max().item() if n > 0 else 0.0\n",
    "\n",
    "    # has_xyz flag\n",
    "    has_xyz = int(bool(getattr(rec, \"has_xyz\", torch.zeros(1, dtype=torch.bool))[0].item())) if hasattr(rec, \"has_xyz\") else 0\n",
    "\n",
    "    # pos-based features\n",
    "    eig3 = np.zeros(3, dtype=np.float32)\n",
    "    extents = np.zeros(3, dtype=np.float32)\n",
    "    rad_stats = np.zeros(3, dtype=np.float32)\n",
    "    pos = getattr(rec, \"pos\", None)\n",
    "    if pos is not None and n > 0 and has_xyz:\n",
    "        P = torch.as_tensor(pos).float()                     # [N,3]\n",
    "        center = P.mean(dim=0, keepdim=True)\n",
    "        C = P - center\n",
    "        cov = (C.T @ C) / max(1, n-1)                       # [3,3]\n",
    "        vals = torch.linalg.eigvalsh(cov).clamp_min(0).sqrt()  # length scales\n",
    "        eig3 = vals.detach().cpu().numpy()\n",
    "        mn, mx = P.min(0).values, P.max(0).values\n",
    "        extents = (mx - mn).detach().cpu().numpy()\n",
    "        r = C.norm(dim=1)\n",
    "        rad_stats = np.array([r.mean().item(), r.std().item(), r.max().item()], dtype=np.float32)\n",
    "\n",
    "    # hop-distance histogram (1,2,3 hops)\n",
    "    hop_hist = np.zeros(3, dtype=np.float32)\n",
    "    D = getattr(rec, \"dist\", None)\n",
    "    if D is not None and n > 0:\n",
    "        Dn = torch.as_tensor(D).float()[:n, :n]\n",
    "        hop_hist = np.array([\n",
    "            (Dn == 1).float().mean().item(),\n",
    "            (Dn == 2).float().mean().item(),\n",
    "            (Dn == 3).float().mean().item()\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    # extra atom features (mean over atoms, 5 dims if present)\n",
    "    extra_atom = getattr(rec, \"extra_atom_feats\", None)\n",
    "    extra_mean = np.zeros(5, dtype=np.float32)\n",
    "    if extra_atom is not None and hasattr(extra_atom, \"shape\") and extra_atom.shape[-1] == 5:\n",
    "        extra_mean = torch.as_tensor(extra_atom).float().mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    # edge RBF (last 32 channels of edge_attr were RBF(d))\n",
    "    rbf_mean = np.zeros(rbf_K, dtype=np.float32)\n",
    "    ea = getattr(rec, \"edge_attr\", None)\n",
    "    if ea is not None:\n",
    "        EA = torch.as_tensor(ea)\n",
    "        if EA.ndim == 2 and EA.shape[1] >= (3 + rbf_K):\n",
    "            rbf = EA[:, -rbf_K:].float()\n",
    "            rbf_mean = rbf.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    scalars = np.array([n, e, deg_mean, deg_max, has_xyz], dtype=np.float32)\n",
    "    return np.concatenate([rd, scalars, eig3, extents, rad_stats, hop_hist, extra_mean, rbf_mean], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e663914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors as rdmd, DataStructs\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def morgan_bits(smiles_list, n_bits=1024, radius=3):\n",
    "    X = np.zeros((len(smiles_list), n_bits), dtype=np.uint8)\n",
    "    for i, s in enumerate(smiles_list):\n",
    "        arr = np.zeros((n_bits,), dtype=np.uint8)\n",
    "        m = Chem.MolFromSmiles(s)\n",
    "        if m is not None:\n",
    "            fp = rdmd.GetMorganFingerprintAsBitVect(m, radius=radius, nBits=n_bits)\n",
    "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        X[i] = arr\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "def build_rf_features_from_lmdb(ids: np.ndarray, lmdb_path: str, smiles_list: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns X = [Morgan1024 | LMDB-3D-global(69)] for each id/smiles.\n",
    "    Assumes ids and smiles_list are aligned with the CSV used to build LMDB.\n",
    "    \"\"\"\n",
    "    base = LMDBDataset(ids, lmdb_path)\n",
    "    # 3D/global block\n",
    "    feats3d = []\n",
    "    for i in range(len(base)):\n",
    "        rec = base[i]\n",
    "        feats3d.append(geom_features_from_rec(rec))  # shape (69,)\n",
    "    X3d = np.vstack(feats3d).astype(np.float32) if feats3d else np.zeros((0, 69), dtype=np.float32)\n",
    "\n",
    "    # Morgan FP block (2D)\n",
    "    Xfp = morgan_bits(smiles_list, n_bits=1024, radius=3)   # (N,1024)\n",
    "\n",
    "    # concat\n",
    "    X = np.hstack([Xfp, X3d]).astype(np.float32)            # (N, 1024+69)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe69f3",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47dc5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Tg shape: (7973, 3)\n",
      "Initial Tg missing:\n",
      "id           0\n",
      "SMILES       0\n",
      "Tg        7462\n",
      "dtype: int64\n",
      "Cleaned Tg shape: (511, 3)\n",
      "Cleaned Tg missing:\n",
      "id        0\n",
      "SMILES    0\n",
      "Tg        0\n",
      "dtype: int64\n",
      "\n",
      "Initial Density shape: (7973, 3)\n",
      "Initial Density missing:\n",
      "id            0\n",
      "SMILES        0\n",
      "Density    7360\n",
      "dtype: int64\n",
      "Cleaned Density shape: (613, 3)\n",
      "Cleaned Density missing:\n",
      "id         0\n",
      "SMILES     0\n",
      "Density    0\n",
      "dtype: int64\n",
      "\n",
      "Initial FFV shape: (7973, 3)\n",
      "Initial FFV missing:\n",
      "id          0\n",
      "SMILES      0\n",
      "FFV       943\n",
      "dtype: int64\n",
      "Cleaned FFV shape: (7030, 3)\n",
      "Cleaned FFV missing:\n",
      "id        0\n",
      "SMILES    0\n",
      "FFV       0\n",
      "dtype: int64\n",
      "\n",
      "Initial Tc shape: (7973, 3)\n",
      "Initial Tc missing:\n",
      "id           0\n",
      "SMILES       0\n",
      "Tc        7236\n",
      "dtype: int64\n",
      "Cleaned Tc shape: (737, 3)\n",
      "Cleaned Tc missing:\n",
      "id        0\n",
      "SMILES    0\n",
      "Tc        0\n",
      "dtype: int64\n",
      "\n",
      "Initial Rg shape: (7973, 3)\n",
      "Initial Rg missing:\n",
      "id           0\n",
      "SMILES       0\n",
      "Rg        7359\n",
      "dtype: int64\n",
      "Cleaned Rg shape: (614, 3)\n",
      "Cleaned Rg missing:\n",
      "id        0\n",
      "SMILES    0\n",
      "Rg        0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build parent-level target DataFrames for RF/tabular\n",
    "train_df = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_df[\"id\"] = train_df[\"id\"].astype(int)\n",
    "\n",
    "# Reuse parents_in_lmdb computed above\n",
    "def build_target_df_from_parents(df: pd.DataFrame, target_col: str, keep_parent_ids: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns DataFrame with ['id','SMILES', target_col] restricted to PARENT ids that\n",
    "    exist in the LMDB; drops missing targets.\n",
    "    \"\"\"\n",
    "    out = df.loc[df[\"id\"].isin(keep_parent_ids), [\"id\", \"SMILES\", target_col]].copy()\n",
    "    print(f\"Initial {target_col} shape:\", out.shape)\n",
    "    print(f\"Initial {target_col} missing:\\n{out.isnull().sum()}\")\n",
    "    out = out.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    print(f\"Cleaned {target_col} shape:\", out.shape)\n",
    "    print(f\"Cleaned {target_col} missing:\\n{out.isnull().sum()}\\n\")\n",
    "    return out\n",
    "\n",
    "# Build all five on PARENTS that exist in LMDB\n",
    "df_tg      = build_target_df_from_parents(train_df, \"Tg\",      parents_in_lmdb)\n",
    "df_density = build_target_df_from_parents(train_df, \"Density\", parents_in_lmdb)\n",
    "df_ffv     = build_target_df_from_parents(train_df, \"FFV\",     parents_in_lmdb)\n",
    "df_tc      = build_target_df_from_parents(train_df, \"Tc\",      parents_in_lmdb)\n",
    "df_rg      = build_target_df_from_parents(train_df, \"Rg\",      parents_in_lmdb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff48e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morgan FP utilities (no 3D, no external descriptors) \n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def smiles_to_morgan_fp(\n",
    "    smi: str,\n",
    "    n_bits: int = 1024,\n",
    "    radius: int = 3,\n",
    "    use_counts: bool = False,\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"Return a 1D numpy array Morgan fingerprint; None if SMILES invalid.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    if use_counts:\n",
    "        fp = rdMolDescriptors.GetMorganFingerprint(mol, radius)\n",
    "        # convert to dense count vector\n",
    "        arr = np.zeros((n_bits,), dtype=np.int32)\n",
    "        for bit_id, count in fp.GetNonzeroElements().items():\n",
    "            arr[bit_id % n_bits] += count\n",
    "        return arr.astype(np.float32)\n",
    "    else:\n",
    "        bv = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "        arr = np.zeros((n_bits,), dtype=np.int8)\n",
    "        Chem.DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "        return arr.astype(np.float32)\n",
    "\n",
    "def prepare_fp_for_target(\n",
    "    df_target: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    *,\n",
    "    fp_bits: int = 1024,\n",
    "    fp_radius: int = 3,\n",
    "    use_counts: bool = False,\n",
    "    save_csv_path: Optional[str] = None,\n",
    "    show_progress: bool = True,\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Drop missing targets, compute Morgan FPs from SMILES only.\n",
    "    Returns (df_clean, y, X_fp) where:\n",
    "      df_clean: ['SMILES', target_col]\n",
    "      y: (N,)\n",
    "      X_fp: (N, fp_bits)\n",
    "    \"\"\"\n",
    "    assert {\"SMILES\", target_col}.issubset(df_target.columns)\n",
    "\n",
    "    # 1) drop missing targets (no imputation)\n",
    "    work = df_target[[\"SMILES\", target_col]].copy()\n",
    "    before = len(work)\n",
    "    work = work.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    after = len(work)\n",
    "    print(f\"[{target_col}] dropped {before - after} missing; kept {after}\")\n",
    "\n",
    "    # 2) compute FPs; skip invalid SMILES\n",
    "    fps, ys, keep_smiles = [], [], []\n",
    "    it = work.itertuples(index=False)\n",
    "    if show_progress:\n",
    "        it = tqdm(it, total=len(work), desc=f\"FPs for {target_col}\")\n",
    "\n",
    "    for row in it:\n",
    "        smi = row.SMILES\n",
    "        yv  = getattr(row, target_col)\n",
    "        arr = smiles_to_morgan_fp(smi, n_bits=fp_bits, radius=fp_radius, use_counts=use_counts)\n",
    "        if arr is None:\n",
    "            continue\n",
    "        fps.append(arr)\n",
    "        ys.append(float(yv))\n",
    "        keep_smiles.append(smi)\n",
    "\n",
    "    X_fp = np.stack(fps, axis=0) if fps else np.zeros((0, fp_bits), dtype=np.float32)\n",
    "    y = np.asarray(ys, dtype=float)\n",
    "    df_clean = pd.DataFrame({\"SMILES\": keep_smiles, target_col: y})\n",
    "\n",
    "    if save_csv_path:\n",
    "        df_clean.to_csv(save_csv_path, index=False)\n",
    "        print(f\"[{target_col}] saved cleaned CSV -> {save_csv_path}\")\n",
    "\n",
    "    print(f\"[{target_col}] X_fp: {X_fp.shape} | y: {y.shape}\")\n",
    "    return df_clean, y, X_fp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371a538",
   "metadata": {},
   "source": [
    "Splits:\n",
    "X_train: (5624, 1024) | X_test: (1406, 1024)\n",
    "y_train: (5624,) | y_test: (1406,)\n",
    "[RF/FFV] val_MAE=0.009095  val_RMSE=0.019753  val_R2=0.5701\n",
    "Splits:\n",
    "X_train: (589, 1024) | X_test: (148, 1024)\n",
    "y_train: (589,) | y_test: (148,)\n",
    "[RF/Tc] val_MAE=0.029866  val_RMSE=0.045109  val_R2=0.7304\n",
    "Splits:\n",
    "X_train: (491, 1024) | X_test: (123, 1024)\n",
    "y_train: (491,) | y_test: (123,)\n",
    "[RF/Rg] val_MAE=1.715067  val_RMSE=2.664982  val_R2=0.6916\n",
    "Splits:\n",
    "X_train: (408, 1024) | X_test: (103, 1024)\n",
    "y_train: (408,) | y_test: (103,)\n",
    "[RF/Tg] val_MAE=61.738193  val_RMSE=78.750171  val_R2=0.5333\n",
    "Splits:\n",
    "X_train: (490, 1024) | X_test: (123, 1024)\n",
    "y_train: (490,) | y_test: (123,)\n",
    "[RF/Density] val_MAE=0.054697  val_RMSE=0.092855  val_R2=0.6311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f37942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] dropped 0 missing; kept 511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2db114a4164008ad7e5289299e5cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Tg:   0%|          | 0/511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] saved cleaned CSV -> cleaned_tg_fp.csv\n",
      "[Tg] X_fp: (511, 1024) | y: (511,)\n",
      "[Density] dropped 0 missing; kept 613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd360f26194143c58c02d6f84795a1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Density:   0%|          | 0/613 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Density] saved cleaned CSV -> cleaned_density_fp.csv\n",
      "[Density] X_fp: (613, 1024) | y: (613,)\n",
      "[FFV] dropped 0 missing; kept 7030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6fa0f48e4f4a48a290cef3be133b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for FFV:   0%|          | 0/7030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FFV] saved cleaned CSV -> cleaned_ffv_fp.csv\n",
      "[FFV] X_fp: (7030, 1024) | y: (7030,)\n",
      "[Tc] dropped 0 missing; kept 737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1867fb0be214181b130ba7ce3a20c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Tc:   0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tc] saved cleaned CSV -> cleaned_tc_fp.csv\n",
      "[Tc] X_fp: (737, 1024) | y: (737,)\n",
      "[Rg] dropped 0 missing; kept 614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b51df3a3e642989f59bb783215e49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Rg:   0%|          | 0/614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rg] saved cleaned CSV -> cleaned_rg_fp.csv\n",
      "[Rg] X_fp: (614, 1024) | y: (614,)\n"
     ]
    }
   ],
   "source": [
    "# Bit vectors (1024, r=3) \n",
    "df_clean_tg,      y_tg,      X_tg      = prepare_fp_for_target(df_tg,      \"Tg\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_tg_fp.csv\")\n",
    "df_clean_density, y_density, X_density = prepare_fp_for_target(df_density, \"Density\", fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_density_fp.csv\")\n",
    "df_clean_ffv,     y_ffv,     X_ffv     = prepare_fp_for_target(df_ffv,     \"FFV\",     fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_ffv_fp.csv\")\n",
    "df_clean_tc,      y_tc,      X_tc      = prepare_fp_for_target(df_tc,      \"Tc\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_tc_fp.csv\")\n",
    "df_clean_rg,      y_rg,      X_rg      = prepare_fp_for_target(df_rg,      \"Rg\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_rg_fp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff620911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "@dataclass\n",
    "class TabularSplits:\n",
    "    # unscaled (for RF)\n",
    "    X_train: np.ndarray\n",
    "    X_test:  np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    y_test:  np.ndarray\n",
    "    # scaled (for KRR/MLP)\n",
    "    X_train_scaled: Optional[np.ndarray] = None\n",
    "    X_test_scaled:  Optional[np.ndarray] = None\n",
    "    y_train_scaled: Optional[np.ndarray] = None  # shape (N,1)\n",
    "    y_test_scaled:  Optional[np.ndarray] = None\n",
    "    x_scaler: Optional[StandardScaler] = None\n",
    "    y_scaler: Optional[StandardScaler] = None\n",
    "\n",
    "def _make_regression_stratify_bins(y: np.ndarray, n_bins: int = 10) -> np.ndarray:\n",
    "    \"\"\"Return integer bins for approximate stratification in regression.\"\"\"\n",
    "    y = y.ravel()\n",
    "    # handle degenerate case\n",
    "    if np.unique(y).size < n_bins:\n",
    "        n_bins = max(2, np.unique(y).size)\n",
    "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "    bins = np.unique(np.quantile(y, quantiles))\n",
    "    # ensure strictly increasing\n",
    "    bins = np.unique(bins)\n",
    "    # np.digitize expects right-open intervals by default\n",
    "    strat = np.digitize(y, bins[1:-1], right=False)\n",
    "    return strat\n",
    "\n",
    "def make_tabular_splits(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    scale_X: bool = True,\n",
    "    scale_y: bool = True,\n",
    "    stratify_regression: bool = False,\n",
    "    n_strat_bins: int = 10,\n",
    "    # if you already decided splits (e.g., scaffold split), pass indices:\n",
    "    train_idx: Optional[np.ndarray] = None,\n",
    "    test_idx: Optional[np.ndarray] = None,\n",
    ") -> TabularSplits:\n",
    "    \"\"\"\n",
    "    Split and (optionally) scale tabular features/targets for a single target.\n",
    "    Returns both scaled and unscaled arrays, plus fitted scalers.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    if train_idx is not None and test_idx is not None:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "    else:\n",
    "        strat = None\n",
    "        if stratify_regression:\n",
    "            strat = _make_regression_stratify_bins(y, n_bins=n_strat_bins)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "        )\n",
    "\n",
    "    # Unscaled outputs (for RF, tree models)\n",
    "    splits = TabularSplits(\n",
    "        X_train=X_train, X_test=X_test,\n",
    "        y_train=y_train, y_test=y_test\n",
    "    )\n",
    "\n",
    "    # Scaled versions (for KRR/MLP)\n",
    "    if scale_X:\n",
    "        xscaler = StandardScaler()\n",
    "        splits.X_train_scaled = xscaler.fit_transform(X_train)\n",
    "        splits.X_test_scaled  = xscaler.transform(X_test)\n",
    "        splits.x_scaler = xscaler\n",
    "    if scale_y:\n",
    "        yscaler = StandardScaler()\n",
    "        splits.y_train_scaled = yscaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        splits.y_test_scaled  = yscaler.transform(y_test.reshape(-1, 1))\n",
    "        splits.y_scaler = yscaler\n",
    "\n",
    "    # Shapes summary\n",
    "    print(\"Splits:\")\n",
    "    print(\"X_train:\", splits.X_train.shape, \"| X_test:\", splits.X_test.shape)\n",
    "    if splits.X_train_scaled is not None:\n",
    "        print(\"X_train_scaled:\", splits.X_train_scaled.shape, \"| X_test_scaled:\", splits.X_test_scaled.shape)\n",
    "    print(\"y_train:\", splits.y_train.shape, \"| y_test:\", splits.y_test.shape)\n",
    "    if splits.y_train_scaled is not None:\n",
    "        print(\"y_train_scaled:\", splits.y_train_scaled.shape, \"| y_test_scaled:\", splits.y_test_scaled.shape)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c284cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def train_eval_rf(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    rf_params: Dict[str, Any],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    save_dir: str = \"saved_models/rf\",\n",
    "    tag: str = \"model\",\n",
    ") -> Tuple[RandomForestRegressor, Dict[str, float], TabularSplits, str]:\n",
    "    \"\"\"\n",
    "    Trains a RandomForest on unscaled features; returns (model, metrics, splits, path).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # Pick a safe number of bins based on dataset size\n",
    "    if stratify_regression:\n",
    "        adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "    else:\n",
    "        adaptive_bins = n_strat_bins\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False,                 # RF doesn't need scaling\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=adaptive_bins\n",
    "    )\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=random_state, n_jobs=-1, **rf_params)\n",
    "    rf.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "    pred_tr = rf.predict(splits.X_train)\n",
    "    pred_te = rf.predict(splits.X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_MAE\": mean_absolute_error(splits.y_train, pred_tr),\n",
    "        \"train_RMSE\": mean_squared_error(splits.y_train, pred_tr, squared=False),\n",
    "        \"train_R2\": r2_score(splits.y_train, pred_tr),\n",
    "        \"val_MAE\": mean_absolute_error(splits.y_test, pred_te),\n",
    "        \"val_RMSE\": mean_squared_error(splits.y_test, pred_te, squared=False),\n",
    "        \"val_R2\": r2_score(splits.y_test, pred_te),\n",
    "    }\n",
    "    print(f\"[RF/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"rf_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": rf, \"metrics\": metrics, \"rf_params\": rf_params}, path)\n",
    "    return rf, metrics, splits, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08d95126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:\n",
      "X_train: (5624, 1024) | X_test: (1406, 1024)\n",
      "y_train: (5624,) | y_test: (1406,)\n",
      "[RF/FFV] val_MAE=0.009095  val_RMSE=0.019753  val_R2=0.5701\n",
      "Splits:\n",
      "X_train: (589, 1024) | X_test: (148, 1024)\n",
      "y_train: (589,) | y_test: (148,)\n",
      "[RF/Tc] val_MAE=0.029866  val_RMSE=0.045109  val_R2=0.7304\n",
      "Splits:\n",
      "X_train: (491, 1024) | X_test: (123, 1024)\n",
      "y_train: (491,) | y_test: (123,)\n",
      "[RF/Rg] val_MAE=1.715067  val_RMSE=2.664982  val_R2=0.6916\n",
      "Splits:\n",
      "X_train: (408, 1024) | X_test: (103, 1024)\n",
      "y_train: (408,) | y_test: (103,)\n",
      "[RF/Tg] val_MAE=61.738193  val_RMSE=78.750171  val_R2=0.5333\n",
      "Splits:\n",
      "X_train: (490, 1024) | X_test: (123, 1024)\n",
      "y_train: (490,) | y_test: (123,)\n",
      "[RF/Density] val_MAE=0.054697  val_RMSE=0.092855  val_R2=0.6311\n"
     ]
    }
   ],
   "source": [
    "rf_cfg = {\n",
    "    \"FFV\": {\"n_estimators\": 100, \"max_depth\": 60},\n",
    "    \"Tc\":  {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "    \"Rg\":  {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "}\n",
    "\n",
    "rf_ffv, m_ffv, splits_ffv, p_ffv = train_eval_rf(X_ffv, y_ffv, rf_params=rf_cfg[\"FFV\"], tag=\"FFV\")\n",
    "rf_tc,  m_tc,  splits_tc,  p_tc  = train_eval_rf(X_tc,  y_tc,  rf_params=rf_cfg[\"Tc\"],  tag=\"Tc\")\n",
    "rf_rg,  m_rg,  splits_rg,  p_rg  = train_eval_rf(X_rg,  y_rg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Rg\")\n",
    "rf_tg,  m_tg,  splits_tg,  p_tg  = train_eval_rf(X_tg,  y_tg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Tg\")\n",
    "rf_density,  m_density,  splits_density,  p_density  = train_eval_rf(X_density,  y_density,  rf_params=rf_cfg[\"Rg\"],  tag=\"Density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af381c92",
   "metadata": {},
   "source": [
    "\n",
    "[RF/FFV] val_MAE=0.009095  val_RMSE=0.019753  val_R2=0.5701\n",
    "\n",
    "[RF/Tc] val_MAE=0.029866  val_RMSE=0.045109  val_R2=0.7304\n",
    "\n",
    "[RF/Rg] val_MAE=1.715067  val_RMSE=2.664982  val_R2=0.6916\n",
    "\n",
    "[RF/Tg] val_MAE=61.738193  val_RMSE=78.750171  val_R2=0.5333\n",
    "\n",
    "[RF/Density] val_MAE=0.054697  val_RMSE=0.092855  val_R2=0.6311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7b55778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "AUG_KEY_MULT = 1000  # must match the builder\n",
    "\n",
    "def build_rf_features_from_lmdb_parents(parent_ids, lmdb_path, smiles_list, *, agg=\"mean\"):\n",
    "    \"\"\"\n",
    "    Uses your EXISTING build_rf_features_from_lmdb(ids, lmdb_path, smiles_list)\n",
    "    but expands each parent id -> its augmented key_ids, extracts features for all,\n",
    "    then reduces (mean/median) back to ONE row per parent (so shapes match y).\n",
    "\n",
    "    Returns:\n",
    "        X_parent  : (N_parents_kept, D)   aggregated features per parent\n",
    "        keep_idx  : indices into the input arrays (parent_ids/smiles_list/y)\n",
    "                    that were actually kept (in order)\n",
    "    \"\"\"\n",
    "    # 1) load augmentation map\n",
    "    pmap_path = lmdb_path + \".parent_map.tsv\"\n",
    "    if os.path.exists(pmap_path):\n",
    "        pmap = pd.read_csv(pmap_path, sep=\"\\t\")  # cols: key_id, parent_id, aug_idx, seed\n",
    "        pmap['key_id'] = pmap['key_id'].astype(np.int64)\n",
    "        pmap['parent_id'] = pmap['parent_id'].astype(np.int64)\n",
    "        group = pmap.groupby('parent_id')['key_id'].apply(list).to_dict()\n",
    "    else:\n",
    "        # derive from ids.txt if parent_map.tsv is missing\n",
    "        lmdb_ids = np.loadtxt(lmdb_path + \".ids.txt\", dtype=np.int64)\n",
    "        if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "        dfmap = pd.DataFrame({\n",
    "            'parent_id': (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "            'key_id': lmdb_ids.astype(np.int64),\n",
    "        })\n",
    "        group = dfmap.groupby('parent_id')['key_id'].apply(list).to_dict()\n",
    "\n",
    "    # 2) expand to augmented keys while tracking slices\n",
    "    flat_keys, flat_smiles, seg_sizes = [], [], []\n",
    "    for pid, smi in zip(parent_ids, smiles_list):\n",
    "        keys = group.get(int(pid), [])\n",
    "        seg_sizes.append(len(keys))\n",
    "        if len(keys):\n",
    "            flat_keys.extend(keys)\n",
    "            flat_smiles.extend([smi] * len(keys))\n",
    "\n",
    "    if len(flat_keys) == 0:\n",
    "        raise ValueError(\"No augmented key_ids found for provided parent ids. \"\n",
    "                         \"Check that LMDB matches this CSV and AUG_KEY_MULT.\")\n",
    "\n",
    "    # 3) call your existing builder on augmented key_ids (NOT parents)\n",
    "    X_all = build_rf_features_from_lmdb(np.array(flat_keys, dtype=np.int64),\n",
    "                                        lmdb_path,\n",
    "                                        flat_smiles)\n",
    "    # 4) fold back to parents by aggregation\n",
    "    out_rows, keep_idx = [], []\n",
    "    i0 = 0\n",
    "    for i, k in enumerate(seg_sizes):\n",
    "        if k == 0:    # parent not present in LMDB (should be rare)\n",
    "            continue\n",
    "        Xi = X_all[i0:i0+k]\n",
    "        i0 += k\n",
    "        if agg == \"mean\":\n",
    "            out_rows.append(Xi.mean(axis=0))\n",
    "        elif agg == \"median\":\n",
    "            out_rows.append(np.median(Xi, axis=0))\n",
    "        elif agg == \"max\":\n",
    "            out_rows.append(Xi.max(axis=0))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported agg={agg}\")\n",
    "        keep_idx.append(i)\n",
    "\n",
    "    X_parent = np.vstack(out_rows).astype(np.float32)\n",
    "    keep_idx = np.asarray(keep_idx, dtype=int)\n",
    "\n",
    "    # minor sanity\n",
    "    assert X_parent.ndim == 2 and X_parent.shape[0] == keep_idx.size, \"bad aggregation folding\"\n",
    "\n",
    "    # optional: report drops\n",
    "    n_drop = (len(parent_ids) - keep_idx.size)\n",
    "    if n_drop:\n",
    "        print(f\"[build_rf_features_from_lmdb_parents] Dropped {n_drop} parents with 0 aug rows in LMDB\")\n",
    "\n",
    "    return X_parent, keep_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7492bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Training RF(+3D) for FFV (gap only)\n",
      "[FFV] appended HOMO cols: 519 -> X=(7030, 1612)\n",
      "Splits:\n",
      "X_train: (5624, 1612) | X_test: (1406, 1612)\n",
      "y_train: (5624,) | y_test: (1406,)\n",
      "[RF/FFV_aug3D_mean+gap+emb] val_MAE=0.008945  val_RMSE=0.018098  val_R2=0.6391\n",
      "[RF+3D/FFV]  val_MAE=0.008945  val_RMSE=0.018098  val_R2=0.6391\n",
      "\n",
      ">>> Training RF(+3D) for Tc (gap only)\n",
      "[Tc] appended HOMO cols: 519 -> X=(737, 1612)\n",
      "Splits:\n",
      "X_train: (589, 1612) | X_test: (148, 1612)\n",
      "y_train: (589,) | y_test: (148,)\n",
      "[RF/Tc_aug3D_mean+gap+emb] val_MAE=0.032474  val_RMSE=0.046433  val_R2=0.7144\n",
      "[RF+3D/Tc]  val_MAE=0.032474  val_RMSE=0.046433  val_R2=0.7144\n",
      "\n",
      ">>> Training RF(+3D) for Rg (gap only)\n",
      "[Rg] appended HOMO cols: 519 -> X=(614, 1612)\n",
      "Splits:\n",
      "X_train: (491, 1612) | X_test: (123, 1612)\n",
      "y_train: (491,) | y_test: (123,)\n",
      "[RF/Rg_aug3D_mean+gap+emb] val_MAE=1.943833  val_RMSE=2.881054  val_R2=0.6395\n",
      "[RF+3D/Rg]  val_MAE=1.943833  val_RMSE=2.881054  val_R2=0.6395\n",
      "\n",
      ">>> Training RF(+3D) for Tg (gap only)\n",
      "[Tg] appended HOMO cols: 519 -> X=(511, 1612)\n",
      "Splits:\n",
      "X_train: (408, 1612) | X_test: (103, 1612)\n",
      "y_train: (408,) | y_test: (103,)\n",
      "[RF/Tg_aug3D_mean+gap+emb] val_MAE=59.381327  val_RMSE=77.921443  val_R2=0.5431\n",
      "[RF+3D/Tg]  val_MAE=59.381327  val_RMSE=77.921443  val_R2=0.5431\n",
      "\n",
      ">>> Training RF(+3D) for Density (gap only)\n",
      "[Density] appended HOMO cols: 519 -> X=(613, 1612)\n",
      "Splits:\n",
      "X_train: (490, 1612) | X_test: (123, 1612)\n",
      "y_train: (490,) | y_test: (123,)\n",
      "[RF/Density_aug3D_mean+gap+emb] val_MAE=0.042626  val_RMSE=0.073426  val_R2=0.7693\n",
      "[RF+3D/Density]  val_MAE=0.042626  val_RMSE=0.073426  val_R2=0.7693\n"
     ]
    }
   ],
   "source": [
    "# === helpers (uses the LMDB feature builders you already added) ===\n",
    "from typing import Optional\n",
    "\n",
    "def train_rf_aug3d_for_target(\n",
    "    target_col: str,\n",
    "    rf_params: dict,\n",
    "    *,\n",
    "    train_csv_path: str,\n",
    "    lmdb_path: str,\n",
    "    save_dir: str = \"saved_models/rf_aug3d\",\n",
    "    tag_prefix: str = \"aug3D\",\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    agg: str = \"mean\",\n",
    "    # ↓↓↓ changed here\n",
    "    homo_df: Optional[pd.DataFrame] = None,\n",
    "    use_gap: bool = True,\n",
    "    use_embed: bool = False,\n",
    "):\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    mask = ~df[target_col].isna()\n",
    "    parent_ids = df.loc[mask, 'id'].astype(int).values\n",
    "    smiles_tr  = df.loc[mask, 'SMILES'].astype(str).tolist()\n",
    "    y          = df.loc[mask, target_col].astype(float).values\n",
    "\n",
    "    # LMDB features aggregated over augmentations\n",
    "    X_parent, keep_idx = build_rf_features_from_lmdb_parents(parent_ids, lmdb_path, smiles_tr, agg=agg)\n",
    "    y_keep = y[keep_idx]\n",
    "\n",
    "    # append HOMO–LUMO (toggle gap/embed)\n",
    "    parents_kept = parent_ids[keep_idx]\n",
    "    if homo_df is not None and (use_gap or use_embed):\n",
    "        X_parent, added_cols = append_homolumo_features(\n",
    "            X_parent, parents_kept, homo_df, use_gap=use_gap, use_embed=use_embed\n",
    "        )\n",
    "        print(f\"[{target_col}] appended HOMO cols: {len(added_cols)} -> X={X_parent.shape}\")\n",
    "\n",
    "    model, metrics, splits, path = train_eval_rf(\n",
    "        X_parent, y_keep,\n",
    "        rf_params=rf_params,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=n_strat_bins,\n",
    "        save_dir=save_dir,\n",
    "        tag=f\"{target_col}_{tag_prefix}_{agg}\"\n",
    "           + (\"+gap\" if use_gap else \"\")\n",
    "           + (\"+emb\" if use_embed else \"\")\n",
    "    )\n",
    "    return model, metrics, splits, path\n",
    "\n",
    "# === per-target configs (start with what worked; tweak later) ===\n",
    "rf_cfg_aug = {\n",
    "    \"FFV\":     {\"n_estimators\": 800, \"max_depth\": 30, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "    \"Tc\":      {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "    \"Rg\":      {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "    # reasonable first passes for the two GNN targets (just to A/B):\n",
    "    \"Tg\":      {\"n_estimators\": 600, \"max_depth\": 60, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "    \"Density\": {\"n_estimators\": 600, \"max_depth\": 40, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "}\n",
    "\n",
    "# === train all five with augmented features ===\n",
    "TRAIN_CSV = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "rf_models, rf_metrics, rf_splits, rf_paths = {}, {}, {}, {}\n",
    "\n",
    "# you already loaded this earlier:\n",
    "# homo = pd.read_csv(HOMO_CSV).drop_duplicates(\"parent_id\").set_index(\"parent_id\").sort_index()\n",
    "\n",
    "for t in [\"FFV\", \"Tc\", \"Rg\", \"Tg\", \"Density\"]:\n",
    "    print(f\"\\n>>> Training RF(+3D) for {t} (gap only)\")\n",
    "    m, met, sp, p = train_rf_aug3d_for_target(\n",
    "        t, rf_cfg_aug[t],\n",
    "        train_csv_path=TRAIN_CSV,\n",
    "        lmdb_path=TRAIN_LMDB,\n",
    "        save_dir=\"saved_models/rf_aug3d\",\n",
    "        tag_prefix=\"aug3D\",\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify_regression=True,\n",
    "        n_strat_bins=10,\n",
    "        agg=\"mean\",\n",
    "        homo_df=homo,         # <<< pass the table\n",
    "        use_gap=True,         # <<< toggle ON/OFF\n",
    "        use_embed=True,      # <<< toggle ON/OFF\n",
    "    )\n",
    "\n",
    "    rf_models[t], rf_metrics[t], rf_splits[t], rf_paths[t] = m, met, sp, p\n",
    "    print(f\"[RF+3D/{t}]  val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b23ce8e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Model Type | Feature | MAE | RMSE | R2 |\n",
    "|---|---|---|---|---|\n",
    "| RF3D_Aug | Tg | 58.143107 | 74.521032 | 0.5821 |\n",
    "| RF3D_Aug | Tg+gap | 57.655998 | 74.087187 | 0.5870 |\n",
    "| RF3D_Aug | Tg+embed | 58.607196  | 76.791752 | 0.5563 |\n",
    "| RF3D_Aug | Tg+gap+embed | 59.381327 | 77.921443 | 0.5431 |\n",
    "| RF3D_Aug | Tc | 0.029675 | 0.044853 | 0.7335 |\n",
    "| RF3D_Aug | Tc+gap | 0.029495 | 0.044756 | 0.7346 |\n",
    "| RF3D_Aug | Tc+embed | 0.032614 | 0.046604 | 0.7122 |\n",
    "| RF3D_Aug | Tc+gap+embed | 0.032474 | 0.046433 | 0.7144 |\n",
    "| RF3D_Aug | Density | 0.037123 | 0.070212 | 0.7891 |\n",
    "| RF3D_Aug | Density+gap | 0.037999 | 0.070560 | 0.7870 |\n",
    "| RF3D_Aug | Density+embed | 0.042827 | 0.072888 | 0.7727 |\n",
    "| RF3D_Aug | Density+gap+embed | 0.042626 | 0.073426 | 0.7693 |\n",
    "| RF3D_Aug | FFV | 0.007578 | 0.017404 | 0.6662 |\n",
    "| RF3D_Aug | FFV+gap | 0.007606 | 0.017523 | 0.6616 |\n",
    "| RF3D_Aug | FFV+embed | 0.008921 | 0.018084 | 0.6397 |\n",
    "| RF3D_Aug | FFV+gap+embed | 0.008945 | 0.018098 | 0.6391 |\n",
    "| RF3D_Aug | Rg | 1.668425 | 2.517235 | 0.7248 |\n",
    "| RF3D_Aug | Rg+gap | 1.683591 | 2.539469 | 0.7199 |\n",
    "| RF3D_Aug | Rg+embed | 1.938364 | 2.872557 | 0.6416 |\n",
    "| RF3D_Aug | Rg+gap+embed | 1.943833 | 2.881054 | 0.6395 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d599b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Tg → parents: train=  408 val=  103 | aug rows: train=  4080 val=  1030\n",
      "    FFV → parents: train= 5624 val= 1406 | aug rows: train= 56240 val= 14060\n",
      "     Tc → parents: train=  589 val=  148 | aug rows: train=  5890 val=  1480\n",
      "Density → parents: train=  490 val=  123 | aug rows: train=  4900 val=  1230\n",
      "     Rg → parents: train=  491 val=  123 | aug rows: train=  4910 val=  1230\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 1: parent-aware IDs & task splits expanded to augmented key_ids ====\n",
    "import os, numpy as np, pandas as pd\n",
    "# after you’ve already built `task_pools` etc.\n",
    "HOMO_CSV = os.path.join(DATA_ROOT, \"homolumo_parent.csv\")\n",
    "homo = pd.read_csv(HOMO_CSV).drop_duplicates(\"parent_id\").set_index(\"parent_id\").sort_index()\n",
    "\n",
    "# key -> parent map (use parent_map.tsv if present; otherwise derive)\n",
    "pmap_path = TRAIN_LMDB + \".parent_map.tsv\"\n",
    "lmdb_ids  = np.loadtxt(TRAIN_LMDB + \".ids.txt\", dtype=np.int64)\n",
    "if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "\n",
    "if os.path.exists(pmap_path):\n",
    "    pmap_df = pd.read_csv(pmap_path, sep=\"\\t\")\n",
    "    key2parent = dict(zip(pmap_df.key_id.astype(np.int64), pmap_df.parent_id.astype(np.int64)))\n",
    "else:\n",
    "    AUG_KEY_MULT = 1000\n",
    "    key2parent = {int(k): int(k // AUG_KEY_MULT) for k in lmdb_ids.tolist()}\n",
    "\n",
    "\n",
    "\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "AUG_KEY_MULT = 1000  # must match your LMDB builder\n",
    "\n",
    "# Paths assumed defined: DATA_ROOT, TRAIN_LMDB\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_csv[\"id\"] = train_csv[\"id\"].astype(int)\n",
    "\n",
    "# --- load LMDB ids (augmented key_ids)\n",
    "lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "if not os.path.exists(lmdb_ids_path):\n",
    "    raise FileNotFoundError(f\"Missing {lmdb_ids_path}\")\n",
    "lmdb_ids = np.loadtxt(lmdb_ids_path, dtype=np.int64)\n",
    "if lmdb_ids.ndim == 0:\n",
    "    lmdb_ids = lmdb_ids.reshape(1)\n",
    "\n",
    "# --- load parent map (preferred); fallback derives from key structure\n",
    "pmap_path = TRAIN_LMDB + \".parent_map.tsv\"\n",
    "if os.path.exists(pmap_path):\n",
    "    pmap = pd.read_csv(pmap_path, sep=\"\\t\")  # cols: key_id, parent_id, aug_idx, seed\n",
    "    pmap[\"key_id\"] = pmap[\"key_id\"].astype(np.int64)\n",
    "    pmap[\"parent_id\"] = pmap[\"parent_id\"].astype(np.int64)\n",
    "else:\n",
    "    # derive parents from integer division if parent_map is missing\n",
    "    pmap = pd.DataFrame({\n",
    "        \"key_id\": lmdb_ids.astype(np.int64),\n",
    "        \"parent_id\": (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "    })\n",
    "parents_in_lmdb = np.sort(pmap[\"parent_id\"].unique().astype(np.int64))\n",
    "\n",
    "# --- helper: which parents have a label for a task\n",
    "def parents_with_label(task: str) -> np.ndarray:\n",
    "    m = ~train_csv[task].isna()\n",
    "    have = train_csv.loc[m, \"id\"].astype(int).values\n",
    "    return np.intersect1d(have, parents_in_lmdb, assume_unique=False)\n",
    "\n",
    "# --- split BY PARENT (no leakage), then expand to aug key_ids\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def task_parent_split(task: str, test_size=0.2, seed=42):\n",
    "    parents_labeled = parents_with_label(task)\n",
    "    if parents_labeled.size == 0:\n",
    "        raise ValueError(f\"No parents with labels for task {task}\")\n",
    "\n",
    "    p_tr, p_va = train_test_split(parents_labeled, test_size=test_size, random_state=seed)\n",
    "    tr_keys = pmap.loc[pmap.parent_id.isin(p_tr), \"key_id\"].astype(np.int64).values\n",
    "    va_keys = pmap.loc[pmap.parent_id.isin(p_va), \"key_id\"].astype(np.int64).values\n",
    "    return np.sort(tr_keys), np.sort(va_keys), np.sort(p_tr), np.sort(p_va)\n",
    "\n",
    "# Build task pools (augmented key_ids) for all tasks\n",
    "task_pools = {}\n",
    "task_parent_splits = {}\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys, p_tr, p_va = task_parent_split(t, test_size=0.2, seed=42)\n",
    "    task_pools[t] = (tr_keys, va_keys)        # for loaders\n",
    "    task_parent_splits[t] = (p_tr, p_va)      # for bookkeeping / analysis\n",
    "\n",
    "# Sanity prints\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys = task_pools[t]\n",
    "    p_tr, p_va = task_parent_splits[t]\n",
    "    print(f\"{t:>7} → parents: train={len(p_tr):5d} val={len(p_va):5d} | \"\n",
    "          f\"aug rows: train={len(tr_keys):6d} val={len(va_keys):6d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3efce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import torch, numpy as np\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def _get_rdkit_feats_from_record(rec):\n",
    "    arr = getattr(rec, \"rdkit_feats\", None)\n",
    "    if arr is None:\n",
    "        return torch.zeros(1, 15, dtype=torch.float32)  # your 15-D globals\n",
    "    v = torch.as_tensor(np.asarray(arr, np.float32).reshape(1, -1), dtype=torch.float32)\n",
    "    return v  # (1, D)\n",
    "\n",
    "class LMDBtoPyGSingleTask(Dataset):\n",
    "    def __init__(self,\n",
    "                 ids,\n",
    "                 lmdb_path,\n",
    "                 target_index=None,\n",
    "                 *,\n",
    "                 use_mixed_edges: bool = True,\n",
    "                 include_extra_atom_feats: bool = True,\n",
    "                 # NEW ↓↓↓\n",
    "                 homo_df: Optional[pd.DataFrame] = None,\n",
    "                 key2parent: Optional[dict] = None,\n",
    "                 use_gap: bool = False,\n",
    "                 use_embed: bool = False,\n",
    "                 gap_clip: Optional[tuple] = None,   # e.g., (0.0, 20.0)\n",
    "                 ):\n",
    "        self.ids  = np.asarray(ids, dtype=np.int64)           # augmented key_ids\n",
    "        self.base = LMDBDataset(self.ids, lmdb_path)\n",
    "        self.t    = target_index\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        self.include_extra_atom_feats = include_extra_atom_feats\n",
    "        # homo config\n",
    "        self.homo_df    = homo_df\n",
    "        self.key2parent = key2parent or {}\n",
    "        self.use_gap    = use_gap\n",
    "        self.use_embed  = use_embed\n",
    "        self.gap_clip   = gap_clip\n",
    "        self.embed_cols: List[str] = []\n",
    "        if self.homo_df is not None and self.use_embed:\n",
    "            self.embed_cols = [c for c in self.homo_df.columns if c.startswith(\"h_embed_\")]\n",
    "\n",
    "    def __len__(self): return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.base[idx]\n",
    "\n",
    "        x  = torch.as_tensor(rec.x, dtype=torch.long)\n",
    "        ei = torch.as_tensor(rec.edge_index, dtype=torch.long)\n",
    "\n",
    "        ea = torch.as_tensor(rec.edge_attr)\n",
    "        if self.use_mixed_edges:\n",
    "            edge_attr = ea.to(torch.float32)        # (E, 3+32)\n",
    "        else:\n",
    "            edge_attr = ea[:, :3].to(torch.long)    # (E, 3)\n",
    "\n",
    "        rdkit_feats = _get_rdkit_feats_from_record(rec)  # (1, D0)\n",
    "\n",
    "        # --- OPTIONAL: append HOMO features (parent-aware) ---\n",
    "        if self.homo_df is not None and (self.use_gap or self.use_embed):\n",
    "            key_id    = int(self.ids[idx])\n",
    "            parent_id = self.key2parent.get(key_id, None)\n",
    "            H_blocks = []\n",
    "\n",
    "            if self.use_gap:\n",
    "                g = 0.0\n",
    "                if parent_id is not None and parent_id in self.homo_df.index:\n",
    "                    g = float(self.homo_df.loc[parent_id, \"gap_pred\"])\n",
    "                if self.gap_clip is not None:\n",
    "                    lo, hi = self.gap_clip\n",
    "                    g = max(lo, min(hi, g))\n",
    "                H_blocks.append(torch.tensor([[g]], dtype=torch.float32))\n",
    "\n",
    "            if self.use_embed and self.embed_cols:\n",
    "                if (parent_id is not None) and (parent_id in self.homo_df.index):\n",
    "                    e = self.homo_df.loc[parent_id, self.embed_cols].to_numpy(dtype=np.float32, copy=True)\n",
    "                    e = np.nan_to_num(e, nan=0.0, posinf=0.0, neginf=0.0, copy=False)\n",
    "                    H_blocks.append(torch.from_numpy(e.reshape(1, -1)))\n",
    "                else:\n",
    "                    H_blocks.append(torch.zeros(1, len(self.embed_cols), dtype=torch.float32))\n",
    "\n",
    "            if H_blocks:\n",
    "                H = torch.cat(H_blocks, dim=1)  # (1, K)\n",
    "                rdkit_feats = torch.cat([rdkit_feats.float(), H.float()], dim=1)  # (1, D0+K)\n",
    "\n",
    "        d = Data(x=x, edge_index=ei, edge_attr=edge_attr, rdkit_feats=rdkit_feats)\n",
    "\n",
    "        if hasattr(rec, \"pos\"):\n",
    "            d.pos = torch.as_tensor(rec.pos, dtype=torch.float32)\n",
    "        if self.include_extra_atom_feats and hasattr(rec, \"extra_atom_feats\"):\n",
    "            d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32)\n",
    "        if hasattr(rec, \"has_xyz\"):\n",
    "            d.has_xyz = torch.as_tensor(rec.has_xyz, dtype=torch.float32)\n",
    "        if hasattr(rec, \"dist\"):\n",
    "            d.hops = torch.as_tensor(rec.dist, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        if (self.t is not None) and hasattr(rec, \"y\"):\n",
    "            yv = torch.as_tensor(rec.y, dtype=torch.float32).view(-1)\n",
    "            if self.t < yv.numel():\n",
    "                d.y = yv[self.t:self.t+1]  # (1,)\n",
    "\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "694612d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdkit_dim = 534\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 3: loaders that use aug key_id pools ====\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "def make_loaders_for_task_from_pools(\n",
    "    task, task_pools, *,\n",
    "    batch_size=64,\n",
    "    use_mixed_edges=True,\n",
    "    include_extra_atom_feats=True,\n",
    "    # NEW ↓↓↓\n",
    "    homo_df: Optional[pd.DataFrame] = None,\n",
    "    key2parent: Optional[dict] = None,\n",
    "    use_gap: bool = False,\n",
    "    use_embed: bool = False,\n",
    "    gap_clip: Optional[tuple] = None,\n",
    "):\n",
    "    t = task2idx[task]\n",
    "    tr_keys, va_keys = task_pools[task]\n",
    "    if len(tr_keys) == 0 or len(va_keys) == 0:\n",
    "        raise ValueError(f\"Empty pools for task {task}. Check task splits.\")\n",
    "\n",
    "    tr_ds = LMDBtoPyGSingleTask(\n",
    "        tr_keys, TRAIN_LMDB, target_index=t,\n",
    "        use_mixed_edges=use_mixed_edges, include_extra_atom_feats=include_extra_atom_feats,\n",
    "        homo_df=homo_df, key2parent=key2parent, use_gap=use_gap, use_embed=use_embed, gap_clip=gap_clip,\n",
    "    )\n",
    "    va_ds = LMDBtoPyGSingleTask(\n",
    "        va_keys, TRAIN_LMDB, target_index=t,\n",
    "        use_mixed_edges=use_mixed_edges, include_extra_atom_feats=include_extra_atom_feats,\n",
    "        homo_df=homo_df, key2parent=key2parent, use_gap=use_gap, use_embed=use_embed, gap_clip=gap_clip,\n",
    "    )\n",
    "    tr = GeoDataLoader(tr_ds, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "    va = GeoDataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    return tr, va\n",
    "\n",
    "\n",
    "# Toggles per task — start conservative based on RF-3D results\n",
    "task_homo_cfg = {\n",
    "    \"Tg\":      dict(use_gap=True,  use_embed=True, gap_clip=(0.0, 20.0)),\n",
    "    \"Tc\":      dict(use_gap=True,  use_embed=True, gap_clip=(0.0, 20.0)),\n",
    "    \"FFV\":     dict(use_gap=True, use_embed=True),\n",
    "    \"Density\": dict(use_gap=True, use_embed=True),\n",
    "    \"Rg\":      dict(use_gap=True, use_embed=True),\n",
    "}\n",
    "\n",
    "# Build loaders with flags\n",
    "train_loader_tg,  val_loader_tg  = make_loaders_for_task_from_pools(\n",
    "    \"Tg\", task_pools, batch_size=64, use_mixed_edges=True, include_extra_atom_feats=True,\n",
    "    homo_df=homo, key2parent=key2parent, **task_homo_cfg[\"Tg\"]\n",
    ")\n",
    "train_loader_tc,  val_loader_tc  = make_loaders_for_task_from_pools(\n",
    "    \"Tc\", task_pools, batch_size=64, use_mixed_edges=True, include_extra_atom_feats=True,\n",
    "    homo_df=homo, key2parent=key2parent, **task_homo_cfg[\"Tc\"]\n",
    ")\n",
    "train_loader_ffv, val_loader_ffv = make_loaders_for_task_from_pools(\n",
    "    \"FFV\", task_pools, batch_size=64, use_mixed_edges=True, include_extra_atom_feats=True,\n",
    "    homo_df=homo, key2parent=key2parent, **task_homo_cfg[\"FFV\"]\n",
    ")\n",
    "train_loader_den, val_loader_den = make_loaders_for_task_from_pools(\n",
    "    \"Density\", task_pools, batch_size=64, use_mixed_edges=True, include_extra_atom_feats=True,\n",
    "    homo_df=homo, key2parent=key2parent, **task_homo_cfg[\"Density\"]\n",
    ")\n",
    "train_loader_rg,  val_loader_rg  = make_loaders_for_task_from_pools(\n",
    "    \"Rg\", task_pools, batch_size=64, use_mixed_edges=True, include_extra_atom_feats=True,\n",
    "    homo_df=homo, key2parent=key2parent, **task_homo_cfg[\"Rg\"]\n",
    ")\n",
    "\n",
    "# Introspect new RDKit+HOMO dimensionality (auto)\n",
    "rd_dim = next(iter(train_loader_tg)).rdkit_feats.shape[-1]\n",
    "print(\"rdkit_dim =\", rd_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983db98",
   "metadata": {},
   "source": [
    "## Step 5: Define the Hybrid GNN Model\n",
    "\n",
    "The final architecture uses both structural and cheminformatics data by combining GNN-learned graph embeddings with SMILES-derived RDKit descriptors. This Hybrid GNN model uses `smiles2graph` for graph construction and augments it with RDKit-based molecular features for improved prediction accuracy.\n",
    "\n",
    "### Model Components:\n",
    "\n",
    "* **AtomEncoder / BondEncoder**\n",
    "  Transforms categorical atom and bond features (provided by OGB) into learnable embeddings using the encoders from `ogb.graphproppred.mol_encoder`. These provide a strong foundation for expressive graph learning.\n",
    "\n",
    "* **GINEConv Layers (x2)**\n",
    "  I use two stacked GINEConv layers (Graph Isomorphism Network with Edge features). These layers perform neighborhood aggregation based on edge attributes, allowing the model to capture localized chemical environments.\n",
    "\n",
    "* **Global Mean Pooling**\n",
    "  After message passing, node level embeddings are aggregated into a fixed size graph level representation using `global_mean_pool`.\n",
    "\n",
    "* **Concatenation with RDKit Descriptors**\n",
    "  The pooled GNN embedding is concatenated with external RDKit descriptors, which capture global molecular properties not easily inferred from graph data alone.\n",
    "\n",
    "* **MLP Prediction Head**\n",
    "  A multilayer perceptron processes the combined feature vector with ReLU activations, dropout regularization, and linear layers to predict the HOMO–LUMO gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82dad355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = float(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        rand = keep + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        rand.floor_()  # 0/1\n",
    "        return x.div(keep) * rand\n",
    "\n",
    "\n",
    "def _act(name: str):\n",
    "    name = (name or \"ReLU\").lower()\n",
    "    if name == \"relu\": return nn.ReLU()\n",
    "    if name == \"gelu\": return nn.GELU()\n",
    "    if name in (\"swish\", \"silu\"): return nn.SiLU()\n",
    "    return nn.ReLU()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0946f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeEncoderMixed(nn.Module):\n",
    "    def __init__(self, emb_dim: int, cont_dim: int = 32, activation=\"GeLU\"):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        # OGB bond categorical widths: type(5), stereo(6), conjugation(2)\n",
    "        self.emb0 = nn.Embedding(5, emb_dim)\n",
    "        self.emb1 = nn.Embedding(6, emb_dim)\n",
    "        self.emb2 = nn.Embedding(2, emb_dim)\n",
    "        self.mlp_cont = nn.Sequential(\n",
    "            nn.Linear(cont_dim, emb_dim),\n",
    "            act,\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        # edge_attr: (E, 3+K)\n",
    "        cat = edge_attr[:, :3].long()\n",
    "        cont = edge_attr[:, 3:].float()\n",
    "        e_cat  = self.emb0(cat[:,0]) + self.emb1(cat[:,1]) + self.emb2(cat[:,2])\n",
    "        e_cont = self.mlp_cont(cont)\n",
    "        return e_cat + e_cont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5380b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtraAtomEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, activation=\"GeLU\"):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            act,\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, extra):\n",
    "        return self.proj(extra)  # (N, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e379ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINEConv\n",
    "\n",
    "class GINEBlock_GNN(nn.Module):\n",
    "    def __init__(self, dim, activation=\"GeLU\", dropout=0.1, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.conv = GINEConv(nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            act,\n",
    "            nn.Linear(dim, dim),\n",
    "        ))\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dp1 = DropPath(drop_path)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, 2*dim),\n",
    "            act,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2*dim, dim),\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dp2 = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_emb):\n",
    "        # pre-norm transformer style\n",
    "        h = self.norm1(x)\n",
    "        h = self.conv(h, edge_index, edge_emb)\n",
    "        h = self.dropout1(h)\n",
    "        x = x + self.dp1(h)\n",
    "\n",
    "        h2 = self.norm2(x)\n",
    "        h2 = self.ffn(h2)\n",
    "        h2 = self.dropout2(h2)\n",
    "        x = x + self.dp2(h2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bef6fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool, global_max_pool, GlobalAttention\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "from torch import nn\n",
    "\n",
    "class HybridGNNv2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gnn_dim: int,\n",
    "        rdkit_dim: int,\n",
    "        hidden_dim: int,\n",
    "        *,\n",
    "        num_layers: int = 8,\n",
    "        activation: str = \"Swish\",\n",
    "        dropout: float = 0.2,\n",
    "        drop_path_rate: float = 0.1,\n",
    "        use_mixed_edges: bool = True,\n",
    "        cont_dim: int = 32,\n",
    "        use_extra_atom_feats: bool = True,\n",
    "        extra_atom_dim: int = 5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gnn_dim = gnn_dim\n",
    "        self.rdkit_dim = rdkit_dim\n",
    "        self.use_extra_atom_feats = use_extra_atom_feats\n",
    "\n",
    "        # encoders\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=gnn_dim)\n",
    "        if use_mixed_edges:\n",
    "            self.edge_encoder = EdgeEncoderMixed(emb_dim=gnn_dim, cont_dim=cont_dim, activation=activation)\n",
    "        else:\n",
    "            self.edge_encoder = BondEncoder(emb_dim=gnn_dim)\n",
    "\n",
    "        if use_extra_atom_feats:\n",
    "            self.extra_atom = ExtraAtomEncoder(in_dim=extra_atom_dim, out_dim=gnn_dim, activation=activation)\n",
    "            self.extra_gate = nn.Sequential(nn.Linear(2*gnn_dim, gnn_dim), _act(activation))\n",
    "\n",
    "        # backbone\n",
    "        dpr = [drop_path_rate * i / max(1, num_layers - 1) for i in range(num_layers)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GINEBlock_GNN(gnn_dim, activation=activation, dropout=dropout, drop_path=dpr[i])\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # pooling (concat of mean/max/attention)\n",
    "        self.att_pool = GlobalAttention(\n",
    "            gate_nn=nn.Sequential(\n",
    "                nn.Linear(gnn_dim, gnn_dim // 2),\n",
    "                _act(activation),\n",
    "                nn.Linear(gnn_dim // 2, 1),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        pooled_dim = 3 * gnn_dim  # mean + max + attention\n",
    "        # plus rdkit globals (+ optional has_xyz scalar)\n",
    "        self.with_has_xyz = True\n",
    "        head_in = pooled_dim + rdkit_dim + (1 if self.with_has_xyz else 0)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(head_in),\n",
    "            nn.Linear(head_in, hidden_dim),\n",
    "            _act(activation),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            _act(activation),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.atom_encoder(data.x)  # (N, D)\n",
    "\n",
    "        if self.use_extra_atom_feats and hasattr(data, \"extra_atom_feats\"):\n",
    "            xa = self.extra_atom(data.extra_atom_feats)  # (N, D)\n",
    "            x = self.extra_gate(torch.cat([x, xa], dim=1))\n",
    "\n",
    "        e = self.edge_encoder(data.edge_attr)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, data.edge_index, e)\n",
    "\n",
    "        # pool\n",
    "        mean = global_mean_pool(x, data.batch)\n",
    "        mmax = global_max_pool(x, data.batch)\n",
    "        attn = self.att_pool(x, data.batch)\n",
    "        g = torch.cat([mean, mmax, attn], dim=1)\n",
    "\n",
    "        rd = data.rdkit_feats.view(g.size(0), -1)\n",
    "        extras = [g, rd]\n",
    "\n",
    "        if self.with_has_xyz and hasattr(data, \"has_xyz\"):\n",
    "            # has_xyz collates to (B,1)\n",
    "            extras.append(data.has_xyz.view(-1, 1).float())\n",
    "\n",
    "        out = torch.cat(extras, dim=1)\n",
    "        return self.head(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc992041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numpy as np, torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, RMSprop\n",
    "from torch.amp import GradScaler, autocast\n",
    "from copy import deepcopy\n",
    "\n",
    "def train_hybrid_gnn_sota(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    *,\n",
    "    lr: float = 5e-4,\n",
    "    optimizer: str = \"AdamW\",\n",
    "    weight_decay: float = 1e-5,\n",
    "    epochs: int = 120,\n",
    "    warmup_epochs: int = 5,\n",
    "    patience: int = 15,\n",
    "    clip_norm: float = 1.0,\n",
    "    amp: bool = True,\n",
    "    loss_name: str = \"mse\",   # \"mse\" or \"huber\"\n",
    "    save_dir: str = \"saved_models/gnn\",\n",
    "    tag: str = \"model_sota\",\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    opt_name = optimizer.lower()\n",
    "    if opt_name == \"rmsprop\":\n",
    "        opt = RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.0)\n",
    "    else:\n",
    "        opt = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # cosine schedule w/ warmup\n",
    "    def lr_factor(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / max(1, warmup_epochs)\n",
    "        t = (epoch - warmup_epochs) / max(1, (epochs - warmup_epochs))\n",
    "        return 0.5 * (1 + math.cos(math.pi * t))\n",
    "    scaler = GradScaler(\"cuda\", enabled=amp)\n",
    "\n",
    "    def loss_fn(pred, target):\n",
    "        if loss_name.lower() == \"huber\":\n",
    "            return F.huber_loss(pred, target, delta=1.0)\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_once(loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        for b in loader:\n",
    "            b = b.to(device)\n",
    "            p = model(b)\n",
    "            preds.append(p.detach().cpu())\n",
    "            trues.append(b.y.view(-1,1).cpu())\n",
    "        preds = torch.cat(preds).numpy(); trues = torch.cat(trues).numpy()\n",
    "        mae = np.mean(np.abs(preds - trues))\n",
    "        rmse = float(np.sqrt(np.mean((preds - trues)**2)))\n",
    "        r2 = float(1 - np.sum((preds - trues)**2) / np.sum((trues - trues.mean())**2))\n",
    "        return mae, rmse, r2\n",
    "\n",
    "    best_mae = float(\"inf\")\n",
    "    best = None\n",
    "    best_path = os.path.join(save_dir, f\"{tag}.pt\")\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # schedule\n",
    "        for g in opt.param_groups:\n",
    "            g[\"lr\"] = lr * lr_factor(ep-1)\n",
    "\n",
    "        model.train()\n",
    "        total, count = 0.0, 0\n",
    "        for b in train_loader:\n",
    "            b = b.to(device)\n",
    "            with autocast(\"cuda\", enabled=amp):\n",
    "                pred = model(b)\n",
    "                loss = loss_fn(pred, b.y.view(-1,1))\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            if clip_norm is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "            scaler.step(opt); scaler.update()\n",
    "\n",
    "            total += loss.item() * b.num_graphs\n",
    "            count += b.num_graphs\n",
    "\n",
    "        tr_mse = total / max(1, count)\n",
    "        mae, rmse, r2 = eval_once(val_loader)\n",
    "        print(f\"Epoch {ep:03d} | tr_MSE {tr_mse:.5f} | val_MAE {mae:.5f} | val_RMSE {rmse:.5f} | R2 {r2:.4f}\")\n",
    "\n",
    "        if mae < best_mae - 1e-6:\n",
    "            best_mae = mae\n",
    "            best = deepcopy(model.state_dict())\n",
    "            torch.save(best, best_path)\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best is not None:\n",
    "        model.load_state_dict(best)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "    final_mae, final_rmse, final_r2 = eval_once(val_loader)\n",
    "    print(f\"[{tag}] Best Val — MAE {final_mae:.6f} | RMSE {final_rmse:.6f} | R2 {final_r2:.4f}\")\n",
    "    return model, best_path, {\"MAE\": final_mae, \"RMSE\": final_rmse, \"R2\": final_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0813a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdkit_dim = 534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | tr_MSE 13993.92983 | val_MAE 61.15255 | val_RMSE 76.70567 | R2 0.3837\n",
      "Epoch 002 | tr_MSE 7647.65189 | val_MAE 61.87417 | val_RMSE 77.29400 | R2 0.3742\n",
      "Epoch 003 | tr_MSE 6243.57543 | val_MAE 57.64693 | val_RMSE 72.49866 | R2 0.4494\n",
      "Epoch 004 | tr_MSE 5548.85468 | val_MAE 56.52554 | val_RMSE 72.06252 | R2 0.4560\n",
      "Epoch 005 | tr_MSE 6917.44974 | val_MAE 64.25526 | val_RMSE 85.26950 | R2 0.2383\n",
      "Epoch 006 | tr_MSE 5871.44264 | val_MAE 70.17628 | val_RMSE 88.51374 | R2 0.1793\n",
      "Epoch 007 | tr_MSE 5140.51730 | val_MAE 57.65596 | val_RMSE 72.12613 | R2 0.4551\n",
      "Epoch 008 | tr_MSE 5148.55241 | val_MAE 57.85924 | val_RMSE 74.50130 | R2 0.4186\n",
      "Epoch 009 | tr_MSE 4711.17384 | val_MAE 57.34474 | val_RMSE 71.45979 | R2 0.4651\n",
      "Epoch 010 | tr_MSE 4864.64484 | val_MAE 57.69729 | val_RMSE 71.95400 | R2 0.4576\n",
      "Epoch 011 | tr_MSE 4536.62997 | val_MAE 54.10973 | val_RMSE 69.31212 | R2 0.4967\n",
      "Epoch 012 | tr_MSE 4330.23838 | val_MAE 59.02861 | val_RMSE 75.16349 | R2 0.4082\n",
      "Epoch 013 | tr_MSE 4171.91256 | val_MAE 57.28464 | val_RMSE 74.98363 | R2 0.4110\n",
      "Epoch 014 | tr_MSE 4124.99231 | val_MAE 57.31318 | val_RMSE 74.55429 | R2 0.4177\n",
      "Epoch 015 | tr_MSE 3961.70944 | val_MAE 55.45193 | val_RMSE 72.57803 | R2 0.4482\n",
      "Epoch 016 | tr_MSE 3840.41347 | val_MAE 56.06129 | val_RMSE 71.75575 | R2 0.4606\n",
      "Epoch 017 | tr_MSE 3756.12058 | val_MAE 53.18792 | val_RMSE 70.10149 | R2 0.4852\n",
      "Epoch 018 | tr_MSE 3593.56239 | val_MAE 55.41759 | val_RMSE 73.07773 | R2 0.4406\n",
      "Epoch 019 | tr_MSE 3516.32122 | val_MAE 53.88855 | val_RMSE 70.32583 | R2 0.4819\n",
      "Epoch 020 | tr_MSE 3386.49656 | val_MAE 54.17081 | val_RMSE 70.32245 | R2 0.4820\n",
      "Epoch 021 | tr_MSE 3425.33926 | val_MAE 54.43021 | val_RMSE 72.22775 | R2 0.4535\n",
      "Epoch 022 | tr_MSE 3398.99427 | val_MAE 55.26068 | val_RMSE 70.65063 | R2 0.4771\n",
      "Epoch 023 | tr_MSE 3217.91478 | val_MAE 59.37690 | val_RMSE 77.65231 | R2 0.3683\n",
      "Epoch 024 | tr_MSE 3159.18174 | val_MAE 53.08670 | val_RMSE 69.82796 | R2 0.4892\n",
      "Epoch 025 | tr_MSE 2912.00997 | val_MAE 58.82219 | val_RMSE 76.87049 | R2 0.3810\n",
      "Epoch 026 | tr_MSE 2856.18915 | val_MAE 57.61105 | val_RMSE 75.34266 | R2 0.4054\n",
      "Epoch 027 | tr_MSE 2756.12528 | val_MAE 56.79610 | val_RMSE 73.79233 | R2 0.4296\n",
      "Epoch 028 | tr_MSE 2631.79067 | val_MAE 56.05808 | val_RMSE 74.07567 | R2 0.4252\n",
      "Epoch 029 | tr_MSE 2527.73148 | val_MAE 55.34780 | val_RMSE 73.83380 | R2 0.4289\n"
     ]
    }
   ],
   "source": [
    "# Introspect dims from a real batch\n",
    "b_tg = next(iter(train_loader_tg))\n",
    "rd_dim = b_tg.rdkit_feats.shape[-1]           # 15 if you rebuilt with 15 globals\n",
    "print(\"rdkit_dim =\", rd_dim)\n",
    "\n",
    "# Tg \n",
    "model_tg = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "\n",
    "model_tg, ckpt_tg, metrics_tg = train_hybrid_gnn_sota(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=0.0005555079210176292, optimizer=\"RMSprop\", weight_decay=9.056299733554687e-06,\n",
    "    epochs=200, warmup_epochs=5, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gnn_tg_v2\", tag=\"hybridgnn_tg_v2\"\n",
    ")\n",
    "# # FFV\n",
    "model_ffv = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "\n",
    "model_ffv, ckpt_ffv, metrics_ffv = train_hybrid_gnn_sota(\n",
    "    model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "    lr=0.0005555079210176292, optimizer=\"RMSprop\", weight_decay=9.056299733554687e-06,\n",
    "    epochs=200, warmup_epochs=5, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gnn_ffv_v2\", tag=\"hybridgnn_ffv_v2\"\n",
    ")\n",
    "\n",
    "# Tc\n",
    "model_tc = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "\n",
    "model_tc, ckpt_tc, metrics_tc = train_hybrid_gnn_sota(\n",
    "    model_tc, train_loader_tc, val_loader_tc,\n",
    "    lr=0.0005555079210176292, optimizer=\"RMSprop\", weight_decay=9.056299733554687e-06,\n",
    "    epochs=200, warmup_epochs=5, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gnn_tc_v2\", tag=\"hybridgnn_tc_v2\"\n",
    ")\n",
    "\n",
    "# Density (use your tuned dims if you like larger backbones)\n",
    "model_den = HybridGNNv2(\n",
    "    gnn_dim=1024, rdkit_dim=rd_dim, hidden_dim=384,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.1, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "model_den, ckpt_den, metrics_den = train_hybrid_gnn_sota(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=5.956024201538505e-04, optimizer=\"AdamW\", weight_decay=8.619671341229739e-06,\n",
    "    epochs=200, warmup_epochs=8, patience=30,\n",
    "    clip_norm=0.5, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gnn_density_v2\", tag=\"hybridgnn_density_v2\"\n",
    ")\n",
    "\n",
    "# Rg (your tuned gnn_dim + swish + RMSprop work fine here)\n",
    "model_rg = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "model_rg, ckpt_rg, metrics_rg = train_hybrid_gnn_sota(\n",
    "    model_rg, train_loader_rg, val_loader_rg,\n",
    "    lr=5.6e-4, optimizer=\"RMSprop\", weight_decay=9.0e-6,\n",
    "    epochs=120, warmup_epochs=6, patience=20,\n",
    "    clip_norm=0.5, amp=True, loss_name=\"huber\",  # Huber often helps Rg\n",
    "    save_dir=\"saved_models/gnn_rg_v2\", tag=\"hybridgnn_rg_v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a821c8d",
   "metadata": {},
   "source": [
    "\n",
    "| Model Type | Feature | MAE | RMSE | R2 |\n",
    "|---|---|---|---|---|\n",
    "| GNN2 | Tg | 47.105114 | 61.480179 | 0.6040 |\n",
    "| GNN2_Aug | Tg | 51.539692 | 70.575638 | 0.4782 |\n",
    "| GNN2_Aug | Tg+gap | 51.549877 | 64.655472 | 0.5621 |\n",
    "| GNN2_Aug | Tg+embed |  |  |  |\n",
    "| GNN2_Aug | Tg+gap+embed |  |  |  |\n",
    "| GNN2 | Tc | 0.025115 | 0.041331 | 0.8000 |\n",
    "| GNN2_Aug | Tc | 0.025252 | 0.039670 | 0.8157 |\n",
    "| GNN2_Aug | Tc+gap | 0.025661 | 0.038501 | 0.8264 |\n",
    "| GNN2_Aug | Tc+embed | 0. | 0. | 0. |\n",
    "| GNN2_Aug | Tc+gap+embed | 0. | 0. | 0. |\n",
    "| GNN2 | Density | 0.031735 | 0.067845 | 0.7379 |\n",
    "| GNN2_Aug | Density | 0.030458 | 0.070372 | 0.7180 |\n",
    "| GNN2_Aug | Density+gap | 0.031054 | 0.069279 | 0.7267 |\n",
    "| GNN2_Aug | Density+embed | 0. | 0. | 0. |\n",
    "| GNN2_Aug | Density+gap+embed | 0. | 0. | 0. |\n",
    "| GNN2 | FFV | 0.013817 | 0.023902 | 0.4473 |\n",
    "| GNN2_Aug | FFV | 0.013092 | 0.022793 | 0.4974 |\n",
    "| GNN2_Aug | FFV+gap | 0.009509 | 0.014558 | 0.7949 |\n",
    "| GNN2_Aug | FFV+embed | 0. | 0. | 0. |\n",
    "| GNN2_Aug | FFV+gap+embed | 0. | 0. | 0. |\n",
    "| GNN2 | Rg | 2.115880 | 2.801481 | 0.6434 |\n",
    "| GNN2_Aug | Rg | 1.532573 | 2.405382 | 0.7371 |\n",
    "| GNN2_Aug | Rg+gap | 1.713354 | 2.783950 | 0.6479 |\n",
    "| GNN2_Aug | Rg+embed | 1. | 2. | 0. |\n",
    "| GNN2_Aug | Rg+gap+embed | 1. | 2. | 0. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1bc6e",
   "metadata": {},
   "source": [
    "\n",
    "| Model Type | Feature | MAE | RMSE | R2 |\n",
    "|---|---|---|---|---|\n",
    "| RF3D | Tg | 58.315801 | 74.296699 | 0.5846 |\n",
    "| RF3D_Aug | Tg | 58.143107 | 74.521032 | 0.5821 |\n",
    "| GNN2 | Tg | 47.105114 | 61.480179 | 0.6040 |\n",
    "| GNN2_Aug | Tg | 51.539692 | 70.575638 | 0.4782 |\n",
    "| RF3D | Tc | 0.029937 | 0.045036 | 0.7313 |\n",
    "| RF3D_Aug | Tc | 0.029675 | 0.044853 | 0.7335 |\n",
    "| GNN2 | Tc | 0.025115 | 0.041331 | 0.8000 |\n",
    "| GNN2_Aug | Tc | 0.025252 | 0.039670 | 0.8157 |\n",
    "| RF3D | Density | 0.037793 | 0.070932 | 0.7847 |\n",
    "| RF3D_Aug | Density | 0.037123 | 0.070212 | 0.7891 |\n",
    "| GNN2 | Density | 0.031735 | 0.067845 | 0.7379 |\n",
    "| GNN2_Aug | Density | 0.030458 | 0.070372 | 0.7180 |\n",
    "| RF3D | FFV | 0.007621 | 0.017553 | 0.6605 |\n",
    "| RF3D_Aug | FFV | 0.007578 | 0.017404 | 0.6662 |\n",
    "| GNN2 | FFV | 0.013817 | 0.023902 | 0.4473 |\n",
    "| GNN2_Aug | FFV | 0.013092 | 0.022793 | 0.4974 |\n",
    "| RF3D | Rg | 1.648818 | 2.493712 | 0.7299 |\n",
    "| RF3D_Aug | Rg | 1.668425 | 2.517235 | 0.7248 |\n",
    "| GNN2 | Rg | 2.115880 | 2.801481 | 0.6434 |\n",
    "| GNN2_Aug | Rg | 1.532573 | 2.405382 | 0.7371 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673460",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
