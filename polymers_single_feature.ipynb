{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c615e0e2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-23T05:01:54.324857Z",
     "iopub.status.busy": "2025-07-23T05:01:54.324606Z",
     "iopub.status.idle": "2025-07-23T05:01:56.365975Z",
     "shell.execute_reply": "2025-07-23T05:01:56.365092Z"
    },
    "papermill": {
     "duration": 2.048016,
     "end_time": "2025-07-23T05:01:56.367363",
     "exception": false,
     "start_time": "2025-07-23T05:01:54.319347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb22f53",
   "metadata": {
    "papermill": {
     "duration": 0.002914,
     "end_time": "2025-07-23T05:01:56.374004",
     "exception": false,
     "start_time": "2025-07-23T05:01:56.371090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NeurIPS 2025 Open Polymer Prediction: Baseline Pipeline\n",
    "\n",
    "This notebook runs the full baseline pipeline for the competition, including:\n",
    "- Data preparation (LMDB creation)\n",
    "- Model training and validation\n",
    "- Test prediction and submission generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0bc8011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T05:01:56.381100Z",
     "iopub.status.busy": "2025-07-23T05:01:56.380771Z",
     "iopub.status.idle": "2025-07-23T05:03:41.036564Z",
     "shell.execute_reply": "2025-07-23T05:03:41.035578Z"
    },
    "papermill": {
     "duration": 104.661117,
     "end_time": "2025-07-23T05:03:41.038168",
     "exception": false,
     "start_time": "2025-07-23T05:01:56.377051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torch_geometric\n",
    "# !pip install rdkit \n",
    "# !pip install ogb\n",
    "# !pip install lmdb\n",
    "# !pip install lz4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47963b58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T05:03:41.090980Z",
     "iopub.status.busy": "2025-07-23T05:03:41.090724Z",
     "iopub.status.idle": "2025-07-23T05:03:58.507303Z",
     "shell.execute_reply": "2025-07-23T05:03:58.506190Z"
    },
    "papermill": {
     "duration": 17.444366,
     "end_time": "2025-07-23T05:03:58.508474",
     "exception": true,
     "start_time": "2025-07-23T05:03:41.064108",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/polymer')\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "from polymer_model import PolymerPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08cd22",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2. Check/Create LMDBs (train & test)\n",
    "If LMDBs are missing, run the builder scripts. Comment out after first run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7895578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: data\n",
      "LMDB directory: data\\processed_chunks\n",
      "Train LMDB: data\\processed_chunks\\polymer_train3d_dist.lmdb\n",
      "Test LMDB: data\\processed_chunks\\polymer_test3d_dist.lmdb\n",
      "LMDBs already exist.\n"
     ]
    }
   ],
   "source": [
    "# Paths - Fixed for Kaggle environment\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  # Writable directory\n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/best_gnn_transformer_hybrid.pt'\n",
    "else:\n",
    "    DATA_ROOT = 'data'\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = 'best_gnn_transformer_hybrid.pt'\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"LMDB directory: {CHUNK_DIR}\")\n",
    "print(f\"Train LMDB: {TRAIN_LMDB}\")\n",
    "print(f\"Test LMDB: {TEST_LMDB}\")\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    # Run the LMDB builders\n",
    "    !python build_polymer_lmdb_fixed.py train\n",
    "    !python build_polymer_lmdb_fixed.py test\n",
    "    print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88fcc8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(DATA_ROOT, \"test.csv\"))\n",
    "\n",
    "def ids_for_task(task):\n",
    "    tcol = label_cols[task2idx[task]]\n",
    "    df = train_df[['id', tcol]].copy()\n",
    "    df = df[~df[tcol].isna()]\n",
    "    return df['id'].values.astype(train_df['id'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8fcea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def make_single_task_model(backbone_ckpt, use_gap=False, freeze=True):\n",
    "    m = PolymerPredictor(\n",
    "        backbone_ckpt=backbone_ckpt,\n",
    "        n_out=1,\n",
    "        freeze=freeze,\n",
    "        use_gap=use_gap  # <- honor the argument\n",
    "    ).to(device)\n",
    "    if freeze:\n",
    "        m.backbone.eval()  # deterministic trunk during head-only stage\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10cccba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "# ---- Helpers ----\n",
    "def filter_ids_with_label(task: str, ids):\n",
    "    \"\"\"Return only those ids (from the given pool) that have a label for this task.\"\"\"\n",
    "    tcol = label_cols[task2idx[task]]\n",
    "    sub = train_df.loc[train_df['id'].isin(ids), ['id', tcol]]\n",
    "    return sub.loc[~sub[tcol].isna(), 'id'].values\n",
    "\n",
    "def make_random_pools(train_ratio=0.9, seed=42):\n",
    "    \"\"\"Create a global pool split; you can reuse these pools for every task.\"\"\"\n",
    "    all_ids = train_df['id'].values\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm = rng.permutation(len(all_ids))\n",
    "    split = int(train_ratio * len(all_ids))\n",
    "    return all_ids[perm[:split]], all_ids[perm[split:]]\n",
    "\n",
    "def make_task_loaders(task, train_pool_ids, val_pool_ids,\n",
    "                      batch_size=128, num_workers=4, shuffle=True):\n",
    "    \"\"\"\n",
    "    Build loaders for a single task.\n",
    "    Both datasets read from TRAIN_LMDB (validation is a subset of training set).\n",
    "    \"\"\"\n",
    "    # Keep only rows that actually have this task’s label\n",
    "    train_ids = filter_ids_with_label(task, train_pool_ids)\n",
    "    val_ids   = filter_ids_with_label(task, val_pool_ids)\n",
    "\n",
    "    # Fallback if val set ends up empty or tiny\n",
    "    if len(val_ids) < max(32, int(0.05 * len(train_ids))):\n",
    "        n_val = min(max(32, int(0.1 * len(train_ids))), len(train_ids)//5 or 1)\n",
    "        val_ids = train_ids[:n_val]\n",
    "        train_ids = train_ids[n_val:]\n",
    "\n",
    "    # Build datasets from the *training* LMDB for both splits\n",
    "    train_ds = LMDBDataset(train_ids, TRAIN_LMDB)\n",
    "    val_ds   = LMDBDataset(val_ids,   TRAIN_LMDB)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, val_loader, train_ids, val_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ded208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Make global pools once (reuse for all tasks)\n",
    "train_pool_ids, val_pool_ids = make_random_pools(train_ratio=0.9, seed=123)\n",
    "\n",
    "# 2) Build loaders per task (example: Tc)\n",
    "train_loader_tc, val_loader_tc, tr_ids_tc, va_ids_tc = make_task_loaders(\n",
    "    \"Tc\", train_pool_ids, val_pool_ids, batch_size=256, num_workers=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60ded8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3. Train/Validation Split\n",
    "Create a stratified split based on label availability (how many non-NaN values each row has).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c87f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mae_1d(pred, true):\n",
    "    # pred: [B,1], true: [B,5] (we’ll slice the right column before calling)\n",
    "    return (pred.squeeze(-1) - true).abs().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eca56a9e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_single_task(task, backbone_ckpt, train_loader, val_loader, use_gap=False,\n",
    "                      epochs_head=10, epochs_ft=0, lr_head=1e-3, lr_bb=1e-6, clip=0.5):\n",
    "    t = task2idx[task]\n",
    "    model = make_single_task_model(backbone_ckpt, use_gap=use_gap, freeze=True)\n",
    "    opt   = torch.optim.AdamW(model.head.parameters(), lr=lr_head, weight_decay=1e-5)\n",
    "    best = {\"mae\": float(\"inf\"), \"path\": f\"best_{task}.pt\", \"epoch\": -1}\n",
    "\n",
    "    # ---- Stage 1: head-only ----\n",
    "    for epoch in range(epochs_head):\n",
    "        model.train(); model.backbone.eval()\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            y = batch.y[:, t]; mask = ~torch.isnan(y)\n",
    "            if mask.sum() == 0: continue\n",
    "            pred = model(batch).squeeze(-1)[mask]\n",
    "            loss = (pred - y[mask]).abs().mean()\n",
    "            opt.zero_grad(set_to_none=True); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.head.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "        # val\n",
    "        model.eval(); s=n=0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                y = batch.y[:, t]; mask = ~torch.isnan(y)\n",
    "                if mask.sum() == 0: continue\n",
    "                pred = model(batch).squeeze(-1)[mask]\n",
    "                s += (pred - y[mask]).abs().sum().item(); n += mask.sum().item()\n",
    "        val_mae = s / max(1, n)\n",
    "        if val_mae < best[\"mae\"]:\n",
    "            best.update(mae=val_mae, epoch=epoch)\n",
    "            torch.save({\"model\": model.state_dict()}, best[\"path\"])\n",
    "        print(f\"[{task}] Stage1 Ep{epoch+1}: val_MAE={val_mae:.6f}\")\n",
    "\n",
    "    # ---- Stage 2: unfreeze last block (optional) ----\n",
    "    if epochs_ft > 0:\n",
    "        layers = getattr(model.backbone.transformer, \"layers\", None) or getattr(model.backbone.transformer, \"encoder_layers\", None)\n",
    "        if layers is not None:\n",
    "            for p in model.backbone.parameters(): p.requires_grad = False\n",
    "            for p in layers[-1].parameters(): p.requires_grad = True\n",
    "            for m in model.backbone.modules():\n",
    "                if isinstance(m, torch.nn.LayerNorm):\n",
    "                    for p in m.parameters(): p.requires_grad = True\n",
    "\n",
    "            opt = torch.optim.AdamW([\n",
    "                {\"params\": model.head.parameters(), \"lr\": lr_head, \"weight_decay\": 1e-5},\n",
    "                {\"params\": layers[-1].parameters(), \"lr\": lr_bb, \"weight_decay\": 1e-5},\n",
    "            ])\n",
    "\n",
    "            for epoch in range(epochs_ft):\n",
    "                model.train()\n",
    "                for batch in train_loader:\n",
    "                    batch = batch.to(device)\n",
    "                    y = batch.y[:, t]; mask = ~torch.isnan(y)\n",
    "                    if mask.sum() == 0: continue\n",
    "                    pred = model(batch).squeeze(-1)[mask]\n",
    "                    loss = (pred - y[mask]).abs().mean()\n",
    "                    opt.zero_grad(set_to_none=True); loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                    opt.step()\n",
    "\n",
    "                model.eval(); s=n=0\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_loader:\n",
    "                        batch = batch.to(device)\n",
    "                        y = batch.y[:, t]; mask = ~torch.isnan(y)\n",
    "                        if mask.sum() == 0: continue\n",
    "                        pred = model(batch).squeeze(-1)[mask]\n",
    "                        s += (pred - y[mask]).abs().sum().item(); n += mask.sum().item()\n",
    "                val_mae = s / max(1, n)\n",
    "                if val_mae < best[\"mae\"]:\n",
    "                    best.update(mae=val_mae, epoch=epoch+epochs_head)\n",
    "                    torch.save({\"model\": model.state_dict()}, best[\"path\"])\n",
    "                print(f\"[{task}] Stage2 Ep{epoch+1}: val_MAE={val_mae:.6f}\")\n",
    "\n",
    "    print(f\"[{task}] Best val_MAE={best['mae']:.6f} (epoch {best['epoch']+1})  -> {best['path']}\")\n",
    "    # free memory before returning\n",
    "    del opt, model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return best[\"path\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2fdd3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tg ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\Downloads\\kaggle_polymers_competition\\polymer_model.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(backbone_ckpt, map_location='cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] Stage1 Ep1: val_MAE=102.855739\n",
      "[Tg] Stage1 Ep2: val_MAE=101.301799\n",
      "[Tg] Stage1 Ep3: val_MAE=99.754010\n",
      "[Tg] Stage1 Ep4: val_MAE=98.209504\n",
      "[Tg] Stage1 Ep5: val_MAE=96.666701\n",
      "[Tg] Stage1 Ep6: val_MAE=95.126932\n",
      "[Tg] Stage1 Ep7: val_MAE=93.591516\n",
      "[Tg] Stage1 Ep8: val_MAE=92.105219\n",
      "[Tg] Stage1 Ep9: val_MAE=91.016789\n",
      "[Tg] Stage1 Ep10: val_MAE=90.065118\n",
      "[Tg] Stage1 Ep11: val_MAE=89.272149\n",
      "[Tg] Stage1 Ep12: val_MAE=88.524456\n",
      "[Tg] Stage1 Ep13: val_MAE=87.868299\n",
      "[Tg] Stage1 Ep14: val_MAE=87.201099\n",
      "[Tg] Stage1 Ep15: val_MAE=86.527837\n",
      "[Tg] Stage1 Ep16: val_MAE=86.013625\n",
      "[Tg] Stage1 Ep17: val_MAE=85.487429\n",
      "[Tg] Stage1 Ep18: val_MAE=84.950045\n",
      "[Tg] Stage1 Ep19: val_MAE=84.402775\n",
      "[Tg] Stage1 Ep20: val_MAE=83.967254\n",
      "[Tg] Stage1 Ep21: val_MAE=83.556573\n",
      "[Tg] Stage1 Ep22: val_MAE=83.192289\n",
      "[Tg] Stage1 Ep23: val_MAE=82.981799\n",
      "[Tg] Stage1 Ep24: val_MAE=82.748520\n",
      "[Tg] Stage1 Ep25: val_MAE=82.486463\n",
      "[Tg] Stage1 Ep26: val_MAE=82.218282\n",
      "[Tg] Stage1 Ep27: val_MAE=81.934487\n",
      "[Tg] Stage1 Ep28: val_MAE=81.655289\n",
      "[Tg] Stage1 Ep29: val_MAE=81.370335\n",
      "[Tg] Stage1 Ep30: val_MAE=81.085558\n",
      "[Tg] Stage1 Ep31: val_MAE=80.796413\n",
      "[Tg] Stage1 Ep32: val_MAE=80.482141\n",
      "[Tg] Stage1 Ep33: val_MAE=80.192139\n",
      "[Tg] Stage1 Ep34: val_MAE=79.900936\n",
      "[Tg] Stage1 Ep35: val_MAE=79.618065\n",
      "[Tg] Stage2 Ep1: val_MAE=79.302423\n",
      "[Tg] Stage2 Ep2: val_MAE=79.045530\n",
      "[Tg] Stage2 Ep3: val_MAE=78.825481\n",
      "[Tg] Stage2 Ep4: val_MAE=78.645778\n",
      "[Tg] Stage2 Ep5: val_MAE=78.416888\n",
      "[Tg] Stage2 Ep6: val_MAE=78.178430\n",
      "[Tg] Stage2 Ep7: val_MAE=77.929449\n",
      "[Tg] Stage2 Ep8: val_MAE=77.732785\n",
      "[Tg] Stage2 Ep9: val_MAE=77.547748\n",
      "[Tg] Stage2 Ep10: val_MAE=77.345989\n",
      "[Tg] Stage2 Ep11: val_MAE=77.126397\n",
      "[Tg] Stage2 Ep12: val_MAE=76.895217\n",
      "[Tg] Stage2 Ep13: val_MAE=76.694113\n",
      "[Tg] Stage2 Ep14: val_MAE=76.465171\n",
      "[Tg] Stage2 Ep15: val_MAE=76.242260\n",
      "[Tg] Best val_MAE=76.242260 (epoch 50)  -> best_Tg.pt\n",
      "\n",
      "=== Density ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\Downloads\\kaggle_polymers_competition\\polymer_model.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(backbone_ckpt, map_location='cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Density] Stage1 Ep1: val_MAE=0.385057\n",
      "[Density] Stage1 Ep2: val_MAE=0.639259\n",
      "[Density] Stage1 Ep3: val_MAE=0.325292\n",
      "[Density] Stage1 Ep4: val_MAE=0.454525\n",
      "[Density] Stage1 Ep5: val_MAE=0.528695\n",
      "[Density] Stage1 Ep6: val_MAE=0.178364\n",
      "[Density] Stage1 Ep7: val_MAE=0.199806\n",
      "[Density] Stage1 Ep8: val_MAE=0.308889\n",
      "[Density] Stage1 Ep9: val_MAE=0.337332\n",
      "[Density] Stage1 Ep10: val_MAE=0.293236\n",
      "[Density] Stage1 Ep11: val_MAE=0.334286\n",
      "[Density] Stage1 Ep12: val_MAE=0.350169\n",
      "[Density] Stage1 Ep13: val_MAE=0.177201\n",
      "[Density] Stage1 Ep14: val_MAE=0.182994\n",
      "[Density] Stage1 Ep15: val_MAE=0.282225\n",
      "[Density] Best val_MAE=0.177201 (epoch 13)  -> best_Density.pt\n",
      "\n",
      "=== FFV ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\Downloads\\kaggle_polymers_competition\\polymer_model.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(backbone_ckpt, map_location='cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FFV] Stage1 Ep1: val_MAE=0.102002\n",
      "[FFV] Stage1 Ep2: val_MAE=0.398115\n",
      "[FFV] Stage1 Ep3: val_MAE=0.369949\n",
      "[FFV] Stage1 Ep4: val_MAE=0.262169\n",
      "[FFV] Stage1 Ep5: val_MAE=0.506806\n",
      "[FFV] Stage1 Ep6: val_MAE=0.823731\n",
      "[FFV] Stage1 Ep7: val_MAE=0.250766\n",
      "[FFV] Stage1 Ep8: val_MAE=0.205098\n",
      "[FFV] Best val_MAE=0.102002 (epoch 1)  -> best_FFV.pt\n",
      "\n",
      "=== Tc ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\Downloads\\kaggle_polymers_competition\\polymer_model.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(backbone_ckpt, map_location='cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tc] Stage1 Ep1: val_MAE=0.660578\n",
      "[Tc] Stage1 Ep2: val_MAE=0.618187\n",
      "[Tc] Stage1 Ep3: val_MAE=0.361392\n",
      "[Tc] Stage1 Ep4: val_MAE=0.452179\n",
      "[Tc] Stage1 Ep5: val_MAE=0.317230\n",
      "[Tc] Stage1 Ep6: val_MAE=0.239269\n",
      "[Tc] Stage1 Ep7: val_MAE=0.267399\n",
      "[Tc] Stage1 Ep8: val_MAE=0.229934\n",
      "[Tc] Stage1 Ep9: val_MAE=0.131891\n",
      "[Tc] Stage1 Ep10: val_MAE=0.167806\n",
      "[Tc] Stage1 Ep11: val_MAE=0.220565\n",
      "[Tc] Stage1 Ep12: val_MAE=0.272527\n",
      "[Tc] Stage1 Ep13: val_MAE=0.341904\n",
      "[Tc] Stage1 Ep14: val_MAE=0.232151\n",
      "[Tc] Stage1 Ep15: val_MAE=0.211043\n",
      "[Tc] Stage1 Ep16: val_MAE=0.154723\n",
      "[Tc] Stage1 Ep17: val_MAE=0.101437\n",
      "[Tc] Stage1 Ep18: val_MAE=0.088941\n",
      "[Tc] Stage2 Ep1: val_MAE=0.891244\n",
      "[Tc] Stage2 Ep2: val_MAE=0.304782\n",
      "[Tc] Stage2 Ep3: val_MAE=0.079850\n",
      "[Tc] Stage2 Ep4: val_MAE=0.265995\n",
      "[Tc] Stage2 Ep5: val_MAE=0.495177\n",
      "[Tc] Best val_MAE=0.079850 (epoch 21)  -> best_Tc.pt\n",
      "\n",
      "=== Rg ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\Downloads\\kaggle_polymers_competition\\polymer_model.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(backbone_ckpt, map_location='cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rg] Stage1 Ep1: val_MAE=12.244028\n",
      "[Rg] Stage1 Ep2: val_MAE=7.063822\n",
      "[Rg] Stage1 Ep3: val_MAE=3.710335\n",
      "[Rg] Stage1 Ep4: val_MAE=4.457699\n",
      "[Rg] Stage1 Ep5: val_MAE=4.955257\n",
      "[Rg] Stage1 Ep6: val_MAE=4.309592\n",
      "[Rg] Stage1 Ep7: val_MAE=3.350820\n",
      "[Rg] Stage1 Ep8: val_MAE=3.448620\n",
      "[Rg] Stage1 Ep9: val_MAE=3.417158\n",
      "[Rg] Stage1 Ep10: val_MAE=3.060979\n",
      "[Rg] Stage1 Ep11: val_MAE=3.007648\n",
      "[Rg] Stage1 Ep12: val_MAE=2.983620\n",
      "[Rg] Stage1 Ep13: val_MAE=2.992419\n",
      "[Rg] Stage1 Ep14: val_MAE=3.080963\n",
      "[Rg] Stage1 Ep15: val_MAE=2.979286\n",
      "[Rg] Stage1 Ep16: val_MAE=2.765390\n",
      "[Rg] Stage1 Ep17: val_MAE=2.729361\n",
      "[Rg] Stage1 Ep18: val_MAE=2.716618\n",
      "[Rg] Stage1 Ep19: val_MAE=2.731581\n",
      "[Rg] Stage1 Ep20: val_MAE=2.711654\n",
      "[Rg] Stage1 Ep21: val_MAE=2.693192\n",
      "[Rg] Stage1 Ep22: val_MAE=2.650998\n",
      "[Rg] Stage1 Ep23: val_MAE=2.665797\n",
      "[Rg] Stage1 Ep24: val_MAE=2.595493\n",
      "[Rg] Stage1 Ep25: val_MAE=2.611911\n",
      "[Rg] Stage2 Ep1: val_MAE=2.630915\n",
      "[Rg] Stage2 Ep2: val_MAE=2.783308\n",
      "[Rg] Stage2 Ep3: val_MAE=2.632620\n",
      "[Rg] Stage2 Ep4: val_MAE=2.732647\n",
      "[Rg] Stage2 Ep5: val_MAE=2.592062\n",
      "[Rg] Stage2 Ep6: val_MAE=2.679921\n",
      "[Rg] Best val_MAE=2.592062 (epoch 30)  -> best_Rg.pt\n",
      "Best checkpoints: {'Tg': 'best_Tg.pt', 'Density': 'best_Density.pt', 'FFV': 'best_FFV.pt', 'Tc': 'best_Tc.pt', 'Rg': 'best_Rg.pt'}\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE  = 256\n",
    "\n",
    "# make global pools once\n",
    "train_pool_ids, val_pool_ids = make_random_pools(train_ratio=0.9, seed=123)\n",
    "\n",
    "task_cfg = {\n",
    "    \"Tg\":      {\"use_gap\": True,  \"epochs_head\": 35, \"epochs_ft\": 15, \"lr_head\": 3e-4, \"lr_bb\": 1e-6},\n",
    "    \"Density\": {\"use_gap\": True,  \"epochs_head\": 15, \"epochs_ft\": 0,  \"lr_head\": 5e-4, \"lr_bb\": 0.0},\n",
    "    \"FFV\":     {\"use_gap\": False, \"epochs_head\": 8,  \"epochs_ft\": 0,  \"lr_head\": 1e-3, \"lr_bb\": 0.0},\n",
    "    \"Tc\":      {\"use_gap\": False, \"epochs_head\": 18, \"epochs_ft\": 5,  \"lr_head\": 5e-4, \"lr_bb\": 1e-6},\n",
    "    \"Rg\":      {\"use_gap\": False, \"epochs_head\": 25, \"epochs_ft\": 6,  \"lr_head\": 5e-4, \"lr_bb\": 1e-6},\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "best_ckpts = {}\n",
    "for task, cfg in task_cfg.items():\n",
    "    print(f\"\\n=== {task} ===\")\n",
    "    train_loader, val_loader, _, _ = make_task_loaders(\n",
    "        task, train_pool_ids, val_pool_ids,\n",
    "        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True\n",
    "    )\n",
    "    ckpt = train_single_task(\n",
    "        task, BACKBONE_PATH, train_loader, val_loader,\n",
    "        use_gap=cfg[\"use_gap\"],\n",
    "        epochs_head=cfg[\"epochs_head\"], epochs_ft=cfg[\"epochs_ft\"],\n",
    "        lr_head=cfg[\"lr_head\"], lr_bb=cfg[\"lr_bb\"],\n",
    "        clip=0.5,\n",
    "    )\n",
    "    best_ckpts[task] = ckpt\n",
    "\n",
    "print(\"Best checkpoints:\", best_ckpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7d142",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4. Data Loading\n",
    "Create DataLoaders using the fast LMDB datasets for both train and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cebcffae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattg\\AppData\\Local\\Temp\\ipykernel_12980\\3944547798.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(ckpt_path, map_location=device)[\"model\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id          Tg       FFV        Tc   Density         Rg\n",
      "0  1109053969  115.004662  0.366079  0.044274  1.272828  16.929520\n",
      "1  1422188626  112.476120  0.299214  0.068348  1.243567  17.482819\n",
      "2  2032016830  130.754288  0.190284  0.083695  1.386783  16.701283 (3, 6)\n"
     ]
    }
   ],
   "source": [
    "def predict_task(task, ckpt_path, test_loader, backbone_ckpt, use_gap=False):\n",
    "    model = make_single_task_model(backbone_ckpt, use_gap=use_gap, freeze=True)\n",
    "    state = torch.load(ckpt_path, map_location=device)[\"model\"]\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    with torch.inference_mode():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch).squeeze(-1).cpu().numpy()\n",
    "            outs.append(pred)\n",
    "    return np.concatenate(outs, axis=0)\n",
    "\n",
    "# test loader (PyG)\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_loader = DataLoader(\n",
    "    LMDBDataset(test_ids, TEST_LMDB),\n",
    "    batch_size=256, shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "preds = {}\n",
    "for task in label_cols:\n",
    "    ckpt = f\"best_{task}.pt\"\n",
    "    preds[task] = predict_task(task, ckpt, test_loader, BACKBONE_PATH,\n",
    "                               use_gap=task_cfg[task][\"use_gap\"])  # <-- key change\n",
    "\n",
    "\n",
    "# Stitch exactly in sample order\n",
    "sample = pd.read_csv(os.path.join(DATA_ROOT, \"sample_submission.csv\"))\n",
    "sub = sample.copy()\n",
    "for k in label_cols:\n",
    "    sub[k] = preds[k].astype(float)\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(sub.head(), sub.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d36e690e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1109053969</td>\n",
       "      <td>115.004662</td>\n",
       "      <td>0.366079</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>1.272828</td>\n",
       "      <td>16.929520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1422188626</td>\n",
       "      <td>112.476120</td>\n",
       "      <td>0.299214</td>\n",
       "      <td>0.068348</td>\n",
       "      <td>1.243567</td>\n",
       "      <td>17.482819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2032016830</td>\n",
       "      <td>130.754288</td>\n",
       "      <td>0.226992</td>\n",
       "      <td>0.083695</td>\n",
       "      <td>1.386783</td>\n",
       "      <td>16.701283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id          Tg       FFV        Tc   Density         Rg\n",
       "0  1109053969  115.004662  0.366079  0.046500  1.272828  16.929520\n",
       "1  1422188626  112.476120  0.299214  0.068348  1.243567  17.482819\n",
       "2  2032016830  130.754288  0.226992  0.083695  1.386783  16.701283"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write submission in sample order (robust to any id ordering)\n",
    "sample = pd.read_csv(os.path.join(DATA_ROOT, \"sample_submission.csv\"))\n",
    "sub = sample[[\"id\"]].copy()  # preserve Kaggle's id order\n",
    "\n",
    "# If ids match already, this is just assignment; otherwise merge handles it.\n",
    "pred_df = pd.DataFrame({\"id\": test_ids})\n",
    "for k in label_cols:\n",
    "    pred_df[k] = preds[k].astype(float)\n",
    "\n",
    "sub = sub.merge(pred_df, on=\"id\", how=\"left\")\n",
    "\n",
    "# (Optional) clip to train ranges to prevent wild outliers\n",
    "for k in label_cols:\n",
    "    lo, hi = np.nanmin(train_df[k].values), np.nanmax(train_df[k].values)\n",
    "    sub[k] = np.clip(sub[k].values, lo, hi)\n",
    "\n",
    "# Sanity checks\n",
    "assert sub[label_cols].notna().all().all(), \"Found NaNs in predictions.\"\n",
    "assert len(sub) == len(sample), \"Submission length mismatch.\"\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12966160,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "datasetId": 7923968,
     "sourceId": 12550608,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 132.263567,
   "end_time": "2025-07-23T05:04:00.360011",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-23T05:01:48.096444",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
