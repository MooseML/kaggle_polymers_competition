{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61979795",
   "metadata": {},
   "source": [
    "# HOMO-LUMO Gap Predictions\n",
    "\n",
    "### Problem Statement & Motivation\n",
    "\n",
    "Accurately predicting quantum chemical properties like the HOMO–LUMO energy gap is essential for advancing materials science, drug discovery, and electronic design. The HOMO–LUMO gap is particularly informative for assessing molecular reactivity and stability. While Density Functional Theory (DFT) provides precise estimates, its high computational cost makes it impractical for large-scale screening of molecular libraries. This notebook explores machine learning alternatives that are fast, scalable, and interpretable, offering solutions that are accessible even on modest hardware.\n",
    "\n",
    "### Related Work & Key Gap\n",
    "\n",
    "Past work has shown that:\n",
    "\n",
    "* DFT is accurate but computationally intensive\n",
    "* ML models like kernel methods and GNNs show promise, but often require large models and expensive hardware\n",
    "\n",
    "Key Gap: A need for lightweight, high-performing models that can run locally and integrate with user-friendly tools for deployment in research or education.\n",
    "\n",
    "### Methodology & Evaluation\n",
    "\n",
    "This notebook:\n",
    "\n",
    "* Benchmarks a variety of 2D-based models using RDKit descriptors, Coulomb matrices, and graph neural networks (GNNs) on a 5k molecule subset\n",
    "* Progresses to a hybrid GNN architecture combining OGB-standard graphs with SMILES-derived cheminformatics features\n",
    "* Achieves **MAE = 0.159 eV**\n",
    "* Visualizes results using parity plots, error inspection, and predicted-vs-true comparisons\n",
    "* Evaluates both random and high-error cases to better understand model behavior\n",
    "\n",
    "| Metric   | Best Model (Hybrid GNN) |\n",
    "| -------- | ----------------------- |\n",
    "| **MAE**  | 0.159 eV                |\n",
    "| **RMSE** | 0.234 eV                |\n",
    "| **R²**   | 0.965                   |\n",
    "\n",
    "\n",
    "### Deployment & Accessibility\n",
    "\n",
    "To make the model practically useful, an **interactive web app** was developed:\n",
    "\n",
    "**Live App**: [HOMO–LUMO Gap Predictor on Hugging Face](https://huggingface.co/spaces/MooseML/homo-lumo-gap-predictor)\n",
    "\n",
    "Features:\n",
    "\n",
    "* **SMILES input** for any organic molecule\n",
    "* **Real-time prediction** of the HOMO–LUMO gap\n",
    "* **Molecular visualization**\n",
    "* Simple **CSV logging** for result tracking\n",
    "\n",
    "GitHub Repository: [MooseML/homo-lumo-gap-models](https://github.com/MooseML/homo-lumo-gap-models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ace_tools_open as tools\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import pickle\n",
    "import joblib\n",
    "import os \n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Module, Sequential, Dropout\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# OGB dataset \n",
    "from ogb.lsc import PygPCQM4Mv2Dataset, PCQM4Mv2Dataset\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# RDKit\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "\n",
    "# ChemML\n",
    "from chemml.chem import Molecule, RDKitFingerprint, CoulombMatrix, tensorise_molecules\n",
    "from chemml.models import MLP, NeuralGraphHidden, NeuralGraphOutput\n",
    "from chemml.utils import regression_metrics\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "589db70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Built with CUDA: True\n",
      "CUDA available: True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device: /physical_device:GPU:0\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: data\n",
      "LMDB directory: data\\processed_chunks\n",
      "Train LMDB: data\\processed_chunks\\polymer_train3d_dist.lmdb\n",
      "Test LMDB: data\\processed_chunks\\polymer_test3d_dist.lmdb\n",
      "LMDBs already exist.\n"
     ]
    }
   ],
   "source": [
    "# Paths - Fixed for Kaggle environment\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  # Writable directory\n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/best_gnn_transformer_hybrid.pt'\n",
    "else:\n",
    "    DATA_ROOT = 'data'\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = 'best_gnn_transformer_hybrid.pt'\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"LMDB directory: {CHUNK_DIR}\")\n",
    "print(f\"Train LMDB: {TRAIN_LMDB}\")\n",
    "print(f\"Test LMDB: {TEST_LMDB}\")\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    # Run the LMDB builders\n",
    "    !python build_polymer_lmdb_fixed.py train\n",
    "    !python build_polymer_lmdb_fixed.py test\n",
    "    print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3c34b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV with shape: (7973, 6)\n",
      "                                                 SMILES  Tg       FFV  \\\n",
      "7560  *C=Cc1ccc2c3ccc(*)cc3n(-c3ccc(OCCCCCCCCCC)c(OC... NaN  0.386695   \n",
      "1405                  *CC(=O)NCCCCCCNC(=O)Cc1ccc(O*)cc1 NaN  0.335504   \n",
      "5196                              *CC(*)c1ccccc1C(=O)NC NaN  0.355580   \n",
      "2087  *c1ccc2c(c1)C(=O)N(c1ccc(Oc3ccc(N4C(=O)c5ccc(-... NaN  0.401573   \n",
      "3337                    *CC(*)OC(=O)c1ccc(-c2ccccc2)cc1 NaN  0.353609   \n",
      "\n",
      "            Tc  Density  Rg  \n",
      "7560       NaN      NaN NaN  \n",
      "1405       NaN      NaN NaN  \n",
      "5196  0.183667      NaN NaN  \n",
      "2087       NaN      NaN NaN  \n",
      "3337       NaN      NaN NaN  \n"
     ]
    }
   ],
   "source": [
    "# /path/to/your_script.py\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_path = os.path.join(DATA_ROOT, 'train.csv')\n",
    "train_df   = pd.read_csv(train_path)\n",
    "\n",
    "#  Keep only the columns we care about \n",
    "target_cols = ['SMILES', 'Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "train_df   = train_df[target_cols]        # drops id and any other columns\n",
    "\n",
    "#  Sample a subset (optional) \n",
    "n = len(train_df)\n",
    "subset_size = n                         # change to whatever you need\n",
    "subset_df = train_df.sample(subset_size, random_state=42)\n",
    "\n",
    "#  Save the subset as a CSV \n",
    "subset_path = os.path.join(DATA_ROOT, 'train_subset.csv')\n",
    "subset_df.to_csv(subset_path, index=False)\n",
    "\n",
    "print(f\"Saved CSV with shape: {subset_df.shape}\")\n",
    "print(subset_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "22f5f955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAIAAAD2HxkiAAAaV0lEQVR4nO3de1TT5/0H8A8x3IKCCihg1Yowb0iV1nvVUtnWOqz1km7zd7K160669hzToztdaN1k3ZwntetOzuZpG1h3Ds4zj7E9bujmbGT1rmhbb3iriihCUVERCCCQPL8/njXNIiDk9vkmfb/+kiR88yHmnc+T5/skT4QQggCAj4q7AIBvOoQQgBlCCMAMIQRghhACMEMIAZghhADMEEIAZgghADOEEIAZQgjADCEEYIYQAjBDCAGYIYQAzBBCAGYIIQAzhBCAGUIIwAwhBGCGEAIwQwgBmCGEAMwQQgBmCCEAM4QQgBlCCMAMIQRghhACMEMIAZghhADMEEIAZgghADOEEIAZQgjADCEEYIYQAjBDCAGYIYQAzBBCAGYIIQAzhBCAGUIIwAwhBGCGEAIwQwgBmCGEAMwQQgBmCCEAM4QQgBlCCMAMIQRghhACMEMIAZghhADMEEIAZgghADOEEIAZQgjADCEEYIYQAjBDCAGYIYQAzBBCAGYIIQAzhBCAGUIIwAwhBGCGEAIwQwgBmCGEAMwQQgBmCCEAM4QQgBlCCMAMIQRghhACMEMIAZgpJYRNTU0LFiwYNmzY2LFjKyoquMsBCB7+EN67d+9Pf/rTmDFjtm/fXltbe/78+ZkzZ5aUlHDXBRAkEUIIrvvu6OjYtGnTm2++WVlZSUQZGRm5ubmHDx8+deoUEc2dO7e4uDgzM5OrPIAgERwcDofVanUFbMKECVar1el0ymutVmtycjIRxcbGmkymzs5OliIBgiPYIXQ6naWlpdnZ2TJ+Y8eOLSkpuT9mt2/f1uv18jaTJ0/+7LPPglwnQNAENYQ2my0nJ0dGa+TIkRaLpaOjw3Xt3r176+vr3W//r3/9a8SIEUQUGRlpNBrb2tqCWS1AcAQphDabbcqUKTJ+w4cPN5vN7okqLy/Pz88nooKCAo9ftNvtRqNRpVIRUUZGxieffBKcggGCJuAh3Ldv3xNPPCHjl5ycbDKZWltbXdceP378mWeekdcmJCSsW7euy4McOHBg/PjxRBQREaHX6xsbGwNdNkDQBDCEhw4dkv2NiBITE00mk91ud1175swZnU4nW1xcXJzBYLh+/XoPR2tvbzeZTFFRUUSUlpa2devWwFUOEEwBCeHJkye1Wq2M34ABA4xGY0NDg+vay5cv6/X6fv36EVFUVJRer6+tre39kadNmyaPrNVqb9y4EYj6AYLJzyGsqKjQarURERFE1L9/f6PRePv2bde1V69e1ev1arVazrXo9fpr16719S4cDofFYunfvz8RDRo0yGKx+PUvAAg2v4Xw7NmzOp1O9jeNRmMwGOrq6lzXXr9+3Wg0xsTEEJFKpdJqtRcvXvTl7iorK7/zne/Ilvj0009XVVX5/BcA8PBDCKuqqlz97f7h5c2bN41GY2xsrCt+58+f9/1OJavVmpSUJGOP0/oQonwKYXl5+U9/+tPo6Gg5vNTpdJWVla5rGxsbTSZTfHy8nNXMz88/duyYr/Xep66uTqfTyZY4Y8aM06dP+/0uAALK+xAWFxfLp75arX7++efd49fU1GQymQYOHChvkJeX9+mnn/qj2m5t3759+PDhOK0Pocj7EA4aNIiIUlNTz54967rQbrebzeahQ4fK+M2aNWvPnj3+qPPBGhoaDAaDPOeRlZV1+PDh4NwvgI+8D2FCQgIRlZWVyR/v3btnsVhSU1Nl/GbOnOm6Kph2794t14WrVKrJkyfjHAYon/chlHmrqamRPxYUFMj4TZs2befOnX4qzxutra2rV6+Wp0mWLVvGWAlAb3j/ecL4+Pimpqa7d+/KqZfa2tqlS5e+/vrrCxYs8O6A/qXT6TZu3JicnHzjxg3uWgB64uUn64UQdrs9IiIiLi5OXpKWlnbw4EGFJJCIZs+eTUQTJkzgLgTgAbwMod1udzqdGo1Gnp1Xgra2tqlTp7pWq8oZmvT0dNaiAB5M7d2vNTc3E5FcO6YQjY2NR48elefu6asKBwwYwFoUwIN52QmbmppIYU9xj9TJChX1MgHQJS9DqMBO6FGS3W4nhVUI0CWfOqGinuIezVmBvRqgSz51QkU9xT06oQJ7NUCXwmc4ik4IISrcJmbQCSHkoBMCMEMnBGDmUyd0rVlTAo/UKXD+FqBLYT47qqgKAboU5u8JFVUhQJfC8z1hR0dHe3u7Wq2W3+8GoGTh2QkV+BoB0J3w7IQKfI0A6E64dUJZkgJfIwC6Ez4LuN2nQxX4GgHQnfA5RYFOCCEqfIaj6IQQorwJYWdnZ1tbm1qtljtMKIHT6WxtbVWpVBqNhtAJIaR4E0JlviEUQsTFxcmvG0UnhBDiTQjvf4rLRuS3ovoOH6GA0OWfTrh+/fqsrKxdu3b5ra4+6nLhqKLWlwN0x5sQXr58mYg6Ozvlj0KIzZs3y1079Xp9Q0ODH+vrJY/Wp8DJW4DueBPCc+fOEdGVK1dqamqIKCIiYs+ePSaTKTo6uri4eNy4cR9++KGfy3yQ6OjoqVOnPvLII/JHBb5rBeiWF/tXVFdXR0ZGElFSUlJpaanr8gsXLuTm5srD5ufnV1dX+2e/jL77wQ9+QER/+9vfuAp4gI4OUV0tbt/mrgMUwZtO+NBDD23ZsmX48OH19fXPPPPMggULamtriSgjI6OsrMxiscTHx2/fvn3ixIlFRUXC2w1nfCGHxErshA4HrVpFiYn06KOUmkqPPkqHD3PXBNy8jq/T6bRYLPJ918CBAy0Wi9PplFfV1tYuWrRIHn/OnDnnzp3zzytGL9y6dauwsDA2Nnbo0KGzZ89m7MZde+MNkZ4uTpwQQoiODlFYKOLjxZUr3GUBJ5/2rBdC1NTULFy4UOZt7ty5X3zxhesqq9U6ZMgQIoqNjTWZTJ2dnT7eV88aGhpWr14t92mLiIiQnyQcOHBgUVGR69WBWUeH6N9ffPTR/1w4dar41a+YCgJF8DWEktVqTU5OJiKNRuOet9u3b+v1ehnRSZMmBWjn+ubmZpPJJLfvJqK8vLwjR47U1tYuWbJEXvL444+77+nN5uJFQSRu3vyfC197TSxezFQQKIJ/QiiEuHXrlitvkydP/uyzz1xX7dixY+TIkUSkVquNRmNra6u/7lTu0Z2SkiLvd9asWZ988on7DUpLS4cNG0ZEMTExhYWF9+7d89dde6O8XBAJjxrWrhVz5jAVBIrgtxBK//znP0eMGEFEkZGRRqOxra1NXm63241Go9zMcPTo0b5vZ9/e3m6xWGTAiGj69Onu87Tu7ty5o9fr5XK27OzsI0eO+HjX3qurE0Ti8uX/ufCVV8SPfsRTDyiDn0Movsqb3KMzMzPTvTUdPHhQbp0bERGh1+vv3r3rxfHb29tLSkpGjx4t4zdx4kSr1frA39q9e3dmZqbsxgaDQa41DZKrV4XBIAoLhRAiPV2sXfv1VS0tIjVVvP++EELU1gavpB69++67cXFx0dHRs2bNamxs5C4n/Pk/hNKBAwfGjRvnypvr/7K9vV2e1iei1NTUjzxmKXrkcDisVqvMEhGNHz/earX2ftKlpaXF1Y3T09N37drV57+qr2prxfLlIjpaEImEBNHcLP7+d6HRCJNJHD8ubDYxd67IyRFtbeLkSREdLfR6wfqk//jjj6dNm+Y+eT5mzJi9e/cylvRNEKgQiq/yFhUVRURpaWlbt251XVVRUTF9+nT535yfn19TU9PzoZxOZ2lpqWtBzKhRoywWi3fTrZ9//nlOTo58ddDpdLdu3fLiIA9WXy+MRqHRCCKhUgmtVrjO0+zZI559VowbJ6ZOFatWCTkcsFiEWi2IxMMPi507A1JSjw4cOPDkk0/KhzcpKenFF180m83y9S6wD5RXbt68+c4777g/o0JaAEMonTx5curUqfJ/V6vV3rhxQ17ucDjMZrM8n56YmLhv377ujmCz2WRsiGjkyJEWi6Wjo8OXkty7cUpKypYtW3w5mqfGRmEyiYQEQSSIRF6eOHasV7944oSYMuW/v6XVek6iBszhw4fz8/Plwzt48ODCwkLX2wRfhi0BIqfi5Cs7EeXk5NTX13MX5auAh1AI4XA4LBaLzNugQYMsFovrqsuXL3/3u99NSkq62dVzzmazuQL80EMPmc1m10yP7y5cuPDEE0+4uvG1a9d8PWJTkzCZxKBBX8fv6NG+HaGjQ5jNIi5OEIkhQ0RJia8l9ejUqVNarVZOWfXv399oNN65c+f+m/V12BIg7e3tRUVFctqPiKKiomTlKSkpH374IUtJ/hKMEEqVlZXf/va35SP49NNPX3FbJnLlviUj+/fvdy1DTU5ONplMLS0tfi+ph0U/fWK32zt//3uRlPR1/A4e9L6sS5dEXt5/DzV/vrh61ftDdePMmTM6nU5OnsXFxRmNxp5Hm/56oLzjMRcwYcKEzZs3OxyOiooK1xCad62yh5qaGoPB8OWXX/by9sELoWS1WhMTE+8/re/iPjpKTEwsLCwM9ARdTU3Ns88+K+9xzpw558+f7/3vytFRWlra6TlzBJGYMUP4Zb7H6RQlJSIxURCJ+HhhNguHww+HFeLy5ct6vV7OTkVFRen1+t4/V+SwRT5QHqujAkTOBWRnZ8s7HTt2bElJiftzxul0lpSUDB48mIgSEhLMZrPDTw+Ud27evGk0GuXXvrz66qu9/K1gh1AIUVdXp9Pp5MM6c+bM06dPy8tPnjzpGh0NGDDAaDQ2NDQErSrXop9eLrJrb29///33hw8fLv+QF596Svz7336uqaZGLFokW+Kp73/fxyW4V65c0ev1arWaiCIjI/V6vXcjcKvVmpSU1MPLqL/0fi6gtrZ28eLF8pZcq6MaGxtNJpNr1WR+fv6xXs4FsIRQ2rZtm3wGR0ZGvvjii4sWLXK9OVm1atVtjo/59HKRnRwdZWRkuEZHfTpT0melpR0jR+aMHev1op/r168bjUa5mFatVut0uosXL/pSkcfqqM8//9yXo93PZrNNmTJFHn/48OG9nAtwrY6KjY0N5uqopqYmk8k0cOBAWXBeXl5fl2eyhVAI0dDQYDAYVCqV/B6K6OjoPo2OAqSHRXZOp9NqtY4ZM0Y+3OPGjfMYHQVI0+3bP/nJT+SL1COPPHK01/M97qMjlUql1Wr7NNjuWXero3yxb98+12yZnAvo0yJHj9VRvX+gvGO3281m89ChQ2XBs2bN2rNnjxfH4QyhtGbNGiLKyMioVcySEfdFdhkZGdu2bRNC2Gy2yZMny4f74Ycf9vpEpdf27t0r8y8X/TQ1NfVwY/mRLvfR0fHjx/1eUnNzc3ero/rq0KFD7nMBJpPJbrd7d6ggrI6ScwGpqamud1W+rMTkD+HGjRuJaNmyZdyFeNq9ezd9RT6b5dxgcXFxe3s7S0nui35GjRr18ccf33+b+0dH7ovpA6G71VG9JOcCZLX+mgsI3OoouWpy1KhRsuBp06Z1t2i59/hD+N577xHRSy+9xF2Ip/r6ejmFKF9T5Zj55Zdf5q5LHD9+/NFHH5VPAq1W6zpb3dzcbDab5Wc4ZfzKy8uDU1IPq6N6cPr0aY8Tlf6dC/Dv6iiPuYCsrCx/zQXwh/Dtt98mop///OfchXiqqqoiohEjRhw8ePDPf/7zL3/5SyIqKCjgrksIITo6Okwmk5xrGTp06HvvvafX612jo1mzZv3nP/8JflXdrY66X2VlpetMiUajMRgMdXV1gSjJY3WUd6f1Zfw85gL8eC6EP4SFhYVEVCg/ZKAkFRUVRDR+/Hj54xtvvEFEa9as4a3K3blz52bPni1f6eXzY/r06TabjbGkHlZHSVVVVa4zJfJEZRCW4PiyOspms02aNCmgcwH8IVy5ciURvf3229yFeDp06JAc9Msfly9fTkRms5m3Kg9Op/Ott95KSEiIj49ft24ddzn/1eXqqOrqaoPBIJtSZGSkTqerrKwMWkleLPqx2WyPPfaY/CtGjBjh+6Ll7vCHUJ5xel9+pk5JbDYbEc2bN0/++MILLxDRBx98wFtVqHA6nR988IH8zhG1Wp2SkiK7X79+/X784x9funSJpapero7at2/f3Llz5c2GDBnS1zMlfcUfwmXLlhHRxo0buQvxtHXrViJauHCh/HHp0qVEtHnzZtaiQkxdXZ1reScRLV68OJhfvdedHlZHHTx4cN68ebLapKSkAC1a9qAmbordvAVfre+7oUOHlpWVrV27trKycv78+a7FZby0Wm1eXl5BQUFRUVFBQcGmTZtWrVoVGxu7YcOGLVu2ENHgwYOXL1++cuVK16mpgOIPoWK3MfMoDF+t7zU5p6UoctJo4cKFL7/88okTJ5577jl5eXx8/IoVK1asWJGQkBC0YvhDqNgnt0dh6IThZ/78+WfPns3NzT169CgR5eXlbdy40XWiNWj4Q6jYJ7fdbqf7QqjAFwvwhUajKS8v//LLL/v16xf8+En8IVR4J8TGo98ErnUOLLzZEMa/FNsJu9x4VIEvFhDq+Duhx6hPOV4fMGDF7NmxyclERE5n0WOPNTgcGo2Guy4IN8whbG1t7ezsjImJkWdyFWXUF1/Qvn302mtERE1N/7d3L8XH01cLxAD8hXk4qug3Wk1NRESyRTc3f/1vAL9iDqGi32jJ4MkXCPdAAviVIjqhQkN4fydUZseGEKeITqjQ4ah78DAchYBBJ+yee/BkV1TmiwWEOHTCbghBdjsRUVwcETohBJAiQqjETtjSQg4HaTQkz51gYgYCRhHDUSV2Qo/Wh4kZCBh0wm54vAnEcBQCBiHsRpedUIF1QuhDCLuBTgjBgveE3YiNpWnT6KtNuXCKAgKHedm0cjvhY4/R4cNERI2NFBNDf/0r/eUvJAR3WRCGmDvh3bt3SZmd0Omk3/2OUlIoJYXi4mjePDp/nr7aKh3Aj9hC2Nzc/NZbb+3fvz8pKekPf/iD3PhBQX7zGyoqoo8+ouZmunWLJkyg3Fy6cYO7LAhHgf5OxfvZ7fZ169bJ3V6JKDIykohSUlK2bNkS/GK61t4uBgwQ7vU4HCIrS6xdy1cThK2gdsL29vaioqLMzMxf/OIX9fX1M2bM2LVr19mzZ+fNm1dXV6fVahcsWFBdXR3MkrpWVUVNTeT2rbWkUtGTT9KpU3w1QfgKTtblrm7p6enyTrOzs61Wq+tap9NZUlIyePBgIkpISDCbzX7c8sYb+/cLlUp41PDmm+LJJ5kKgnAW8BD2fof32traJUuWyJs9/vjjZ8+eDXRt3bp6VRAJj92CXnpJvPACU0EQzgIYQu92eC8tLR02bBgRxcTEFBYW3rt3L3AVdsvhEGlp4o9//PqS1laRliaUt2sNhIGAhNDpdO7YseP555+X8Rs9evSGDRt6v6vbnTt39Hq93HMvOzv7yJEjgSjyATZsEPHxoqhIVFaK8nLx1FNi4kQR+L1B4BvI/yEsKyt76qmnUlNTU1NTFyxY4PUO73v27PnWt75FRGq12mAwNDc3+73UBygtFbm5IjVVjB0rXn1V+LbZMkB3/BnC8vLyJUuWyPhlZWWtX7++ra3NlwO2tLQYjUa5qXJ6evquXbv8VSqAcvgnhJ9++ulzzz0n4zd+/Pj169f7cVPFY8eO5eTkEFFERIROp7uFjgThpW8h3LRp0507d9wvOXPmjF6vl/HLzMxcs2bN3bt3/VmgEEKIjo4Ok8kkd1pW1ml9AJ/1NoTXrl3buXPnypUrL126VFZWJoQ4d+6cXq9PS0tLTU3NyMgIUPzcXbhwITc3V0725OfnX7t2LaB3BxAcvV0xo1arq6qq9u/fX1xcHBMTs3Hjxnnz5m3bti0mJuaVV145cuTIqlWrAr2taUZGRllZmcViiY+P3759e1ZWVlFRkcAnGyDU9T6vJpPJarUuXbq0oaGhuro6MzPztddeq6urC9gLRLdqamoWLVok658zZ8758+eDXwOAv/QhhCdOnHA6nRUVFfKMX1NTU8Cq6hWr1ZqcnExEsbGxJpOp9+chARQlQoTycO7OnTsFBQVFRUVENGnSpHfffXfGjBncRQH0TWiHUNqxY8fPfvazq1evRkREfO9739u2bRt3RQB9EA4hJKKWlpYf/vCHpaWlRLR79+65c+dyVwTQW/zbZfuFRqP5xz/+MXr0aCKKwpdQQEgJkxC6k7M1AKEirEKo6D2eALoRViFU7h5PAN0Lk4kZInI6nWq1OiIiorOzU34WESAkhE8nlB84jIuLQwIhtIRPCJX7jfoAPQqfECr3G/UBehQ+IcTUKISo8AkhpkYhRIVPCNEJIUSFTwjRCSFEhU8IVSrVjBkzXN81DBAqwieEDQ0NVVVV3FUA9Fn4hNButxNRXFwcdyEAfRM+IWxpaSGEEEJQ+IRQdkKNRsNdCEDfhE8IsWIGQlT4hBDvCSFEhU8I0QkhRIVPCOXEDN4TQsgJnxDKTojhKISc8AmhfE+I4SiEnHALITohhBw1dwF+Exsb29HRgRBCyAmfL3pqbW2NioqSe2sDhJBwGI46nU4hxIoVK+Q+bdzlAPRNOHTCM2fOrF69WqVSZWZmRkZG/vrXv+auCKAPwiGERFRdXV1YWDhp0iSDwcBdC0DfhMNwlIiKi4t/+9vf1tTU3Lx5k7sWgL4Jk04IELrCpBMChC6EEIAZQgjADCEEYIYQAjBDCAGYIYQAzBBCAGYIIQAzhBCAGUIIwAwhBGCGEAIwQwgBmCGEAMwQQgBmCCEAM4QQgBlCCMAMIQRghhACMEMIAZghhADMEEIAZgghADOEEIAZQgjADCEEYIYQAjBDCAGYIYQAzBBCAGYIIQAzhBCAGUIIwAwhBGCGEAIwQwgBmCGEAMwQQgBmCCEAM4QQgBlCCMAMIQRghhACMEMIAZghhADMEEIAZgghADOEEIAZQgjADCEEYIYQAjBDCAGYIYQAzBBCAGYIIQAzhBCAGUIIwAwhBGCGEAIwQwgBmCGEAMwQQgBmCCEAM4QQgBlCCMAMIQRghhACMEMIAZghhADMEEIAZgghADOEEIAZQgjADCEEYIYQAjBDCAGYIYQAzP4f+bJfHGOisBEAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=300x300>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol = Molecule(subset_df['SMILES'][0], input_type='smiles')\n",
    "mol.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c0f557b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7973 molecules.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(subset_path)\n",
    "print(f\"Loaded {len(df)} molecules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "04007d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*C=Cc1ccc2c3ccc(*)cc3n(-c3ccc(OCCCCCCCCCC)c(OC...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.386695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*CC(=O)NCCCCCCNC(=O)Cc1ccc(O*)cc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.335504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*CC(*)c1ccccc1C(=O)NC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.355580</td>\n",
       "      <td>0.183667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*c1ccc2c(c1)C(=O)N(c1ccc(Oc3ccc(N4C(=O)c5ccc(-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.401573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*CC(*)OC(=O)c1ccc(-c2ccccc2)cc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.353609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              SMILES  Tg       FFV        Tc  \\\n",
       "0  *C=Cc1ccc2c3ccc(*)cc3n(-c3ccc(OCCCCCCCCCC)c(OC... NaN  0.386695       NaN   \n",
       "1                  *CC(=O)NCCCCCCNC(=O)Cc1ccc(O*)cc1 NaN  0.335504       NaN   \n",
       "2                              *CC(*)c1ccccc1C(=O)NC NaN  0.355580  0.183667   \n",
       "3  *c1ccc2c(c1)C(=O)N(c1ccc(Oc3ccc(N4C(=O)c5ccc(-... NaN  0.401573       NaN   \n",
       "4                    *CC(*)OC(=O)c1ccc(-c2ccccc2)cc1 NaN  0.353609       NaN   \n",
       "\n",
       "   Density  Rg  \n",
       "0      NaN NaN  \n",
       "1      NaN NaN  \n",
       "2      NaN NaN  \n",
       "3      NaN NaN  \n",
       "4      NaN NaN  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1779d696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values by Column:\n",
      "         Total Missing  Percent Missing\n",
      "SMILES               0         0.000000\n",
      "Tg                7462        93.590869\n",
      "FFV                943        11.827418\n",
      "Tc                7236        90.756303\n",
      "Density           7360        92.311551\n",
      "Rg                7359        92.299009\n",
      "\n",
      "Feature Statistics (Min, Max, Mean, etc.):\n",
      "               Tg          FFV          Tc     Density          Rg\n",
      "count  511.000000  7030.000000  737.000000  613.000000  614.000000\n",
      "mean    96.452314     0.367212    0.256334    0.985484   16.419787\n",
      "std    111.228279     0.029609    0.089538    0.146189    4.608640\n",
      "min   -148.029738     0.226992    0.046500    0.748691    9.728355\n",
      "25%     13.674509     0.349549    0.186000    0.890243   12.540328\n",
      "50%     74.040183     0.364264    0.236000    0.948193   15.052194\n",
      "75%    161.147595     0.380790    0.330500    1.062096   20.411067\n",
      "max    472.250000     0.777097    0.524000    1.840999   34.672906\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "total_rows = len(df)\n",
    "percent_missing = (missing_values / total_rows) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Total Missing': missing_values,\n",
    "    'Percent Missing': percent_missing\n",
    "})\n",
    "\n",
    "print(\"Missing Values by Column:\")\n",
    "print(missing_df)\n",
    "print(\"\\nFeature Statistics (Min, Max, Mean, etc.):\")\n",
    "print(df[['Tg', 'FFV', 'Tc', 'Density', 'Rg']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125f621",
   "metadata": {},
   "source": [
    "The only property that appears will succeed with a simple imputation strategy is FFV. All other properties contain very high percent missing. Therefore, I will impute median for FFV, train a model for FFV, and train separate models for other properties. I will attempt to filter out missing values for each property. If this yields uncessful, I may explore sampling techniques or use the trained model to impute values to train a secondaery model. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe69f3",
   "metadata": {},
   "source": [
    "# Rg Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bc711963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Rg DataFrame shape: (7973, 2)\n",
      "Initial Rg Missing Values:\n",
      "SMILES       0\n",
      "Rg        7359\n",
      "dtype: int64\n",
      "\n",
      "Cleaned Rg DataFrame shape: (614, 2)\n",
      "Cleaned Rg Missing Values:\n",
      "SMILES    0\n",
      "Rg        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a new DataFrame with only the SMILES and Rg columns\n",
    "df_Rg = df[['SMILES', 'Rg']].copy()\n",
    "\n",
    "print(\"Initial Rg DataFrame shape:\", df_Rg.shape)\n",
    "print(\"Initial Rg Missing Values:\")\n",
    "print(df_Rg.isnull().sum())\n",
    "\n",
    "# 2. Drop all rows where the 'Rg' value is missing\n",
    "df_Rg.dropna(subset=['Rg'], inplace=True)\n",
    "\n",
    "print(\"\\nCleaned Rg DataFrame shape:\", df_Rg.shape)\n",
    "print(\"Cleaned Rg Missing Values:\")\n",
    "print(df_Rg.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d169da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import AllChem, Descriptors, HybridizationType, SanitizeFlags\n",
    "def rdkit_ogb_agree(smi: str) -> bool:\n",
    "    m = Chem.MolFromSmiles(smi)\n",
    "    if m is None:\n",
    "        return False\n",
    "    return m.GetNumAtoms() == smiles2graph(smi)[\"num_nodes\"]\n",
    "\n",
    "def canonicalize_polymer_smiles(smiles: str, cap_atomic_num: int = 6) -> str:\n",
    "    \"\"\"\n",
    "    Turn every '*' (dummy atom) into a real atom (default C) in the RDKit graph,\n",
    "    preserving existing bond orders/stereo; sanitize, remove explicit Hs, and\n",
    "    return canonical isomeric SMILES.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles, sanitize=False)\n",
    "    if mol is None:\n",
    "        raise ValueError(f\"RDKit could not parse SMILES: {smiles}\")\n",
    "\n",
    "    rw = Chem.RWMol(mol)\n",
    "    for a in rw.GetAtoms():\n",
    "        if a.GetAtomicNum() == 0:   # '*'\n",
    "            a.SetAtomicNum(cap_atomic_num)  # 6 = carbon\n",
    "            a.SetFormalCharge(0)\n",
    "            a.SetIsAromatic(False)\n",
    "            a.SetNoImplicit(False)\n",
    "            a.SetNumExplicitHs(0)\n",
    "\n",
    "    mol2 = rw.GetMol()\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol2)\n",
    "    except Exception:\n",
    "        Chem.SanitizeMol(mol2, sanitizeOps=SanitizeFlags.SANITIZE_ALL ^ SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "        Chem.Kekulize(mol2, clearAromaticFlags=True)\n",
    "\n",
    "    mol2 = Chem.RemoveHs(mol2)\n",
    "    return Chem.MolToSmiles(mol2, isomericSmiles=True, canonical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cff48e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed on *c1ccc(-c2cc(-c3ccc(OCCCCCC)cc3)cc(-c3ccc(-c4ccc5c(c4)C(CCCCCC)(CCCCCC)c4cc(*)ccc4-5)cc3)c2-c2ccc(OCCCCCC)cc2)cc1 | Reason: Bad Conformer Id\n",
      "Kept 613 molecules after filtering.\n",
      "Kept 613 molecules after filtering.\n",
      "Saved cleaned Rg dataset to 'cleaned_Rg_dataset.csv'.\n",
      "Target shape: (613,)\n",
      "RDKit FP shape: (613, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Build the molecule list\n",
    "valid_mol_objs = []\n",
    "valid_targets = []  # Now stores an array with one value per molecule\n",
    "\n",
    "for i, row in df_Rg.iterrows():\n",
    "    smi = row['SMILES']\n",
    "    \n",
    "    # 2.a Clean the SMILES first\n",
    "    cleaned_smiles = canonicalize_polymer_smiles(smi)\n",
    "\n",
    "    try:\n",
    "        # 2.b Create your custom Molecule from the cleaned string\n",
    "        mol = Molecule(cleaned_smiles, input_type='smiles')\n",
    "        mol.hydrogens('add')\n",
    "        mol.to_xyz(optimizer='MMFF', maxIters=200)\n",
    "\n",
    "        # 2.c Only keep molecules that got a 3-D geometry\n",
    "        if mol.xyz is not None:\n",
    "            valid_mol_objs.append(mol)\n",
    "            \n",
    "            # Keep only the 'Tc' target column as a NumPy array\n",
    "            valid_targets.append(\n",
    "                row[['Rg']].values\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Skipped bc missing xyz: {smi}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {smi} | Reason: {e}\")\n",
    "\n",
    "print(f\"Kept {len(valid_mol_objs)} molecules after filtering.\")\n",
    "\n",
    "df_clean = pd.DataFrame({\n",
    "    'SMILES': [m.smiles for m in valid_mol_objs],\n",
    "    'Rg': [t[0] for t in valid_targets],\n",
    "})\n",
    "print(f\"Kept {len(df_clean)} molecules after filtering.\")\n",
    "df_clean.to_csv('cleaned_Rg_dataset.csv', index=False)\n",
    "print(\"Saved cleaned Rg dataset to 'cleaned_Rg_dataset.csv'.\")\n",
    "\n",
    "y = np.array([t[0] for t in valid_targets])\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "# Your feature computation will now work correctly\n",
    "fp_featurizer = RDKitFingerprint(\n",
    "    fingerprint_type='morgan', vector='bit', n_bits=1024, radius=3\n",
    ")\n",
    "X_fp = fp_featurizer.represent(valid_mol_objs)\n",
    "\n",
    "print(\"RDKit FP shape:\", X_fp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ff620911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Feature Splits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table id=\"itables_9abb24a8_f4cd_484d_8415_255f19533c11\" class=\"display nowrap\" data-quarto-disable-processing=\"true\" style=\"table-layout:auto;width:auto;margin:auto;caption-side:bottom\">\n",
       "<thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      \n",
       "      <th>Split</th>\n",
       "      <th>Shape</th>\n",
       "    </tr>\n",
       "  </thead><tbody><tr>\n",
       "<td style=\"vertical-align:middle; text-align:left\">\n",
       "<a href=https://mwouts.github.io/itables/><svg class=\"main-svg\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
       "width=\"64\" viewBox=\"0 0 500 400\" style=\"font-family: 'Droid Sans', sans-serif;\">\n",
       "    <g style=\"fill:#d9d7fc\">\n",
       "        <path d=\"M100,400H500V357H100Z\" />\n",
       "        <path d=\"M100,300H400V257H100Z\" />\n",
       "        <path d=\"M0,200H400V157H0Z\" />\n",
       "        <path d=\"M100,100H500V57H100Z\" />\n",
       "        <path d=\"M100,350H500V307H100Z\" />\n",
       "        <path d=\"M100,250H400V207H100Z\" />\n",
       "        <path d=\"M0,150H400V107H0Z\" />\n",
       "        <path d=\"M100,50H500V7H100Z\" />\n",
       "    </g>\n",
       "    <g style=\"fill:#1a1366;stroke:#1a1366;\">\n",
       "   <rect x=\"100\" y=\"7\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "      <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;500\"\n",
       "      dur=\"5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"0\" y=\"107\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"3.5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "    <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"0;0;400\"\n",
       "      dur=\"3.5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"100\" y=\"207\" width=\"300\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;300;0\"\n",
       "      dur=\"3s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "    <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;400\"\n",
       "      dur=\"3s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"100\" y=\"307\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"4s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "      <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;500\"\n",
       "      dur=\"4s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <g style=\"fill:transparent;stroke-width:8; stroke-linejoin:round\" rx=\"5\">\n",
       "            <g transform=\"translate(45 50) rotate(-45)\">\n",
       "                <circle r=\"33\" cx=\"0\" cy=\"0\" />\n",
       "                <rect x=\"-8\" y=\"32\" width=\"16\" height=\"30\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(450 152)\">\n",
       "                <polyline points=\"-15,-20 -35,-20 -35,40 25,40 25,20\" />\n",
       "                <rect x=\"-15\" y=\"-40\" width=\"60\" height=\"60\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(50 352)\">\n",
       "                <polygon points=\"-35,-5 0,-40 35,-5\" />\n",
       "                <polygon points=\"-35,10 0,45 35,10\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(75 250)\">\n",
       "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
       "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(425 250) rotate(180)\">\n",
       "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
       "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
       "            </g>\n",
       "        </g>\n",
       "    </g>\n",
       "</svg>\n",
       "</a>\n",
       "Loading ITables v2.3.0 from the internet...\n",
       "(need <a href=https://mwouts.github.io/itables/troubleshooting.html>help</a>?)</td>\n",
       "</tr></tbody>\n",
       "</table>\n",
       "<link href=\"https://www.unpkg.com/dt_for_itables@2.2.0/dt_bundle.css\" rel=\"stylesheet\">\n",
       "<script type=\"module\">\n",
       "    import {DataTable, jQuery as $} from 'https://www.unpkg.com/dt_for_itables@2.2.0/dt_bundle.js';\n",
       "\n",
       "    document.querySelectorAll(\"#itables_9abb24a8_f4cd_484d_8415_255f19533c11:not(.dataTable)\").forEach(table => {\n",
       "        if (!(table instanceof HTMLTableElement))\n",
       "            return;\n",
       "\n",
       "        // Define the table data\n",
       "        const data = [[\"X_train_fp_scaled\", \"(490, 1024)\"], [\"X_test_fp_scaled\", \"(123, 1024)\"], [\"y_train_scaled\", \"(490, 1)\"], [\"y_test_scaled\", \"(123, 1)\"], [\"X_train_fp_unscaled\", \"(490, 1024)\"], [\"X_test_fp_unscaled\", \"(123, 1024)\"], [\"y_train_unscaled\", \"(490,)\"], [\"y_test_unscaled\", \"(123,)\"]];\n",
       "\n",
       "        // Define the dt_args\n",
       "        let dt_args = {\"layout\": {\"topStart\": null, \"topEnd\": null, \"bottomStart\": null, \"bottomEnd\": null}, \"order\": [], \"warn_on_selected_rows_not_rendered\": true};\n",
       "        dt_args[\"data\"] = data;\n",
       "\n",
       "        \n",
       "        new DataTable(table, dt_args);\n",
       "    });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # 1. make separate train/test splits for both scaled and unscaled targets\n",
    "# # scaled targets (MLP, KRR, GNN)\n",
    "# X_train_fp_scaled, X_test_fp_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
    "#     X_fp, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "# # X_train_cm_scaled, X_test_cm_scaled, _, _ = train_test_split(\n",
    "# #     X_cm, y, test_size=0.2, random_state=42\n",
    "# # )\n",
    "\n",
    "# xscaler_fp = StandardScaler()\n",
    "# # xscaler_cm = StandardScaler()\n",
    "# yscaler = StandardScaler()\n",
    "\n",
    "# X_train_fp_scaled = xscaler_fp.fit_transform(X_train_fp_scaled)\n",
    "# X_test_fp_scaled  = xscaler_fp.transform(X_test_fp_scaled)\n",
    "\n",
    "# # X_train_cm_scaled = xscaler_cm.fit_transform(X_train_cm_scaled)\n",
    "# # X_test_cm_scaled  = xscaler_cm.transform(X_test_cm_scaled)\n",
    "\n",
    "# y_train_scaled = yscaler.fit_transform(y_train_scaled)\n",
    "# y_test_scaled  = yscaler.transform(y_test_scaled)\n",
    "\n",
    "# # b) unscaled targets (Random Forest)\n",
    "# y_unscaled = y              \n",
    "# X_train_fp_unscaled, X_test_fp_unscaled, y_train_unscaled, y_test_unscaled = train_test_split(\n",
    "#     X_fp, y_unscaled, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# # 2. show shapes\n",
    "# tools.display_dataframe_to_user(\n",
    "#     name=\"Cleaned Feature Splits\",\n",
    "#     dataframe=pd.DataFrame({\n",
    "#         \"Split\": [\n",
    "#             \"X_train_fp_scaled\", \"X_test_fp_scaled\",\n",
    "#             # \"X_train_cm_scaled\", \"X_test_cm_scaled\",\n",
    "#             \"y_train_scaled\",   \"y_test_scaled\",\n",
    "#             \"X_train_fp_unscaled\", \"X_test_fp_unscaled\",\n",
    "#             \"y_train_unscaled\",   \"y_test_unscaled\"\n",
    "#         ],\n",
    "#         \"Shape\": [\n",
    "#             X_train_fp_scaled.shape, X_test_fp_scaled.shape,\n",
    "#             # X_train_cm_scaled.shape, X_test_cm_scaled.shape,\n",
    "#             y_train_scaled.shape,   y_test_scaled.shape,\n",
    "#             X_train_fp_unscaled.shape, X_test_fp_unscaled.shape,\n",
    "#             y_train_unscaled.shape,   y_test_unscaled.shape\n",
    "#         ]\n",
    "#     })\n",
    "# )\n",
    "\n",
    "# 1. make separate train/test splits for both scaled and unscaled targets\n",
    "# a) Scaled targets (for KRR)\n",
    "# Your y is now a 1D array of FFV values.\n",
    "X_train_fp, X_test_fp, y_train, y_test = train_test_split(\n",
    "    X_fp, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "xscaler_fp = StandardScaler()\n",
    "yscaler = StandardScaler()\n",
    "\n",
    "X_train_fp_scaled = xscaler_fp.fit_transform(X_train_fp)\n",
    "X_test_fp_scaled = xscaler_fp.transform(X_test_fp)\n",
    "\n",
    "# Reshape y arrays for the StandardScaler\n",
    "y_train_scaled = yscaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = yscaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# b) Unscaled targets (for models that don't need scaling, like Random Forest)\n",
    "# These are the original, unscaled splits. You can use the variables you already created.\n",
    "X_train_fp_unscaled = X_train_fp\n",
    "X_test_fp_unscaled = X_test_fp\n",
    "y_train_unscaled = y_train\n",
    "y_test_unscaled = y_test\n",
    "\n",
    "# 2. show shapes\n",
    "# The shape display now reflects the single target variable\n",
    "tools.display_dataframe_to_user(\n",
    "    name=\"Cleaned Feature Splits\",\n",
    "    dataframe=pd.DataFrame({\n",
    "        \"Split\": [\n",
    "            \"X_train_fp_scaled\", \"X_test_fp_scaled\",\n",
    "            \"y_train_scaled\", \"y_test_scaled\",\n",
    "            \"X_train_fp_unscaled\", \"X_test_fp_unscaled\",\n",
    "            \"y_train_unscaled\", \"y_test_unscaled\"\n",
    "        ],\n",
    "        \"Shape\": [\n",
    "            X_train_fp_scaled.shape, X_test_fp_scaled.shape,\n",
    "            y_train_scaled.shape, y_test_scaled.shape,\n",
    "            X_train_fp_unscaled.shape, X_test_fp_unscaled.shape,\n",
    "            y_train_unscaled.shape, y_test_unscaled.shape\n",
    "        ]\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489dc183",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression baseline first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d9778d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Ridge (RDKit FP)\n",
      "        MAE      RMSE  r_squared\n",
      "0  2.556085  3.273124   0.476583\n"
     ]
    }
   ],
   "source": [
    "# Kernel Ridge on RDKit fingerprints\n",
    "krr = KernelRidge(kernel='rbf', alpha=1.0)\n",
    "krr.fit(X_train_fp_scaled, y_train_scaled)\n",
    "\n",
    "# predict on scaled test set\n",
    "y_pred_krr_scaled = krr.predict(X_test_fp_scaled)\n",
    "\n",
    "# Inverse transform predictions and test targets to compare with unscaled values\n",
    "# You must reshape y_pred_krr_scaled and y_test_scaled to 2D before inverse transforming\n",
    "y_pred_krr = yscaler.inverse_transform(y_pred_krr_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_krr = yscaler.inverse_transform(y_test_scaled).flatten()\n",
    "\n",
    "# Eval against true unscaled test target\n",
    "print(\"Kernel Ridge (RDKit FP)\")\n",
    "metrics_krr = regression_metrics(y_test_krr, y_pred_krr)\n",
    "print(metrics_krr[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0fedd",
   "metadata": {},
   "source": [
    "## Random Forest Regression baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c3ea7be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (RDKit FP)\n",
      "        MAE      RMSE  r_squared\n",
      "0  1.787365  2.473684   0.701041\n"
     ]
    }
   ],
   "source": [
    "# Random Forest (RDKit FP) \n",
    "rfr = RandomForestRegressor(n_estimators=100, max_depth=30, random_state=42)\n",
    "rfr.fit(X_train_fp_unscaled, y_train_unscaled)\n",
    "# predict\n",
    "y_pred_rfr = rfr.predict(X_test_fp_unscaled)\n",
    "# eval\n",
    "print(\"Random Forest (RDKit FP)\")\n",
    "metrics_rfr = regression_metrics(y_test_unscaled, y_pred_rfr)\n",
    "print(metrics_rfr[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74973657",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron with Morgan Fingerprints baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "70bbb566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3798\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6049\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4321\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3493\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3129\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2840\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2677\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2406\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2335\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2321\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2324\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2270\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2261\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2240\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2208\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2196\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2182\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2192\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2191\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2198\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2174\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2165\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2147\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2154\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2148\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2121\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2154\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2139\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2129\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2119\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2110\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2120\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2114\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2092\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2090\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2082\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2071\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2071\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2054\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2064\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2044\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2020\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2019\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2017\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2017\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2016\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2003\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1988\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1981\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1979\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1968\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1970\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1960\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1949\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1956\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1946\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1938\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1977\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1964\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1929\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1916\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1914\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1918\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1902\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1932\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1926\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1897\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1899\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1874\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1870\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1865\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1852\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1843\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1847\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1838\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1818\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1830\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1813\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1812\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1807\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1800\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1803\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1796\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1794\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1789\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1775\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1767\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1767\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1774\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1773\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1759\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1751\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1748\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1746\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1733\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1723\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1724\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1712\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1710\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1702\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1700\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1692\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1698\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1695\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1684\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1692\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1692\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1664\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1675\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1655\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1652\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1651\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1637\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1632\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1630\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1626\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1621\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1619\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1622\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1620\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1611\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1599\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1594\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1590\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1594\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1582\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1583\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1577\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1564\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1563\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1558\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1567\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1556\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1562\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1541\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1539\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1545\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1531\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1539\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1524\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1511\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1513\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1507\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1506\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1521\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1515\n",
      "Epoch 148/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1517\n",
      "Epoch 149/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1496\n",
      "Epoch 150/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1492\n",
      "Epoch 151/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1479\n",
      "Epoch 152/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1481\n",
      "Epoch 153/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1481\n",
      "Epoch 154/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1467\n",
      "Epoch 155/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1471\n",
      "Epoch 156/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1486\n",
      "Epoch 157/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1488\n",
      "Epoch 158/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1477\n",
      "Epoch 159/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1461\n",
      "Epoch 160/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1455\n",
      "Epoch 161/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1440\n",
      "Epoch 162/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1435\n",
      "Epoch 163/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1444\n",
      "Epoch 164/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1436\n",
      "Epoch 165/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1428\n",
      "Epoch 166/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1451\n",
      "Epoch 167/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1441\n",
      "Epoch 168/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1429\n",
      "Epoch 169/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1415\n",
      "Epoch 170/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1424\n",
      "Epoch 171/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1412\n",
      "Epoch 172/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1393\n",
      "Epoch 173/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1396\n",
      "Epoch 174/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1385\n",
      "Epoch 175/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1380\n",
      "Epoch 176/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1380\n",
      "Epoch 177/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1376\n",
      "Epoch 178/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1369\n",
      "Epoch 179/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1371\n",
      "Epoch 180/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1358\n",
      "Epoch 181/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1360\n",
      "Epoch 182/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1360\n",
      "Epoch 183/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1349\n",
      "Epoch 184/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1348\n",
      "Epoch 185/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1349\n",
      "Epoch 186/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1351\n",
      "Epoch 187/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1344\n",
      "Epoch 188/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1336\n",
      "Epoch 189/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1325\n",
      "Epoch 190/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1324\n",
      "Epoch 191/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1319\n",
      "Epoch 192/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1314\n",
      "Epoch 193/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1315\n",
      "Epoch 194/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1312\n",
      "Epoch 195/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1309\n",
      "Epoch 196/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1301\n",
      "Epoch 197/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1306\n",
      "Epoch 198/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1301\n",
      "Epoch 199/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1301\n",
      "Epoch 200/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1296\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "MLP (RDKit FP)\n",
      "        MAE      RMSE  r_squared\n",
      "0  2.079622  2.805338   0.615503\n"
     ]
    }
   ],
   "source": [
    "# MLP (Fingerprint)\n",
    "mlp_fp = MLP(\n",
    "    engine='tensorflow',\n",
    "    nfeatures=X_train_fp_scaled.shape[1],\n",
    "    nneurons=[64, 128], # These are the hidden layers\n",
    "    activations=['ReLU', 'ReLU'],\n",
    "    learning_rate=0.01,\n",
    "    alpha=0.001,\n",
    "    nepochs=200,\n",
    "    batch_size=64,\n",
    "    loss='mean_squared_error',\n",
    "    is_regression=True\n",
    ")\n",
    "\n",
    "mlp_fp.fit(X=X_train_fp_scaled, y=y_train_scaled.ravel()) # Use .ravel() to convert to 1D\n",
    "y_pred_fp_scaled = mlp_fp.predict(X_test_fp_scaled)\n",
    "# Reshape the output from predict() to 2D before inverse transforming\n",
    "y_pred_fp = yscaler.inverse_transform(y_pred_fp_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_fp = yscaler.inverse_transform(y_test_scaled).flatten()\n",
    "\n",
    "# Eval against true unscaled test target\n",
    "print(\"MLP (RDKit FP)\")\n",
    "metrics_mlp = regression_metrics(y_test, y_pred_fp)\n",
    "print(metrics_mlp[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61abe3c7",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron with Coulomb Matrix representation baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "64495ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MLP (Coulomb matrix)\n",
    "# mlp_cm = MLP(\n",
    "#     engine='tensorflow', \n",
    "#     nfeatures=X_train_cm_scaled.shape[1], \n",
    "#     nneurons=[64, 128], \n",
    "#     activations=['ReLU', 'ReLU'],\n",
    "#     learning_rate=0.0001, \n",
    "#     alpha=0.001, \n",
    "#     nepochs=100, \n",
    "#     batch_size=64, \n",
    "#     loss='mean_squared_error', \n",
    "#     is_regression=True\n",
    "#     )\n",
    "\n",
    "# mlp_cm.fit(X=X_train_cm_scaled, y=y_train_scaled)\n",
    "# y_pred_cm_scaled = mlp_cm.predict(X_test_cm_scaled)\n",
    "# y_pred_cm = yscaler.inverse_transform(y_pred_cm_scaled)\n",
    "# y_test_cm = yscaler.inverse_transform(y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7101926e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Comparison\n",
      "\n",
      "Kernel Ridge (RDKit FP)\n",
      "        MAE      RMSE  r_squared\n",
      "0  2.556085  3.273124   0.476583\n",
      "\n",
      "Random Forest (RDKit FP)\n",
      "        MAE      RMSE  r_squared\n",
      "0  1.787365  2.473684   0.701041\n",
      "\n",
      "MLP (RDKit FP)\n",
      "        MAE      RMSE  r_squared\n",
      "0  2.079622  2.805338   0.615503\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "results = {\"Kernel Ridge (RDKit FP)\": regression_metrics(y_test_krr, y_pred_krr),\n",
    "           \"Random Forest (RDKit FP)\": regression_metrics(y_test_unscaled, y_pred_rfr),\n",
    "           \"MLP (RDKit FP)\": regression_metrics(y_test_fp, y_pred_fp),\n",
    "        #    \"MLP (Coulomb Matrix)\": regression_metrics(y_test_cm, y_pred_cm)\n",
    "        }\n",
    "\n",
    "# display\n",
    "print(\"Final Model Comparison\")\n",
    "for name, metrics_df in results.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(metrics_df[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e7366",
   "metadata": {},
   "source": [
    "## Parity Plots and Residuals Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "86ff8791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_regression_results(y_true, y_pred, title=\"Model Evaluation\", save_dir=\"plots\"):\n",
    "    residuals = y_true.flatten() - y_pred.flatten()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # parity plot\n",
    "    sns.scatterplot(ax=axes[0], x=y_true.flatten(), y=y_pred.flatten(), alpha=0.5)\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    axes[0].plot([min_val, max_val], [min_val, max_val], '--r')\n",
    "    axes[0].set_xlabel(\"Actual HOMO-LUMO Gap\")\n",
    "    axes[0].set_ylabel(\"Predicted HOMO-LUMO Gap\")\n",
    "    axes[0].set_title(\"Parity Plot\")\n",
    "    axes[0].grid(True)\n",
    "    axes[0].axis('equal')\n",
    "\n",
    "    # residuals histogram\n",
    "    sns.histplot(ax=axes[1], data=residuals, bins=30, kde=True)\n",
    "    axes[1].set_title(\"Residuals Histogram\")\n",
    "    axes[1].set_xlabel(\"Residual (Actual - Predicted)\")\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    # overall title\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "\n",
    "    # save fig as pdf for best overleaf upload format \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filename = os.path.join(save_dir, f\"{title.lower().replace(' ', '_')}_plots.pdf\")\n",
    "    fig.savefig(filename, bbox_inches='tight')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_regression_results(y_test_fp, y_pred_fp, title=\"MLP Untuned(RDKit FP)\")\n",
    "# plot_regression_results(y_test_cm, y_pred_cm, title=\"MLP Untuned (Coulomb Matrix)\")\n",
    "plot_regression_results(y_test_krr, y_pred_krr, title=\"Kernel Ridge Untuned (RDKit FP)\")\n",
    "plot_regression_results(y_test_unscaled, y_pred_rfr, title=\"Random Forest Untuned (RDKit FP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dce050",
   "metadata": {},
   "source": [
    "## Tune hyperparameters for baseline models with Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "66717bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:02:10,039] A new study created in memory with name: no-name-524eaa50-61df-44ab-a1be-08c1f3f70993\n",
      "[I 2025-09-04 21:02:10,058] Trial 0 finished with value: 1.9302987284656619 and parameters: {'alpha': 0.14513458760480907, 'kernel': 'poly'}. Best is trial 0 with value: 1.9302987284656619.\n",
      "[I 2025-09-04 21:02:10,071] Trial 1 finished with value: 2.840112245906479 and parameters: {'alpha': 0.08146890746382296, 'kernel': 'linear'}. Best is trial 0 with value: 1.9302987284656619.\n",
      "[I 2025-09-04 21:02:10,088] Trial 2 finished with value: 2.8676532758232227 and parameters: {'alpha': 0.04779091471260243, 'kernel': 'linear'}. Best is trial 0 with value: 1.9302987284656619.\n",
      "[I 2025-09-04 21:02:10,112] Trial 3 finished with value: 3.3494238061946806 and parameters: {'alpha': 0.18780668448559307, 'kernel': 'rbf', 'gamma': 0.025122889316061988}. Best is trial 0 with value: 1.9302987284656619.\n",
      "[I 2025-09-04 21:02:10,155] Trial 4 finished with value: 3.3815444602396028 and parameters: {'alpha': 0.05369205541120655, 'kernel': 'rbf', 'gamma': 0.2861109093431796}. Best is trial 0 with value: 1.9302987284656619.\n",
      "[I 2025-09-04 21:02:10,170] Trial 5 finished with value: 1.9649705865312743 and parameters: {'alpha': 0.01523844420712483, 'kernel': 'poly'}. Best is trial 0 with value: 1.9302987284656619.\n",
      "[I 2025-09-04 21:02:10,186] Trial 6 finished with value: 1.9477399269954978 and parameters: {'alpha': 0.059225988847633776, 'kernel': 'poly'}. Best is trial 0 with value: 1.9302987284656619.\n",
      "[I 2025-09-04 21:02:10,202] Trial 7 finished with value: 2.932247213558719 and parameters: {'alpha': 0.07592537001646538, 'kernel': 'rbf', 'gamma': 0.0028876610843324136}. Best is trial 0 with value: 1.9302987284656619.\n",
      "[I 2025-09-04 21:02:10,216] Trial 8 finished with value: 2.852731124728181 and parameters: {'alpha': 0.06541914772264672, 'kernel': 'linear'}. Best is trial 0 with value: 1.9302987284656619.\n",
      "[I 2025-09-04 21:02:10,231] Trial 9 finished with value: 2.869790386108232 and parameters: {'alpha': 0.026362318764440347, 'kernel': 'rbf', 'gamma': 0.0026339714528868276}. Best is trial 0 with value: 1.9302987284656619.\n",
      "[I 2025-09-04 21:02:10,249] Trial 10 finished with value: 1.909181241893026 and parameters: {'alpha': 0.8098356646813913, 'kernel': 'poly'}. Best is trial 10 with value: 1.909181241893026.\n",
      "[I 2025-09-04 21:02:10,267] Trial 11 finished with value: 1.9075640197103647 and parameters: {'alpha': 0.6701924879968298, 'kernel': 'poly'}. Best is trial 11 with value: 1.9075640197103647.\n",
      "[I 2025-09-04 21:02:10,286] Trial 12 finished with value: 1.9100128057984314 and parameters: {'alpha': 0.855320373174862, 'kernel': 'poly'}. Best is trial 11 with value: 1.9075640197103647.\n",
      "[I 2025-09-04 21:02:10,303] Trial 13 finished with value: 1.9130576888403408 and parameters: {'alpha': 0.9703324034042838, 'kernel': 'poly'}. Best is trial 11 with value: 1.9075640197103647.\n",
      "[I 2025-09-04 21:02:10,322] Trial 14 finished with value: 1.9068537017273484 and parameters: {'alpha': 0.47738969013543214, 'kernel': 'poly'}. Best is trial 14 with value: 1.9068537017273484.\n",
      "[I 2025-09-04 21:02:10,341] Trial 15 finished with value: 1.9089368856343827 and parameters: {'alpha': 0.3562792144104637, 'kernel': 'poly'}. Best is trial 14 with value: 1.9068537017273484.\n",
      "[I 2025-09-04 21:02:10,360] Trial 16 finished with value: 1.9094164435941208 and parameters: {'alpha': 0.345583192250033, 'kernel': 'poly'}. Best is trial 14 with value: 1.9068537017273484.\n",
      "[I 2025-09-04 21:02:10,380] Trial 17 finished with value: 1.9073580913019355 and parameters: {'alpha': 0.40157649785664123, 'kernel': 'poly'}. Best is trial 14 with value: 1.9068537017273484.\n",
      "[I 2025-09-04 21:02:10,399] Trial 18 finished with value: 1.9081766038251151 and parameters: {'alpha': 0.37426526406075855, 'kernel': 'poly'}. Best is trial 14 with value: 1.9068537017273484.\n",
      "[I 2025-09-04 21:02:10,414] Trial 19 finished with value: 2.7609690815606913 and parameters: {'alpha': 0.20273611286530568, 'kernel': 'linear'}. Best is trial 14 with value: 1.9068537017273484.\n",
      "[I 2025-09-04 21:02:10,432] Trial 20 finished with value: 1.9068485215430695 and parameters: {'alpha': 0.5513734819638962, 'kernel': 'poly'}. Best is trial 20 with value: 1.9068485215430695.\n",
      "[I 2025-09-04 21:02:10,449] Trial 21 finished with value: 1.9068009942575272 and parameters: {'alpha': 0.5077585351771438, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,467] Trial 22 finished with value: 1.9068021734707434 and parameters: {'alpha': 0.5232419003431815, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,484] Trial 23 finished with value: 1.9167539631142698 and parameters: {'alpha': 0.24001445942713193, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,502] Trial 24 finished with value: 1.9069083175209551 and parameters: {'alpha': 0.5700097324257337, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,519] Trial 25 finished with value: 1.914046746176497 and parameters: {'alpha': 0.2686270379264338, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,542] Trial 26 finished with value: 1.906921219624932 and parameters: {'alpha': 0.5733200740460987, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,567] Trial 27 finished with value: 1.933636130874164 and parameters: {'alpha': 0.126197251869498, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,585] Trial 28 finished with value: 2.7123435110315595 and parameters: {'alpha': 0.3072734549959064, 'kernel': 'linear'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,604] Trial 29 finished with value: 2.1122341251646337 and parameters: {'alpha': 0.16251089135513658, 'kernel': 'rbf', 'gamma': 5.249941072179048e-05}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,622] Trial 30 finished with value: 1.906802175810516 and parameters: {'alpha': 0.5057565571256272, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,640] Trial 31 finished with value: 1.9068047505660397 and parameters: {'alpha': 0.5025183390607257, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,658] Trial 32 finished with value: 1.9366856552260345 and parameters: {'alpha': 0.1097718231948461, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,675] Trial 33 finished with value: 1.9077126981105132 and parameters: {'alpha': 0.6861326085998498, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,691] Trial 34 finished with value: 2.6660988710852185 and parameters: {'alpha': 0.44739265693993857, 'kernel': 'linear'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,708] Trial 35 finished with value: 1.9193967234524072 and parameters: {'alpha': 0.21660450094816883, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,724] Trial 36 finished with value: 1.9558703048269739 and parameters: {'alpha': 0.03619697199135077, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,743] Trial 37 finished with value: 3.3507685549329227 and parameters: {'alpha': 0.7084087529005116, 'kernel': 'rbf', 'gamma': 1.1593331249095692e-05}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,759] Trial 38 finished with value: 1.9124142006441507 and parameters: {'alpha': 0.2900451393710649, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,773] Trial 39 finished with value: 2.791509455494766 and parameters: {'alpha': 0.15137468870851287, 'kernel': 'linear'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,796] Trial 40 finished with value: 3.3884275561564547 and parameters: {'alpha': 0.012256174476918083, 'kernel': 'rbf', 'gamma': 0.9160754714151067}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,814] Trial 41 finished with value: 1.906801309388567 and parameters: {'alpha': 0.5071697795518156, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,831] Trial 42 finished with value: 1.9068066591185338 and parameters: {'alpha': 0.5285433493316796, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,848] Trial 43 finished with value: 1.913904530466969 and parameters: {'alpha': 0.9982078224872573, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,866] Trial 44 finished with value: 1.9070661928740114 and parameters: {'alpha': 0.4346513343417575, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,884] Trial 45 finished with value: 1.9076939936452133 and parameters: {'alpha': 0.6841882485201981, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,901] Trial 46 finished with value: 1.9090989433058414 and parameters: {'alpha': 0.8038347572253048, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,918] Trial 47 finished with value: 1.9109350402142733 and parameters: {'alpha': 0.3145275129475885, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,937] Trial 48 finished with value: 1.906947721088846 and parameters: {'alpha': 0.45416114751645476, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,955] Trial 49 finished with value: 1.941374714249126 and parameters: {'alpha': 0.09408292092737064, 'kernel': 'rbf', 'gamma': 0.00011993049324958293}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,974] Trial 50 finished with value: 1.9072635124678226 and parameters: {'alpha': 0.6333505867310995, 'kernel': 'poly'}. Best is trial 21 with value: 1.9068009942575272.\n",
      "[I 2025-09-04 21:02:10,992] Trial 51 finished with value: 1.9068005640539563 and parameters: {'alpha': 0.5086593985213539, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,011] Trial 52 finished with value: 1.9068103561484875 and parameters: {'alpha': 0.4975196034892755, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,034] Trial 53 finished with value: 1.9081206381343105 and parameters: {'alpha': 0.37564338653120016, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,058] Trial 54 finished with value: 1.9094994018221028 and parameters: {'alpha': 0.8324121962996354, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,079] Trial 55 finished with value: 1.9141492197443317 and parameters: {'alpha': 0.26745780737593566, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,097] Trial 56 finished with value: 1.9613862613808926 and parameters: {'alpha': 0.023580676289726713, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,111] Trial 57 finished with value: 2.6790619056977514 and parameters: {'alpha': 0.40439452025236183, 'kernel': 'linear'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,128] Trial 58 finished with value: 1.9244965412106543 and parameters: {'alpha': 0.1808135061316816, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,145] Trial 59 finished with value: 1.9070189769429309 and parameters: {'alpha': 0.5944521014010852, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,165] Trial 60 finished with value: 1.9096876290862304 and parameters: {'alpha': 0.3397370783054607, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,182] Trial 61 finished with value: 1.9068036466861515 and parameters: {'alpha': 0.5252672685073451, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,201] Trial 62 finished with value: 1.906830854935557 and parameters: {'alpha': 0.486065048480029, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,219] Trial 63 finished with value: 1.909085220777408 and parameters: {'alpha': 0.802827120385094, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,237] Trial 64 finished with value: 1.9068837569746109 and parameters: {'alpha': 0.5631549352584648, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,256] Trial 65 finished with value: 1.9080127159352582 and parameters: {'alpha': 0.7154309184608255, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,274] Trial 66 finished with value: 1.9180981182691887 and parameters: {'alpha': 0.2271970818181664, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,295] Trial 67 finished with value: 3.3700585283615045 and parameters: {'alpha': 0.38246663887972604, 'kernel': 'rbf', 'gamma': 0.04375651477645404}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,313] Trial 68 finished with value: 1.9068028496439828 and parameters: {'alpha': 0.5048014436431097, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,328] Trial 69 finished with value: 2.569585813386379 and parameters: {'alpha': 0.9158264225368385, 'kernel': 'linear'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,345] Trial 70 finished with value: 1.9071772391825943 and parameters: {'alpha': 0.6209903397967173, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,364] Trial 71 finished with value: 1.9068012377678678 and parameters: {'alpha': 0.5216718174542663, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,381] Trial 72 finished with value: 1.9071818028037737 and parameters: {'alpha': 0.4198739570714441, 'kernel': 'poly'}. Best is trial 51 with value: 1.9068005640539563.\n",
      "[I 2025-09-04 21:02:11,399] Trial 73 finished with value: 1.9067995277774679 and parameters: {'alpha': 0.5169858567703334, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,417] Trial 74 finished with value: 1.9099297770032668 and parameters: {'alpha': 0.33463350578385576, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,436] Trial 75 finished with value: 1.9069929088138466 and parameters: {'alpha': 0.4459663787089312, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,454] Trial 76 finished with value: 1.9150246970364289 and parameters: {'alpha': 0.2578723248222873, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,472] Trial 77 finished with value: 1.9071897708767585 and parameters: {'alpha': 0.6228566129630926, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,490] Trial 78 finished with value: 1.908195280413189 and parameters: {'alpha': 0.7318774962779206, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,508] Trial 79 finished with value: 1.9124752516735437 and parameters: {'alpha': 0.28922134949359085, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,529] Trial 80 finished with value: 2.024340707669014 and parameters: {'alpha': 0.04379725525781395, 'kernel': 'rbf', 'gamma': 0.0003992581655176187}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,556] Trial 81 finished with value: 1.9068002040969085 and parameters: {'alpha': 0.5095400495738981, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,580] Trial 82 finished with value: 1.906800317367138 and parameters: {'alpha': 0.5196940531128494, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,597] Trial 83 finished with value: 1.9082740958482205 and parameters: {'alpha': 0.3718830911207889, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,615] Trial 84 finished with value: 1.9068770420746157 and parameters: {'alpha': 0.5611225147503559, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,633] Trial 85 finished with value: 1.908670326673005 and parameters: {'alpha': 0.7712852096263699, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,651] Trial 86 finished with value: 1.9112729362659477 and parameters: {'alpha': 0.9042206617264782, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,666] Trial 87 finished with value: 2.662821968633318 and parameters: {'alpha': 0.45878080586230124, 'kernel': 'linear'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,684] Trial 88 finished with value: 1.907298378412548 and parameters: {'alpha': 0.40734817159435877, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,701] Trial 89 finished with value: 1.9077500033543855 and parameters: {'alpha': 0.6899635564802423, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,719] Trial 90 finished with value: 1.9072262449049875 and parameters: {'alpha': 0.6281452668209552, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,736] Trial 91 finished with value: 1.9068049107759353 and parameters: {'alpha': 0.5267462992132583, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,754] Trial 92 finished with value: 1.9068017636482517 and parameters: {'alpha': 0.5063973938537016, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,773] Trial 93 finished with value: 1.9069048037143157 and parameters: {'alpha': 0.4633120985781954, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,791] Trial 94 finished with value: 1.9114033336379608 and parameters: {'alpha': 0.3057043725657219, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,810] Trial 95 finished with value: 1.9090805683085768 and parameters: {'alpha': 0.35302479012969784, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,829] Trial 96 finished with value: 1.9068835594108933 and parameters: {'alpha': 0.5630962481749057, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,847] Trial 97 finished with value: 1.90727102028801 and parameters: {'alpha': 0.4101244904634187, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,865] Trial 98 finished with value: 1.9448679144999381 and parameters: {'alpha': 0.06991205699612053, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,885] Trial 99 finished with value: 3.364902738470447 and parameters: {'alpha': 0.501225262487587, 'kernel': 'rbf', 'gamma': 0.02807824093991723}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,903] Trial 100 finished with value: 1.9072275989581227 and parameters: {'alpha': 0.6283377472798781, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,923] Trial 101 finished with value: 1.906869911898723 and parameters: {'alpha': 0.4723656653082902, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,941] Trial 102 finished with value: 1.9068034748977054 and parameters: {'alpha': 0.5039941930117765, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,960] Trial 103 finished with value: 1.9069865261010086 and parameters: {'alpha': 0.5880327334562891, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,977] Trial 104 finished with value: 1.908418992981973 and parameters: {'alpha': 0.7509635584867353, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:11,995] Trial 105 finished with value: 1.9075479457223197 and parameters: {'alpha': 0.6683964673427906, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:12,010] Trial 106 finished with value: 2.671295512672682 and parameters: {'alpha': 0.4297706015076257, 'kernel': 'linear'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:12,028] Trial 107 finished with value: 1.906824626145797 and parameters: {'alpha': 0.5407706283746772, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:12,054] Trial 108 finished with value: 1.90799396719757 and parameters: {'alpha': 0.37879175865555226, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:12,077] Trial 109 finished with value: 1.9068337696501219 and parameters: {'alpha': 0.48480735446603673, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:12,097] Trial 110 finished with value: 1.9100277597621844 and parameters: {'alpha': 0.3325985738664738, 'kernel': 'poly'}. Best is trial 73 with value: 1.9067995277774679.\n",
      "[I 2025-09-04 21:02:12,115] Trial 111 finished with value: 1.9067993265370458 and parameters: {'alpha': 0.5154876533478521, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,133] Trial 112 finished with value: 1.9069531510908648 and parameters: {'alpha': 0.5808749203203921, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,151] Trial 113 finished with value: 1.9070662242938172 and parameters: {'alpha': 0.4346468773520895, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,169] Trial 114 finished with value: 1.9068046581428588 and parameters: {'alpha': 0.5264649725706075, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,187] Trial 115 finished with value: 1.9100193313798817 and parameters: {'alpha': 0.8555772224554613, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,206] Trial 116 finished with value: 1.907540662367325 and parameters: {'alpha': 0.6675774613561696, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,224] Trial 117 finished with value: 1.9080876121747055 and parameters: {'alpha': 0.3764603141134483, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,243] Trial 118 finished with value: 1.9068326254667285 and parameters: {'alpha': 0.4852942375672266, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,264] Trial 119 finished with value: 2.2526469338512785 and parameters: {'alpha': 0.40650718256763335, 'kernel': 'rbf', 'gamma': 0.000646542345060659}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,284] Trial 120 finished with value: 1.964095107854569 and parameters: {'alpha': 0.017366369825704723, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,303] Trial 121 finished with value: 1.9068026613271052 and parameters: {'alpha': 0.5239596325461444, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,321] Trial 122 finished with value: 1.9070709714901766 and parameters: {'alpha': 0.6039032075715376, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,340] Trial 123 finished with value: 1.9069201603433585 and parameters: {'alpha': 0.4598415473594532, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,357] Trial 124 finished with value: 1.906819849087896 and parameters: {'alpha': 0.538122352963165, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,376] Trial 125 finished with value: 1.9075400479782947 and parameters: {'alpha': 0.6675082230905252, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,394] Trial 126 finished with value: 1.9082646784733994 and parameters: {'alpha': 0.7379126339673086, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,409] Trial 127 finished with value: 2.6456910710227337 and parameters: {'alpha': 0.5220315614643861, 'kernel': 'linear'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,427] Trial 128 finished with value: 1.9070289869403705 and parameters: {'alpha': 0.5963440074072851, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,446] Trial 129 finished with value: 1.907133347161474 and parameters: {'alpha': 0.42571396158253094, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,464] Trial 130 finished with value: 1.9088124033356646 and parameters: {'alpha': 0.35913444452132537, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,483] Trial 131 finished with value: 1.9068019885837801 and parameters: {'alpha': 0.5060415111307971, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,503] Trial 132 finished with value: 1.906867888256229 and parameters: {'alpha': 0.4729570903961174, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,523] Trial 133 finished with value: 1.932365622137892 and parameters: {'alpha': 0.13328252573862187, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,549] Trial 134 finished with value: 1.90686345130143 and parameters: {'alpha': 0.5567371170048621, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,573] Trial 135 finished with value: 1.9068046997024664 and parameters: {'alpha': 0.5025736015892597, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,593] Trial 136 finished with value: 1.9073403781655902 and parameters: {'alpha': 0.6435501563365781, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,611] Trial 137 finished with value: 1.9070411106743963 and parameters: {'alpha': 0.4383063036977099, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,630] Trial 138 finished with value: 1.9070319152989268 and parameters: {'alpha': 0.596890402759413, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,649] Trial 139 finished with value: 1.9105201350076202 and parameters: {'alpha': 0.32262415702005626, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,668] Trial 140 finished with value: 1.9077262989258497 and parameters: {'alpha': 0.38558190334684855, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,686] Trial 141 finished with value: 1.9067996223957488 and parameters: {'alpha': 0.5174416997376802, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,704] Trial 142 finished with value: 1.9068374181156875 and parameters: {'alpha': 0.5468563464499299, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,723] Trial 143 finished with value: 1.9068467286738902 and parameters: {'alpha': 0.4797921286447958, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,743] Trial 144 finished with value: 1.907019144585735 and parameters: {'alpha': 0.44168238641192537, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,761] Trial 145 finished with value: 1.9068016819172224 and parameters: {'alpha': 0.5224550190650671, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,781] Trial 146 finished with value: 1.9076534998627293 and parameters: {'alpha': 0.6799219216671174, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,802] Trial 147 finished with value: 3.2374779948100656 and parameters: {'alpha': 0.5287297508274539, 'kernel': 'rbf', 'gamma': 1.1183317258213672e-05}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,821] Trial 148 finished with value: 1.9073390442246139 and parameters: {'alpha': 0.40337740626850743, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,840] Trial 149 finished with value: 1.9088711139204926 and parameters: {'alpha': 0.7868249641762028, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,856] Trial 150 finished with value: 2.628949187995469 and parameters: {'alpha': 0.5905017835189958, 'kernel': 'linear'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,876] Trial 151 finished with value: 1.9068042099208329 and parameters: {'alpha': 0.503119839748441, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,895] Trial 152 finished with value: 1.906941890644105 and parameters: {'alpha': 0.4553121619017315, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,915] Trial 153 finished with value: 1.9415984277356482 and parameters: {'alpha': 0.08484092550797959, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,934] Trial 154 finished with value: 1.9068682607849554 and parameters: {'alpha': 0.5583351485414108, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,953] Trial 155 finished with value: 1.9071697202838673 and parameters: {'alpha': 0.6198576109779701, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,972] Trial 156 finished with value: 1.9068080428139973 and parameters: {'alpha': 0.49937483746663797, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:12,991] Trial 157 finished with value: 1.9071298567634358 and parameters: {'alpha': 0.42615241933033293, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,017] Trial 158 finished with value: 1.9080813930282507 and parameters: {'alpha': 0.7217217012831387, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,041] Trial 159 finished with value: 1.9068564771282897 and parameters: {'alpha': 0.5543151315213695, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,061] Trial 160 finished with value: 1.906887592196041 and parameters: {'alpha': 0.46753913690953774, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,080] Trial 161 finished with value: 1.9067993628667275 and parameters: {'alpha': 0.5130299473961293, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,099] Trial 162 finished with value: 1.9068010731055232 and parameters: {'alpha': 0.5076064109499606, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,117] Trial 163 finished with value: 1.907364186546859 and parameters: {'alpha': 0.6465819908533386, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,137] Trial 164 finished with value: 1.9068027835737509 and parameters: {'alpha': 0.5241312833554609, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,155] Trial 165 finished with value: 1.907402403276115 and parameters: {'alpha': 0.3975190072862857, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,174] Trial 166 finished with value: 1.9069576650551903 and parameters: {'alpha': 0.5818819880655492, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,193] Trial 167 finished with value: 1.9069014876973762 and parameters: {'alpha': 0.46409624601715505, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,211] Trial 168 finished with value: 1.9068015647787444 and parameters: {'alpha': 0.5067258912314729, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,230] Trial 169 finished with value: 1.9084428528623256 and parameters: {'alpha': 0.36781400042627055, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,249] Trial 170 finished with value: 1.90711119578153 and parameters: {'alpha': 0.4285415488302769, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,268] Trial 171 finished with value: 1.9481246515686301 and parameters: {'alpha': 0.057930804536488405, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,288] Trial 172 finished with value: 1.9068028999226654 and parameters: {'alpha': 0.5047340030632811, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,308] Trial 173 finished with value: 1.9068744041071564 and parameters: {'alpha': 0.5603015449603047, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,327] Trial 174 finished with value: 1.9072396745946936 and parameters: {'alpha': 0.6300428242571382, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,345] Trial 175 finished with value: 1.9068427702602404 and parameters: {'alpha': 0.4812386836479118, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,365] Trial 176 finished with value: 1.9068077218451045 and parameters: {'alpha': 0.5295338140937037, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,402] Trial 177 finished with value: 3.3909556796986275 and parameters: {'alpha': 0.586962761833159, 'kernel': 'rbf', 'gamma': 0.14631403113284572}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,422] Trial 178 finished with value: 1.9069976960199828 and parameters: {'alpha': 0.4451620844417877, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,442] Trial 179 finished with value: 1.9067998295282873 and parameters: {'alpha': 0.5182541565016187, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,461] Trial 180 finished with value: 1.907762003533411 and parameters: {'alpha': 0.6911830578184762, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,482] Trial 181 finished with value: 1.9068048876937005 and parameters: {'alpha': 0.5023706392928955, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,502] Trial 182 finished with value: 1.9068374715197776 and parameters: {'alpha': 0.5468794825216036, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,521] Trial 183 finished with value: 1.9068777791820808 and parameters: {'alpha': 0.4701473213644853, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,543] Trial 184 finished with value: 1.9072825886203626 and parameters: {'alpha': 0.40893962966821634, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,569] Trial 185 finished with value: 1.9070804690939025 and parameters: {'alpha': 0.6055388199264338, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,593] Trial 186 finished with value: 1.9067994207705212 and parameters: {'alpha': 0.5163428734130744, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,609] Trial 187 finished with value: 2.6643485071437945 and parameters: {'alpha': 0.45344862055082685, 'kernel': 'linear'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,627] Trial 188 finished with value: 1.9068960802610435 and parameters: {'alpha': 0.5666964877117645, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,645] Trial 189 finished with value: 1.906809711450818 and parameters: {'alpha': 0.49801494033638366, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,663] Trial 190 finished with value: 1.9072789382286417 and parameters: {'alpha': 0.6354522286251307, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,682] Trial 191 finished with value: 1.9068041148693649 and parameters: {'alpha': 0.5258370066377068, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,701] Trial 192 finished with value: 1.9068381505680345 and parameters: {'alpha': 0.4830166302817525, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,720] Trial 193 finished with value: 1.9068581054926308 and parameters: {'alpha': 0.5548926510068234, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,739] Trial 194 finished with value: 1.9071667076321053 and parameters: {'alpha': 0.42164640831133565, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,758] Trial 195 finished with value: 1.9067999983649013 and parameters: {'alpha': 0.5188062623167777, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,777] Trial 196 finished with value: 1.9070538685566927 and parameters: {'alpha': 0.6008920004219704, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,796] Trial 197 finished with value: 1.906950638090232 and parameters: {'alpha': 0.45359442498724284, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,814] Trial 198 finished with value: 1.906799731518939 and parameters: {'alpha': 0.5110278350958132, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,833] Trial 199 finished with value: 1.9074057023646085 and parameters: {'alpha': 0.39722389889039644, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,853] Trial 200 finished with value: 1.966683298506331 and parameters: {'alpha': 0.010557466775041173, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,871] Trial 201 finished with value: 1.9068003976752927 and parameters: {'alpha': 0.5090484087873932, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,892] Trial 202 finished with value: 1.9067993521576996 and parameters: {'alpha': 0.5157759351115674, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,911] Trial 203 finished with value: 1.9068003369880242 and parameters: {'alpha': 0.5091975403709617, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,930] Trial 204 finished with value: 1.9068466334215506 and parameters: {'alpha': 0.5506427021585547, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,949] Trial 205 finished with value: 1.9068339524972777 and parameters: {'alpha': 0.48473032858694903, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,969] Trial 206 finished with value: 1.9070521905400912 and parameters: {'alpha': 0.43666720183329233, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:13,988] Trial 207 finished with value: 1.906981033808096 and parameters: {'alpha': 0.5868975815849183, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,009] Trial 208 finished with value: 1.906801134500496 and parameters: {'alpha': 0.5214773975775604, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,028] Trial 209 finished with value: 1.9073196915534856 and parameters: {'alpha': 0.6408696050878325, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,055] Trial 210 finished with value: 1.906799577957721 and parameters: {'alpha': 0.5172368751879282, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,080] Trial 211 finished with value: 1.9068006301326317 and parameters: {'alpha': 0.5204390143182579, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,100] Trial 212 finished with value: 1.9068745206536266 and parameters: {'alpha': 0.47105121103259107, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,119] Trial 213 finished with value: 1.9068212503428241 and parameters: {'alpha': 0.5389275278688265, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,137] Trial 214 finished with value: 1.9584415502259551 and parameters: {'alpha': 0.0301560362113711, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,157] Trial 215 finished with value: 1.936671733372365 and parameters: {'alpha': 0.1098450381923668, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,177] Trial 216 finished with value: 1.906905279195419 and parameters: {'alpha': 0.5692040413598708, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,196] Trial 217 finished with value: 1.9068162694338866 and parameters: {'alpha': 0.49353193801866185, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,219] Trial 218 finished with value: 3.3293504650458985 and parameters: {'alpha': 0.4432284354362548, 'kernel': 'rbf', 'gamma': 0.010410180505211091}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,238] Trial 219 finished with value: 1.9068143977082526 and parameters: {'alpha': 0.5347014754651701, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,259] Trial 220 finished with value: 1.9072189342450463 and parameters: {'alpha': 0.6271014478219262, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,279] Trial 221 finished with value: 1.906801308695035 and parameters: {'alpha': 0.5218024110011401, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,300] Trial 222 finished with value: 1.9068235397297342 and parameters: {'alpha': 0.48951563159515965, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,320] Trial 223 finished with value: 1.9068006483165258 and parameters: {'alpha': 0.5204795224259963, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,339] Trial 224 finished with value: 1.9069490372930307 and parameters: {'alpha': 0.5799452297881541, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,360] Trial 225 finished with value: 1.9069387839061152 and parameters: {'alpha': 0.45593583271452154, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,380] Trial 226 finished with value: 1.9068214255381182 and parameters: {'alpha': 0.5390264223026541, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,399] Trial 227 finished with value: 1.9069433692212556 and parameters: {'alpha': 0.57864479267664, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,415] Trial 228 finished with value: 2.672830847417923 and parameters: {'alpha': 0.4246643438163496, 'kernel': 'linear'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,434] Trial 229 finished with value: 1.9068016070085996 and parameters: {'alpha': 0.5066549603593866, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,454] Trial 230 finished with value: 1.9076386511179884 and parameters: {'alpha': 0.6783371193198922, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,473] Trial 231 finished with value: 1.9068027608107545 and parameters: {'alpha': 0.5049217865253429, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,493] Trial 232 finished with value: 1.9068756660188355 and parameters: {'alpha': 0.47073118272275827, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,513] Trial 233 finished with value: 1.9068097295812996 and parameters: {'alpha': 0.5312504257371579, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,533] Trial 234 finished with value: 1.9069622169810923 and parameters: {'alpha': 0.5828842618468845, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,551] Trial 235 finished with value: 1.9068795967633596 and parameters: {'alpha': 0.46965173688355344, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,573] Trial 236 finished with value: 1.9068027380436532 and parameters: {'alpha': 0.5240677047468334, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,600] Trial 237 finished with value: 1.924598043509277 and parameters: {'alpha': 0.18013678651930679, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,623] Trial 238 finished with value: 1.9072735569972343 and parameters: {'alpha': 0.40986327084642515, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,642] Trial 239 finished with value: 1.9071061017589968 and parameters: {'alpha': 0.6098344486247443, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,661] Trial 240 finished with value: 1.9068453166392592 and parameters: {'alpha': 0.48030064760098834, 'kernel': 'poly'}. Best is trial 111 with value: 1.9067993265370458.\n",
      "[I 2025-09-04 21:02:14,680] Trial 241 finished with value: 1.9067993207610952 and parameters: {'alpha': 0.5154107743801493, 'kernel': 'poly'}. Best is trial 241 with value: 1.9067993207610952.\n",
      "[I 2025-09-04 21:02:14,700] Trial 242 finished with value: 1.9068215381232199 and parameters: {'alpha': 0.5390897742951712, 'kernel': 'poly'}. Best is trial 241 with value: 1.9067993207610952.\n",
      "[I 2025-09-04 21:02:14,721] Trial 243 finished with value: 1.906801148741191 and parameters: {'alpha': 0.5074636341567386, 'kernel': 'poly'}. Best is trial 241 with value: 1.9067993207610952.\n",
      "[I 2025-09-04 21:02:14,740] Trial 244 finished with value: 1.9069561751419155 and parameters: {'alpha': 0.45253465001456245, 'kernel': 'poly'}. Best is trial 241 with value: 1.9067993207610952.\n",
      "[I 2025-09-04 21:02:14,760] Trial 245 finished with value: 1.9069064552778967 and parameters: {'alpha': 0.5695171723282816, 'kernel': 'poly'}. Best is trial 241 with value: 1.9067993207610952.\n",
      "[I 2025-09-04 21:02:14,780] Trial 246 finished with value: 1.9068005397820993 and parameters: {'alpha': 0.5087144883806298, 'kernel': 'poly'}. Best is trial 241 with value: 1.9067993207610952.\n",
      "[I 2025-09-04 21:02:14,800] Trial 247 finished with value: 1.9067992899003565 and parameters: {'alpha': 0.5141388481477017, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:14,819] Trial 248 finished with value: 1.9068907288827133 and parameters: {'alpha': 0.46673814789242163, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:14,838] Trial 249 finished with value: 1.906915873087892 and parameters: {'alpha': 0.5719690856976516, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:14,859] Trial 250 finished with value: 1.906819618453669 and parameters: {'alpha': 0.49159016747440076, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:14,878] Trial 251 finished with value: 1.906817254730876 and parameters: {'alpha': 0.5365578006109963, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:14,898] Trial 252 finished with value: 1.9071047765844795 and parameters: {'alpha': 0.4293817036940389, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:14,918] Trial 253 finished with value: 1.9072580452770933 and parameters: {'alpha': 0.6325985447846596, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:14,938] Trial 254 finished with value: 1.9068033699741653 and parameters: {'alpha': 0.5249162619303578, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:14,969] Trial 255 finished with value: 2.2467154361454007 and parameters: {'alpha': 0.46303998600542734, 'kernel': 'rbf', 'gamma': 0.000590276317624202}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:14,998] Trial 256 finished with value: 1.9070325504007646 and parameters: {'alpha': 0.5970084950672171, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,021] Trial 257 finished with value: 1.9074464758885485 and parameters: {'alpha': 0.39365078604212406, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,045] Trial 258 finished with value: 1.9068043160242016 and parameters: {'alpha': 0.5029992466047164, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,064] Trial 259 finished with value: 1.906881574370986 and parameters: {'alpha': 0.5625029517683265, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,089] Trial 260 finished with value: 1.9070537237235095 and parameters: {'alpha': 0.43644352517613505, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,114] Trial 261 finished with value: 2.613095277291991 and parameters: {'alpha': 0.6632406544094853, 'kernel': 'linear'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,134] Trial 262 finished with value: 1.9068031259421643 and parameters: {'alpha': 0.5044365432793468, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,153] Trial 263 finished with value: 1.9068722473129578 and parameters: {'alpha': 0.559620156817408, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,173] Trial 264 finished with value: 1.9068463354893446 and parameters: {'alpha': 0.47993292428374906, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,193] Trial 265 finished with value: 1.9071196266701949 and parameters: {'alpha': 0.6120367765680107, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,212] Trial 266 finished with value: 1.9070586645029173 and parameters: {'alpha': 0.4357277202211493, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,232] Trial 267 finished with value: 1.9068133573023949 and parameters: {'alpha': 0.5339834401681324, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,252] Trial 268 finished with value: 1.9068774314549664 and parameters: {'alpha': 0.47024282174248855, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,271] Trial 269 finished with value: 1.9068023977648687 and parameters: {'alpha': 0.5054305835494766, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,291] Trial 270 finished with value: 1.906900284847478 and parameters: {'alpha': 0.5678558504196247, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,311] Trial 271 finished with value: 1.9074644922993251 and parameters: {'alpha': 0.39241225346122643, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,330] Trial 272 finished with value: 1.907192383392131 and parameters: {'alpha': 0.6232423729736806, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,351] Trial 273 finished with value: 1.9079769432847669 and parameters: {'alpha': 0.7120999621549376, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,371] Trial 274 finished with value: 1.9070224738949169 and parameters: {'alpha': 0.4411591671907698, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,390] Trial 275 finished with value: 1.9068069432621846 and parameters: {'alpha': 0.500344623306858, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,411] Trial 276 finished with value: 1.906852511088294 and parameters: {'alpha': 0.552874872057773, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,434] Trial 277 finished with value: 3.308926717102993 and parameters: {'alpha': 0.46430447384733925, 'kernel': 'rbf', 'gamma': 0.008537871917559416}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,453] Trial 278 finished with value: 1.9068131041991943 and parameters: {'alpha': 0.5338048805012824, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,474] Trial 279 finished with value: 1.9070773745970608 and parameters: {'alpha': 0.6050086517997, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,495] Trial 280 finished with value: 1.9071657113787583 and parameters: {'alpha': 0.4217648119599527, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,515] Trial 281 finished with value: 1.906812316617849 and parameters: {'alpha': 0.4960977799539588, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,531] Trial 282 finished with value: 2.6929800911202286 and parameters: {'alpha': 0.36160802287105137, 'kernel': 'linear'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,555] Trial 283 finished with value: 1.9068612417356035 and parameters: {'alpha': 0.5559839476803136, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,586] Trial 284 finished with value: 1.9074000237523483 and parameters: {'alpha': 0.6510457371692147, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,608] Trial 285 finished with value: 1.9068684915637597 and parameters: {'alpha': 0.4727798190875896, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,627] Trial 286 finished with value: 1.9068008303998503 and parameters: {'alpha': 0.5208712184523671, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,647] Trial 287 finished with value: 1.9533146833922885 and parameters: {'alpha': 0.043241160206120566, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,666] Trial 288 finished with value: 1.9070365524025525 and parameters: {'alpha': 0.597749330403967, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,685] Trial 289 finished with value: 1.9068006087978775 and parameters: {'alpha': 0.520391138705195, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,706] Trial 290 finished with value: 1.9070484749743932 and parameters: {'alpha': 0.43721238182777583, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,725] Trial 291 finished with value: 1.9068380986376443 and parameters: {'alpha': 0.5471500133981354, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,746] Trial 292 finished with value: 1.906865240097405 and parameters: {'alpha': 0.4737450834709012, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,766] Trial 293 finished with value: 1.907564089885512 and parameters: {'alpha': 0.6702002948801837, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,787] Trial 294 finished with value: 1.907267909470793 and parameters: {'alpha': 0.4104459218491739, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,808] Trial 295 finished with value: 1.9068040496822658 and parameters: {'alpha': 0.5257593651217476, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,828] Trial 296 finished with value: 1.9638795385914063 and parameters: {'alpha': 0.017876916722659293, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,849] Trial 297 finished with value: 1.9070013976106737 and parameters: {'alpha': 0.5910325285786334, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,869] Trial 298 finished with value: 1.9068095740300781 and parameters: {'alpha': 0.4981225286255729, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,890] Trial 299 finished with value: 1.9069051108593862 and parameters: {'alpha': 0.46324013541469844, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,912] Trial 300 finished with value: 2.4693556356278656 and parameters: {'alpha': 0.562900836664984, 'kernel': 'rbf', 'gamma': 6.715813910169008e-05}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,933] Trial 301 finished with value: 1.9068027691480822 and parameters: {'alpha': 0.524111183715056, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,954] Trial 302 finished with value: 1.9073915977135796 and parameters: {'alpha': 0.3984921578951779, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,974] Trial 303 finished with value: 1.9070860633915419 and parameters: {'alpha': 0.6064907157969416, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:15,994] Trial 304 finished with value: 1.9070121674851002 and parameters: {'alpha': 0.44279311427488527, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,012] Trial 305 finished with value: 2.5984671226604203 and parameters: {'alpha': 0.7395981733377065, 'kernel': 'linear'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,032] Trial 306 finished with value: 1.906820695228337 and parameters: {'alpha': 0.49100137553197787, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,054] Trial 307 finished with value: 1.906819780999234 and parameters: {'alpha': 0.5380825604342536, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,083] Trial 308 finished with value: 1.9073704035102952 and parameters: {'alpha': 0.6473647275777595, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,109] Trial 309 finished with value: 1.9069296592426952 and parameters: {'alpha': 0.575397878901786, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,129] Trial 310 finished with value: 1.9069113663948887 and parameters: {'alpha': 0.46179801809297444, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,148] Trial 311 finished with value: 1.9068003831490394 and parameters: {'alpha': 0.5090837244516392, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,168] Trial 312 finished with value: 1.9068197168637833 and parameters: {'alpha': 0.49153569969685773, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,187] Trial 313 finished with value: 1.9071914700650259 and parameters: {'alpha': 0.41875940740547385, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,208] Trial 314 finished with value: 1.9203674069324916 and parameters: {'alpha': 0.20951032718071377, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,229] Trial 315 finished with value: 1.9069026257852488 and parameters: {'alpha': 0.5684915584045821, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,249] Trial 316 finished with value: 1.907948511912008 and parameters: {'alpha': 0.3799315594849532, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,270] Trial 317 finished with value: 1.9069259015331983 and parameters: {'alpha': 0.4586050863307891, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,290] Trial 318 finished with value: 1.90679952878621 and parameters: {'alpha': 0.5169911461580707, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,311] Trial 319 finished with value: 1.9071345099481962 and parameters: {'alpha': 0.6144133298963637, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,330] Trial 320 finished with value: 1.9068271821852767 and parameters: {'alpha': 0.5420881755652344, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,351] Trial 321 finished with value: 1.9068117850395778 and parameters: {'alpha': 0.4964718214290448, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,372] Trial 322 finished with value: 1.9077411727500733 and parameters: {'alpha': 0.6890622343361292, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,392] Trial 323 finished with value: 1.9070504572292428 and parameters: {'alpha': 0.43692097587470147, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,423] Trial 324 finished with value: 3.3998894341393107 and parameters: {'alpha': 0.5785762738711459, 'kernel': 'rbf', 'gamma': 0.7875155956187312}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,445] Trial 325 finished with value: 1.9067993284258802 and parameters: {'alpha': 0.5133965252672009, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,464] Trial 326 finished with value: 1.9067993277347755 and parameters: {'alpha': 0.5155028919194202, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,484] Trial 327 finished with value: 1.9068468248872876 and parameters: {'alpha': 0.47975776839138207, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,505] Trial 328 finished with value: 1.907328933513146 and parameters: {'alpha': 0.6420726614629957, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,523] Trial 329 finished with value: 1.9094159707617546 and parameters: {'alpha': 0.3455935094611848, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,545] Trial 330 finished with value: 1.9068292411907874 and parameters: {'alpha': 0.5431076131670632, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,564] Trial 331 finished with value: 1.9070179951801491 and parameters: {'alpha': 0.44186402748088305, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,593] Trial 332 finished with value: 1.906990592737346 and parameters: {'alpha': 0.5888634793111958, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,617] Trial 333 finished with value: 2.6518541490578142 and parameters: {'alpha': 0.4985305944934922, 'kernel': 'linear'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,638] Trial 334 finished with value: 1.9072766412355222 and parameters: {'alpha': 0.40954673714564854, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,658] Trial 335 finished with value: 1.9068277513814738 and parameters: {'alpha': 0.5423735233678124, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,678] Trial 336 finished with value: 1.906866908635917 and parameters: {'alpha': 0.473246691648254, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,701] Trial 337 finished with value: 1.9071227894386626 and parameters: {'alpha': 0.6125458160085314, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,722] Trial 338 finished with value: 1.9067997156254812 and parameters: {'alpha': 0.517831498741792, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,744] Trial 339 finished with value: 1.9069895892946747 and parameters: {'alpha': 0.44653046079014636, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,764] Trial 340 finished with value: 1.9068808679462461 and parameters: {'alpha': 0.5622901907318409, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,785] Trial 341 finished with value: 1.9068098585441067 and parameters: {'alpha': 0.4979005782081853, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,807] Trial 342 finished with value: 1.9073910008681645 and parameters: {'alpha': 0.3985462072175499, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,827] Trial 343 finished with value: 1.9068643698914658 and parameters: {'alpha': 0.47400764628599046, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,850] Trial 344 finished with value: 1.9304636273418903 and parameters: {'alpha': 0.1441737458646002, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,872] Trial 345 finished with value: 1.9075918627088873 and parameters: {'alpha': 0.6732673649551462, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,903] Trial 346 finished with value: 3.387278676277127 and parameters: {'alpha': 0.5381258813324402, 'kernel': 'rbf', 'gamma': 0.10937320816239461}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,923] Trial 347 finished with value: 1.9070046855220804 and parameters: {'alpha': 0.5916820503384775, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,944] Trial 348 finished with value: 1.944374474949449 and parameters: {'alpha': 0.07181490785985514, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,964] Trial 349 finished with value: 1.9071094315891393 and parameters: {'alpha': 0.42877148411464544, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:16,986] Trial 350 finished with value: 1.9067995966999343 and parameters: {'alpha': 0.5173250345359807, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,007] Trial 351 finished with value: 1.9069040743170278 and parameters: {'alpha': 0.46348344255207197, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,025] Trial 352 finished with value: 2.6494759465775455 and parameters: {'alpha': 0.5074963144227203, 'kernel': 'linear'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,044] Trial 353 finished with value: 1.9073038348739966 and parameters: {'alpha': 0.6387840819719273, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,066] Trial 354 finished with value: 1.9069107726905674 and parameters: {'alpha': 0.5706531650934638, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,090] Trial 355 finished with value: 1.9080733338227438 and parameters: {'alpha': 0.3768143545966863, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,121] Trial 356 finished with value: 1.9087854944766758 and parameters: {'alpha': 0.7802665442933617, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,145] Trial 357 finished with value: 1.9068000976361834 and parameters: {'alpha': 0.5191005850370035, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,165] Trial 358 finished with value: 1.9163032745992215 and parameters: {'alpha': 0.24441727251909265, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,185] Trial 359 finished with value: 1.9069395779969154 and parameters: {'alpha': 0.4557757160444481, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,206] Trial 360 finished with value: 1.9068797944610465 and parameters: {'alpha': 0.5619652105372569, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,227] Trial 361 finished with value: 1.9068119819142586 and parameters: {'alpha': 0.49633236044065077, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,247] Trial 362 finished with value: 1.907124216426491 and parameters: {'alpha': 0.6127747661862599, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,269] Trial 363 finished with value: 1.9070770279070084 and parameters: {'alpha': 0.43313117629781644, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,290] Trial 364 finished with value: 1.9068212432867844 and parameters: {'alpha': 0.538923536869956, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,311] Trial 365 finished with value: 1.9068393214492352 and parameters: {'alpha': 0.48255603494061977, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,330] Trial 366 finished with value: 1.9416675164814476 and parameters: {'alpha': 0.08450208251812573, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,352] Trial 367 finished with value: 1.9078899336370616 and parameters: {'alpha': 0.7038297621965829, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,372] Trial 368 finished with value: 1.9071944245707375 and parameters: {'alpha': 0.41842186810371257, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,395] Trial 369 finished with value: 2.2004988937110572 and parameters: {'alpha': 0.5300597851650358, 'kernel': 'rbf', 'gamma': 0.0001912026621747039}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,418] Trial 370 finished with value: 1.9069843353981517 and parameters: {'alpha': 0.5875818018887224, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,439] Trial 371 finished with value: 1.9068441026135252 and parameters: {'alpha': 0.48074440223070963, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,462] Trial 372 finished with value: 1.9067992925943575 and parameters: {'alpha': 0.5148660664453245, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,482] Trial 373 finished with value: 1.907259951571991 and parameters: {'alpha': 0.63286120150906, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,505] Trial 374 finished with value: 1.9069244673986379 and parameters: {'alpha': 0.45891108966287053, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,526] Trial 375 finished with value: 1.9068676789198977 and parameters: {'alpha': 0.5581446836290284, 'kernel': 'poly'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,545] Trial 376 finished with value: 2.8261454565764716 and parameters: {'alpha': 0.10023901226583527, 'kernel': 'linear'}. Best is trial 247 with value: 1.9067992899003565.\n",
      "[I 2025-09-04 21:02:17,566] Trial 377 finished with value: 1.9067992888692293 and parameters: {'alpha': 0.5147211168611996, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,587] Trial 378 finished with value: 1.9068009458482338 and parameters: {'alpha': 0.5211077716895823, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,612] Trial 379 finished with value: 1.907002366669192 and parameters: {'alpha': 0.5912244597021264, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,641] Trial 380 finished with value: 1.9073260900459998 and parameters: {'alpha': 0.4046231379429272, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,665] Trial 381 finished with value: 1.906931746212483 and parameters: {'alpha': 0.4573768224961904, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,685] Trial 382 finished with value: 1.9068051737383265 and parameters: {'alpha': 0.5020683371116061, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,707] Trial 383 finished with value: 1.9068853281148759 and parameters: {'alpha': 0.5636193702658877, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,726] Trial 384 finished with value: 1.9077321558679001 and parameters: {'alpha': 0.6881384103403819, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,747] Trial 385 finished with value: 1.9083401930784598 and parameters: {'alpha': 0.3702812185513962, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,768] Trial 386 finished with value: 1.907227500388684 and parameters: {'alpha': 0.6283237443965818, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,789] Trial 387 finished with value: 1.9070246805744078 and parameters: {'alpha': 0.4408147261121757, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,809] Trial 388 finished with value: 1.9605081101682782 and parameters: {'alpha': 0.025545654270313762, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,830] Trial 389 finished with value: 1.9136701486984014 and parameters: {'alpha': 0.9905126608944008, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,850] Trial 390 finished with value: 1.9068095285922961 and parameters: {'alpha': 0.5310863315605293, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,871] Trial 391 finished with value: 1.9068505451313509 and parameters: {'alpha': 0.4784559863316713, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,893] Trial 392 finished with value: 1.9069097722456365 and parameters: {'alpha': 0.5703917812310179, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,917] Trial 393 finished with value: 2.698694197322171 and parameters: {'alpha': 0.4988157630943356, 'kernel': 'rbf', 'gamma': 0.0016381860574544572}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,939] Trial 394 finished with value: 1.907168445148263 and parameters: {'alpha': 0.421440336290026, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,960] Trial 395 finished with value: 1.9068454668932786 and parameters: {'alpha': 0.5501842343565478, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:17,981] Trial 396 finished with value: 1.9068900678214051 and parameters: {'alpha': 0.4669057309389787, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,003] Trial 397 finished with value: 1.9071948575011195 and parameters: {'alpha': 0.62360666801873, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,021] Trial 398 finished with value: 2.6475848315832384 and parameters: {'alpha': 0.514717488310709, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,042] Trial 399 finished with value: 1.9069865410641125 and parameters: {'alpha': 0.5880358051500582, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,063] Trial 400 finished with value: 1.9069747287943055 and parameters: {'alpha': 0.44912370559646764, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,085] Trial 401 finished with value: 1.9068059170204386 and parameters: {'alpha': 0.5278094268486367, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,107] Trial 402 finished with value: 1.9074299668785661 and parameters: {'alpha': 0.3950813623749603, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,127] Trial 403 finished with value: 1.9068171326516103 and parameters: {'alpha': 0.49301390182505667, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,149] Trial 404 finished with value: 1.9073870313996508 and parameters: {'alpha': 0.6494407641976987, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,178] Trial 405 finished with value: 1.9068550684798489 and parameters: {'alpha': 0.5538091962410331, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,205] Trial 406 finished with value: 1.910467654218672 and parameters: {'alpha': 0.8731274503748401, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,226] Trial 407 finished with value: 1.9069197080066935 and parameters: {'alpha': 0.4599402888374412, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,245] Trial 408 finished with value: 1.9068029701159512 and parameters: {'alpha': 0.5046406396441322, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,266] Trial 409 finished with value: 1.9567246426828946 and parameters: {'alpha': 0.03400902774252839, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,289] Trial 410 finished with value: 1.907013239057952 and parameters: {'alpha': 0.5933500502154417, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,311] Trial 411 finished with value: 1.9071607706102827 and parameters: {'alpha': 0.4223547001636018, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,332] Trial 412 finished with value: 1.907933035925712 and parameters: {'alpha': 0.7079574852407697, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,356] Trial 413 finished with value: 1.9068161053187982 and parameters: {'alpha': 0.5358296214381628, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,377] Trial 414 finished with value: 1.9092348007870374 and parameters: {'alpha': 0.3495792859135611, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,399] Trial 415 finished with value: 1.906874732076693 and parameters: {'alpha': 0.4709919444705204, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,423] Trial 416 finished with value: 2.7720088709553177 and parameters: {'alpha': 0.5720175466487554, 'kernel': 'rbf', 'gamma': 3.173997295929621e-05}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,445] Trial 417 finished with value: 1.9516237851904261 and parameters: {'alpha': 0.04795350247162844, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,467] Trial 418 finished with value: 1.9068052143730652 and parameters: {'alpha': 0.5020260031236268, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,490] Trial 419 finished with value: 1.9072406783508695 and parameters: {'alpha': 0.6301836396084993, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,509] Trial 420 finished with value: 2.6724709556545005 and parameters: {'alpha': 0.42585724964670396, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,529] Trial 421 finished with value: 1.9117148765692775 and parameters: {'alpha': 0.30000897957717937, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,549] Trial 422 finished with value: 1.9068183017831406 and parameters: {'alpha': 0.5372016474657004, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,569] Trial 423 finished with value: 1.9069456162275207 and parameters: {'alpha': 0.4545738218454016, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,590] Trial 424 finished with value: 1.9068071146020937 and parameters: {'alpha': 0.5001890460590455, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,611] Trial 425 finished with value: 1.9068702498497563 and parameters: {'alpha': 0.5589805661967269, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,631] Trial 426 finished with value: 1.9071481648025042 and parameters: {'alpha': 0.6165531515366349, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,653] Trial 427 finished with value: 1.9068497039082615 and parameters: {'alpha': 0.47874590907794395, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,677] Trial 428 finished with value: 1.9085339093929443 and parameters: {'alpha': 0.7603870521394646, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,706] Trial 429 finished with value: 1.90757025803319 and parameters: {'alpha': 0.38962972065929463, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,730] Trial 430 finished with value: 1.9068098407345637 and parameters: {'alpha': 0.5313405112221579, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,751] Trial 431 finished with value: 1.923051039367624 and parameters: {'alpha': 0.19059629545549708, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,772] Trial 432 finished with value: 1.907150243443244 and parameters: {'alpha': 0.4236267999090381, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,792] Trial 433 finished with value: 1.9070014053549624 and parameters: {'alpha': 0.5910340640609788, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,813] Trial 434 finished with value: 1.906849208683378 and parameters: {'alpha': 0.4789177689451211, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,835] Trial 435 finished with value: 1.9075444069306948 and parameters: {'alpha': 0.6679989444546965, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,857] Trial 436 finished with value: 1.9068046264821439 and parameters: {'alpha': 0.5264292603998997, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,880] Trial 437 finished with value: 3.3398597207010705 and parameters: {'alpha': 0.44580310805901485, 'kernel': 'rbf', 'gamma': 0.011944274858972636}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,902] Trial 438 finished with value: 1.906929096377993 and parameters: {'alpha': 0.5752612814141903, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,924] Trial 439 finished with value: 1.9068100474185181 and parameters: {'alpha': 0.497754916476222, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,946] Trial 440 finished with value: 1.9068102361287327 and parameters: {'alpha': 0.5316572575597577, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,968] Trial 441 finished with value: 1.9069043969128296 and parameters: {'alpha': 0.46340758292233813, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:18,990] Trial 442 finished with value: 1.9069784709198128 and parameters: {'alpha': 0.5863625305387989, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,012] Trial 443 finished with value: 1.9074872230373165 and parameters: {'alpha': 0.661463577059695, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,043] Trial 444 finished with value: 1.9075895558037483 and parameters: {'alpha': 0.38912544950558825, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,072] Trial 445 finished with value: 2.6510056945527714 and parameters: {'alpha': 0.5017146143104401, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,106] Trial 446 finished with value: 1.9070827376286108 and parameters: {'alpha': 0.4323432726333946, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,131] Trial 447 finished with value: 1.9068706223824154 and parameters: {'alpha': 0.5591004910316355, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,152] Trial 448 finished with value: 1.9068680084184237 and parameters: {'alpha': 0.47292171759304275, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,172] Trial 449 finished with value: 1.9068068957165398 and parameters: {'alpha': 0.528769630153895, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,194] Trial 450 finished with value: 1.9071496834812092 and parameters: {'alpha': 0.616788846054733, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,224] Trial 451 finished with value: 1.906814157218674 and parameters: {'alpha': 0.49485959533812446, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,250] Trial 452 finished with value: 1.9070479914236236 and parameters: {'alpha': 0.43728366062388313, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,272] Trial 453 finished with value: 1.906864080229116 and parameters: {'alpha': 0.556949239976291, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,293] Trial 454 finished with value: 1.9635048624881295 and parameters: {'alpha': 0.018754922534491302, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,314] Trial 455 finished with value: 1.9070737884001001 and parameters: {'alpha': 0.60439094063452, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,335] Trial 456 finished with value: 1.9068044894874792 and parameters: {'alpha': 0.5262735194971233, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,357] Trial 457 finished with value: 1.9068927059051255 and parameters: {'alpha': 0.46624076116762425, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,378] Trial 458 finished with value: 1.9072943359986287 and parameters: {'alpha': 0.4077528418273687, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,399] Trial 459 finished with value: 1.9068134614457324 and parameters: {'alpha': 0.495317885332473, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,421] Trial 460 finished with value: 1.9075672943925395 and parameters: {'alpha': 0.6705564797494069, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,443] Trial 461 finished with value: 1.9068765238114913 and parameters: {'alpha': 0.5609622742111701, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,486] Trial 462 finished with value: 3.38334705602008 and parameters: {'alpha': 0.12271461179491865, 'kernel': 'rbf', 'gamma': 0.29515743486433127}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,508] Trial 463 finished with value: 1.9087540436820056 and parameters: {'alpha': 0.360484674812506, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,530] Trial 464 finished with value: 1.9069492964739179 and parameters: {'alpha': 0.4538543397543727, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,549] Trial 465 finished with value: 1.9655547465578636 and parameters: {'alpha': 0.013748647525212326, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,572] Trial 466 finished with value: 1.9068001827776595 and parameters: {'alpha': 0.519339025608551, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,590] Trial 467 finished with value: 2.623257775996736 and parameters: {'alpha': 0.6154437771661678, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,611] Trial 468 finished with value: 1.9068067663234958 and parameters: {'alpha': 0.5286463147673988, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,633] Trial 469 finished with value: 1.9280432297352303 and parameters: {'alpha': 0.15855388065957376, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,654] Trial 470 finished with value: 1.9082596560925291 and parameters: {'alpha': 0.7374795153816568, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,676] Trial 471 finished with value: 1.9069001176378177 and parameters: {'alpha': 0.5678101805414797, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,698] Trial 472 finished with value: 1.9068214049972578 and parameters: {'alpha': 0.490621568899122, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,719] Trial 473 finished with value: 1.9474390622799502 and parameters: {'alpha': 0.06023972987783364, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,741] Trial 474 finished with value: 1.907127667224561 and parameters: {'alpha': 0.42642878414487256, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,764] Trial 475 finished with value: 1.9070485546746234 and parameters: {'alpha': 0.5999380971691354, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,791] Trial 476 finished with value: 1.906801831314198 and parameters: {'alpha': 0.5227019941479167, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,819] Trial 477 finished with value: 1.9068850062143736 and parameters: {'alpha': 0.46821091015254074, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,840] Trial 478 finished with value: 1.9073522032699568 and parameters: {'alpha': 0.6450629100335802, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,862] Trial 479 finished with value: 1.9068620054847216 and parameters: {'alpha': 0.5562457047216247, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,882] Trial 480 finished with value: 1.9068085628147498 and parameters: {'alpha': 0.498937874818435, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,907] Trial 481 finished with value: 2.439108123123454 and parameters: {'alpha': 0.4029283720015844, 'kernel': 'rbf', 'gamma': 0.0010243936661029158}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,928] Trial 482 finished with value: 1.906898222564395 and parameters: {'alpha': 0.4648817051496977, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,949] Trial 483 finished with value: 1.9068415863697552 and parameters: {'alpha': 0.5486177776697864, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,970] Trial 484 finished with value: 1.9070627118137147 and parameters: {'alpha': 0.43514693728364284, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:19,992] Trial 485 finished with value: 1.9067992997868977 and parameters: {'alpha': 0.5138531159752318, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,014] Trial 486 finished with value: 1.9070388490817782 and parameters: {'alpha': 0.5981719434711159, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,035] Trial 487 finished with value: 1.9079128925020619 and parameters: {'alpha': 0.7060362028850248, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,056] Trial 488 finished with value: 1.9068033149846886 and parameters: {'alpha': 0.5248451242213326, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,074] Trial 489 finished with value: 2.7031222795780847 and parameters: {'alpha': 0.3324389498667604, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,097] Trial 490 finished with value: 1.9069114590853613 and parameters: {'alpha': 0.4617769769011434, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,118] Trial 491 finished with value: 1.9068757063998332 and parameters: {'alpha': 0.560708506317846, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,144] Trial 492 finished with value: 1.9068191639887855 and parameters: {'alpha': 0.49184348061007616, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,167] Trial 493 finished with value: 1.9073360507232235 and parameters: {'alpha': 0.6429930617460161, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,190] Trial 494 finished with value: 1.907013259911319 and parameters: {'alpha': 0.5933540794753043, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,214] Trial 495 finished with value: 1.9074563990762436 and parameters: {'alpha': 0.3928010623058346, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,235] Trial 496 finished with value: 1.9068020538972228 and parameters: {'alpha': 0.5230570106903342, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,256] Trial 497 finished with value: 1.9068480392996496 and parameters: {'alpha': 0.4793271537912676, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,278] Trial 498 finished with value: 1.9069814296002972 and parameters: {'alpha': 0.44794017881219234, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,299] Trial 499 finished with value: 1.906890306758446 and parameters: {'alpha': 0.5650653580091046, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,321] Trial 500 finished with value: 1.9068011359256805 and parameters: {'alpha': 0.5214801168741945, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,346] Trial 501 finished with value: 1.9071481345606422 and parameters: {'alpha': 0.42388419317046916, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,376] Trial 502 finished with value: 1.9072000070647712 and parameters: {'alpha': 0.6243617392490778, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,398] Trial 503 finished with value: 1.9068059548884801 and parameters: {'alpha': 0.5012788749845901, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,422] Trial 504 finished with value: 3.215449607248278 and parameters: {'alpha': 0.26763548280496396, 'kernel': 'rbf', 'gamma': 0.00559334075855494}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,443] Trial 505 finished with value: 1.9068417175201906 and parameters: {'alpha': 0.5486718159514612, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,465] Trial 506 finished with value: 1.9070034026608957 and parameters: {'alpha': 0.44421700768596156, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,486] Trial 507 finished with value: 1.9081850207392999 and parameters: {'alpha': 0.37405867699147904, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,509] Trial 508 finished with value: 1.9091186459782725 and parameters: {'alpha': 0.8052779430491073, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,531] Trial 509 finished with value: 1.9069841418239604 and parameters: {'alpha': 0.5875418400906669, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,550] Trial 510 finished with value: 2.6100822320483257 and parameters: {'alpha': 0.6783444342149173, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,572] Trial 511 finished with value: 1.9068451502379742 and parameters: {'alpha': 0.4803611071058531, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,593] Trial 512 finished with value: 1.9068114491124952 and parameters: {'alpha': 0.5325955508515787, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,613] Trial 513 finished with value: 1.9068376848684327 and parameters: {'alpha': 0.4832018278634984, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,635] Trial 514 finished with value: 1.907125802064736 and parameters: {'alpha': 0.4266650168460311, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,657] Trial 515 finished with value: 1.906976231642072 and parameters: {'alpha': 0.5858921716297301, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,678] Trial 516 finished with value: 1.9067992997360816 and parameters: {'alpha': 0.5138542357916994, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,700] Trial 517 finished with value: 1.907376390678495 and parameters: {'alpha': 0.648115140326363, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,722] Trial 518 finished with value: 1.9068370678283055 and parameters: {'alpha': 0.5467042053369247, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,745] Trial 519 finished with value: 1.9069025987127999 and parameters: {'alpha': 0.46383203228207276, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,765] Trial 520 finished with value: 1.9068013538401647 and parameters: {'alpha': 0.5218843546058033, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,787] Trial 521 finished with value: 1.9073082768847995 and parameters: {'alpha': 0.4063651050578545, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,810] Trial 522 finished with value: 1.9070972917902662 and parameters: {'alpha': 0.6083766863103817, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,831] Trial 523 finished with value: 1.906809775939323 and parameters: {'alpha': 0.49796470108571345, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,857] Trial 524 finished with value: 1.9068836111570062 and parameters: {'alpha': 0.5631116258104574, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,890] Trial 525 finished with value: 1.9069902624868482 and parameters: {'alpha': 0.446415635964068, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,914] Trial 526 finished with value: 1.9622302882779528 and parameters: {'alpha': 0.02167830599360005, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,934] Trial 527 finished with value: 1.9068233449446994 and parameters: {'alpha': 0.4896144070102332, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,958] Trial 528 finished with value: 2.1946536711292355 and parameters: {'alpha': 0.559922794856613, 'kernel': 'rbf', 'gamma': 0.0002587307016480093}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:20,978] Trial 529 finished with value: 1.9080321794531212 and parameters: {'alpha': 0.7172273405249046, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,000] Trial 530 finished with value: 1.9068014913137388 and parameters: {'alpha': 0.522128571459781, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,022] Trial 531 finished with value: 1.9071310722268924 and parameters: {'alpha': 0.6138685995058283, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,043] Trial 532 finished with value: 1.9069388900245696 and parameters: {'alpha': 0.45591440718196086, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,061] Trial 533 finished with value: 2.6769355579612926 and parameters: {'alpha': 0.4112309502656106, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,083] Trial 534 finished with value: 1.9068147439557104 and parameters: {'alpha': 0.49448159530926095, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,105] Trial 535 finished with value: 1.9069440329889384 and parameters: {'alpha': 0.5787982864111235, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,128] Trial 536 finished with value: 1.9068163037931218 and parameters: {'alpha': 0.5359570506718135, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,149] Trial 537 finished with value: 1.9075027480303082 and parameters: {'alpha': 0.6632593604641231, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,172] Trial 538 finished with value: 1.906945309765252 and parameters: {'alpha': 0.4546341721033626, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,194] Trial 539 finished with value: 1.9068108842188132 and parameters: {'alpha': 0.497124750143829, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,216] Trial 540 finished with value: 1.9069581757502199 and parameters: {'alpha': 0.5819950891293915, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,238] Trial 541 finished with value: 1.9075286891646477 and parameters: {'alpha': 0.3907195311823738, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,259] Trial 542 finished with value: 1.906825347946657 and parameters: {'alpha': 0.5411489562511711, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,282] Trial 543 finished with value: 1.906870780552041 and parameters: {'alpha': 0.4721145350718094, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,304] Trial 544 finished with value: 1.9072242692571797 and parameters: {'alpha': 0.62786395127604, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,327] Trial 545 finished with value: 1.9070459405454356 and parameters: {'alpha': 0.4375868248900528, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,349] Trial 546 finished with value: 1.9067993209413556 and parameters: {'alpha': 0.5134945969297267, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,380] Trial 547 finished with value: 1.9067993096882738 and parameters: {'alpha': 0.5136647225140974, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,406] Trial 548 finished with value: 1.906802263242836 and parameters: {'alpha': 0.5233782330557606, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,428] Trial 549 finished with value: 1.9068392465192878 and parameters: {'alpha': 0.48258529894187074, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,449] Trial 550 finished with value: 1.906998926373129 and parameters: {'alpha': 0.5905411766115313, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,473] Trial 551 finished with value: 3.380939530197338 and parameters: {'alpha': 0.5465758632948766, 'kernel': 'rbf', 'gamma': 0.059037051038668624}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,495] Trial 552 finished with value: 1.9070240810680388 and parameters: {'alpha': 0.44090811983347217, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,518] Trial 553 finished with value: 1.9067992947714945 and parameters: {'alpha': 0.5139761645511992, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,540] Trial 554 finished with value: 1.9072679959157663 and parameters: {'alpha': 0.4104369733947166, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,563] Trial 555 finished with value: 1.9083740646570058 and parameters: {'alpha': 0.36946441197665764, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,582] Trial 556 finished with value: 2.65520464688039 and parameters: {'alpha': 0.48611314630678426, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,604] Trial 557 finished with value: 1.9071105182721464 and parameters: {'alpha': 0.6105582508497492, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,626] Trial 558 finished with value: 1.9182976227666915 and parameters: {'alpha': 0.22533347317859947, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,649] Trial 559 finished with value: 1.9068868934787155 and parameters: {'alpha': 0.5640781498035917, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,671] Trial 560 finished with value: 1.9074928249189458 and parameters: {'alpha': 0.6621134645894567, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,693] Trial 561 finished with value: 1.9069026051921463 and parameters: {'alpha': 0.46383049585712827, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,715] Trial 562 finished with value: 1.9067994546560434 and parameters: {'alpha': 0.5165676585394736, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,739] Trial 563 finished with value: 1.9070633194795028 and parameters: {'alpha': 0.4350601640184819, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,762] Trial 564 finished with value: 1.9068064452483595 and parameters: {'alpha': 0.5008071625538074, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,784] Trial 565 finished with value: 1.9068595307894847 and parameters: {'alpha': 0.5553919429479193, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,806] Trial 566 finished with value: 1.9667745761342845 and parameters: {'alpha': 0.010267606117263638, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,828] Trial 567 finished with value: 1.906852136973732 and parameters: {'alpha': 0.4779140970610026, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,850] Trial 568 finished with value: 1.907127343574018 and parameters: {'alpha': 0.6132749484401957, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,873] Trial 569 finished with value: 1.906806609782956 and parameters: {'alpha': 0.5284957177347441, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,895] Trial 570 finished with value: 1.9071954093851935 and parameters: {'alpha': 0.41830967316077206, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,927] Trial 571 finished with value: 1.9079992097257363 and parameters: {'alpha': 0.7141778077574059, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,955] Trial 572 finished with value: 1.9069046140224277 and parameters: {'alpha': 0.5690262214931869, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:21,994] Trial 573 finished with value: 3.3949406522872088 and parameters: {'alpha': 0.4696117666290015, 'kernel': 'rbf', 'gamma': 0.4252966653703051}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,016] Trial 574 finished with value: 1.9068007371351867 and parameters: {'alpha': 0.5082809444024856, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,037] Trial 575 finished with value: 1.9091506219874126 and parameters: {'alpha': 0.35145374896954185, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,060] Trial 576 finished with value: 1.9069139204833747 and parameters: {'alpha': 0.5714685352780373, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,081] Trial 577 finished with value: 1.9073935901091488 and parameters: {'alpha': 0.6502528194847415, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,110] Trial 578 finished with value: 1.906978832814599 and parameters: {'alpha': 0.44839598516784535, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,134] Trial 579 finished with value: 2.6474543197759646 and parameters: {'alpha': 0.5152188778246534, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,162] Trial 580 finished with value: 1.9068265658531423 and parameters: {'alpha': 0.4880306861146395, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,187] Trial 581 finished with value: 1.907197070211303 and parameters: {'alpha': 0.41812081873853874, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,213] Trial 582 finished with value: 1.906846809912123 and parameters: {'alpha': 0.550711594055765, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,234] Trial 583 finished with value: 1.9433117839474188 and parameters: {'alpha': 0.07652004409627534, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,255] Trial 584 finished with value: 1.9071883536173448 and parameters: {'alpha': 0.6226468721342912, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,277] Trial 585 finished with value: 1.907498642046211 and parameters: {'alpha': 0.3915103343017921, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,301] Trial 586 finished with value: 1.9069441449462994 and parameters: {'alpha': 0.4548641791283341, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,326] Trial 587 finished with value: 1.9068037816228374 and parameters: {'alpha': 0.5254344447474738, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,347] Trial 588 finished with value: 1.9069334603327355 and parameters: {'alpha': 0.5763132823273601, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,370] Trial 589 finished with value: 1.9068196466810106 and parameters: {'alpha': 0.4915745304346712, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,393] Trial 590 finished with value: 1.9068035241895898 and parameters: {'alpha': 0.5251132805705994, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,417] Trial 591 finished with value: 1.9069532166113285 and parameters: {'alpha': 0.45309834675983296, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,441] Trial 592 finished with value: 1.9071048915699382 and parameters: {'alpha': 0.609635312019275, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,469] Trial 593 finished with value: 1.9068590203770117 and parameters: {'alpha': 0.5552137934043944, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,500] Trial 594 finished with value: 1.9068157619424182 and parameters: {'alpha': 0.4938428201915502, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,528] Trial 595 finished with value: 2.1570244024691596 and parameters: {'alpha': 0.0966216079100729, 'kernel': 'rbf', 'gamma': 2.4290833923238545e-05}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,551] Trial 596 finished with value: 1.907881810270879 and parameters: {'alpha': 0.7030447310026784, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,573] Trial 597 finished with value: 1.9071639776769644 and parameters: {'alpha': 0.42197129158114727, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,596] Trial 598 finished with value: 1.9068826009048896 and parameters: {'alpha': 0.4688454503172967, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,618] Trial 599 finished with value: 1.9070435859920993 and parameters: {'alpha': 0.5990378701397641, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,640] Trial 600 finished with value: 1.9068169435486886 and parameters: {'alpha': 0.5363629540213906, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,660] Trial 601 finished with value: 2.6514205452902226 and parameters: {'alpha': 0.5001557765587561, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,682] Trial 602 finished with value: 1.9086738234916394 and parameters: {'alpha': 0.7715607475333462, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,704] Trial 603 finished with value: 1.907941594337094 and parameters: {'alpha': 0.3801054889493323, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,727] Trial 604 finished with value: 1.907375672809112 and parameters: {'alpha': 0.648025338670777, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,750] Trial 605 finished with value: 1.9070029261110164 and parameters: {'alpha': 0.4442953734738618, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,772] Trial 606 finished with value: 1.910710741339857 and parameters: {'alpha': 0.31887081159992664, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,795] Trial 607 finished with value: 1.906867676029297 and parameters: {'alpha': 0.5581437355230581, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,818] Trial 608 finished with value: 1.9068026564116105 and parameters: {'alpha': 0.5050652241893668, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,841] Trial 609 finished with value: 1.9069926341946457 and parameters: {'alpha': 0.589277482692066, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,863] Trial 610 finished with value: 1.9068785161256627 and parameters: {'alpha': 0.4699456619068483, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,885] Trial 611 finished with value: 1.955673059358585 and parameters: {'alpha': 0.03673875093313719, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,908] Trial 612 finished with value: 1.9068216588567182 and parameters: {'alpha': 0.5391575389776573, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,930] Trial 613 finished with value: 1.9072467800631774 and parameters: {'alpha': 0.41266195177687814, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,951] Trial 614 finished with value: 1.907548730541874 and parameters: {'alpha': 0.6684845238447156, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,974] Trial 615 finished with value: 1.9068392978268793 and parameters: {'alpha': 0.4825652576090398, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:22,996] Trial 616 finished with value: 1.9069730671484328 and parameters: {'alpha': 0.5852228079046937, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,028] Trial 617 finished with value: 3.3616374992062874 and parameters: {'alpha': 0.5230232181577948, 'kernel': 'rbf', 'gamma': 0.02211649000203364}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,055] Trial 618 finished with value: 1.9070069845591022 and parameters: {'alpha': 0.44363115319116714, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,077] Trial 619 finished with value: 1.9068306077782573 and parameters: {'alpha': 0.5437656452648668, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,098] Trial 620 finished with value: 1.9071811111373982 and parameters: {'alpha': 0.6215698117275922, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,120] Trial 621 finished with value: 1.9068228521147967 and parameters: {'alpha': 0.4898661779657473, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,143] Trial 622 finished with value: 1.9069950956114077 and parameters: {'alpha': 0.44559764920874806, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,164] Trial 623 finished with value: 1.9069132436922156 and parameters: {'alpha': 0.5712941151902639, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,183] Trial 624 finished with value: 2.648192618467864 and parameters: {'alpha': 0.5123877194801064, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,206] Trial 625 finished with value: 1.9074292982181056 and parameters: {'alpha': 0.3951397592772358, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,228] Trial 626 finished with value: 1.9068409651008666 and parameters: {'alpha': 0.4819211312659871, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,250] Trial 627 finished with value: 1.9068268721921273 and parameters: {'alpha': 0.5419315874669975, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,273] Trial 628 finished with value: 1.9071515781296502 and parameters: {'alpha': 0.6170822619455065, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,296] Trial 629 finished with value: 1.907014851915473 and parameters: {'alpha': 0.442363441266138, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,317] Trial 630 finished with value: 1.9076221278866012 and parameters: {'alpha': 0.6765601864427756, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,341] Trial 631 finished with value: 1.9068012509174785 and parameters: {'alpha': 0.5216962043682675, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,362] Trial 632 finished with value: 1.9359533796504083 and parameters: {'alpha': 0.11364426780122765, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,385] Trial 633 finished with value: 1.9069432845100498 and parameters: {'alpha': 0.5786251800921469, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,408] Trial 634 finished with value: 1.9068720778700474 and parameters: {'alpha': 0.4717424520613234, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,430] Trial 635 finished with value: 1.9072673329883143 and parameters: {'alpha': 0.4105056211229362, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,453] Trial 636 finished with value: 1.906800026550947 and parameters: {'alpha': 0.5100393422003459, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,476] Trial 637 finished with value: 1.9086149915541561 and parameters: {'alpha': 0.3637324062447959, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,503] Trial 638 finished with value: 1.9069740995727393 and parameters: {'alpha': 0.585441797405157, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,537] Trial 639 finished with value: 2.7621260424719605 and parameters: {'alpha': 0.47693939243902234, 'kernel': 'rbf', 'gamma': 0.0018645488312286533}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,560] Trial 640 finished with value: 1.9068254119473809 and parameters: {'alpha': 0.5411822558441544, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,582] Trial 641 finished with value: 1.9070751391029281 and parameters: {'alpha': 0.43339378733182193, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,604] Trial 642 finished with value: 1.9072275059545325 and parameters: {'alpha': 0.6283245351240376, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,628] Trial 643 finished with value: 1.9068012201663764 and parameters: {'alpha': 0.5073314666172105, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,650] Trial 644 finished with value: 1.9068718652150594 and parameters: {'alpha': 0.5594984547952164, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,670] Trial 645 finished with value: 2.601413909354946 and parameters: {'alpha': 0.7235860573677728, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,692] Trial 646 finished with value: 1.906906110653328 and parameters: {'alpha': 0.46300665422180276, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,715] Trial 647 finished with value: 1.9067995920851886 and parameters: {'alpha': 0.5173035794710186, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,738] Trial 648 finished with value: 1.9072079073826407 and parameters: {'alpha': 0.4168992745755192, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,761] Trial 649 finished with value: 1.9068121034718006 and parameters: {'alpha': 0.49624680389509546, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,784] Trial 650 finished with value: 1.9317680248656248 and parameters: {'alpha': 0.13666707274224202, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,808] Trial 651 finished with value: 1.9509259236607415 and parameters: {'alpha': 0.04991550731484125, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,830] Trial 652 finished with value: 1.9069676399832989 and parameters: {'alpha': 0.5840617167189027, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,853] Trial 653 finished with value: 1.9074034877886865 and parameters: {'alpha': 0.6514711866150641, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,876] Trial 654 finished with value: 1.906817200469111 and parameters: {'alpha': 0.5365239434802626, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,899] Trial 655 finished with value: 1.9069366336124254 and parameters: {'alpha': 0.4563718832691875, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,923] Trial 656 finished with value: 1.9068135073447083 and parameters: {'alpha': 0.4952873025704796, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,947] Trial 657 finished with value: 1.9068493659676102 and parameters: {'alpha': 0.5516959719714194, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,970] Trial 658 finished with value: 1.907455277567018 and parameters: {'alpha': 0.3928967222865099, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:23,992] Trial 659 finished with value: 1.9071252335333446 and parameters: {'alpha': 0.6129376829929862, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,021] Trial 660 finished with value: 1.911096918474896 and parameters: {'alpha': 0.8974677563439212, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,061] Trial 661 finished with value: 3.3859400317647705 and parameters: {'alpha': 0.4570088079839641, 'kernel': 'rbf', 'gamma': 0.1272938596771626}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,085] Trial 662 finished with value: 1.9068058154142673 and parameters: {'alpha': 0.5277058578748339, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,107] Trial 663 finished with value: 1.9255119773765985 and parameters: {'alpha': 0.17425614107281506, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,129] Trial 664 finished with value: 1.9068268671537574 and parameters: {'alpha': 0.48788759997874553, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,153] Trial 665 finished with value: 1.9461948302007686 and parameters: {'alpha': 0.06481730084479115, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,175] Trial 666 finished with value: 1.9069055771132137 and parameters: {'alpha': 0.5692835144220376, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,195] Trial 667 finished with value: 2.671547621328907 and parameters: {'alpha': 0.42892904108804764, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,218] Trial 668 finished with value: 1.9072878855858 and parameters: {'alpha': 0.6366579607847778, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,241] Trial 669 finished with value: 1.9068002874140368 and parameters: {'alpha': 0.5196171335369066, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,264] Trial 670 finished with value: 1.9068921721046284 and parameters: {'alpha': 0.4663745017316846, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,286] Trial 671 finished with value: 1.9069587372841013 and parameters: {'alpha': 0.5821192566968145, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,309] Trial 672 finished with value: 1.9067997089067115 and parameters: {'alpha': 0.5178049145299715, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,331] Trial 673 finished with value: 1.906804011079632 and parameters: {'alpha': 0.5033494384417496, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,354] Trial 674 finished with value: 1.907245540852613 and parameters: {'alpha': 0.41279374386514006, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,376] Trial 675 finished with value: 1.966178534220787 and parameters: {'alpha': 0.01205591592059194, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,399] Trial 676 finished with value: 1.9075824315573344 and parameters: {'alpha': 0.6722308417082121, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,421] Trial 677 finished with value: 1.9068930545539124 and parameters: {'alpha': 0.565847532431201, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,445] Trial 678 finished with value: 1.9069269810617064 and parameters: {'alpha': 0.45837596888956855, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,468] Trial 679 finished with value: 1.9083659503436847 and parameters: {'alpha': 0.3696598371226995, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,492] Trial 680 finished with value: 1.9068163374526472 and parameters: {'alpha': 0.5359785896771788, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,518] Trial 681 finished with value: 1.9071211083347361 and parameters: {'alpha': 0.6122755217483599, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,550] Trial 682 finished with value: 1.9068304237668139 and parameters: {'alpha': 0.4862561288941168, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,574] Trial 683 finished with value: 1.9090296541069711 and parameters: {'alpha': 0.7987255916743479, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,596] Trial 684 finished with value: 1.9070485892278404 and parameters: {'alpha': 0.43719555117847075, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,632] Trial 685 finished with value: 3.2261516660859884 and parameters: {'alpha': 0.5375181985096604, 'kernel': 'rbf', 'gamma': 0.005391252322043012}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,667] Trial 686 finished with value: 1.9068286275657365 and parameters: {'alpha': 0.4870672817615437, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,698] Trial 687 finished with value: 1.9069938949243816 and parameters: {'alpha': 0.5895321631194034, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,723] Trial 688 finished with value: 1.9068030328439216 and parameters: {'alpha': 0.52447234386612, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,746] Trial 689 finished with value: 1.9080487767279688 and parameters: {'alpha': 0.7187506213671561, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,765] Trial 690 finished with value: 2.6701134626780827 and parameters: {'alpha': 0.433732694455734, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,788] Trial 691 finished with value: 1.9068558696598765 and parameters: {'alpha': 0.4766760160120068, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,810] Trial 692 finished with value: 1.906924566822603 and parameters: {'alpha': 0.5741518836793682, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,833] Trial 693 finished with value: 1.9068001158108878 and parameters: {'alpha': 0.5191524904285922, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,856] Trial 694 finished with value: 1.9097198141449998 and parameters: {'alpha': 0.3390525110687781, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,879] Trial 695 finished with value: 1.907219986620977 and parameters: {'alpha': 0.627252188988786, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,902] Trial 696 finished with value: 1.9072878225215302 and parameters: {'alpha': 0.40840886366371815, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,924] Trial 697 finished with value: 1.9068701161496915 and parameters: {'alpha': 0.47230647006951504, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,946] Trial 698 finished with value: 1.906829398486186 and parameters: {'alpha': 0.5431840802901537, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,969] Trial 699 finished with value: 1.9069472823032414 and parameters: {'alpha': 0.4542469134789332, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:24,992] Trial 700 finished with value: 1.9070339108839325 and parameters: {'alpha': 0.5972609800414025, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,014] Trial 701 finished with value: 1.90755533475859 and parameters: {'alpha': 0.6692240217824776, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,048] Trial 702 finished with value: 1.90680708163076 and parameters: {'alpha': 0.5002188484676634, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,078] Trial 703 finished with value: 1.9069175417993323 and parameters: {'alpha': 0.5723937781652648, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,101] Trial 704 finished with value: 1.9079910146165437 and parameters: {'alpha': 0.3788656330622644, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,123] Trial 705 finished with value: 1.9067997677171635 and parameters: {'alpha': 0.5180309547204267, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,146] Trial 706 finished with value: 1.9070553000843025 and parameters: {'alpha': 0.43621432172651964, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,172] Trial 707 finished with value: 2.4510111895027027 and parameters: {'alpha': 0.5595177383086737, 'kernel': 'rbf', 'gamma': 7.031572148062147e-05}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,196] Trial 708 finished with value: 1.9068397719546757 and parameters: {'alpha': 0.482380686906354, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,219] Trial 709 finished with value: 1.9071750881027854 and parameters: {'alpha': 0.6206672915128384, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,244] Trial 710 finished with value: 1.906804176093566 and parameters: {'alpha': 0.5259094589492761, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,267] Trial 711 finished with value: 1.906940933452149 and parameters: {'alpha': 0.4555035303514784, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,287] Trial 712 finished with value: 2.6784497775136096 and parameters: {'alpha': 0.40635413336064014, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,311] Trial 713 finished with value: 1.9067998981234442 and parameters: {'alpha': 0.510439382128829, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,334] Trial 714 finished with value: 1.9068717073696249 and parameters: {'alpha': 0.5594480910988905, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,357] Trial 715 finished with value: 1.9588669209523955 and parameters: {'alpha': 0.029206143883775865, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,381] Trial 716 finished with value: 1.907521513494754 and parameters: {'alpha': 0.6654082535889517, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,404] Trial 717 finished with value: 1.9132525203000323 and parameters: {'alpha': 0.2788912364619423, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,427] Trial 718 finished with value: 1.906870968758402 and parameters: {'alpha': 0.47206033631151223, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,451] Trial 719 finished with value: 1.9068456665971034 and parameters: {'alpha': 0.5502631111673022, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,474] Trial 720 finished with value: 1.9070985419277815 and parameters: {'alpha': 0.4302071438799987, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,498] Trial 721 finished with value: 1.9070937719556171 and parameters: {'alpha': 0.6077889315556166, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,521] Trial 722 finished with value: 1.9541516677421007 and parameters: {'alpha': 0.04092638737635298, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,549] Trial 723 finished with value: 1.9068143850274668 and parameters: {'alpha': 0.494711939520914, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,581] Trial 724 finished with value: 1.9067993193280535 and parameters: {'alpha': 0.5135170838775538, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,608] Trial 725 finished with value: 1.9068385159098005 and parameters: {'alpha': 0.5473288584173702, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,630] Trial 726 finished with value: 1.9069481586580084 and parameters: {'alpha': 0.45407575523288135, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,654] Trial 727 finished with value: 1.9068078807995743 and parameters: {'alpha': 0.4995136676644465, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,677] Trial 728 finished with value: 1.9082914423124133 and parameters: {'alpha': 0.7402114103247995, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,704] Trial 729 finished with value: 2.9358962618023807 and parameters: {'alpha': 0.5998823801561499, 'kernel': 'rbf', 'gamma': 2.352475021634776e-05}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,727] Trial 730 finished with value: 1.9067992993758105 and parameters: {'alpha': 0.515044915868523, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,750] Trial 731 finished with value: 1.9077973000403652 and parameters: {'alpha': 0.38376218463051104, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,773] Trial 732 finished with value: 1.9071062540205874 and parameters: {'alpha': 0.42918747204192526, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,797] Trial 733 finished with value: 1.906854954074893 and parameters: {'alpha': 0.5537678406460347, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,819] Trial 734 finished with value: 1.9068517210249067 and parameters: {'alpha': 0.4780548594168017, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,841] Trial 735 finished with value: 2.6157375655937765 and parameters: {'alpha': 0.6502482019035346, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,864] Trial 736 finished with value: 1.9067998056105182 and parameters: {'alpha': 0.5181693683186253, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,887] Trial 737 finished with value: 1.906986593824837 and parameters: {'alpha': 0.5880466352178055, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,911] Trial 738 finished with value: 1.9068986999931217 and parameters: {'alpha': 0.46476600341673635, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,934] Trial 739 finished with value: 1.9067993073708462 and parameters: {'alpha': 0.5152028657181463, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,957] Trial 740 finished with value: 1.9215623299677234 and parameters: {'alpha': 0.2009644355704621, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:25,981] Trial 741 finished with value: 1.9070923961242703 and parameters: {'alpha': 0.43103021162086524, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,003] Trial 742 finished with value: 1.9068592073671573 and parameters: {'alpha': 0.5552791422969365, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,027] Trial 743 finished with value: 1.906814343905108 and parameters: {'alpha': 0.4947385083919233, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,049] Trial 744 finished with value: 1.9073622934752628 and parameters: {'alpha': 0.4011839465826698, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,080] Trial 745 finished with value: 1.907150089257066 and parameters: {'alpha': 0.6168517452898794, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,110] Trial 746 finished with value: 1.9069180969575656 and parameters: {'alpha': 0.4602935891196434, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,133] Trial 747 finished with value: 1.9068441103109803 and parameters: {'alpha': 0.5496440502872241, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,155] Trial 748 finished with value: 1.9076830830090619 and parameters: {'alpha': 0.6830465264830737, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,178] Trial 749 finished with value: 1.9068057746106606 and parameters: {'alpha': 0.50145671130593, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,202] Trial 750 finished with value: 1.906981049741863 and parameters: {'alpha': 0.44800663304206784, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,225] Trial 751 finished with value: 1.9069507493554976 and parameters: {'alpha': 0.5803335543625966, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,251] Trial 752 finished with value: 2.020034372182458 and parameters: {'alpha': 0.01604770284763774, 'kernel': 'rbf', 'gamma': 0.00014666636003886935}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,274] Trial 753 finished with value: 1.9068041184814597 and parameters: {'alpha': 0.503224827845974, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,299] Trial 754 finished with value: 1.9071496789957634 and parameters: {'alpha': 0.6167881505878879, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,323] Trial 755 finished with value: 1.907284330451626 and parameters: {'alpha': 0.4087626300339321, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,347] Trial 756 finished with value: 1.906868334878415 and parameters: {'alpha': 0.47282577985041224, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,371] Trial 757 finished with value: 1.9068300525213917 and parameters: {'alpha': 0.5434999844094888, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,391] Trial 758 finished with value: 2.6441499220941536 and parameters: {'alpha': 0.5280458026052395, 'kernel': 'linear'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,415] Trial 759 finished with value: 1.9069248975491113 and parameters: {'alpha': 0.5742335093769116, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,437] Trial 760 finished with value: 1.906848451090622 and parameters: {'alpha': 0.4791824123006044, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,461] Trial 761 finished with value: 1.9071101929081746 and parameters: {'alpha': 0.42867216897721894, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,484] Trial 762 finished with value: 1.9158578315647656 and parameters: {'alpha': 0.2489572632980808, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,507] Trial 763 finished with value: 1.9072660075551002 and parameters: {'alpha': 0.6336925335742207, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,530] Trial 764 finished with value: 1.9086993595044413 and parameters: {'alpha': 0.3617567153362516, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,559] Trial 765 finished with value: 1.9068016697609143 and parameters: {'alpha': 0.5224345917848016, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,614] Trial 766 finished with value: 1.9068887509329322 and parameters: {'alpha': 0.4672415097350207, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,648] Trial 767 finished with value: 1.9070108601991431 and parameters: {'alpha': 0.5928892370848647, 'kernel': 'poly'}. Best is trial 377 with value: 1.9067992888692293.\n",
      "[I 2025-09-04 21:02:26,678] Trial 768 finished with value: 1.906799286179563 and parameters: {'alpha': 0.5144309895570267, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:26,700] Trial 769 finished with value: 1.9079508575990225 and parameters: {'alpha': 0.7096462659508513, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:26,722] Trial 770 finished with value: 1.9068296977583803 and parameters: {'alpha': 0.5433290375127895, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:26,747] Trial 771 finished with value: 1.9070083499349488 and parameters: {'alpha': 0.44340928509327737, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:26,770] Trial 772 finished with value: 1.906810514747536 and parameters: {'alpha': 0.4974000224365107, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:26,793] Trial 773 finished with value: 1.907456779664835 and parameters: {'alpha': 0.3927686212581538, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:26,819] Trial 774 finished with value: 3.0262782253491176 and parameters: {'alpha': 0.08983450644899929, 'kernel': 'rbf', 'gamma': 0.0034769947520790104}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:26,842] Trial 775 finished with value: 1.906948340983102 and parameters: {'alpha': 0.579786710687926, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:26,865] Trial 776 finished with value: 1.9068056930162218 and parameters: {'alpha': 0.5015380257055481, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:26,889] Trial 777 finished with value: 1.9069941607277519 and parameters: {'alpha': 0.44575501002236967, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:26,912] Trial 778 finished with value: 1.9073790959351309 and parameters: {'alpha': 0.6484531312875157, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:26,935] Trial 779 finished with value: 1.9623422295804098 and parameters: {'alpha': 0.02142445980671901, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:26,959] Trial 780 finished with value: 1.9068310454244202 and parameters: {'alpha': 0.5439734397579861, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:26,980] Trial 781 finished with value: 2.656592501133873 and parameters: {'alpha': 0.4810414350283276, 'kernel': 'linear'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,003] Trial 782 finished with value: 1.9068827767455943 and parameters: {'alpha': 0.5628631157671667, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,026] Trial 783 finished with value: 1.9071584716189713 and parameters: {'alpha': 0.42263072119575934, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,049] Trial 784 finished with value: 1.9067993128971925 and parameters: {'alpha': 0.5136126874993746, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,073] Trial 785 finished with value: 1.9068412387932123 and parameters: {'alpha': 0.48181668439088843, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,097] Trial 786 finished with value: 1.9071218365513238 and parameters: {'alpha': 0.6123926835415591, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,130] Trial 787 finished with value: 1.9068042861984795 and parameters: {'alpha': 0.5260386402330232, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,154] Trial 788 finished with value: 1.9070020022611622 and parameters: {'alpha': 0.4444475804208163, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,177] Trial 789 finished with value: 1.906939634899918 and parameters: {'alpha': 0.5777750943080783, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,201] Trial 790 finished with value: 1.9075091118367866 and parameters: {'alpha': 0.3912344897133365, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,224] Trial 791 finished with value: 1.9068015625253023 and parameters: {'alpha': 0.5067296947668702, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,247] Trial 792 finished with value: 1.9110205349788416 and parameters: {'alpha': 0.31289238529913777, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,271] Trial 793 finished with value: 1.9077376257237462 and parameters: {'alpha': 0.6886992474578539, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,294] Trial 794 finished with value: 1.9068884129313548 and parameters: {'alpha': 0.4673281149139044, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,318] Trial 795 finished with value: 1.906821360228005 and parameters: {'alpha': 0.53898960049428, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,345] Trial 796 finished with value: 3.3887383283013457 and parameters: {'alpha': 0.8429019802179104, 'kernel': 'rbf', 'gamma': 0.05392821450098093}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,371] Trial 797 finished with value: 1.907248886195513 and parameters: {'alpha': 0.6313299638836107, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,395] Trial 798 finished with value: 1.9068021885815518 and parameters: {'alpha': 0.5057374648004959, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,419] Trial 799 finished with value: 1.907174207107672 and parameters: {'alpha': 0.4207608255264459, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,444] Trial 800 finished with value: 1.906941566404715 and parameters: {'alpha': 0.5782262442058563, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,467] Trial 801 finished with value: 1.9068780689836509 and parameters: {'alpha': 0.4700679003594507, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,490] Trial 802 finished with value: 2.6368795593318093 and parameters: {'alpha': 0.5571904254143492, 'kernel': 'linear'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,515] Trial 803 finished with value: 1.9067992861901562 and parameters: {'alpha': 0.5144812999948022, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,540] Trial 804 finished with value: 1.9069610372706693 and parameters: {'alpha': 0.45162058328911514, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,563] Trial 805 finished with value: 1.9092389740117868 and parameters: {'alpha': 0.3494867336129708, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,587] Trial 806 finished with value: 1.9071873851119372 and parameters: {'alpha': 0.6225033510284738, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,610] Trial 807 finished with value: 1.9068006942868214 and parameters: {'alpha': 0.5083724166450835, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,647] Trial 808 finished with value: 1.907227935231049 and parameters: {'alpha': 0.41468897678496214, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,673] Trial 809 finished with value: 1.9068441717033273 and parameters: {'alpha': 0.5496686639477933, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,697] Trial 810 finished with value: 1.9068598612163157 and parameters: {'alpha': 0.4753985519691815, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,720] Trial 811 finished with value: 1.9069879921613442 and parameters: {'alpha': 0.5883331636088069, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,742] Trial 812 finished with value: 1.9068101410292098 and parameters: {'alpha': 0.497683206731481, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,766] Trial 813 finished with value: 1.9069513414216581 and parameters: {'alpha': 0.4534586629053413, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,790] Trial 814 finished with value: 1.90748081303226 and parameters: {'alpha': 0.6607172415066331, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,814] Trial 815 finished with value: 1.906820349414705 and parameters: {'alpha': 0.538412815206514, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,839] Trial 816 finished with value: 1.9080074951726702 and parameters: {'alpha': 0.37845357187667533, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,864] Trial 817 finished with value: 1.9070002007166111 and parameters: {'alpha': 0.5907948949092536, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,887] Trial 818 finished with value: 1.9068201432588083 and parameters: {'alpha': 0.4913012447312208, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,911] Trial 819 finished with value: 1.9071489528098267 and parameters: {'alpha': 0.4237842212235274, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,938] Trial 820 finished with value: 3.3686087111397107 and parameters: {'alpha': 0.7337146772268017, 'kernel': 'rbf', 'gamma': 0.022103483796991}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,962] Trial 821 finished with value: 1.9068148252090618 and parameters: {'alpha': 0.5349895010563067, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:27,986] Trial 822 finished with value: 1.9068606131559012 and parameters: {'alpha': 0.47516289411259355, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,010] Trial 823 finished with value: 1.9068420732349158 and parameters: {'alpha': 0.5488179806565041, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,034] Trial 824 finished with value: 1.907089686417169 and parameters: {'alpha': 0.6071027851496832, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,055] Trial 825 finished with value: 2.664354060919776 and parameters: {'alpha': 0.4534293077541732, 'kernel': 'linear'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,078] Trial 826 finished with value: 1.9068015400001985 and parameters: {'alpha': 0.5067678194320128, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,102] Trial 827 finished with value: 1.9074773578897013 and parameters: {'alpha': 0.6603137421124957, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,135] Trial 828 finished with value: 1.9071482481857074 and parameters: {'alpha': 0.42387030286852106, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,162] Trial 829 finished with value: 1.906832292629992 and parameters: {'alpha': 0.5445581630510756, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,186] Trial 830 finished with value: 1.9068178258180177 and parameters: {'alpha': 0.49260716414842126, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,209] Trial 831 finished with value: 1.9068879307628235 and parameters: {'alpha': 0.564380039759357, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,233] Trial 832 finished with value: 1.907652675275783 and parameters: {'alpha': 0.3874833493844352, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,257] Trial 833 finished with value: 1.9070971941563135 and parameters: {'alpha': 0.6083604247439622, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,282] Trial 834 finished with value: 1.906936525087165 and parameters: {'alpha': 0.45639398764390104, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,305] Trial 835 finished with value: 1.9068031772851808 and parameters: {'alpha': 0.5043702174327994, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,329] Trial 836 finished with value: 1.9068034918101755 and parameters: {'alpha': 0.5250722113687198, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,353] Trial 837 finished with value: 1.9069592471576513 and parameters: {'alpha': 0.5822318272162481, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,379] Trial 838 finished with value: 1.9070813039989796 and parameters: {'alpha': 0.4325402723961145, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,402] Trial 839 finished with value: 1.9069131003631141 and parameters: {'alpha': 0.46140592483338194, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,429] Trial 840 finished with value: 2.2464520853173697 and parameters: {'alpha': 0.68156453205695, 'kernel': 'rbf', 'gamma': 0.0003183987146014569}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,465] Trial 841 finished with value: 1.906801273359865 and parameters: {'alpha': 0.5217376393783716, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,494] Trial 842 finished with value: 1.9069488025859958 and parameters: {'alpha': 0.57989183532929, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,523] Trial 843 finished with value: 1.9068413526076382 and parameters: {'alpha': 0.4817733548294927, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,554] Trial 844 finished with value: 1.9072736497344736 and parameters: {'alpha': 0.40985373622840154, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,577] Trial 845 finished with value: 1.9072551043962231 and parameters: {'alpha': 0.6321924158750937, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,601] Trial 846 finished with value: 1.906818261575014 and parameters: {'alpha': 0.537177246012427, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,624] Trial 847 finished with value: 1.9068401885180348 and parameters: {'alpha': 0.4822194511403569, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,645] Trial 848 finished with value: 2.6371114042693713 and parameters: {'alpha': 0.556240896155293, 'kernel': 'linear'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,675] Trial 849 finished with value: 1.9088331059314 and parameters: {'alpha': 0.7839255201052436, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,711] Trial 850 finished with value: 1.9069795408547952 and parameters: {'alpha': 0.44827135210344965, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,735] Trial 851 finished with value: 1.9068062834309063 and parameters: {'alpha': 0.5009609568399402, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,758] Trial 852 finished with value: 1.9071418896020007 and parameters: {'alpha': 0.6155744420400885, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,782] Trial 853 finished with value: 1.9074345130925363 and parameters: {'alpha': 0.3946852698364005, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,807] Trial 854 finished with value: 1.9068134549172535 and parameters: {'alpha': 0.5340518864093101, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,831] Trial 855 finished with value: 1.9069982316958047 and parameters: {'alpha': 0.44507274639851563, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,855] Trial 856 finished with value: 1.9069375281339325 and parameters: {'alpha': 0.5772797267463271, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,880] Trial 857 finished with value: 1.9068167585233944 and parameters: {'alpha': 0.4932368085183254, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,903] Trial 858 finished with value: 1.9075121045950787 and parameters: {'alpha': 0.6643337288719553, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,927] Trial 859 finished with value: 1.9068004046125184 and parameters: {'alpha': 0.5199119850360729, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,950] Trial 860 finished with value: 1.9068993243559673 and parameters: {'alpha': 0.464615138123055, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:28,973] Trial 861 finished with value: 1.9483175622715443 and parameters: {'alpha': 0.057357900203899595, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,014] Trial 862 finished with value: 3.3922060959610043 and parameters: {'alpha': 0.34263665028258805, 'kernel': 'rbf', 'gamma': 0.47898833244043326}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,038] Trial 863 finished with value: 1.906932754919303 and parameters: {'alpha': 0.5761443159932826, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,062] Trial 864 finished with value: 1.9070781855020293 and parameters: {'alpha': 0.43297071703855605, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,087] Trial 865 finished with value: 1.9068028868864604 and parameters: {'alpha': 0.5242740506875183, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,111] Trial 866 finished with value: 1.929470960161565 and parameters: {'alpha': 0.14999899518097745, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,136] Trial 867 finished with value: 1.9071765631031115 and parameters: {'alpha': 0.6208888932137813, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,160] Trial 868 finished with value: 1.9068317485926756 and parameters: {'alpha': 0.48567326608828665, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,183] Trial 869 finished with value: 2.6409777145314224 and parameters: {'alpha': 0.5406034530790396, 'kernel': 'linear'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,215] Trial 870 finished with value: 1.907252966953328 and parameters: {'alpha': 0.4120070407991916, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,244] Trial 871 finished with value: 1.9134534559287781 and parameters: {'alpha': 0.9833850319865092, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,267] Trial 872 finished with value: 1.9068965613075874 and parameters: {'alpha': 0.566830308492998, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,291] Trial 873 finished with value: 1.9343358331076395 and parameters: {'alpha': 0.12235760688417227, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,316] Trial 874 finished with value: 1.9068600966135758 and parameters: {'alpha': 0.47532461419525707, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,339] Trial 875 finished with value: 1.9068009154987309 and parameters: {'alpha': 0.5079140647972422, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,363] Trial 876 finished with value: 1.9070445000295404 and parameters: {'alpha': 0.5992040885488118, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,388] Trial 877 finished with value: 1.9078500049870983 and parameters: {'alpha': 0.6999486117414355, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,413] Trial 878 finished with value: 1.9069894107160887 and parameters: {'alpha': 0.44656095758164305, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,437] Trial 879 finished with value: 1.9087027166314636 and parameters: {'alpha': 0.36167843105176156, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,462] Trial 880 finished with value: 1.906821811805957 and parameters: {'alpha': 0.539243131512909, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,486] Trial 881 finished with value: 1.9068105729415985 and parameters: {'alpha': 0.4973563615975969, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,510] Trial 882 finished with value: 1.907304832028869 and parameters: {'alpha': 0.6389160470949872, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,535] Trial 883 finished with value: 1.9069060597284515 and parameters: {'alpha': 0.4630185184581716, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,561] Trial 884 finished with value: 1.9074276774959975 and parameters: {'alpha': 0.3952814527503627, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,589] Trial 885 finished with value: 2.4252590494838633 and parameters: {'alpha': 0.5705611779655715, 'kernel': 'rbf', 'gamma': 0.0008970580811622271}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,613] Trial 886 finished with value: 1.906799876924354 and parameters: {'alpha': 0.5184168569376641, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,637] Trial 887 finished with value: 1.926405864092533 and parameters: {'alpha': 0.16863078671268927, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,663] Trial 888 finished with value: 1.9071136976761867 and parameters: {'alpha': 0.4282167017034906, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,689] Trial 889 finished with value: 1.9068540742860618 and parameters: {'alpha': 0.47726599959955374, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,711] Trial 890 finished with value: 2.628715908893059 and parameters: {'alpha': 0.5915067190117023, 'kernel': 'linear'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,738] Trial 891 finished with value: 1.9068073646236596 and parameters: {'alpha': 0.5292081041855823, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,773] Trial 892 finished with value: 1.9074089835592696 and parameters: {'alpha': 0.6521440768920843, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,799] Trial 893 finished with value: 1.9068298501768381 and parameters: {'alpha': 0.486512462285842, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,822] Trial 894 finished with value: 1.9068641710917638 and parameters: {'alpha': 0.5569798050327761, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,847] Trial 895 finished with value: 1.9068000408551085 and parameters: {'alpha': 0.5189345857211846, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,871] Trial 896 finished with value: 1.9070542025967328 and parameters: {'alpha': 0.43637381434586303, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,895] Trial 897 finished with value: 1.9070232336833781 and parameters: {'alpha': 0.5952612901964311, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,919] Trial 898 finished with value: 1.9069284312576258 and parameters: {'alpha': 0.4580698095120126, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,943] Trial 899 finished with value: 1.9074360177462262 and parameters: {'alpha': 0.39455453750762476, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,968] Trial 900 finished with value: 1.9068505863222596 and parameters: {'alpha': 0.5521574818136847, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:29,992] Trial 901 finished with value: 1.906821347463542 and parameters: {'alpha': 0.490652120003264, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,016] Trial 902 finished with value: 1.9084757165984645 and parameters: {'alpha': 0.7556444929935102, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,042] Trial 903 finished with value: 1.9072358998230698 and parameters: {'alpha': 0.6295120227607387, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,065] Trial 904 finished with value: 1.906799942169834 and parameters: {'alpha': 0.5186305498968271, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,089] Trial 905 finished with value: 1.9071053453810975 and parameters: {'alpha': 0.42930686501480814, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,114] Trial 906 finished with value: 1.9068579823270733 and parameters: {'alpha': 0.47599419275801264, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,142] Trial 907 finished with value: 2.31868544302286 and parameters: {'alpha': 0.5692679875522751, 'kernel': 'rbf', 'gamma': 0.00011248913298790594}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,167] Trial 908 finished with value: 1.9068109214665867 and parameters: {'alpha': 0.532193296853727, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,190] Trial 909 finished with value: 1.9076563472314194 and parameters: {'alpha': 0.6802245433983991, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,224] Trial 910 finished with value: 1.9069471245900553 and parameters: {'alpha': 0.45427777389150237, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,260] Trial 911 finished with value: 1.9070995388121257 and parameters: {'alpha': 0.6087502911058877, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,286] Trial 912 finished with value: 2.6508003069964277 and parameters: {'alpha': 0.5024878071606608, 'kernel': 'linear'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,316] Trial 913 finished with value: 1.9073218143977104 and parameters: {'alpha': 0.4050381472473354, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,340] Trial 914 finished with value: 1.9429380786682497 and parameters: {'alpha': 0.07832089459316394, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,374] Trial 915 finished with value: 1.9068790878800235 and parameters: {'alpha': 0.5617501897130389, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,401] Trial 916 finished with value: 1.9068617615520482 and parameters: {'alpha': 0.47480589548858154, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,426] Trial 917 finished with value: 1.9067992956875734 and parameters: {'alpha': 0.5149556001761403, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,449] Trial 918 finished with value: 1.9070465354832833 and parameters: {'alpha': 0.43749873791159394, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,474] Trial 919 finished with value: 1.9068352463766731 and parameters: {'alpha': 0.5459019271989942, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,498] Trial 920 finished with value: 1.90836167873391 and parameters: {'alpha': 0.36976277776923083, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,523] Trial 921 finished with value: 1.9071517415189096 and parameters: {'alpha': 0.6171075328346494, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,548] Trial 922 finished with value: 1.9068052703454796 and parameters: {'alpha': 0.5019679306181333, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,573] Trial 923 finished with value: 1.9069349271459315 and parameters: {'alpha': 0.5766633150667833, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,599] Trial 924 finished with value: 1.906871365536727 and parameters: {'alpha': 0.47194631917709107, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,623] Trial 925 finished with value: 1.9067994968884132 and parameters: {'alpha': 0.5168181952721428, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,650] Trial 926 finished with value: 1.9068013202736893 and parameters: {'alpha': 0.5218235132723399, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,673] Trial 927 finished with value: 1.9072020092436341 and parameters: {'alpha': 0.4175618051640918, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,701] Trial 928 finished with value: 1.9080907626357282 and parameters: {'alpha': 0.7225698333007639, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,732] Trial 929 finished with value: 3.3862133133849786 and parameters: {'alpha': 0.5870884709540827, 'kernel': 'rbf', 'gamma': 0.08232212646998967}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,756] Trial 930 finished with value: 1.9068913989066638 and parameters: {'alpha': 0.4665689472576746, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,781] Trial 931 finished with value: 1.9073701358919875 and parameters: {'alpha': 0.6473311078475571, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,807] Trial 932 finished with value: 1.9067995229098393 and parameters: {'alpha': 0.5169601770106297, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,832] Trial 933 finished with value: 1.9068059006214075 and parameters: {'alpha': 0.5277927641110979, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,857] Trial 934 finished with value: 1.9070387358577758 and parameters: {'alpha': 0.43866296286511186, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,895] Trial 935 finished with value: 1.9068081296228676 and parameters: {'alpha': 0.49930098916041993, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,920] Trial 936 finished with value: 2.6372772618473768 and parameters: {'alpha': 0.5555624495292635, 'kernel': 'linear'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,944] Trial 937 finished with value: 1.9068536459455088 and parameters: {'alpha': 0.47740824797941545, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,969] Trial 938 finished with value: 1.906924212345211 and parameters: {'alpha': 0.5740642850426667, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:30,993] Trial 939 finished with value: 1.907342607360153 and parameters: {'alpha': 0.4030377703496147, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,016] Trial 940 finished with value: 1.9071744774605275 and parameters: {'alpha': 0.620575438383316, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,041] Trial 941 finished with value: 1.906800963471969 and parameters: {'alpha': 0.5078188897609841, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,066] Trial 942 finished with value: 1.906950746311787 and parameters: {'alpha': 0.45357351317644795, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,091] Trial 943 finished with value: 1.9068233638215168 and parameters: {'alpha': 0.5400962281511665, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,117] Trial 944 finished with value: 1.9075971856897318 and parameters: {'alpha': 0.6738501617237891, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,142] Trial 945 finished with value: 1.9068172498678133 and parameters: {'alpha': 0.49294455865271386, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,167] Trial 946 finished with value: 1.9070856078645144 and parameters: {'alpha': 0.4319505144543861, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,191] Trial 947 finished with value: 1.9070429378687255 and parameters: {'alpha': 0.5989198390396643, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,217] Trial 948 finished with value: 1.9068307639321231 and parameters: {'alpha': 0.5438399467575196, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,241] Trial 949 finished with value: 1.906894983936036 and parameters: {'alpha': 0.46567452429044104, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,266] Trial 950 finished with value: 1.9079602553500283 and parameters: {'alpha': 0.37963657758199515, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,291] Trial 951 finished with value: 1.9067993975189736 and parameters: {'alpha': 0.5127386203935139, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,338] Trial 952 finished with value: 3.392407328369922 and parameters: {'alpha': 0.5721269988026696, 'kernel': 'rbf', 'gamma': 0.19163356728445397}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,365] Trial 953 finished with value: 1.911835516297244 and parameters: {'alpha': 0.2979463611763537, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,398] Trial 954 finished with value: 1.9068354068013944 and parameters: {'alpha': 0.4841250517671564, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,430] Trial 955 finished with value: 1.9071944983482962 and parameters: {'alpha': 0.4184134575487662, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,454] Trial 956 finished with value: 1.9072553673910577 and parameters: {'alpha': 0.632228780685491, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,479] Trial 957 finished with value: 1.9068191117513384 and parameters: {'alpha': 0.5376879677281736, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,500] Trial 958 finished with value: 2.664927068749204 and parameters: {'alpha': 0.4514400719115519, 'kernel': 'linear'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,525] Trial 959 finished with value: 1.906812246165398 and parameters: {'alpha': 0.4961468988369256, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,551] Trial 960 finished with value: 1.9070356051603787 and parameters: {'alpha': 0.5975744919947285, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,575] Trial 961 finished with value: 1.9368613944770947 and parameters: {'alpha': 0.1088489509887471, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,600] Trial 962 finished with value: 1.906864227581346 and parameters: {'alpha': 0.5569987971922363, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,624] Trial 963 finished with value: 1.9454862672912645 and parameters: {'alpha': 0.0675342202054856, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,649] Trial 964 finished with value: 1.9068043978601974 and parameters: {'alpha': 0.5029071136386007, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,675] Trial 965 finished with value: 1.9100543116072735 and parameters: {'alpha': 0.33205007113849816, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,700] Trial 966 finished with value: 1.9069853504608183 and parameters: {'alpha': 0.44725859633874504, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,724] Trial 967 finished with value: 1.9068143285833274 and parameters: {'alpha': 0.5346545317944905, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,749] Trial 968 finished with value: 1.9078782498366291 and parameters: {'alpha': 0.70269992828632, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,775] Trial 969 finished with value: 1.9073185903761405 and parameters: {'alpha': 0.40535236844702116, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,800] Trial 970 finished with value: 1.9068595551451997 and parameters: {'alpha': 0.4754949141129302, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,824] Trial 971 finished with value: 1.9070083311748143 and parameters: {'alpha': 0.5923967542481341, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,856] Trial 972 finished with value: 1.90680232030317 and parameters: {'alpha': 0.5055429703691622, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,893] Trial 973 finished with value: 1.9073171234909874 and parameters: {'alpha': 0.6405337017035339, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,928] Trial 974 finished with value: 1.9069213228331958 and parameters: {'alpha': 0.4595886884011355, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,975] Trial 975 finished with value: 3.198402580584264 and parameters: {'alpha': 0.5616870196029248, 'kernel': 'rbf', 'gamma': 1.2933693068946463e-05}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:31,999] Trial 976 finished with value: 1.908160045606031 and parameters: {'alpha': 0.3746721869513421, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,023] Trial 977 finished with value: 1.919618139593121 and parameters: {'alpha': 0.21497407265773194, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,049] Trial 978 finished with value: 1.9070945279237859 and parameters: {'alpha': 0.4307436390049899, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,073] Trial 979 finished with value: 1.9068005203989846 and parameters: {'alpha': 0.5201885605731641, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,100] Trial 980 finished with value: 1.9068947968890924 and parameters: {'alpha': 0.5663379388387345, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,123] Trial 981 finished with value: 2.6565090674558482 and parameters: {'alpha': 0.48134515606195216, 'kernel': 'linear'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,148] Trial 982 finished with value: 1.9072913954701638 and parameters: {'alpha': 0.6371283624088917, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,173] Trial 983 finished with value: 1.906800043258578 and parameters: {'alpha': 0.5189417356196425, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,198] Trial 984 finished with value: 1.9070425496741654 and parameters: {'alpha': 0.4380911245612024, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,222] Trial 985 finished with value: 1.9069089613916188 and parameters: {'alpha': 0.5701791210095527, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,247] Trial 986 finished with value: 1.90683170472961 and parameters: {'alpha': 0.4856923644158348, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,273] Trial 987 finished with value: 1.906802810973514 and parameters: {'alpha': 0.524169347672497, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,298] Trial 988 finished with value: 1.9073324366104605 and parameters: {'alpha': 0.40401065785103274, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,323] Trial 989 finished with value: 1.9070406857393676 and parameters: {'alpha': 0.5985085977893002, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,348] Trial 990 finished with value: 1.9068785192234479 and parameters: {'alpha': 0.4699448163173242, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,374] Trial 991 finished with value: 1.9076214461997214 and parameters: {'alpha': 0.6764865673740035, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,399] Trial 992 finished with value: 1.9068243744807394 and parameters: {'alpha': 0.54063749655626, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,424] Trial 993 finished with value: 1.9577899388356135 and parameters: {'alpha': 0.03161413288090932, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,453] Trial 994 finished with value: 1.9068134018183134 and parameters: {'alpha': 0.4953576912608796, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,485] Trial 995 finished with value: 1.9070861232782417 and parameters: {'alpha': 0.4318802172800349, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,519] Trial 996 finished with value: 3.4003788965921498 and parameters: {'alpha': 0.5869115461469078, 'kernel': 'rbf', 'gamma': 0.9641204341415052}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,543] Trial 997 finished with value: 1.9086361487834844 and parameters: {'alpha': 0.7685823488852247, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,568] Trial 998 finished with value: 1.906819249193236 and parameters: {'alpha': 0.5377695258843073, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n",
      "[I 2025-09-04 21:02:32,592] Trial 999 finished with value: 1.9068628252061954 and parameters: {'alpha': 0.47447830223351706, 'kernel': 'poly'}. Best is trial 768 with value: 1.906799286179563.\n"
     ]
    }
   ],
   "source": [
    "def objective_krr(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 1.0, log=True)\n",
    "    kernel = trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly'])\n",
    "    gamma = trial.suggest_float('gamma', 1e-5, 1.0, log=True) if kernel == 'rbf' else None\n",
    "    model = KernelRidge(alpha=alpha, kernel=kernel, gamma=gamma) if gamma else KernelRidge(alpha=alpha, kernel=kernel)\n",
    "    model.fit(X_train_fp_scaled, y_train_scaled)\n",
    "    preds_scaled = model.predict(X_test_fp_scaled).reshape(-1, 1)\n",
    "    preds = yscaler.inverse_transform(preds_scaled)\n",
    "    y_test_inv = yscaler.inverse_transform(y_test_scaled)\n",
    "    metrics = regression_metrics(y_test_inv, preds)\n",
    "    return metrics['MAE'][0]\n",
    "\n",
    "study_krr = optuna.create_study(direction='minimize')\n",
    "study_krr.optimize(objective_krr, n_trials=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "83942b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:02:34,119] A new study created in memory with name: no-name-55547504-e701-445c-911b-e99188802bbb\n",
      "[I 2025-09-04 21:02:34,414] Trial 0 finished with value: 1.796754007845293 and parameters: {'n_estimators': 100, 'max_depth': 90}. Best is trial 0 with value: 1.796754007845293.\n",
      "[I 2025-09-04 21:02:34,615] Trial 1 finished with value: 1.792214427147921 and parameters: {'n_estimators': 50, 'max_depth': 70}. Best is trial 1 with value: 1.792214427147921.\n",
      "[I 2025-09-04 21:02:35,253] Trial 2 finished with value: 1.746548499580389 and parameters: {'n_estimators': 250, 'max_depth': 100}. Best is trial 2 with value: 1.746548499580389.\n",
      "[I 2025-09-04 21:02:35,933] Trial 3 finished with value: 1.7438558801344457 and parameters: {'n_estimators': 300, 'max_depth': 20}. Best is trial 3 with value: 1.7438558801344457.\n",
      "[I 2025-09-04 21:02:36,116] Trial 4 finished with value: 1.7889717633362614 and parameters: {'n_estimators': 50, 'max_depth': 30}. Best is trial 3 with value: 1.7438558801344457.\n",
      "[I 2025-09-04 21:02:36,397] Trial 5 finished with value: 1.7972490741402796 and parameters: {'n_estimators': 100, 'max_depth': 40}. Best is trial 3 with value: 1.7438558801344457.\n",
      "[I 2025-09-04 21:02:37,111] Trial 6 finished with value: 1.746117275711783 and parameters: {'n_estimators': 300, 'max_depth': 90}. Best is trial 3 with value: 1.7438558801344457.\n",
      "[I 2025-09-04 21:02:37,506] Trial 7 finished with value: 1.7520013078742553 and parameters: {'n_estimators': 150, 'max_depth': 60}. Best is trial 3 with value: 1.7438558801344457.\n",
      "[I 2025-09-04 21:02:37,893] Trial 8 finished with value: 1.7520013078742556 and parameters: {'n_estimators': 150, 'max_depth': 100}. Best is trial 3 with value: 1.7438558801344457.\n",
      "[I 2025-09-04 21:02:38,077] Trial 9 finished with value: 1.7922144271479206 and parameters: {'n_estimators': 50, 'max_depth': 70}. Best is trial 3 with value: 1.7438558801344457.\n",
      "[I 2025-09-04 21:02:38,509] Trial 10 finished with value: 1.7870630978126378 and parameters: {'n_estimators': 250, 'max_depth': 10}. Best is trial 3 with value: 1.7438558801344457.\n",
      "[I 2025-09-04 21:02:39,019] Trial 11 finished with value: 1.7841560708123292 and parameters: {'n_estimators': 300, 'max_depth': 10}. Best is trial 3 with value: 1.7438558801344457.\n",
      "[I 2025-09-04 21:02:39,733] Trial 12 finished with value: 1.7454093937655288 and parameters: {'n_estimators': 300, 'max_depth': 40}. Best is trial 3 with value: 1.7438558801344457.\n",
      "[I 2025-09-04 21:02:40,332] Trial 13 finished with value: 1.7390763542753838 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 13 with value: 1.7390763542753838.\n",
      "[I 2025-09-04 21:02:40,931] Trial 14 finished with value: 1.7390763542753835 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 14 with value: 1.7390763542753835.\n",
      "[I 2025-09-04 21:02:41,415] Trial 15 finished with value: 1.7512233743404875 and parameters: {'n_estimators': 200, 'max_depth': 40}. Best is trial 14 with value: 1.7390763542753835.\n",
      "[I 2025-09-04 21:02:42,015] Trial 16 finished with value: 1.7390763542753833 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:42,505] Trial 17 finished with value: 1.7523974979719137 and parameters: {'n_estimators': 200, 'max_depth': 50}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:42,966] Trial 18 finished with value: 1.7505153322948404 and parameters: {'n_estimators': 200, 'max_depth': 20}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:43,591] Trial 19 finished with value: 1.7468271159843076 and parameters: {'n_estimators': 250, 'max_depth': 50}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:44,191] Trial 20 finished with value: 1.7390763542753833 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:44,797] Trial 21 finished with value: 1.7390763542753838 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:45,263] Trial 22 finished with value: 1.750515332294841 and parameters: {'n_estimators': 200, 'max_depth': 20}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:45,881] Trial 23 finished with value: 1.7390763542753835 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:46,378] Trial 24 finished with value: 1.7523974979719141 and parameters: {'n_estimators': 200, 'max_depth': 60}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:47,092] Trial 25 finished with value: 1.7454093937655286 and parameters: {'n_estimators': 300, 'max_depth': 40}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:47,676] Trial 26 finished with value: 1.7428239272260897 and parameters: {'n_estimators': 250, 'max_depth': 20}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:47,969] Trial 27 finished with value: 1.7902594705201138 and parameters: {'n_estimators': 150, 'max_depth': 10}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:48,590] Trial 28 finished with value: 1.746827115984308 and parameters: {'n_estimators': 250, 'max_depth': 50}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:49,317] Trial 29 finished with value: 1.7399882352436553 and parameters: {'n_estimators': 300, 'max_depth': 30}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:49,794] Trial 30 finished with value: 1.7505153322948408 and parameters: {'n_estimators': 200, 'max_depth': 20}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:50,412] Trial 31 finished with value: 1.7390763542753838 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:51,027] Trial 32 finished with value: 1.7459589638889224 and parameters: {'n_estimators': 250, 'max_depth': 40}. Best is trial 16 with value: 1.7390763542753833.\n",
      "[I 2025-09-04 21:02:51,633] Trial 33 finished with value: 1.7390763542753827 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:02:52,311] Trial 34 finished with value: 1.743855880134446 and parameters: {'n_estimators': 300, 'max_depth': 20}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:02:52,821] Trial 35 finished with value: 1.751223374340488 and parameters: {'n_estimators': 200, 'max_depth': 40}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:02:53,031] Trial 36 finished with value: 1.8200882582655293 and parameters: {'n_estimators': 100, 'max_depth': 10}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:02:53,758] Trial 37 finished with value: 1.7399882352436558 and parameters: {'n_estimators': 300, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:02:54,392] Trial 38 finished with value: 1.7468271159843074 and parameters: {'n_estimators': 250, 'max_depth': 50}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:02:54,777] Trial 39 finished with value: 1.7520013078742553 and parameters: {'n_estimators': 150, 'max_depth': 80}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:02:55,392] Trial 40 finished with value: 1.746548499580389 and parameters: {'n_estimators': 250, 'max_depth': 60}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:02:56,008] Trial 41 finished with value: 1.739076354275384 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:02:56,942] Trial 42 finished with value: 1.7428239272260901 and parameters: {'n_estimators': 250, 'max_depth': 20}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:02:57,680] Trial 43 finished with value: 1.7399882352436558 and parameters: {'n_estimators': 300, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:02:58,200] Trial 44 finished with value: 1.751223374340488 and parameters: {'n_estimators': 200, 'max_depth': 40}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:02:58,813] Trial 45 finished with value: 1.7390763542753838 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:02:59,492] Trial 46 finished with value: 1.7438558801344457 and parameters: {'n_estimators': 300, 'max_depth': 20}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:00,123] Trial 47 finished with value: 1.745958963888923 and parameters: {'n_estimators': 250, 'max_depth': 40}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:00,504] Trial 48 finished with value: 1.791639332482224 and parameters: {'n_estimators': 200, 'max_depth': 10}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:00,692] Trial 49 finished with value: 1.7889717633362616 and parameters: {'n_estimators': 50, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:01,330] Trial 50 finished with value: 1.7459589638889228 and parameters: {'n_estimators': 250, 'max_depth': 40}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:01,923] Trial 51 finished with value: 1.7390763542753838 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:02,696] Trial 52 finished with value: 1.7428239272260904 and parameters: {'n_estimators': 250, 'max_depth': 20}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:04,683] Trial 53 finished with value: 1.7399882352436555 and parameters: {'n_estimators': 300, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:06,324] Trial 54 finished with value: 1.7468271159843076 and parameters: {'n_estimators': 250, 'max_depth': 50}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:07,636] Trial 55 finished with value: 1.751223374340487 and parameters: {'n_estimators': 200, 'max_depth': 40}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:09,118] Trial 56 finished with value: 1.74282392722609 and parameters: {'n_estimators': 250, 'max_depth': 20}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:11,001] Trial 57 finished with value: 1.7399882352436558 and parameters: {'n_estimators': 300, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:12,316] Trial 58 finished with value: 1.751223374340488 and parameters: {'n_estimators': 200, 'max_depth': 40}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:13,882] Trial 59 finished with value: 1.7428239272260908 and parameters: {'n_estimators': 250, 'max_depth': 20}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:15,550] Trial 60 finished with value: 1.7465484995803895 and parameters: {'n_estimators': 250, 'max_depth': 70}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:17,162] Trial 61 finished with value: 1.7390763542753838 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:18,790] Trial 62 finished with value: 1.7390763542753835 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:20,423] Trial 63 finished with value: 1.7465484995803886 and parameters: {'n_estimators': 250, 'max_depth': 100}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:22,342] Trial 64 finished with value: 1.7399882352436555 and parameters: {'n_estimators': 300, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:23,593] Trial 65 finished with value: 1.7505153322948408 and parameters: {'n_estimators': 200, 'max_depth': 20}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:25,212] Trial 66 finished with value: 1.7390763542753838 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:26,859] Trial 67 finished with value: 1.746827115984308 and parameters: {'n_estimators': 250, 'max_depth': 50}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:27,883] Trial 68 finished with value: 1.7516183777575165 and parameters: {'n_estimators': 150, 'max_depth': 40}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:28,696] Trial 69 finished with value: 1.791639332482225 and parameters: {'n_estimators': 200, 'max_depth': 10}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:30,473] Trial 70 finished with value: 1.7438558801344453 and parameters: {'n_estimators': 300, 'max_depth': 20}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:32,187] Trial 71 finished with value: 1.739076354275384 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:33,821] Trial 72 finished with value: 1.7459589638889226 and parameters: {'n_estimators': 250, 'max_depth': 40}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:35,435] Trial 73 finished with value: 1.7390763542753838 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:37,040] Trial 74 finished with value: 1.739076354275384 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:38,671] Trial 75 finished with value: 1.7459589638889221 and parameters: {'n_estimators': 250, 'max_depth': 40}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:39,337] Trial 76 finished with value: 1.7839245321947972 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:41,052] Trial 77 finished with value: 1.7465484995803893 and parameters: {'n_estimators': 250, 'max_depth': 90}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:42,961] Trial 78 finished with value: 1.7399882352436558 and parameters: {'n_estimators': 300, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:44,607] Trial 79 finished with value: 1.7459589638889228 and parameters: {'n_estimators': 250, 'max_depth': 40}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:45,933] Trial 80 finished with value: 1.743702699059841 and parameters: {'n_estimators': 200, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:47,562] Trial 81 finished with value: 1.7390763542753838 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:49,178] Trial 82 finished with value: 1.7390763542753838 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:50,801] Trial 83 finished with value: 1.7428239272260906 and parameters: {'n_estimators': 250, 'max_depth': 20}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:52,427] Trial 84 finished with value: 1.7390763542753838 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:53,908] Trial 85 finished with value: 1.7428239272260904 and parameters: {'n_estimators': 250, 'max_depth': 20}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:55,545] Trial 86 finished with value: 1.7459589638889228 and parameters: {'n_estimators': 250, 'max_depth': 40}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:56,870] Trial 87 finished with value: 1.7512233743404875 and parameters: {'n_estimators': 200, 'max_depth': 40}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:03:58,856] Trial 88 finished with value: 1.7399882352436562 and parameters: {'n_estimators': 300, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:04:00,540] Trial 89 finished with value: 1.7390763542753829 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:04:02,093] Trial 90 finished with value: 1.7428239272260901 and parameters: {'n_estimators': 250, 'max_depth': 20}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:04:03,775] Trial 91 finished with value: 1.7390763542753842 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:04:05,458] Trial 92 finished with value: 1.7390763542753833 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:04:07,146] Trial 93 finished with value: 1.7390763542753833 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:04:08,792] Trial 94 finished with value: 1.7459589638889221 and parameters: {'n_estimators': 250, 'max_depth': 40}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:04:10,408] Trial 95 finished with value: 1.7390763542753842 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:04:12,025] Trial 96 finished with value: 1.7390763542753833 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:04:13,642] Trial 97 finished with value: 1.739076354275384 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:04:15,206] Trial 98 finished with value: 1.7428239272260901 and parameters: {'n_estimators': 250, 'max_depth': 20}. Best is trial 33 with value: 1.7390763542753827.\n",
      "[I 2025-09-04 21:04:16,825] Trial 99 finished with value: 1.7390763542753838 and parameters: {'n_estimators': 250, 'max_depth': 30}. Best is trial 33 with value: 1.7390763542753827.\n"
     ]
    }
   ],
   "source": [
    "def objective_rfr(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 100, step=10)\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train_fp_unscaled, y_train_unscaled)\n",
    "    preds = model.predict(X_test_fp_unscaled)\n",
    "    metrics = regression_metrics(y_test_unscaled, preds)\n",
    "    return metrics['MAE'][0]\n",
    "\n",
    "study_rfr = optuna.create_study(direction='minimize')\n",
    "study_rfr.optimize(objective_rfr, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0b6bd272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:04:29,626] A new study created in memory with name: no-name-cd6b28a5-d1de-4ee0-aa4f-f53d3170a226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 22.5776\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20.8674\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.8113\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 16.7403\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.8612\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.1484\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.6355\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.2910\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.1004\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.0492\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.1188\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.2976\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5726\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.9317\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3651\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.8649\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4213\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0301\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.6858\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.3817\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1136\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8750\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6622\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4796\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3174\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1676\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0394\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.9302\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8285\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.7401\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6654\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5969\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5385\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4816\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4315\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3890\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3507\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3225\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2912\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2665\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2425\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2271\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2110\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1920\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1731\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1565\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1451\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1346\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1245\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1177\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1116\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1075\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1095\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1089\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1175\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1211\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1298\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1002\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0923\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0872\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0816\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0821\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0753\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0748\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0704\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0715\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0668\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0676\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0639\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0620\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0621\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0636\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0617\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0603\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0606\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0612\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0615\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0636\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0662\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0640\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0614\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0654\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0631\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0599\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0625\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0663\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0598\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0596\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0690\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0714\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0644\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0603\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0609\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0577\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0633\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0653\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0647\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0646\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0650\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0630\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:04:32,585] Trial 0 finished with value: 1.9455822398152114 and parameters: {'lr': 0.012506948929669975, 'alpha': 0.02867458166589874, 'activation': 'relu', 'n1': 384, 'n2': 128}. Best is trial 0 with value: 1.9455822398152114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.4041\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.5621\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.0882\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.8265\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.5905\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.3807\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.1880\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.9998\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.8203\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.6429\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.4699\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.2991\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.1317\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.9663\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.8019\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.6413\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.4810\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.3228\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 15.1672\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 15.0132\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.8601\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.7087\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.5592\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.4112\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.2653\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.1200\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.9769\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 13.8354\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 13.6948\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 13.5559\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.4184\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.2822\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.1475\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.0145\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.8818\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.7516\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.6222\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 12.4944\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 12.3675\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.2422\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.1178\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.9951\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.8737\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.7534\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6346\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.5164\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.3999\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.2839\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.1697\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.0565\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 10.9443\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8335\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.7241\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.6150\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.5085\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.4018\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2958\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.1918\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 10.0894\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 9.9867\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 9.8852\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.7849\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.6864\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5885\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.4908\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.3951\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.3000\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 9.2054\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1124\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0202\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 8.9289\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 8.8389\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7488\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6605\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5733\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4862\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4006\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 8.3158\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2308\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1483\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0653\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9840\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9034\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 7.8231\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7447\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6662\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5882\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 7.5117\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4355\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3604\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2858\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2123\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 7.1392\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 7.0676\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 6.9964\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.9254\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8551\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7865\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7170\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 6.6495\n",
      "4/4 [==============================] - 0s 1000us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:04:34,568] Trial 1 finished with value: 2.198563752587031 and parameters: {'lr': 0.0013221575642792637, 'alpha': 0.024001613905772243, 'activation': 'tanh', 'n1': 384, 'n2': 128}. Best is trial 0 with value: 1.9455822398152114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5536\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1015\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6295\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4481\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.4168\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.3555\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2739\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2333\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1944\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1457\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1318\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1347\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1386\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1079\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0828\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0707\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0620\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0583\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0575\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0584\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0769\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0764\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0027\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9877\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9838\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9737\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9703\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9530\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9562\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9453\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9440\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9291\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9199\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9124\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8971\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8917\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8790\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8728\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8695\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8638\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8552\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8552\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8475\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8299\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8253\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8200\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8199\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8108\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8009\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8164\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8017\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7800\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7813\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7686\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7606\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.7548\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7448\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7395\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.7364\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7428\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.7257\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7255\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7138\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7072\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7015\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6995\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6927\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6863\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6806\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6718\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6765\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6662\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6635\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6684\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6525\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6444\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6406\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6317\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6293\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6448\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6198\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6294\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6284\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6301\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6379\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6135\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6152\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5925\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5891\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5774\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5676\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5695\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5656\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5719\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5553\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5604\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5515\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5415\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5346\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5366\n",
      "4/4 [==============================] - 0s 667us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:04:36,575] Trial 2 finished with value: 2.2463471004773186 and parameters: {'lr': 0.017974971095635398, 'alpha': 0.0014731462697211782, 'activation': 'sigmoid', 'n1': 320, 'n2': 256}. Best is trial 0 with value: 1.9455822398152114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2236\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0330\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8468\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5677\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6216\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4674\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3784\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3314\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3452\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3421\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3043\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2936\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2860\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2552\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2546\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2635\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2430\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2538\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2306\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2326\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2430\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2474\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2378\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2477\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2305\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2227\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2187\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2368\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2327\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2297\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2159\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2164\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2145\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2142\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2182\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2179\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2182\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2175\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2189\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2097\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2093\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2090\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2094\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2088\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2108\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2096\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2061\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2071\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2184\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2084\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2070\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2086\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2052\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2045\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2120\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2094\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2128\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2286\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2206\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2121\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2088\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2051\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2023\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2011\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2013\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2075\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2060\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2075\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2035\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2026\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2018\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2101\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2112\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2077\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2083\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2072\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2070\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2063\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2080\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2209\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2119\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2119\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2260\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2234\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2178\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2049\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2011\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2009\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1971\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1967\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1998\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1989\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1990\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2013\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1952\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1959\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1961\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1950\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1997\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:04:38,605] Trial 3 finished with value: 2.1951429042858828 and parameters: {'lr': 0.013536128399178482, 'alpha': 0.00021892196515532164, 'activation': 'sigmoid', 'n1': 384, 'n2': 256}. Best is trial 0 with value: 1.9455822398152114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5388\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9978\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6560\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5344\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4626\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4113\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3773\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3578\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3411\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3309\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3218\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3142\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3086\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3030\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2986\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2945\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2915\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2877\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2851\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2830\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2810\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2792\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2773\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2760\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2748\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2734\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2725\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2718\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2708\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2702\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2693\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2688\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2681\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2677\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2671\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2667\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2664\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2662\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2657\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2654\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2650\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2649\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2645\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2643\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2642\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2641\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2638\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2635\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2634\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2635\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2632\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2631\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2630\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2628\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2629\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2626\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2626\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2624\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2626\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2627\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2623\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2621\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2620\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2621\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2618\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2618\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2618\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2616\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2616\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2615\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2615\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2614\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2612\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2611\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2615\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2611\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2613\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2609\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2611\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2611\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2608\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2608\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2609\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2606\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2607\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2607\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2604\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2606\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2605\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2604\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2602\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2603\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2602\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2602\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2605\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2602\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2601\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2602\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2599\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2600\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:04:40,835] Trial 4 finished with value: 2.7010818153872056 and parameters: {'lr': 0.0010552589581914332, 'alpha': 0.0003529229942610836, 'activation': 'gelu', 'n1': 256, 'n2': 320}. Best is trial 0 with value: 1.9455822398152114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4418\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.2581\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.2261\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2278\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2123\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2050\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2031\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1963\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1923\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1887\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1848\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.1803\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1759\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.1737\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1674\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1650\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1625\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.1559\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.1522\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1480\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1443\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1403\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1359\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1318\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1282\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1240\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.1201\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.1168\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.1120\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1083\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1040\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1000\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0962\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.0916\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.0875\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0841\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.0797\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.0753\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0712\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0669\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0632\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0589\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0544\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.0501\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.0466\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.0422\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.0383\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0331\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0291\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0242\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.0198\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.0157\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0112\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0073\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0024\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9980\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9933\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9910\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9839\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9798\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.9754\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9707\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9660\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9615\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9570\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9525\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9477\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9428\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9385\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9341\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9286\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.9239\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.9191\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9143\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9097\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9051\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9000\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8953\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8903\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8855\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8815\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8765\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8712\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8673\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8608\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8564\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8518\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8464\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8415\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8376\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8321\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8278\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8223\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8170\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8128\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8079\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8034\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7978\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7933\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7885\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:04:42,771] Trial 5 finished with value: 2.9604539094678772 and parameters: {'lr': 0.0002293737813871796, 'alpha': 0.0034657558847675223, 'activation': 'sigmoid', 'n1': 128, 'n2': 128}. Best is trial 0 with value: 1.9455822398152114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1009\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6750\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4697\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3379\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2698\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2289\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2025\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1827\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1556\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1470\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1393\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1342\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1297\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1251\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1227\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1196\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1172\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1155\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1144\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1127\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1112\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1100\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1090\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1085\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1074\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1072\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1067\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1059\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1056\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1049\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1044\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1043\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1033\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1035\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1031\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1030\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1026\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1025\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1020\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1020\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1019\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1018\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1019\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1017\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1015\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1011\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1010\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1011\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1006\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1008\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1009\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1004\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1013\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1008\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1004\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1003\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1010\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1005\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1003\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1000\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1003\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1005\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0999\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1002\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1000\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0997\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0998\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0998\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0997\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1001\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0994\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0995\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0999\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0996\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1000\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0998\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0992\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0997\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0993\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0994\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0995\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0993\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0998\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0996\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0992\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0993\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0991\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0991\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0990\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0992\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0989\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0993\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0995\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0992\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0990\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0994\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0987\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0990\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:04:44,931] Trial 6 finished with value: 2.2940382800404358 and parameters: {'lr': 0.0011918327556436098, 'alpha': 0.00014123006160084872, 'activation': 'tanh', 'n1': 192, 'n2': 384}. Best is trial 0 with value: 1.9455822398152114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9196\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7210\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5608\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4029\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2679\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1704\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0897\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0259\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9715\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9272\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8893\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8570\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8294\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.8047\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.7832\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7637\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.7460\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7296\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7155\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7024\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6909\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6796\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6691\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6602\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6515\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6437\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6364\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6295\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6228\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6171\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6115\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6059\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6010\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5961\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5919\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5876\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5836\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5798\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5763\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5731\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5700\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5671\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5641\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5615\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5589\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5566\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5543\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5520\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5500\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5480\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5460\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5441\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5423\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5405\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5389\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5373\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5358\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5344\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5328\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5316\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5303\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5289\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5277\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5265\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5254\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5242\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5232\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5222\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5212\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5204\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5193\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5185\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5175\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5167\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5158\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5150\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5142\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5135\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5127\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5120\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5113\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5105\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5099\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5092\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5086\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5080\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5074\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5068\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5062\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5057\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5051\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5046\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5040\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5035\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5030\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5025\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5020\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5016\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5011\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5006\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:04:46,907] Trial 7 finished with value: 2.8339399743944855 and parameters: {'lr': 0.00016468246480052408, 'alpha': 0.0013031901456353012, 'activation': 'relu', 'n1': 128, 'n2': 128}. Best is trial 0 with value: 1.9455822398152114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8415\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2076\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9754\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8579\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7846\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7405\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7144\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6936\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6765\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6658\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6567\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6485\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6429\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6382\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6338\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6308\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6277\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6249\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6230\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6213\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6196\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6181\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6168\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6156\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6148\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6137\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6132\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6124\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6118\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6111\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6104\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6099\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6095\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6091\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6082\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6081\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6076\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6073\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6068\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6064\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6059\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6058\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6056\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6053\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6052\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6048\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6044\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6040\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6037\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6037\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6032\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6031\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6030\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6025\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6030\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6024\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6021\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6017\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6021\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6015\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6012\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6008\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6010\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6009\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6003\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6003\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6000\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5996\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5996\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5994\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5993\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5994\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5987\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5985\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5989\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5983\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5987\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5980\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5977\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5976\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5974\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5973\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5972\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5968\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5971\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5967\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5962\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5962\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5959\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5958\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5955\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5955\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5952\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5954\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5954\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5949\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5947\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5948\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5941\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5942\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:04:49,084] Trial 8 finished with value: 2.5802767851253594 and parameters: {'lr': 0.0009249797276505639, 'alpha': 0.0008902416378976463, 'activation': 'tanh', 'n1': 256, 'n2': 256}. Best is trial 0 with value: 1.9455822398152114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.6501\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.5099\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.3401\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.1717\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.0230\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.8973\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.7854\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.6901\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.6032\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.5254\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 15.4518\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.3868\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 15.3247\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15.2656\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 15.2105\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.1571\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.1064\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.0571\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.0099\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.9641\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.9189\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.8759\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14.8328\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14.7912\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.7501\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.7098\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.6701\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.6309\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.5921\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.5543\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.5167\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.4791\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.4422\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 14.4057\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14.3696\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.3338\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.2983\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.2630\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.2280\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.1934\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.1590\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.1249\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.0906\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14.0571\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.0236\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.9902\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.9572\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.9241\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.8916\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.8590\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.8266\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.7945\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.7623\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.7306\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.6988\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 13.6672\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.6358\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.6044\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.5733\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.5422\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.5114\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.4805\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.4499\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.4192\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.3889\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 13.3585\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 13.3284\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.2983\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.2683\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.2385\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.2087\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.1791\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.1494\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.1200\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.0905\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 13.0613\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.0321\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 13.0030\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.9739\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.9450\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.9163\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.8873\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.8587\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.8301\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.8017\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.7733\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 12.7450\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.7168\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.6885\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.6606\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.6324\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.6045\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.5767\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.5489\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.5212\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.4936\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.4660\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 12.4386\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.4112\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.3838\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:04:52,977] Trial 9 finished with value: 2.339623548854667 and parameters: {'lr': 0.0001830846545746065, 'alpha': 0.037185633721884666, 'activation': 'gelu', 'n1': 128, 'n2': 320}. Best is trial 0 with value: 1.9455822398152114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.3672\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.6576\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.3803\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.2473\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.0640\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.9418\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 5.8473\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.7605\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.6772\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.5981\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.5216\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.4471\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.3745\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.3035\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2323\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.1638\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.0945\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 5.0270\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.9608\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.8957\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.8307\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7667\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7038\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.6416\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.5808\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.5200\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4606\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.4022\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.3440\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.2867\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2300\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1743\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.1192\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0653\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0112\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9587\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9062\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.8560\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.8047\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.7543\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.7048\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6560\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6079\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5603\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5136\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4677\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4217\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3766\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3320\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.2883\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.2454\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2026\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1608\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1190\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.0788\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.0378\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.9981\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.9580\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.9200\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8817\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.8433\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.8065\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7695\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.7334\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6968\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6618\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6267\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5921\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5585\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.5247\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2.4921\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.4593\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4274\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3952\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3645\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3327\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3026\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.2723\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2423\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2132\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2.1840\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.1555\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1277\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0993\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0727\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0454\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0183\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9921\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9659\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9400\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.9146\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.8898\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.8649\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8409\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8172\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7934\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7697\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7470\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7236\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7014\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:04:57,115] Trial 10 finished with value: 2.0526118415424173 and parameters: {'lr': 0.004841760492185288, 'alpha': 0.00855843041528723, 'activation': 'relu', 'n1': 320, 'n2': 192}. Best is trial 0 with value: 1.9455822398152114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.5857\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.8154\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.5289\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.3161\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.1204\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.9073\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7356\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.5654\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.4046\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.2548\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 7.1094\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.9643\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.8253\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.6893\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.5554\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.4261\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.2973\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1718\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.0495\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 5.9299\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.8117\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.6960\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.5829\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.4720\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.3642\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2573\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.1531\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.0516\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.9512\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.8530\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.7568\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.6625\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.5702\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4801\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3907\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3040\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2186\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1362\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0538\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.9735\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.8946\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.8179\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.7421\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6682\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5959\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5251\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4551\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3867\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3197\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2542\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1902\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.1271\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0659\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0051\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9468\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8882\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8313\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7749\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7212\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6676\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.6144\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.5635\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.5131\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.4639\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4145\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3675\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3209\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2752\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2308\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.1868\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.1441\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1021\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.0607\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.0203\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.9811\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9418\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9036\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8672\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8296\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7946\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7590\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7246\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6913\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.6578\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.6264\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5946\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5630\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5328\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5023\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4734\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.4444\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4164\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3885\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.3619\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.3358\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3098\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2840\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2596\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2341\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2108\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:01,068] Trial 11 finished with value: 1.918677925474689 and parameters: {'lr': 0.005299012937491008, 'alpha': 0.011755305480802326, 'activation': 'relu', 'n1': 320, 'n2': 192}. Best is trial 11 with value: 1.918677925474689.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.8797\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.1112\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 6.7215\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.4895\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.3280\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1925\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.0781\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.9753\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.8669\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.7664\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.6690\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.5729\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 5.4806\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.3896\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2992\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2121\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.1249\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.0400\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.9567\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.8756\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.7944\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7148\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 4.6368\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.5598\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4854\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4104\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3379\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2669\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1960\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1269\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0587\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9917\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9254\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.8613\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.7968\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.7347\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6728\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6134\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5534\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4945\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4365\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3803\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3244\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2696\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2159\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1632\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1110\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0594\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0089\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9594\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9111\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8628\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8164\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7693\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7248\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.6797\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.6351\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5920\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5507\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5086\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4663\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4268\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3866\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3480\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3085\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2709\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2.2336\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.1968\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1611\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1256\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0915\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0571\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.0237\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9901\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.9583\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9253\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.8937\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.8638\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.8322\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8028\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7729\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7440\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7159\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6874\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6608\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6337\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6065\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.5807\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.5544\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.5293\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5040\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4798\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4554\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4322\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4093\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3865\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3635\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3421\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.3190\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.2982\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:05,048] Trial 12 finished with value: 2.1352081259572335 and parameters: {'lr': 0.005748525141893343, 'alpha': 0.009033914667184286, 'activation': 'relu', 'n1': 320, 'n2': 192}. Best is trial 11 with value: 1.918677925474689.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.5804\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.7190\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.3311\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.0550\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.7742\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.5182\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.2967\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10.0794\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.8631\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.6564\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.4552\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.2579\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.0662\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.8785\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.6943\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.5156\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.3386\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.1663\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 7.9982\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.8335\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.6716\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5130\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.3580\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2061\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.0582\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.9120\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.7697\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.6309\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.4939\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 6.3602\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.2292\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1008\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.9750\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.8524\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.7313\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.6138\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.4979\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.3861\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2748\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 5.1659\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.0596\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.9556\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.8537\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7539\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.6564\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.5609\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4670\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3749\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2850\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1970\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 4.1113\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.0267\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9442\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.8630\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.7846\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.7065\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6303\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5557\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4838\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4126\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.3419\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.2740\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2068\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1415\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0763\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0138\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9520\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8915\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8328\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.7747\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2.7184\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.6630\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6085\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5550\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5035\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4517\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.4018\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3534\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3044\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2583\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2.2118\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.1668\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1231\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 2.0794\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2.0382\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.9966\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.9556\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9159\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8764\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8386\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.8009\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.7646\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7285\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6938\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6600\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6264\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5929\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5612\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5286\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.4982\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:09,281] Trial 13 finished with value: 1.989560627521282 and parameters: {'lr': 0.004597473076722533, 'alpha': 0.014064972226338323, 'activation': 'relu', 'n1': 384, 'n2': 192}. Best is trial 11 with value: 1.918677925474689.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1509\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6141\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4154\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0699\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9219\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8560\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8032\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2.7665\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.7147\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6763\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6423\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6071\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5766\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5459\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.5147\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4854\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.4555\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.4268\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.3986\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.3716\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3438\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3161\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2889\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2617\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.2361\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2094\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.1838\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1597\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.1334\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.1094\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.0850\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0602\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0359\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0127\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.9885\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9658\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9428\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9211\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8985\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8760\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.8543\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.8328\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8116\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7905\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7697\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7494\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7291\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7091\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6889\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6693\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6503\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.6306\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.6122\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.5929\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5752\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5573\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5386\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5204\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.5039\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4865\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4678\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4523\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.4346\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.4190\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.4015\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3858\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3696\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3536\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3382\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3224\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3073\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2926\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.2770\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.2624\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.2484\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2334\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2186\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2060\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1911\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1779\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1637\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1506\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1377\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.1243\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.1128\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0998\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.0865\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0741\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0612\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.0492\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.0367\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.0249\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.0129\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.0016\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.9905\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.9790\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9675\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.9575\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.9452\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9348\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:13,333] Trial 14 finished with value: 2.0866594743011864 and parameters: {'lr': 0.009187223885627122, 'alpha': 0.003998701290540148, 'activation': 'relu', 'n1': 320, 'n2': 192}. Best is trial 11 with value: 1.918677925474689.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 37.0252\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 35.7697\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 34.4798\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 33.2597\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 32.0805\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 30.9274\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 29.8184\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 28.7514\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 27.7216\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 26.7316\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 25.7755\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 24.8546\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 23.9674\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 23.1123\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 22.2870\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 21.4926\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20.7256\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19.9861\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19.2738\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 18.5873\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 17.9248\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 17.2861\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.6704\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.0765\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.5047\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.9524\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.4205\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.9078\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.4130\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.9362\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.4762\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12.0328\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.6052\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.1930\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.7954\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.4123\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.0429\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.6869\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.3427\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.0114\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.6916\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.3840\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.0868\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.8004\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5243\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2582\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.0012\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.7536\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.5148\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.2850\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.0627\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.8487\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.6425\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.4433\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2519\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.0665\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.8880\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7158\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.5502\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3905\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2360\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.0867\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.9435\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.8054\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6715\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5432\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4191\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2991\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1839\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0726\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9654\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8620\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.7621\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.6657\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.5735\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4836\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3978\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3142\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2339\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.1567\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0818\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0100\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9408\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.8735\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.8096\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7471\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6868\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6292\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5732\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.5196\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4674\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4176\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3691\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.3231\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.2784\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2347\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1930\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1529\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1136\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0767\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:17,332] Trial 15 finished with value: 1.9224143100049267 and parameters: {'lr': 0.002316520943814998, 'alpha': 0.04811100867603011, 'activation': 'relu', 'n1': 384, 'n2': 128}. Best is trial 11 with value: 1.918677925474689.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 37.2453\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 35.6044\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 34.2684\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32.8554\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 31.4308\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 30.0730\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 28.7578\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 27.5071\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 26.3153\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 25.1775\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 24.0898\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 23.0496\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 22.0560\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 21.1051\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20.1951\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19.3255\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.4929\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.6962\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.9347\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.2063\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.5089\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14.8418\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14.2036\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.5926\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.0092\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.4497\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.9152\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.4040\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.9144\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.4463\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.9981\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.5695\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.1591\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.7668\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.3911\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.0321\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.6886\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.3601\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.0447\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.7435\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.4550\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1798\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.9158\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.6636\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.4222\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.1914\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.9701\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7584\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.5560\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3627\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1771\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9997\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.8304\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.6678\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5131\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3641\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2218\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0857\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.9560\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8317\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7122\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5975\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4888\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3846\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.2841\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.1889\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.0974\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0095\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9259\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8457\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7691\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6957\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.6254\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5581\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.4943\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.4323\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.3737\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.3175\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2630\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2121\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1621\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1151\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0703\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0265\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9859\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9460\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.9076\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8717\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.8367\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8038\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7714\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7413\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7119\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6846\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6582\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6325\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6078\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5848\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5615\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5409\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:21,295] Trial 16 finished with value: 1.8627321129015857 and parameters: {'lr': 0.0027144823731241317, 'alpha': 0.049599748539040193, 'activation': 'relu', 'n1': 320, 'n2': 192}. Best is trial 16 with value: 1.8627321129015857.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.2543\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.5423\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.2902\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.1061\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.9614\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.8350\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.7191\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.6081\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.4970\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.3946\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.2915\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.1898\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.0909\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 7.9935\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 7.8966\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.8021\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.7084\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.6158\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5248\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 7.4350\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.3461\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2581\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.1715\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.0858\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.0018\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.9179\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.8356\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.7543\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.6739\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.5944\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.5157\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.4382\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.3614\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.2859\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.2107\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1370\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.0639\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.9925\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.9209\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.8502\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.7804\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.7119\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.6437\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5765\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5104\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.4449\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.3799\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.3158\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2526\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.1905\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.1288\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.0678\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.0075\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.9480\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.8896\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.8312\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7738\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7170\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.6615\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.6065\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.5513\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 4.4975\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.4437\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3913\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3388\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2873\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2366\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1862\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1368\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0874\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0392\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9914\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.9437\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.8968\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.8512\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.8050\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.7604\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.7153\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6716\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6281\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5850\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5425\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5009\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.4590\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.4185\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3779\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3375\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2983\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2593\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2206\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1824\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1449\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1076\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0711\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.0351\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.9990\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9634\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9285\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8935\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8596\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:25,414] Trial 17 finished with value: 2.2502517772452246 and parameters: {'lr': 0.0025465225202623626, 'alpha': 0.014686366604809714, 'activation': 'relu', 'n1': 256, 'n2': 192}. Best is trial 16 with value: 1.8627321129015857.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.4754\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.0971\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.7844\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6271\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5473\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4903\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4488\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4167\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3912\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3707\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3538\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3390\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.3266\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.3152\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3053\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2958\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2875\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2791\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2721\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2648\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2586\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2526\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.2462\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2406\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2353\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2298\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2249\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2200\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2151\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2106\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2056\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2014\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1965\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.1921\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1880\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1838\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1794\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1755\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1714\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1674\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1634\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1597\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1556\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.1519\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1482\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1444\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1408\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1370\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1335\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1299\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1263\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1228\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1193\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1158\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.1123\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1089\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1055\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1020\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0988\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0955\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0921\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0886\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.0853\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0821\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.0787\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.0754\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0723\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0688\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0656\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0625\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0592\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0560\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0528\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0495\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0466\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.0433\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0402\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0369\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0338\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0307\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0275\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0244\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0214\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0182\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0152\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.0120\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0088\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0059\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0028\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9997\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9966\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9936\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.9905\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.9875\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9846\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.9814\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.9784\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.9754\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9723\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.9693\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:29,753] Trial 18 finished with value: 2.8827969981649897 and parameters: {'lr': 0.000673619916423313, 'alpha': 0.004692912785713113, 'activation': 'gelu', 'n1': 256, 'n2': 320}. Best is trial 16 with value: 1.8627321129015857.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.2056\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.7802\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.2760\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.0584\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.8383\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.7270\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.6575\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.5928\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.5420\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.4982\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.4601\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.4249\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.3935\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.3645\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.3365\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 8.3105\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.2853\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.2611\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.2383\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.2162\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.1950\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.1741\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.1536\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.1336\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.1146\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.0952\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.0765\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.0581\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.0399\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.0220\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.0041\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.9866\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.9690\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.9519\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.9348\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.9178\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.9009\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.8843\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.8677\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.8512\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.8348\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.8186\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.8023\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7863\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7702\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7543\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7385\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.7226\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.7069\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.6913\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.6756\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.6601\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.6446\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.6291\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.6138\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5985\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5832\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5679\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5528\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.5377\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.5226\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5074\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.4924\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.4775\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.4625\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.4476\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.4328\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.4179\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.4031\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 7.3885\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.3737\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.3591\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.3444\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.3298\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.3153\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.3007\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2863\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2717\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2573\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.2429\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.2285\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.2142\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2000\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.1856\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.1715\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.1572\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.1430\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.1290\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.1148\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.1008\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.0866\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 7.0727\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.0587\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.0448\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.0309\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.0169\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.0031\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.9893\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.9754\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.9617\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:33,597] Trial 19 finished with value: 2.710048937628059 and parameters: {'lr': 0.00039823898482520574, 'alpha': 0.015528436977033454, 'activation': 'relu', 'n1': 192, 'n2': 256}. Best is trial 16 with value: 1.8627321129015857.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.3193\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.4124\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.1628\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.0228\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.9386\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.8787\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.8351\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.8041\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.7748\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.7497\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.7234\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.6983\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.6747\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.6512\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.6276\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.6052\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.5822\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.5595\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.5374\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.5156\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.4934\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4714\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4496\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4278\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4068\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3850\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3640\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3429\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3219\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.3010\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.2800\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2594\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2386\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2184\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1976\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1774\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1572\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1378\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.1175\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0973\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.0774\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.0578\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.0380\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0184\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9992\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9799\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9605\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9412\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9220\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9033\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.8846\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 3.8657\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.8471\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.8284\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.8103\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.7915\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.7732\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.7546\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.7371\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.7194\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.7008\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6833\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6651\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6477\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6296\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6122\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5948\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5772\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5602\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5428\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5259\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.5091\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4918\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4748\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4586\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4414\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4251\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4082\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3920\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3755\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3592\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3430\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3271\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3107\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2951\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2791\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2630\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2475\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2318\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2159\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2005\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.1851\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1696\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1545\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1396\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1242\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1090\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0942\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0789\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0642\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:37,793] Trial 20 finished with value: 2.257748169793616 and parameters: {'lr': 0.0022482717862801474, 'alpha': 0.0067614661226536804, 'activation': 'relu', 'n1': 320, 'n2': 192}. Best is trial 16 with value: 1.8627321129015857.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 32.3282\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 30.9233\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 30.0674\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 29.1956\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 28.3509\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 27.5252\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 26.7309\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 25.9675\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 25.2242\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 24.5063\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 23.8076\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 23.1301\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 22.4721\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 21.8334\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 21.2125\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20.6102\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20.0245\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 19.4555\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.9032\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 18.3669\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 17.8453\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 17.3388\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.8467\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.3686\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.9047\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.4531\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.0152\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.5897\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.1761\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.7744\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 13.3840\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 13.0048\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.6363\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.2785\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.9307\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.5931\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.2651\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.9467\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.6365\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.3356\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.0430\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.7593\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.4832\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.2152\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.9549\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.7020\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.4560\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.2172\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.9852\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7601\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5409\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.3282\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.1216\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.9208\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.7259\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.5363\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.3522\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1733\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.9998\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.8311\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.6668\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5072\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.3524\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.2022\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.0557\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.9139\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7760\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.6417\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.5116\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.3850\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.2621\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1427\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.0265\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9137\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.8046\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6978\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5948\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4939\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3965\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3016\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2092\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1198\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0331\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9481\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8665\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.7865\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.7087\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.6336\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5603\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4893\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4200\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3532\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2878\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2247\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1635\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1032\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.0451\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.9888\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.9335\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8806\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:41,793] Trial 21 finished with value: 1.9101167062632631 and parameters: {'lr': 0.0021461638629805806, 'alpha': 0.041384766960634464, 'activation': 'relu', 'n1': 384, 'n2': 128}. Best is trial 16 with value: 1.8627321129015857.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.9469\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.0959\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 16.3184\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.6733\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.1839\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 14.7465\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.3472\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.9881\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.6349\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.2993\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.9739\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.6578\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.3516\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.0536\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.7631\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 11.4803\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.2047\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.9350\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.6733\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.4180\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.1686\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.9257\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.6884\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.4570\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.2317\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.0112\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.7968\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.5871\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.3826\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.1830\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.9880\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7979\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.6124\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.4314\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2543\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.0819\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 6.9135\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.7494\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.5887\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.4323\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.2793\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1304\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.9848\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.8428\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.7043\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5691\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.4368\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.3079\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.1819\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.0595\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.9392\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.8224\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.7084\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.5966\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.4887\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3820\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2781\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1769\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.0788\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.9822\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.8879\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.7956\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.7064\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6191\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5331\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4501\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3686\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2888\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2115\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.1358\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0619\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9901\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9192\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8504\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7841\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7180\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6547\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5921\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.5308\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.4718\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.4134\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.3569\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3019\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2475\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1956\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.1439\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.0932\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0446\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9966\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9501\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.9041\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.8600\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.8163\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7744\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7332\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6927\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6530\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6149\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5766\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5403\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:45,794] Trial 22 finished with value: 1.9888310108366127 and parameters: {'lr': 0.003109196855683419, 'alpha': 0.02406387305333381, 'activation': 'relu', 'n1': 320, 'n2': 128}. Best is trial 16 with value: 1.8627321129015857.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 34.2057\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 32.0072\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 29.5774\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 27.1476\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 24.7676\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 22.5556\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20.5354\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.6883\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 17.0036\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15.4716\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.0776\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.8096\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.6576\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.6098\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.6558\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.7900\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.0008\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2836\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.6319\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.0399\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.5001\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.0090\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.5629\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1560\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.7879\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4509\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1452\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8694\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6147\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3858\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1771\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.9870\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.8128\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.6558\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5117\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3818\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2640\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1580\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0597\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9715\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8894\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8180\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7491\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6927\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6314\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5822\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5335\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4921\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4545\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4189\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3877\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3572\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3321\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3067\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2861\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2677\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2470\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2330\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2176\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2057\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1900\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1791\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1671\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1591\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1494\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1431\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1360\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1296\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1253\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1198\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1166\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1118\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1072\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1008\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0979\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0937\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0892\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0865\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0849\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0826\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0792\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0790\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0759\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0751\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0774\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0806\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0745\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0754\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0743\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0738\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0720\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0715\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0703\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0693\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0728\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0710\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0705\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0695\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0668\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0642\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:49,415] Trial 23 finished with value: 1.8665964125741594 and parameters: {'lr': 0.0060865350597227525, 'alpha': 0.04603699215086594, 'activation': 'relu', 'n1': 320, 'n2': 192}. Best is trial 16 with value: 1.8627321129015857.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 37.4933\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 35.9430\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 34.9708\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 34.0367\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 33.1300\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 32.2652\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 31.4299\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 30.6152\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 29.8232\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 29.0558\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 28.3068\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 27.5788\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 26.8696\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 26.1791\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 25.5063\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 24.8516\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 24.2132\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 23.5914\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 22.9861\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 22.3965\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 21.8217\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 21.2618\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20.7164\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20.1849\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19.6677\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19.1630\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.6720\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.1935\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 17.7272\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 17.2729\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 16.8303\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.3991\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.9789\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.5698\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.1710\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.7827\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.4044\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.0359\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.6762\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.3262\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 12.9850\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.6530\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.3293\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.0140\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.7069\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.4077\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.1159\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8318\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.5551\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2857\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.0227\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.7668\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5175\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.2745\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0381\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8073\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5825\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.3636\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1505\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9429\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7403\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5428\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3507\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.1637\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.9810\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8035\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6304\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4614\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2972\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1370\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9810\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8290\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6808\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5364\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3962\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2589\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1258\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9954\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8689\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7455\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6250\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5079\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3940\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2824\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1743\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0686\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9655\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8654\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7677\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6726\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5796\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4895\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4013\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3158\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2325\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1506\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0714\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9941\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9185\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8454\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:52,337] Trial 24 finished with value: 2.0340077800955365 and parameters: {'lr': 0.0018164168325739444, 'alpha': 0.04418223295918186, 'activation': 'relu', 'n1': 384, 'n2': 192}. Best is trial 16 with value: 1.8627321129015857.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.0370\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.1355\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 13.3041\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.4284\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6388\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.9100\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2321\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.6088\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0211\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4750\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9591\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4777\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 7.0256\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6014\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2026\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8293\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4777\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.1480\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8388\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5496\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 4.2768\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0203\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7791\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5518\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 3.3402\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 3.1398\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9521\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7781\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6103\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4569\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3117\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1742\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0441\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9238\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8092\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.7026\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.6024\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5096\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4206\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3376\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2590\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1888\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1226\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0657\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0026\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.9392\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.8853\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.8369\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7870\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7417\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7024\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6608\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6301\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5896\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5558\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5251\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4965\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4674\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4455\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4245\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3985\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3799\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3592\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3425\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3262\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3101\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2930\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2800\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2653\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2398\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2273\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2153\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2035\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1943\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1838\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1734\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1667\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1597\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1521\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1445\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1398\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1330\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1275\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1259\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1237\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1156\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1137\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1106\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1062\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1029\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0993\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0975\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0929\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0935\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0920\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0900\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0872\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0799\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:54,325] Trial 25 finished with value: 1.8452508383976476 and parameters: {'lr': 0.009005105419531683, 'alpha': 0.020943708147144375, 'activation': 'relu', 'n1': 320, 'n2': 128}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.4489\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.1810\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.4065\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.7025\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.0995\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.3670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7737\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2535\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7526\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2903\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8701\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4645\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0768\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7246\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4043\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0934\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7934\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5188\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2605\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0203\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7974\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5818\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3847\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2050\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0217\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8505\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6937\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5360\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4054\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2662\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1518\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0320\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9141\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8110\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7304\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6354\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5433\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4552\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3835\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3102\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2566\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1903\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1233\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0623\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0063\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9543\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9044\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8589\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8172\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7803\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7418\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7044\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6723\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6450\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6113\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5866\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5606\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5361\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5173\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4869\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4721\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4527\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4272\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4105\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3927\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3786\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3722\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3602\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3413\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3312\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3281\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3115\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2982\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2973\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2781\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2758\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2748\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2526\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2438\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2352\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2300\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2225\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2265\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2284\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2285\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2178\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2119\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1959\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1903\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1886\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1834\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1820\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1755\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1807\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1763\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1717\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1661\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1600\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1587\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1592\n",
      "4/4 [==============================] - 0s 1000us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:56,457] Trial 26 finished with value: 2.117426683369244 and parameters: {'lr': 0.008134072448071333, 'alpha': 0.02230193200003664, 'activation': 'sigmoid', 'n1': 192, 'n2': 192}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 34.6661\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 32.9760\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 31.0776\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 29.4206\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 27.7215\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 26.1190\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 24.6186\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 23.2035\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 21.8714\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.6200\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.4384\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.3254\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.2780\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 16.2910\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.3606\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.4846\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.6585\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.8799\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.1468\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.4559\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8046\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.1907\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.6122\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0665\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.5529\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.0680\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6116\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1816\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7756\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3933\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.0327\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.6932\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.3726\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0708\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7859\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.5179\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2650\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0271\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8018\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5900\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3902\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2022\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0247\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8577\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7000\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5516\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4114\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2789\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1544\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0374\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9263\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8220\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7241\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6309\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5445\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4619\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3833\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3106\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2422\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1770\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1148\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0564\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0021\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9509\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9019\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8563\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8131\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7719\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7336\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6974\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6632\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6311\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6003\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5714\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5453\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5191\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4951\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4728\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4505\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4309\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4112\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3935\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3768\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3604\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3458\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3310\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3171\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3048\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2924\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2811\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2698\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2600\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2419\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2340\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2262\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2174\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2107\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2023\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1966\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:05:58,813] Trial 27 finished with value: 2.0381709359913938 and parameters: {'lr': 0.0035792148136236236, 'alpha': 0.049991761656705984, 'activation': 'gelu', 'n1': 320, 'n2': 128}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.6354\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.6545\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.0803\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.4852\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 10.9327\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.4026\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.9029\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.4321\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.9785\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5508\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1435\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7566\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3892\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0371\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7018\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3842\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 6.0799\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 5.7913\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5172\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2566\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0110\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7708\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 4.5442\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3284\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1250\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9297\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7461\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5748\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4002\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2431\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0923\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9496\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8087\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6805\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5516\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4295\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3145\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2088\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1032\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0049\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9093\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8227\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7402\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6635\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5801\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5110\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4415\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3734\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3103\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2465\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1894\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1357\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0865\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0391\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9884\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9559\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9333\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8774\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8316\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7992\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7593\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7322\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6954\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6636\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6326\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6054\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5790\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5520\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5270\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5016\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4772\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4574\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4377\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4186\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4009\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3819\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3659\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3511\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3380\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3247\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3098\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2993\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2865\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2748\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2677\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2606\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2496\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2380\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2331\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2237\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2164\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2084\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2036\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2052\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2043\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1988\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1996\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1848\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1716\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1690\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:00,883] Trial 28 finished with value: 2.302189637119125 and parameters: {'lr': 0.007785081051888344, 'alpha': 0.019071865273859012, 'activation': 'tanh', 'n1': 256, 'n2': 256}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 22.6707\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.0928\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.7719\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.2862\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.8915\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.7958\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9730\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4729\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2491\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2588\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4572\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8030\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2815\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8557\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5149\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2375\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0153\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8329\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6924\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5816\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4967\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4177\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3670\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3078\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2653\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2272\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2006\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1755\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1619\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1495\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1468\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1362\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1294\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1181\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1076\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0997\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0969\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1002\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1146\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0959\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0910\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0895\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0911\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0799\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0790\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0760\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0777\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0727\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0745\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0976\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0998\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1115\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1152\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1310\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1214\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0979\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0996\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0893\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0931\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0808\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0790\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0718\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0725\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0783\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0866\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0864\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1000\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0956\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0956\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0854\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0899\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0819\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0858\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0850\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0866\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0809\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0840\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0797\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0877\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0926\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0937\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0880\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0797\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0798\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0808\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0755\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0729\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0770\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0757\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0757\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0759\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0785\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0851\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0985\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1232\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1012\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0937\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0854\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0849\n",
      "4/4 [==============================] - 0s 1000us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:02,984] Trial 29 finished with value: 1.9000355510184874 and parameters: {'lr': 0.019687161944383347, 'alpha': 0.029758153966546585, 'activation': 'relu', 'n1': 320, 'n2': 128}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 19.7568\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.6494\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.5142\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.3766\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.2545\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.1474\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.0509\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.9690\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.8959\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.8304\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.7701\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.7167\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.6666\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 18.6223\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.5787\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.5390\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.5011\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.4653\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.4316\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.3990\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.3679\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.3384\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.3087\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.2808\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.2535\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.2271\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.2014\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.1763\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.1509\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.1271\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.1034\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.0794\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.0567\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.0337\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.0116\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.9894\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.9675\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.9462\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.9248\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.9037\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.8829\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.8624\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.8416\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.8216\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.8015\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.7815\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.7618\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.7420\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.7226\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.7033\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.6839\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.6649\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.6458\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.6269\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.6081\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.5894\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.5708\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.5523\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.5337\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.5154\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.4972\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.4788\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.4608\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.4426\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.4246\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.4067\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.3888\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.3710\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.3533\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.3356\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.3179\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.3003\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.2827\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.2652\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 17.2478\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.2304\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.2130\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.1957\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.1784\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.1612\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.1440\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.1267\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.1097\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.0926\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.0756\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.0586\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.0416\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.0247\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.0078\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.9910\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.9741\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.9573\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.9405\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.9238\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.9071\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.8904\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.8737\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.8571\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.8405\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.8240\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:05,027] Trial 30 finished with value: 2.4817498118084216 and parameters: {'lr': 0.00010194512966059571, 'alpha': 0.029453763367055772, 'activation': 'relu', 'n1': 256, 'n2': 192}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 23.9625\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 21.6977\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.1220\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.5789\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.2947\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.2759\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.5192\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.9954\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.6940\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5795\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6287\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8160\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1235\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5322\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0273\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5954\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2251\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9105\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6432\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4168\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2212\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0536\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9087\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7924\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6867\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5962\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5217\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4632\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4081\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3627\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3240\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2861\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2540\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2220\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1993\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1782\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1635\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1522\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1380\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1306\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1185\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1188\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1292\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1236\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1143\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1040\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0971\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0922\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0853\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0818\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0818\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0909\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0904\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0973\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1025\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1112\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0849\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0800\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0791\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0751\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0785\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0719\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0716\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0683\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0727\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0687\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0764\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0788\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0692\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0685\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0703\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0681\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0686\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0659\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0649\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0644\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0694\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0662\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0671\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0627\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0698\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0664\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0681\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0758\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0739\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0719\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0712\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0819\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0809\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0727\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0701\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0703\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0702\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0779\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0727\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0758\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0719\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0728\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0709\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:07,144] Trial 31 finished with value: 2.0188758540431517 and parameters: {'lr': 0.013142291333341773, 'alpha': 0.03405253292242659, 'activation': 'relu', 'n1': 320, 'n2': 128}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.7216\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.7894\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.6089\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.3833\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.5028\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8288\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3922\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1782\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1614\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3111\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6028\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0124\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5204\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.1106\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.7667\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.4855\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2470\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0521\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8910\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7548\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6471\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5519\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4788\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4143\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3605\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3070\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2693\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2388\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2130\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1967\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1882\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1841\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1640\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1545\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1244\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1147\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1068\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1026\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0901\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0870\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0848\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0869\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0891\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0961\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0887\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0973\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1015\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0945\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0845\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0872\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0871\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0909\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0835\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1031\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0910\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1133\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1151\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1155\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1027\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1137\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0921\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1112\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0839\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0802\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0727\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0898\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0871\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0784\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0756\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0719\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0695\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0709\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0713\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0679\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0681\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0680\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0641\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0688\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0687\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0660\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0744\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0750\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0726\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0727\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0815\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0774\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0708\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0723\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0802\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0716\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0710\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0732\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0728\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0962\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0958\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1019\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0950\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1060\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1018\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:09,143] Trial 32 finished with value: 1.9603446837411516 and parameters: {'lr': 0.019501981529700906, 'alpha': 0.026157085594100515, 'activation': 'relu', 'n1': 320, 'n2': 128}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.8250\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.6848\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.7187\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.7518\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.8400\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.0057\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.2334\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5108\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8452\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2249\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6482\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1127\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6163\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1541\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7244\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.3253\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.9536\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6089\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2888\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9924\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7160\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4580\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2180\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9950\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7879\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5951\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4157\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2542\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0954\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9540\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8219\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6976\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5801\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4728\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3718\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2802\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1934\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1155\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0383\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9696\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9043\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8462\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.7949\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7507\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6987\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6493\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6043\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5665\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5287\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4980\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4649\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4340\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4081\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3827\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3600\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3402\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3199\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2976\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2827\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2690\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2497\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2377\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2210\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2098\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1972\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1895\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1786\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1722\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1685\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1576\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1493\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1462\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1359\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1292\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1254\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1192\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1121\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1109\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1062\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1055\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1025\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0994\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0979\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0930\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0916\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0851\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0791\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0780\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0739\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0713\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0675\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0662\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0638\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0626\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0640\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0627\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0600\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0597\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0594\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0578\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:11,633] Trial 33 finished with value: 1.8490723870770651 and parameters: {'lr': 0.01108595934156824, 'alpha': 0.019678405030832277, 'activation': 'relu', 'n1': 384, 'n2': 128}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.0350\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.9494\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.3943\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9968\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6466\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3107\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0187\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7306\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4632\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2125\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9751\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7461\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5309\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3231\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1236\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9334\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7491\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5724\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4028\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2405\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0828\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9313\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7854\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6444\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5104\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3797\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2547\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1360\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0188\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9081\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8009\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6971\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5975\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5022\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4097\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3208\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2359\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1544\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0746\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9982\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9247\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8544\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7860\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7223\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6586\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5984\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5386\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4833\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4288\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3772\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3262\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2769\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2319\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1868\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1443\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1045\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0648\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0270\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9901\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9551\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9187\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8887\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8559\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8257\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7946\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7695\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7397\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7146\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6875\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6638\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6397\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6181\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5947\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5742\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5557\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5349\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5145\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4978\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4818\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4634\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4471\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4322\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4172\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4026\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3917\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3801\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3657\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3553\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3419\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3301\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3175\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3071\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2962\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2870\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2783\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2696\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2600\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2521\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2430\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2353\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:13,767] Trial 34 finished with value: 1.8761042610590979 and parameters: {'lr': 0.010618809820398449, 'alpha': 0.010984051629475652, 'activation': 'relu', 'n1': 384, 'n2': 128}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5942\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6246\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4026\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1363\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1010\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7647\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5776\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4779\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3405\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2448\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 5.1502\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0742\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 4.9847\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8714\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.8171\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7489\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6585\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5919\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5095\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4534\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 4.3919\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 4.3276\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2714\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2182\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1545\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0825\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0258\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9656\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9216\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8577\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8144\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7616\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7028\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6483\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6132\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5583\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5041\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4555\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4151\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3667\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3330\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2859\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2382\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1884\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 3.1414\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0965\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0523\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0106\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9716\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9344\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8933\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.8528\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.8149\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.7793\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7385\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7035\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6665\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6287\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6003\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5582\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.5278\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.4938\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4564\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4244\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3910\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3580\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3317\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2991\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2716\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2387\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2082\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1760\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1510\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1263\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0915\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0670\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0456\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0104\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9807\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9545\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9284\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9017\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8823\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8586\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8421\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8102\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7872\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7606\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7365\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7108\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6863\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6662\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6432\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6299\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6086\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5855\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5614\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5396\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5172\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4991\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:15,804] Trial 35 finished with value: 2.219940951018825 and parameters: {'lr': 0.006672762328383692, 'alpha': 0.006420245029894681, 'activation': 'sigmoid', 'n1': 384, 'n2': 256}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 15.1013\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 14.0531\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 12.9600\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.8574\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8886\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.9885\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 9.1647\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4114\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7087\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0711\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4859\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 5.9507\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4603\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0091\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 4.5964\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2191\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8712\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5522\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2618\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9953\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7541\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5269\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.3236\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.1325\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9638\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8089\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6660\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5386\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4174\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3079\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2081\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1165\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0308\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9634\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8886\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.8164\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7449\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6880\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6317\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5869\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5393\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5052\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4763\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4562\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4120\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3894\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3556\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3367\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3148\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2853\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2643\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2463\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2352\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2220\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2097\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2197\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2205\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2164\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1861\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1736\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1556\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1555\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1403\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1298\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1248\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1244\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1232\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1175\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1145\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1070\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1080\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0966\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0929\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0900\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0892\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0891\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0860\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0895\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0869\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0862\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0798\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0770\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0753\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0746\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0770\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0811\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0736\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0703\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0749\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0742\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0788\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0869\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0854\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0944\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0881\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0821\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0741\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0770\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0662\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0686\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:17,770] Trial 36 finished with value: 2.2788241191573615 and parameters: {'lr': 0.013664727209120167, 'alpha': 0.018818320228411834, 'activation': 'tanh', 'n1': 384, 'n2': 128}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6145\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2758\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9180\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7026\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5848\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4891\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4533\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4368\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4269\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4230\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4162\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4141\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4126\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4110\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4097\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4090\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4074\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4065\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4063\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4067\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4060\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4053\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4039\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4026\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4031\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4012\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4005\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4015\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3991\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4001\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3996\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3982\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3970\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3971\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3955\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3949\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3945\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3949\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3937\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3928\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3920\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3918\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3902\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3897\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3894\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3888\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3882\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3877\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3867\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3864\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3861\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3845\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3847\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3837\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3833\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3834\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3818\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3808\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3823\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3824\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3790\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3804\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3779\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3787\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3771\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3771\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3762\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3753\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3747\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3737\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3735\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3735\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3719\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3711\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3717\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3702\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3691\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3697\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3683\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3682\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3670\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3667\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3666\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3657\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3668\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3656\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3642\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3633\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3627\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3620\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3610\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3607\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3599\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3598\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3598\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3598\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3582\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3588\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3562\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3562\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:19,852] Trial 37 finished with value: 2.1287499298293637 and parameters: {'lr': 0.010595177029982746, 'alpha': 0.00048811613960717806, 'activation': 'relu', 'n1': 384, 'n2': 192}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3761\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4061\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 3.2898\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4079\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2024\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0570\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8883\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8342\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.7746\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7285\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.6544\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6000\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5672\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.5263\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4995\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4708\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4577\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4306\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4174\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4049\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3784\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3672\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3391\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3223\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3071\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2914\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2788\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2719\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2692\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2648\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2388\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2172\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2076\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1973\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1876\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1796\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1721\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1576\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1493\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1382\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1320\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1193\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1111\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1005\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0917\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0819\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0743\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0654\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0579\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0485\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0401\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0318\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0232\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0191\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0125\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0003\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9944\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9848\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9830\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9721\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9688\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9571\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9466\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9411\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9292\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9224\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9152\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9072\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9002\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8935\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8871\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8791\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8717\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8642\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8587\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8534\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8426\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8364\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8302\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8232\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8159\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8076\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8021\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7982\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.7903\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.7829\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7746\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7667\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7603\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7535\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7473\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7408\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7348\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7276\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7239\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7186\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.7130\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.7048\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6996\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6906\n",
      "4/4 [==============================] - 0s 1000us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:21,948] Trial 38 finished with value: 2.214644053001177 and parameters: {'lr': 0.0037965090801510702, 'alpha': 0.0030774583737178405, 'activation': 'sigmoid', 'n1': 320, 'n2': 256}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.7192\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.9864\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.3026\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.7211\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2215\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.7512\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.3191\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.9142\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.5261\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1580\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8052\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4688\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1481\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8411\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5473\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2676\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9984\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7417\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4967\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2626\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.0377\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8223\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6168\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4198\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2324\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0517\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8797\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7158\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5572\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4070\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2627\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1246\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9917\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8658\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7441\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6289\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5182\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4126\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3103\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2133\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1201\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0315\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9461\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8651\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7875\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7125\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6418\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5735\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5080\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4461\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3855\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3272\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2742\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2214\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1713\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1236\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0763\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0334\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9928\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9537\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9127\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8758\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8402\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8079\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7746\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7448\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7149\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6856\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6583\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6335\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6086\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5846\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5616\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5394\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5202\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4987\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4788\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4618\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4445\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4274\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4105\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3960\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3820\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3682\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3590\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3441\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3304\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3186\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3077\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2958\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2844\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2751\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2646\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2569\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2431\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2332\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2255\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2163\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2100\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:24,211] Trial 39 finished with value: 2.059073029515335 and parameters: {'lr': 0.00669972469960471, 'alpha': 0.020103098491958203, 'activation': 'gelu', 'n1': 256, 'n2': 128}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1003\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4045\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1811\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0647\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0270\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9942\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9778\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9667\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9584\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9523\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9472\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9421\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9386\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9350\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9315\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9287\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9253\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9226\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9200\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9183\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9161\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9129\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9108\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9078\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9065\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9035\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9016\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.9007\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8969\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8959\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8941\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8913\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8891\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8874\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8844\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8827\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8807\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8784\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8762\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8742\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8719\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8698\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8679\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8659\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8644\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8617\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8602\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8580\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8559\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8541\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8513\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8489\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8478\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8452\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8435\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8420\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8388\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8372\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8361\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8341\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8310\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8295\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8276\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8264\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8234\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8222\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8199\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8171\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8153\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8136\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8117\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8101\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8071\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8055\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8041\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8017\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7994\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7988\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7956\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7941\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7916\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7900\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.7884\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.7860\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.7854\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7829\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7805\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7784\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7761\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7747\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7721\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7705\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7684\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7670\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7654\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7633\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7610\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7600\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7568\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7553\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:26,229] Trial 40 finished with value: 2.479198291780881 and parameters: {'lr': 0.0016599010932206123, 'alpha': 0.002054639334747989, 'activation': 'tanh', 'n1': 384, 'n2': 384}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4094\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6364\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0375\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5010\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1719\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8570\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5760\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3135\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.0671\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8329\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6109\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3961\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1925\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9958\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8068\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6261\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4508\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2831\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1223\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9684\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.8192\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6757\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5371\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4036\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2775\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1528\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0347\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9232\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8120\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7082\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6073\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5092\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4148\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3254\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2379\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1542\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0744\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9981\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9235\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8511\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7825\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7172\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6534\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5931\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5330\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4758\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4212\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3692\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3182\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2703\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2233\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1774\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1354\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0947\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0553\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0186\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.9816\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.9436\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9100\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8777\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8436\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8151\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7843\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7564\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7279\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7026\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6775\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6523\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6291\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6074\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5862\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5671\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5452\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5264\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5100\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4898\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4705\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4545\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4395\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4229\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4082\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3940\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3806\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3678\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3581\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3469\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3341\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3244\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3122\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3006\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2893\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2796\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2707\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2614\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2558\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2473\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2405\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2311\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2244\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2166\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:28,339] Trial 41 finished with value: 1.9156895303341988 and parameters: {'lr': 0.011273187682443358, 'alpha': 0.010483746382504357, 'activation': 'relu', 'n1': 384, 'n2': 128}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 22.5276\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.3388\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.9885\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.7590\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.6703\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.8026\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.1716\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7652\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5353\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4870\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5826\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8056\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1384\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 3.5639\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0717\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6502\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2866\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9740\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7065\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.4813\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2864\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1135\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9656\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8418\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7399\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6419\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5615\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4949\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4342\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3839\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3445\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3097\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2748\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2429\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2132\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1925\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1759\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1657\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1542\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1459\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1364\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1309\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1277\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1210\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1082\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0936\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0919\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0893\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0850\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0817\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0778\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0823\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0858\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0904\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0983\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0791\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0763\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0798\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0718\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0751\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0694\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0682\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0637\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0657\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0623\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0654\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0694\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0672\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0704\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0694\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0677\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0667\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0669\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0673\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0665\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0848\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0785\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0847\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0725\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0773\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0708\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0770\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0716\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0668\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0636\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0638\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0636\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0606\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0568\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0583\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0578\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0594\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0665\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0729\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0691\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0660\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0685\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0689\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:30,372] Trial 42 finished with value: 1.9369948342577172 and parameters: {'lr': 0.015094297513332171, 'alpha': 0.02859800948838268, 'activation': 'relu', 'n1': 384, 'n2': 128}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 28.8372\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 27.1305\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 24.4456\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 21.6350\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.2070\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 17.0055\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.0296\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.2919\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.7529\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.3967\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 9.1973\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1374\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2013\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 6.3731\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 5.6414\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9950\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4224\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9168\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4708\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0770\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7291\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4201\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1466\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.9059\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6937\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5066\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3403\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1981\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0645\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9481\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8490\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7592\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6805\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6066\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5450\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4890\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4411\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4006\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3598\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3278\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2953\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2730\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2559\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2437\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2153\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1943\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1770\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1631\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1497\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1389\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1306\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1222\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1171\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1121\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1082\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1149\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1271\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1025\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1045\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0995\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0897\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0853\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0787\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0764\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0736\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0714\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0710\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0706\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0687\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0671\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0675\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0681\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0672\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0684\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0738\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0698\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0710\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0786\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0693\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0690\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0664\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0694\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0652\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0637\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0644\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0674\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0613\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0631\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0666\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0611\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0591\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0617\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0569\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0593\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0622\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0640\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0601\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0587\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0602\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0583\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:32,385] Trial 43 finished with value: 1.8793986194551933 and parameters: {'lr': 0.009580100417231561, 'alpha': 0.03737141376728426, 'activation': 'relu', 'n1': 384, 'n2': 128}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4534\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6864\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2854\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.9514\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6747\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4321\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2328\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0409\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8580\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6833\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5124\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3475\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1875\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.0322\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.8795\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7330\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5884\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4480\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3120\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1795\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0492\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9227\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7993\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6788\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.5626\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4476\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3365\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2289\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1227\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0199\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9194\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8213\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7258\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6332\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5419\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4541\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3677\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2850\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2025\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1226\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0445\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9690\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8947\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8229\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7529\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.6844\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6177\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5524\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4889\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4271\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3671\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3082\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2514\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1951\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1415\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0883\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0360\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9859\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9376\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8898\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8423\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7973\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7527\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7101\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6673\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6265\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5865\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5473\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5097\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4724\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4369\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4014\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3670\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3329\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3008\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2681\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2367\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.2075\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.1770\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1487\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1202\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0931\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0670\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0404\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.0169\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9921\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9675\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9440\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9204\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8986\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8756\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8549\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8338\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8142\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7947\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7755\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7565\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7391\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7203\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7035\n",
      "4/4 [==============================] - 0s 1000us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:34,458] Trial 44 finished with value: 1.9156715762632037 and parameters: {'lr': 0.006434377056684691, 'alpha': 0.012226612152438683, 'activation': 'relu', 'n1': 320, 'n2': 128}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6702\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8847\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7174\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6166\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5158\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4475\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3991\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3565\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3098\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2708\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2336\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1945\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1582\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 4.1222\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 4.0860\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0522\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0166\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9819\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9483\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9151\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8815\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8479\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8151\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7825\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7511\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7187\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6871\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6565\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6252\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5942\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5635\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5334\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5030\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4743\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4440\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4152\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3854\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3578\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3288\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3008\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2718\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2447\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2167\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1895\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1626\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1362\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1091\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0826\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0562\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0304\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0048\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9791\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9544\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9289\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9047\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8799\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8557\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8318\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8082\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7844\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7597\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7367\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7143\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6912\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6674\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6449\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6224\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6003\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5781\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5566\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5350\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5135\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4926\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4713\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4511\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4291\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4090\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3885\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3676\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3482\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3280\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3084\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2891\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2694\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2510\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2318\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.2125\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1941\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1759\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.1569\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1386\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1206\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1027\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0851\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0679\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.0502\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0325\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0161\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9980\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.9816\n",
      "4/4 [==============================] - 0s 666us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:36,531] Trial 45 finished with value: 2.23226945728389 and parameters: {'lr': 0.004277889600864285, 'alpha': 0.006246750356023465, 'activation': 'relu', 'n1': 320, 'n2': 192}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3016\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5062\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2982\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2371\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1827\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1472\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1361\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1283\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1184\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1157\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1133\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1108\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1103\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1091\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1082\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1085\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1073\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1069\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1068\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1068\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1064\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1060\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1059\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1057\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1063\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1059\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1058\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1062\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1062\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1059\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1058\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1057\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1063\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1057\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1058\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1055\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1064\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1059\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1061\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1059\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1056\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1061\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1055\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1057\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1057\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1057\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1056\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1058\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1056\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1059\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1065\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1060\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1050\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1055\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1052\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1052\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1050\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1052\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1052\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1056\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1052\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1048\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1048\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1052\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1046\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1048\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1049\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1050\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1045\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1049\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1049\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1046\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:38,575] Trial 46 finished with value: 2.1849734864777495 and parameters: {'lr': 0.0029598653502571695, 'alpha': 0.00010876847115786186, 'activation': 'relu', 'n1': 384, 'n2': 192}. Best is trial 25 with value: 1.8452508383976476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6855\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.0885\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1885\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4609\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7587\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1062\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.5083\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9645\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4568\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 5.0102\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5864\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2065\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8605\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5420\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2524\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9873\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.7432\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5178\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3131\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1264\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9565\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7966\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6510\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5198\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3991\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2867\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.1810\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0922\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0065\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9324\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8648\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7907\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.7277\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6720\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6184\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5716\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5276\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4898\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4504\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4184\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3875\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3629\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3399\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3321\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3007\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2809\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2565\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2393\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2214\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2110\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2001\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1901\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1780\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1766\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1623\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1753\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1743\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1594\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1356\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1375\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1248\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1201\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1045\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0994\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0942\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0896\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0849\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0835\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0802\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0791\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0760\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0736\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0741\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0722\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0746\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0711\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0683\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0679\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0648\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0638\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0618\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0603\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0586\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0564\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0590\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0623\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0604\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0696\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0612\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0594\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0545\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0558\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0544\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0549\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0564\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0578\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0534\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0570\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0544\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0507\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:40,524] Trial 47 finished with value: 1.843448312569594 and parameters: {'lr': 0.015501594115799253, 'alpha': 0.016663207576012973, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 47 with value: 1.843448312569594.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.5176\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.7538\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.8610\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0026\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4073\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5291\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8247\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2114\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7056\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2228\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7798\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3714\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0464\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6734\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3549\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0877\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8212\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5985\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3907\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1852\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0130\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8826\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7270\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6034\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4451\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3273\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2373\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1385\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0528\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9705\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9206\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8530\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8113\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7393\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7134\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6693\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6346\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5732\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5316\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4939\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4485\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4266\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4050\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3914\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3595\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3521\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3442\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3015\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2951\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2742\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2612\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2661\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2590\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2462\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2125\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2146\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2128\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2004\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2039\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2715\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2382\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2972\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2364\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2238\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2068\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2295\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2625\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2350\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3019\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2257\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2295\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1762\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1860\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2018\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2180\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1510\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1576\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1587\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1466\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1422\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1528\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1757\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1621\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1509\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1723\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1545\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1423\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1553\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1693\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2842\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2376\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1790\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1911\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1957\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1645\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1499\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1335\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1226\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1296\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1397\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:42,708] Trial 48 finished with value: 2.3452508460224375 and parameters: {'lr': 0.01595515231870465, 'alpha': 0.016655010630108328, 'activation': 'sigmoid', 'n1': 256, 'n2': 192}. Best is trial 47 with value: 1.843448312569594.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.5862\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.1950\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.9225\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.7014\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.4318\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.3506\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.3530\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4507\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6224\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8717\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1852\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5598\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9904\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4710\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.9963\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5649\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1703\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8103\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4825\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1845\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9113\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.6618\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4346\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2254\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0379\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8632\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7051\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5643\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4300\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3120\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2033\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1038\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0109\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9282\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8515\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7830\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7203\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6656\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6131\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5673\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5234\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4888\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4527\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4348\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3923\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3562\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3289\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3053\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2824\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2628\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2448\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2265\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2122\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1985\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1864\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1767\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1655\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1537\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1480\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1435\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1317\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1280\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1185\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1140\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1103\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1070\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1018\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1003\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1008\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1025\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0928\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0897\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0838\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0789\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0733\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0763\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0748\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0706\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0745\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0680\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0694\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0656\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0664\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0695\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0647\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0657\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0650\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0618\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0616\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0607\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0590\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0590\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0608\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0623\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0614\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0623\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0593\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0562\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:44,800] Trial 49 finished with value: 1.8254005014245944 and parameters: {'lr': 0.007863133981898437, 'alpha': 0.03459059408253292, 'activation': 'relu', 'n1': 192, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.9738\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.1320\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.7893\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.3705\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.0963\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.9345\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.9032\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.9606\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1101\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3422\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6489\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0222\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4559\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9436\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4801\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0615\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6821\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3387\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0290\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7492\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4961\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2659\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0581\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8689\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6996\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5452\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4053\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2836\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1655\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0647\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9723\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8889\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8113\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7428\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6785\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6230\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5716\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5271\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4822\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4455\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4116\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3803\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3506\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3262\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3089\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2826\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2719\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2697\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2476\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2341\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2109\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1953\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1819\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1752\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1653\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1610\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1582\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1469\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1515\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1478\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1362\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1267\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1161\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1128\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1085\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1065\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1032\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1009\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0989\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0971\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0965\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0937\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0955\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0915\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0908\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0852\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0821\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0838\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0825\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0809\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0784\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0826\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0787\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1042\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1181\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1095\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0873\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0887\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0876\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0816\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0811\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0771\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0755\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0737\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0766\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0791\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0757\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0732\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0704\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0696\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:47,161] Trial 50 finished with value: 2.038447106822703 and parameters: {'lr': 0.008549584996613681, 'alpha': 0.03459312328903979, 'activation': 'gelu', 'n1': 192, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.4046\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.1958\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.5922\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.1470\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.5910\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.1253\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.6938\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2781\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.8784\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4997\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1378\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7889\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4552\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1347\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8255\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5296\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2440\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9696\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.7061\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4531\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2088\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9739\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7482\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5307\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3229\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1213\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9284\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7435\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 4.5642\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3926\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2272\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0680\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9146\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7681\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6258\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4902\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3592\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2342\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1123\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9953\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8828\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7752\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6709\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5713\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4755\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3829\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2944\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2088\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1263\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0475\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9710\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8971\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8280\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7594\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6944\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6322\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5707\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5134\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4583\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4055\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3525\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3030\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2549\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2102\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1646\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1232\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0817\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0416\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0037\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9679\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9330\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8990\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8657\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8342\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8049\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7753\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7469\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7213\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6953\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6705\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6459\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6235\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6018\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5804\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5625\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5424\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5220\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5038\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4863\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4694\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4526\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4372\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4219\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4081\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3947\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3811\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3677\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3558\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3429\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3323\n",
      "4/4 [==============================] - 0s 836us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:49,301] Trial 51 finished with value: 1.8992012660345252 and parameters: {'lr': 0.005135353848419676, 'alpha': 0.02311597145945337, 'activation': 'relu', 'n1': 192, 'n2': 320}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.2132\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.6120\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.0613\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5516\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 8.1961\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9878\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9535\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0664\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3106\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6705\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1268\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6654\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2745\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9418\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6595\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4192\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2147\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0413\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8963\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7754\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6726\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5831\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5083\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4433\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3884\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3392\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3037\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2756\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2468\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2252\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2115\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1912\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1739\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1531\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1420\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1240\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1161\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1119\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1022\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0990\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0901\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0891\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0913\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1018\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0914\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0918\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0894\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0844\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0787\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0777\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0730\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0734\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0729\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0740\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0720\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0955\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0872\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0797\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0894\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0831\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1067\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0936\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0942\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0801\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0755\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0707\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0715\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0738\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0723\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0726\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0730\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0693\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0703\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0668\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0662\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0626\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0673\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0675\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0710\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0678\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0761\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0729\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0734\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0816\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0843\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0806\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0809\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0736\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0724\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0653\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0654\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0621\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0641\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0710\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0771\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0722\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0678\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0757\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0729\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:51,367] Trial 52 finished with value: 1.8744747828031354 and parameters: {'lr': 0.012295315810679736, 'alpha': 0.03775346553345508, 'activation': 'relu', 'n1': 128, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 23.8999\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.5267\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.5698\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.7576\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5975\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1576\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3040\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9179\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8941\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1408\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5856\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1856\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8872\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6786\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5187\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4146\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3340\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2723\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2203\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1912\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1700\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1538\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1306\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1270\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1256\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1251\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1203\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1245\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1100\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1223\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1230\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1112\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1036\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1052\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1028\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0955\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1008\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1014\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0965\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1041\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0883\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0912\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1049\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1141\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1050\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1087\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1149\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1162\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1210\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1181\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1142\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1241\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1075\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1040\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0985\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0980\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1155\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1238\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1294\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1192\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1075\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1035\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0967\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1017\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1021\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1193\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1086\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1000\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1033\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0985\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0970\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1011\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0997\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0954\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0938\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0957\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0964\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1090\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1078\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0944\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1193\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1459\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1306\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1085\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1061\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1178\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1309\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1593\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1577\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1446\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1336\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1343\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1367\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1157\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1138\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1166\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1092\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:53,502] Trial 53 finished with value: 1.8798354399120738 and parameters: {'lr': 0.016321334479236005, 'alpha': 0.04936021858860191, 'activation': 'relu', 'n1': 192, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7708\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1798\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8170\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6936\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6361\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6006\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5865\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5786\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5723\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5698\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5667\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5635\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5625\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5611\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5586\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5580\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5560\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5546\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5533\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5533\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5514\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5497\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5484\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5466\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5463\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5444\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5433\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5434\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5408\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5405\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5391\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5377\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5362\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5357\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5337\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5327\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5313\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5312\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5296\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5281\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5268\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5257\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5244\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5232\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5222\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5214\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5201\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5190\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5173\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5162\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5153\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5138\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5135\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5116\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5112\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5106\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5087\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5070\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5071\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5060\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5031\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5039\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5018\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5016\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4991\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4987\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4974\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4960\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4948\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4934\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4927\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4919\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4902\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4893\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4889\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4870\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4855\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4856\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4835\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4830\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4813\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4805\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4800\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4783\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4783\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4770\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4753\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4746\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4736\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4721\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4710\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4700\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4687\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4680\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4674\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4663\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4647\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4650\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4624\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4619\n",
      "4/4 [==============================] - 0s 1000us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:55,627] Trial 54 finished with value: 2.1470805369649195 and parameters: {'lr': 0.007486539100886744, 'alpha': 0.0009571614030680792, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4657\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6586\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1982\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0404\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8972\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7832\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6977\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.6228\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5492\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4824\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4139\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3487\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2860\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2245\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1629\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1040\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0452\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9874\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9310\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.8758\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8205\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7663\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7130\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6604\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6094\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5580\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5081\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4590\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4101\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3621\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3145\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2679\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2217\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1770\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1316\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0879\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0441\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0023\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9596\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9179\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8766\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8366\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7963\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7570\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7184\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6805\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6427\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6052\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5685\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5326\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4974\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4620\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4280\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3934\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3609\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3273\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2942\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2620\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2315\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2002\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1685\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1387\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1085\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0793\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0496\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0210\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9928\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9648\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9377\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9102\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8841\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8581\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8313\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8056\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7809\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7555\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7313\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7075\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6829\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6601\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6365\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6138\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5917\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5690\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5483\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5264\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5046\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4838\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4630\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4427\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4225\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4028\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3831\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3643\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3458\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3271\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3082\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2906\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2715\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2545\n",
      "4/4 [==============================] - 0s 837us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:57,778] Trial 55 finished with value: 2.1174724363580015 and parameters: {'lr': 0.005558925370928615, 'alpha': 0.00798059282945104, 'activation': 'relu', 'n1': 256, 'n2': 192}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.3660\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6320\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1645\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.9975\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8769\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7990\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7438\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6926\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6491\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6095\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5733\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5380\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5050\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4720\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.4401\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4088\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3781\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3470\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3168\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2873\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2576\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2282\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1989\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1699\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1415\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1127\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0847\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0566\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0286\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0009\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9731\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9456\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9181\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8910\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8639\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8371\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.8104\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.7839\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7570\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7307\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7042\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6782\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6521\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6262\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6006\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5749\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5493\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5239\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4987\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4738\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4486\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4236\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3989\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3742\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3499\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3253\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3010\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2768\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2529\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2291\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2051\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1811\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1574\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1341\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1104\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0872\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0641\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0407\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.0179\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9950\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9721\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9495\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9268\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9043\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8823\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8598\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8379\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8155\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7937\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7719\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7499\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7284\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7070\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6853\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6642\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6428\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6214\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6006\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5796\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.5587\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5377\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5172\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4966\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4762\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4560\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4354\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4152\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3953\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3750\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3552\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:06:59,884] Trial 56 finished with value: 2.7024646578789078 and parameters: {'lr': 0.0010897042904421453, 'alpha': 0.013169681148261513, 'activation': 'relu', 'n1': 192, 'n2': 192}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.9251\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.4001\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.2276\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.9246\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.7456\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6317\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.6157\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.6814\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.8304\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0542\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3477\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7032\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1155\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5792\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0895\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6455\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2387\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8683\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5321\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2253\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9493\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6906\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4581\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2446\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0538\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8794\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7187\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5795\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4433\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3263\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2192\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1206\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0287\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9544\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8731\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8000\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7307\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6799\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6207\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5771\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5262\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4859\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4562\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4265\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3840\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3610\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3279\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3045\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2837\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2617\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2461\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2292\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2193\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2100\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1942\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1978\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2165\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1760\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1677\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1556\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1498\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1349\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1239\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1194\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1141\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1113\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1113\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1077\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1110\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1178\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1160\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0993\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1036\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0943\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0858\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0891\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0909\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0855\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0826\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0787\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0751\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0744\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0740\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0765\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0754\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0739\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0759\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0755\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0751\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0953\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0785\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0778\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0753\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0707\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0657\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0676\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0625\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0654\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:02,018] Trial 57 finished with value: 2.2720629195896094 and parameters: {'lr': 0.009347710790043571, 'alpha': 0.029236351119286384, 'activation': 'tanh', 'n1': 256, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6591\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8699\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5952\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4020\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.2233\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0708\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9300\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7889\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6547\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5273\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4055\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2834\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1664\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0529\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.9404\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8321\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7243\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6202\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5189\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4200\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3225\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2272\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1343\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0434\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9553\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.8678\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7833\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7007\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6194\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5401\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4625\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3868\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3124\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2402\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1689\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1000\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0320\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9671\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9016\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8380\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7755\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7155\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6557\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5978\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5412\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4862\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4317\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3783\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3264\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2761\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2266\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1779\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1311\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0842\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0398\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9951\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9515\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9092\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8686\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8284\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7883\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7494\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7116\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6750\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6381\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6032\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5685\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5343\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5015\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4690\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4376\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4066\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3760\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3462\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3179\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2890\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2615\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2349\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2075\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1827\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1565\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1320\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1081\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0840\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0619\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0389\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0161\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9946\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9733\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9530\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9322\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9126\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8931\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8746\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8563\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8382\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8201\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8032\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7853\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7695\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:04,147] Trial 58 finished with value: 1.868977357253303 and parameters: {'lr': 0.004161099139392123, 'alpha': 0.01664290204956268, 'activation': 'relu', 'n1': 128, 'n2': 256}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.3167\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.1677\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.6798\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.2628\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.8575\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.4753\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.1113\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.7580\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.4133\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.0812\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.7566\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.4404\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.1332\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.8335\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.5407\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.2568\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.9785\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.7078\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.4442\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.1871\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.9358\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.6911\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4523\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.2195\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.9930\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7713\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5556\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.3453\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.1402\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9399\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7446\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5542\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3686\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1878\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0109\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8390\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6710\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5081\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3478\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1920\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.0397\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8919\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7472\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6063\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4691\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3352\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2042\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0767\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9523\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.8314\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7131\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5977\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4855\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3756\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2694\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1646\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0628\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9634\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8673\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7730\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6807\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5908\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5033\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4183\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3344\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2535\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1743\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0968\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0217\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9480\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8765\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8067\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7379\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6713\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6071\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5433\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4821\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4218\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3627\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3060\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2498\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1954\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1425\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0903\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0405\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9910\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9424\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8958\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8500\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8055\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.7617\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7194\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6778\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6380\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5988\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5603\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5226\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4864\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4501\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.4158\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:06,288] Trial 59 finished with value: 1.9848127944914173 and parameters: {'lr': 0.0032247342236219373, 'alpha': 0.023949823242467923, 'activation': 'relu', 'n1': 320, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 29.6096\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 26.8062\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 23.5405\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.3060\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.4186\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.8854\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.7025\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 10.8228\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.2221\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.8558\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6934\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7046\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8641\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1486\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5409\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0210\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5797\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2056\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8877\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6192\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3885\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1937\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0246\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8891\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7668\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6632\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5770\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5112\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4490\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3972\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3597\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3160\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2807\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2483\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2214\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1952\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1760\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1661\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1484\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1391\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1236\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1211\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1225\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1208\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1102\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1105\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1020\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0976\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0891\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0867\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0827\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0778\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0776\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0774\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0755\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0875\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0994\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0863\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0811\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0827\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0784\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0875\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0812\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0770\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0779\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0745\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0814\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0842\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0858\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0746\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0818\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0726\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0707\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0702\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0701\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0659\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0689\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0664\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0654\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0618\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0657\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0645\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0617\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0663\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0733\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0732\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0815\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0923\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0944\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0900\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0804\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0817\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0755\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0751\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0709\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0679\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0645\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0649\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0645\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:08,316] Trial 60 finished with value: 1.9970807862139188 and parameters: {'lr': 0.011705896575735364, 'alpha': 0.03913218859464359, 'activation': 'relu', 'n1': 320, 'n2': 192}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5310\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7714\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4746\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2824\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0776\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9083\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.7600\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6159\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4705\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3358\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2048\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0788\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9569\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8382\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7218\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6098\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4989\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3914\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2869\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1852\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0853\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9877\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8928\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 3.8000\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7105\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 3.6216\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5357\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4522\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3699\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2900\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2120\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1359\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0612\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9890\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9178\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8490\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7814\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7170\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6522\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5891\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5276\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4683\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4096\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3527\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2973\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2433\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1901\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1381\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0877\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0388\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9909\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9438\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8987\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8535\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.8107\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.7677\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7259\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6856\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6471\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6093\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5706\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5341\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4978\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4636\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4286\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3956\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3631\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3311\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3006\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2700\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2412\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2120\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1838\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1555\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1295\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1024\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0770\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0525\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0272\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0045\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9805\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9582\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9365\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9144\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.8947\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8737\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8529\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8336\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8141\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7958\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7770\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7596\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7419\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7257\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7094\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6937\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6770\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6620\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6460\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6322\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:10,369] Trial 61 finished with value: 2.0197571716309826 and parameters: {'lr': 0.0047548429995717244, 'alpha': 0.015781810836925685, 'activation': 'relu', 'n1': 128, 'n2': 320}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1557\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4559\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1562\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9141\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7072\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5241\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3576\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1982\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.0379\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8871\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7403\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5982\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4600\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3257\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1944\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0679\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9436\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8224\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7049\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5907\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4785\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3691\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2628\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1589\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0588\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9595\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8636\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7703\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6786\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5896\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5028\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4178\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3348\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2544\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1753\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0991\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0239\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9522\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8806\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8107\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7424\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6768\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6115\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5487\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4875\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4275\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3693\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3117\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2561\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2019\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1490\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0970\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0474\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9975\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9505\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9035\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8572\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8135\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7708\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7286\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6866\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6463\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6069\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5694\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5310\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4951\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4594\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4244\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3909\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3579\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3259\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2941\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2630\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2327\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2042\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1751\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1472\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1212\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0934\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0686\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0425\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0182\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9949\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9708\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9493\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9266\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9043\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8836\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8623\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8427\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8222\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8034\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7846\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7667\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7493\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7318\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7143\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6981\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6810\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6660\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:12,547] Trial 62 finished with value: 2.026063135010921 and parameters: {'lr': 0.004260569805780942, 'alpha': 0.01797332408852466, 'activation': 'relu', 'n1': 128, 'n2': 256}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9357\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0619\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8516\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7255\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6055\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5198\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4401\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3744\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3125\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2534\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1986\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1416\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0863\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0326\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9805\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9305\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8794\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8300\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7817\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7352\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6885\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6423\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5976\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5531\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5108\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.4676\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4260\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3863\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3451\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3055\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2664\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2281\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1904\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1544\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1169\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0814\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0460\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0127\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9786\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9448\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9116\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8798\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8477\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8166\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7862\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7561\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7268\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6979\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6688\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6407\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6134\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5859\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5602\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5331\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5080\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4833\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4574\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4330\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4100\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3866\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3619\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3410\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3176\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2963\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2733\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2532\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2318\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2108\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1910\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1710\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1525\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1330\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1149\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0953\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0785\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0589\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0409\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0251\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0068\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9904\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9734\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9576\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9421\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9257\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9117\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8965\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8811\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8668\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8518\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8385\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8238\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8105\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7969\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7845\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7716\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7592\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7462\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7348\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7214\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7101\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:14,753] Trial 63 finished with value: 2.1172867821992747 and parameters: {'lr': 0.005726060167662867, 'alpha': 0.009347709881279778, 'activation': 'relu', 'n1': 128, 'n2': 256}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.4790\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.5162\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.9679\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.4480\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.9878\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.5413\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.1092\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.6991\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.2961\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.9084\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.5310\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.1641\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.8086\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.4623\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.1254\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.7986\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.4801\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.1703\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8694\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.5768\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2917\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.0146\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.7450\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4825\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.2278\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.9791\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7378\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5030\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2744\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0521\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8357\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6252\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4204\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2213\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0273\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8389\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6557\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.4775\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3033\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1343\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9697\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8100\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6543\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5030\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3558\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2126\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0729\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9372\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.8051\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6771\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5518\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4303\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3121\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1969\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0854\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9761\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8700\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7668\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6668\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5693\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4741\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3812\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2917\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2044\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1187\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0362\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9555\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8768\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8006\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7264\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6541\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5840\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5153\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4487\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3845\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3210\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2603\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2003\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1420\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0859\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0306\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9772\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9255\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8744\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8259\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7777\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7306\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6855\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6411\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5984\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5562\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5159\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4760\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4380\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4007\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3640\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3283\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2941\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2598\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2276\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:16,913] Trial 64 finished with value: 2.014901874196543 and parameters: {'lr': 0.002626713517356385, 'alpha': 0.0321633221224034, 'activation': 'relu', 'n1': 192, 'n2': 256}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 25.3123\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 24.6219\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 23.9683\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 23.4439\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 22.9696\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 22.5169\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 22.0837\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 21.6641\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 21.2537\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.8556\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20.4643\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.0812\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.7065\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.3402\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.9801\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.6279\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.2824\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.9428\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.6103\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.2845\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.9642\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.6501\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.3419\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.0395\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.7432\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.4517\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.1664\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.8862\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.6111\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.3412\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.0761\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.8162\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.5609\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.3105\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.0647\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.8236\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.5870\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.3548\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.1266\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.9029\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.6832\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.4678\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.2563\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.0487\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8451\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.6451\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.4489\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2563\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.0673\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.8820\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.6997\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.5211\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.3458\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1736\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0050\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8391\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6763\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5167\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.3601\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2065\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0554\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.9070\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7618\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6192\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4789\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3416\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2068\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0741\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9444\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8168\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6916\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5689\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.4482\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3297\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2141\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0997\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9882\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8780\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7704\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.6646\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5607\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.4590\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.3591\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2607\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1646\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0700\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9770\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8861\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7967\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7090\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6228\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5384\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4554\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3741\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2944\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2157\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1388\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0634\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9890\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9164\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:19,083] Trial 65 finished with value: 2.019633121081428 and parameters: {'lr': 0.0013474490510396763, 'alpha': 0.042990415043100874, 'activation': 'relu', 'n1': 192, 'n2': 320}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.7747\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.2977\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.3436\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.4001\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.5948\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.8413\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.1460\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.4898\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2810\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.7278\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.2050\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7118\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2456\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8038\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3878\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9925\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6190\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2665\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9336\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.6175\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.3180\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0353\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7672\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5149\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2747\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0484\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8356\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6311\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4403\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.2586\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0869\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9235\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7701\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6237\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4867\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3565\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2339\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1159\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0055\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9009\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8023\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7089\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6219\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5377\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4585\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3828\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3120\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2444\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1820\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1224\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0633\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0111\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9601\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9128\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8684\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8231\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7825\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7470\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7115\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6746\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6405\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6077\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5823\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5521\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5517\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5185\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4932\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4683\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4392\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4177\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3998\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3796\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3614\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3472\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3291\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3142\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3014\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2888\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2756\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2631\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2527\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2432\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2325\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2265\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2162\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2069\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1982\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1910\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1833\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1760\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1701\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1631\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1586\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1553\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1530\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1465\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1403\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1337\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1296\n",
      "4/4 [==============================] - 0s 885us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:21,394] Trial 66 finished with value: 2.084658020003696 and parameters: {'lr': 0.007558600437393456, 'alpha': 0.022186816279345836, 'activation': 'gelu', 'n1': 320, 'n2': 192}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7926\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0282\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6070\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4262\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1560\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9170\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 6.7109\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 6.5189\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3365\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 6.1634\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9957\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8337\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 5.6774\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5255\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.3770\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2342\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0936\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9574\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8258\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6979\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5723\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4503\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3316\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2158\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1046\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9946\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8886\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7857\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6849\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5871\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4917\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3991\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3085\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2213\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1352\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0527\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9717\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8941\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8168\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7421\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6692\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5993\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5297\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4632\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3982\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3351\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2736\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2131\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1548\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0982\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0431\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9890\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9373\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8858\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8371\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7885\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7411\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6958\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6522\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6096\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5664\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5260\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4857\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4476\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4094\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3726\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3369\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3022\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2685\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2355\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2039\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1728\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1414\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1115\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0834\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0544\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0271\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0013\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9745\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9499\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9248\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9013\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8787\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8555\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8352\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8134\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7920\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7721\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7518\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7331\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7129\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6954\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6773\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6608\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6441\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6280\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6116\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5969\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5806\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5666\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:23,503] Trial 67 finished with value: 2.0234412123780925 and parameters: {'lr': 0.006342421409116605, 'alpha': 0.013214797378485044, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4472\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9039\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5657\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3926\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2827\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2136\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1613\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1239\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0937\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0696\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0495\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0332\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0196\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0067\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9967\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9864\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9782\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9695\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9624\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9562\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9501\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9448\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9391\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9342\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9299\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9254\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9215\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9175\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9137\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9103\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9066\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9034\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9000\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8970\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8939\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8911\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8882\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8854\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8827\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8800\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8774\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8750\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8723\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8699\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8675\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8651\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8628\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8604\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8582\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8559\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8536\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8515\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8493\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8471\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8450\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8429\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8408\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8386\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8367\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8347\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8326\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8304\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8284\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8265\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8244\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8224\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8205\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8184\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8165\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8146\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8126\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8107\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8087\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8068\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8051\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8030\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8013\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7992\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7973\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7954\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7935\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7916\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7899\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7879\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7861\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7842\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7823\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7805\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7786\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7768\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7749\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7731\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7713\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7695\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7678\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7658\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7640\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7623\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7604\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7586\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:25,605] Trial 68 finished with value: 2.543905464343335 and parameters: {'lr': 0.0006361056607724142, 'alpha': 0.005024704983179999, 'activation': 'relu', 'n1': 128, 'n2': 192}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0661\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2880\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8917\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8167\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7057\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5687\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4932\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4165\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3580\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3288\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3096\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2897\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2844\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2637\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2549\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2434\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2370\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2326\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2318\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2289\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2253\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2231\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2280\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2184\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2153\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2146\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2127\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2161\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2134\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2177\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2141\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2120\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2082\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2057\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2061\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2053\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2069\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2025\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2015\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2080\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2050\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2007\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1997\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2012\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2038\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2019\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1996\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2005\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2007\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1993\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1963\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1954\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1953\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1958\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1941\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1933\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1928\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1940\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1938\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1986\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1967\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1940\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1916\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1908\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1907\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1909\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1896\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1916\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1918\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1928\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1904\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1889\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1884\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1891\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1920\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1910\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1909\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1927\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1923\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1916\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1905\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1902\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1876\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1905\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1882\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1905\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1904\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1876\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1852\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1854\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1847\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1844\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1844\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1857\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1854\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1833\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1831\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1836\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1833\n",
      "4/4 [==============================] - 0s 963us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:27,786] Trial 69 finished with value: 2.1803837198680687 and parameters: {'lr': 0.013667562039020172, 'alpha': 0.0002687389452813897, 'activation': 'sigmoid', 'n1': 320, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.1528\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.8512\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.3459\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.9395\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.5963\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.2661\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.9523\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.6574\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.3658\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.0844\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.8075\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.5366\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.2714\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.0111\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.7548\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.5041\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.2571\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 14.0143\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.7762\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.5424\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.3121\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.0860\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.8636\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.6451\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.4307\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.2193\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.0122\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 11.8085\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.6082\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.4113\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.2174\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.0273\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8403\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 10.6565\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.4756\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 10.2981\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.1237\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 9.9524\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 9.7833\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.6175\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4544\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.2944\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1369\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.9821\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8301\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 8.6807\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5335\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 8.3890\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 8.2470\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1077\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.9703\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8355\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7030\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5726\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4450\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3188\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 7.1949\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 7.0734\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 6.9541\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8367\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7210\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.6071\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4957\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3862\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2780\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1721\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0679\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9653\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 5.8648\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7658\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6685\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5731\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.4788\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3864\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2961\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2064\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1192\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0324\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9479\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8645\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7824\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7019\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.6230\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5448\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4687\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3934\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3192\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2468\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1753\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1051\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0359\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9683\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9015\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8362\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7720\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7083\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6461\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5851\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5246\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4657\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:29,774] Trial 70 finished with value: 2.0877816255482284 and parameters: {'lr': 0.0019235455143018164, 'alpha': 0.027652264186557158, 'activation': 'relu', 'n1': 320, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.6157\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.3775\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.1054\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.9425\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8531\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8717\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9914\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2154\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5223\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 4.9098\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3651\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8820\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4543\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0742\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7361\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4373\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1709\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9345\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7260\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5411\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3771\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2305\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0999\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9830\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8823\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7883\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7061\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6413\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5757\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5213\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4756\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4346\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3920\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3559\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3218\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2926\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2706\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2542\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2354\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2155\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2009\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1914\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1774\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1590\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1466\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1351\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1264\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1191\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1099\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1002\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0961\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0943\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0918\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0915\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0962\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1040\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0892\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0910\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0904\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0804\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0782\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0711\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0708\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0682\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0683\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0659\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0696\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0697\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0671\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0706\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0701\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0678\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0664\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0678\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0620\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0620\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0623\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0635\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0633\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0647\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0661\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0636\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0618\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0671\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0734\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0689\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0683\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0701\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0638\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0615\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0613\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0607\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0591\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0656\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0634\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0584\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0598\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0578\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0551\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:31,801] Trial 71 finished with value: 1.9047996773489146 and parameters: {'lr': 0.009656622085842623, 'alpha': 0.03595322974104714, 'activation': 'relu', 'n1': 128, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.4968\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.6451\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.8115\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.8338\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1797\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7599\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5663\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5766\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7621\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0970\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5509\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1048\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7385\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4379\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1922\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9915\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8251\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6938\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5857\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5024\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4277\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3686\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3242\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2777\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2462\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2145\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1934\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1818\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1724\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1565\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1595\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1466\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1342\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1199\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1119\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1008\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0982\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1014\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1012\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0954\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0854\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0895\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1028\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1072\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0918\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0889\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0842\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0793\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0750\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0817\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0744\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0737\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0754\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0760\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0807\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0838\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1029\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0956\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0953\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0953\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0899\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0981\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0907\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0797\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0796\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0767\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0758\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0756\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0825\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0957\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0993\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0956\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0845\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0844\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0876\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0798\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0764\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0703\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0712\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0757\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0804\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0866\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0949\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0942\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1153\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1189\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1028\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1039\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1017\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0854\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0772\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0718\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0706\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0742\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0760\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0815\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0815\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0820\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0738\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:33,952] Trial 72 finished with value: 1.9598290401553409 and parameters: {'lr': 0.013195626316638525, 'alpha': 0.041891843054138, 'activation': 'relu', 'n1': 128, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.9653\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9549\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5732\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2920\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0415\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8240\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6261\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4434\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.2646\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0957\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9296\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7697\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6150\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4650\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3182\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1769\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0385\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9040\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7739\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6475\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5239\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4038\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2871\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1733\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0634\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9554\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8509\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7497\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6504\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5540\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4601\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3689\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2799\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1936\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1093\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0276\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9480\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8708\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7947\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7213\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6494\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5802\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5123\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4465\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3824\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3202\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2592\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1999\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1422\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.0866\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0318\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9787\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9273\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8768\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8286\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7805\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7340\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6889\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6456\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6030\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5609\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5200\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4809\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4430\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4048\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3690\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3335\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2987\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2654\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2328\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2013\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1703\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1400\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1107\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0827\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0545\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0278\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0016\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9754\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9512\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9263\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9029\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8804\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8575\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8368\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8151\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7940\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7742\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7546\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.7357\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7167\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6991\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6812\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6648\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6485\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6323\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6159\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6010\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5853\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5714\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:36,064] Trial 73 finished with value: 1.9126222071405021 and parameters: {'lr': 0.0039035202315495948, 'alpha': 0.021421885681414142, 'activation': 'relu', 'n1': 128, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.6645\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.4246\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.1991\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.0263\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0979\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4666\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1057\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9865\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0718\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3271\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7193\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2277\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8267\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4998\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2358\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0187\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8431\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6997\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5841\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4944\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4225\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3569\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3108\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2675\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2365\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2038\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1806\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1652\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1463\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1373\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1332\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1359\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1423\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1556\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1267\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1265\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1108\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1059\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0962\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0896\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0852\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0884\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0924\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0853\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0913\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0874\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0929\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0886\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0954\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0910\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0959\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0930\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1023\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0916\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1030\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0977\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0854\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0949\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0849\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1017\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0875\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0869\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0838\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0840\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0882\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0927\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0969\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0896\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0919\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0847\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0873\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0780\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0903\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0810\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0836\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0785\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0852\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0818\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0952\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1098\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0868\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0891\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0907\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0841\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0808\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0735\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0721\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0687\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0744\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0956\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0955\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0842\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0963\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0853\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:38,165] Trial 74 finished with value: 1.8864464481296288 and parameters: {'lr': 0.011953629483621999, 'alpha': 0.04777473854010333, 'activation': 'relu', 'n1': 128, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.1521\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.9064\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.4564\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.0795\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.8940\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0776\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5656\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3292\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3222\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5083\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8495\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3188\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8896\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5438\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2649\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0525\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8768\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7571\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6642\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5807\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4540\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3791\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3247\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2795\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2459\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2155\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1890\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1744\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1581\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1521\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1595\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1597\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1450\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1475\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1374\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1236\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1146\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1079\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1012\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1004\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0935\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1008\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1323\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1563\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1343\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1251\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1042\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0959\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0910\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0860\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0822\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0849\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0861\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0953\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1006\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1305\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1333\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1209\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1023\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1004\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0919\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0955\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0936\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0871\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0845\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0852\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0859\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0895\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0883\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0958\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0938\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0878\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0880\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0943\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0895\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0897\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0860\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0886\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0950\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0893\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0876\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0915\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0811\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0897\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0906\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0988\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0949\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0956\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0930\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0887\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0890\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0842\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0849\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0818\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0846\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0826\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0792\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0801\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0915\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0925\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:40,286] Trial 75 finished with value: 2.3144573906515125 and parameters: {'lr': 0.01693798725339765, 'alpha': 0.034286218291026016, 'activation': 'tanh', 'n1': 192, 'n2': 256}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.3684\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4304\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 8.6642\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8932\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2157\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5908\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0365\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5176\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0481\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6266\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2355\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8818\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5582\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2622\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9911\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7438\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5168\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3093\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1203\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9477\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7901\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6444\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.5095\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.3866\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2758\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1708\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0766\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9964\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9135\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8430\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7788\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7208\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6629\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6137\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5648\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5218\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4857\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4563\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4245\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3961\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3666\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3471\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3215\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2982\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2736\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2537\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2359\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2212\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2065\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1933\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1815\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1701\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1606\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1510\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1437\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1407\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1372\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1250\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1246\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1223\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1103\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1073\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0983\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0948\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0907\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0868\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0866\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0831\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0791\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0753\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0736\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0720\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0718\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0719\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0718\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0675\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0645\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0627\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0633\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0639\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0654\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0677\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0688\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0674\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0694\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0719\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0649\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0671\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0663\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0613\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0598\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0584\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0559\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0558\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0556\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0562\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0521\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0526\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0518\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0508\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:42,345] Trial 76 finished with value: 1.8795279246001695 and parameters: {'lr': 0.009919108772772222, 'alpha': 0.02636876347315577, 'activation': 'relu', 'n1': 128, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6916\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.7608\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2334\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.6356\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1953\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7673\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.3715\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9970\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6403\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.3062\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.9841\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6776\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3861\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1073\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8406\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5867\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3430\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1104\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8885\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6772\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4739\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2798\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0941\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9158\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7476\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5843\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4293\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2823\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1394\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0048\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8754\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7513\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6318\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5191\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4102\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3072\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2084\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1147\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0238\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9367\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8536\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7750\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6979\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6259\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5568\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4904\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4274\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3671\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3088\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2542\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2005\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1488\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1025\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0553\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0108\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9686\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9273\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8897\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8544\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8196\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7834\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7534\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7210\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6934\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6652\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6398\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6137\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5913\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5678\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5449\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5245\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5027\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4817\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4622\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4415\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4238\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4052\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3896\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3738\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3588\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3439\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3315\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3188\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3064\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2978\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2868\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2744\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2646\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2542\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2450\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2349\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2275\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2192\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2117\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2057\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1986\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1908\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1849\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1772\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1723\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:44,844] Trial 77 finished with value: 1.865445691160843 and parameters: {'lr': 0.007585298936430933, 'alpha': 0.018006697289553956, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.7107\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8817\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.5706\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.3349\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.1132\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.9148\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.7273\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5475\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.3640\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1928\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0234\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8588\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6975\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5398\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.3847\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2338\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.0843\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9382\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7951\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6545\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5161\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3803\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2472\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1164\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9885\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8620\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7385\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.6175\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4981\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3810\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2661\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1532\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0423\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9338\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8266\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7220\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6188\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5189\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4192\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.3213\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2255\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1316\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0393\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9486\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8599\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7727\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6866\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6022\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5195\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4385\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3590\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2807\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2039\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1282\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0549\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9817\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9102\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8398\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7717\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7042\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6374\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5725\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5084\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4459\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3837\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3233\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2640\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2054\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1485\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0919\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.0371\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9828\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9292\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8765\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8256\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7747\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7256\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6766\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6287\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5822\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5358\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4907\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4466\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4025\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3606\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3182\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2765\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2361\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1963\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1573\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1187\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0813\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0440\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0082\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9728\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9374\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9030\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8694\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8358\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8037\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:46,975] Trial 78 finished with value: 2.0119786943924316 and parameters: {'lr': 0.0033352711714537877, 'alpha': 0.01697122770642895, 'activation': 'relu', 'n1': 256, 'n2': 192}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6214\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.9064\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.3575\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.7907\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.3832\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0362\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7155\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.4119\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.1225\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8465\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5811\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3249\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0791\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8415\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6119\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3913\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1766\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9697\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7706\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5787\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3922\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2116\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0377\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8690\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7077\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5500\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3987\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2534\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1107\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9746\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8423\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7147\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5908\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4721\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3561\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2454\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1377\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0342\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9329\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8359\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7418\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6514\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5634\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4791\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3976\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3182\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2424\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1684\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0969\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0284\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9610\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8958\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8352\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.7748\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7166\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6607\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6053\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5537\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5045\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4567\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4070\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3612\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3169\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2755\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2332\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1944\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1554\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1174\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0811\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0474\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0137\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9811\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9499\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9190\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8912\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8613\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8332\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8082\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7826\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7575\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7328\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7104\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6887\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6667\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6478\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6267\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6066\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5877\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5697\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5521\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5345\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5185\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5021\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4876\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4739\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4608\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4452\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4318\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4176\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4058\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:49,236] Trial 79 finished with value: 2.0677590813033175 and parameters: {'lr': 0.007272412628731773, 'alpha': 0.014453085449172988, 'activation': 'gelu', 'n1': 256, 'n2': 384}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.3996\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.6037\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.0696\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5704\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0693\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6156\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1999\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7997\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4204\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0656\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7279\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4057\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1012\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8114\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5344\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2722\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.0211\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7827\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5561\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3416\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1360\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9397\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7533\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5752\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4075\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2460\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0927\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9491\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8082\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6776\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5521\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4323\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3172\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2089\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1047\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0066\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9131\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8249\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7398\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6584\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5810\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5085\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4372\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3715\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3077\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2473\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1892\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1351\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0822\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0334\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9862\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9398\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8983\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8574\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8185\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7822\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7460\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7121\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6824\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6542\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6235\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5987\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5732\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5512\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5268\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5051\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4837\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4648\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4446\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4239\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4076\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3880\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3694\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3520\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3360\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3210\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3069\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2944\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2826\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2703\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2586\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2491\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2390\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2291\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2233\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2158\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2052\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1983\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1905\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1837\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1764\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1713\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1663\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1605\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1557\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1506\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1453\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1412\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1353\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1302\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:51,324] Trial 80 finished with value: 1.849000878596612 and parameters: {'lr': 0.00824439682800439, 'alpha': 0.018072686105061794, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6838\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.7290\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2770\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.6411\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.1455\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6924\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2635\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8683\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4939\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1410\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8060\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4862\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1838\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8957\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6199\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3588\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.1089\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.8711\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6447\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4303\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2249\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0289\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8420\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6628\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4946\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3319\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.1781\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0325\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8915\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7591\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6322\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5110\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3948\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2855\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1799\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0804\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9854\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8962\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8093\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7259\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6472\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5725\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4998\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4322\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3671\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3048\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2461\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1902\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1360\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0856\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0375\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9893\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9508\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9040\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8644\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8260\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7883\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7544\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7230\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6924\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6597\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6331\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6047\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5794\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5548\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5318\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5089\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4891\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4689\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4486\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4324\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4137\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3965\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3791\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3606\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3444\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3290\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3162\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3030\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2902\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2776\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2678\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2570\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2467\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2393\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2309\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2213\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2140\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2050\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1978\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1888\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1828\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1771\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1700\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1654\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1590\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1532\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1489\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1422\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1381\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:53,435] Trial 81 finished with value: 1.8475760349320616 and parameters: {'lr': 0.00805965131096429, 'alpha': 0.018123716625583222, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5355\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9720\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5305\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0483\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7030\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3672\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0707\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7991\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5272\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2834\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0542\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8364\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6341\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4421\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2578\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0863\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9226\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7683\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6229\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4869\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3568\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2326\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1144\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0034\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8987\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7993\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7050\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6194\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5321\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4549\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3808\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3102\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2398\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1757\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1146\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0574\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0038\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9538\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9044\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8592\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8150\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7752\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7384\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7074\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6697\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6318\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5981\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5690\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5396\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5153\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4887\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4646\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4449\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4245\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4050\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3927\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3765\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3484\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3351\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3205\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3019\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2927\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2768\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2667\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2530\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2446\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2323\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2271\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2145\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2039\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1935\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1875\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1775\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1691\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1649\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1571\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1495\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1436\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1394\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1349\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1309\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1281\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1292\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1235\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1273\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1147\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1095\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1037\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0997\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0937\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0901\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0872\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0849\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0822\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0810\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0786\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0758\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0744\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0721\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0705\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:55,540] Trial 82 finished with value: 1.8693278647217035 and parameters: {'lr': 0.014616056614375324, 'alpha': 0.011346503271688136, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.9021\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.9728\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.2555\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.5494\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.8942\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.2989\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7444\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2241\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7369\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2832\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8545\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4520\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0742\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.7186\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.3834\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0697\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7730\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4942\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2326\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9877\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7552\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5366\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3311\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1369\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9559\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.7841\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6229\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4732\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3285\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1959\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0703\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9514\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8376\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7325\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6325\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5396\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4520\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3711\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2936\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2210\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1511\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0886\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0279\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9790\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9203\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8681\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8174\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7736\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7289\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6890\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6517\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6155\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5859\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5515\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5208\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4939\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4675\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4410\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4208\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4021\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3779\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3622\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3417\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3271\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3116\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2963\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2826\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2711\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2599\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2437\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2331\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2225\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2087\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1990\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1891\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1822\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1709\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1651\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1577\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1503\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1448\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1388\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1336\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1277\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1256\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1252\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1183\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1173\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1116\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1072\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1023\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0999\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0973\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0917\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0929\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0889\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0864\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0830\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0786\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0759\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:57,688] Trial 83 finished with value: 1.8589799259864115 and parameters: {'lr': 0.00889965946619541, 'alpha': 0.020595121085819294, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 49 with value: 1.8254005014245944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.2468\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.3673\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6666\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8493\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.1824\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5647\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0018\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4721\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.9725\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5070\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0694\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6584\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2731\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9102\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5684\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2477\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9444\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6593\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3915\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1400\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9019\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6777\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4665\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2671\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0813\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9038\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7378\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5832\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4344\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2968\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1662\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0429\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9260\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8173\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7139\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6175\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5263\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4423\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3617\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2851\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2127\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1467\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0804\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0212\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9642\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9116\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8606\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8143\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7684\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7276\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6876\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6492\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6159\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5823\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5513\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5245\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4958\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4700\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4464\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4240\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3996\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3796\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3586\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3420\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3241\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3099\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2959\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2825\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2712\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2611\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2442\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2297\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2212\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2061\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1975\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1864\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1782\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1685\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1605\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1528\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1476\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1405\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1341\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1313\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1272\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1202\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1170\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1120\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1082\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1039\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1011\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0986\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0952\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0941\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0916\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0887\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0872\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0845\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0809\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:07:59,808] Trial 84 finished with value: 1.8050954250882123 and parameters: {'lr': 0.008707837531230094, 'alpha': 0.020786563242333297, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.0661\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.9655\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.2610\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.5618\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.9089\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.3312\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8011\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.2956\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8273\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3900\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9788\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5902\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2257\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8810\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5556\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2494\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9592\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.6855\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4279\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1854\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9548\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7372\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5319\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3376\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1561\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9822\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8194\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6667\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5199\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3837\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2538\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1309\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0145\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9057\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8018\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7048\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6130\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5274\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4452\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3669\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2935\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2247\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1576\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0964\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0381\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9830\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9308\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8825\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8351\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7927\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7514\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7104\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6758\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6401\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6065\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5760\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5463\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5183\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4941\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4738\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4467\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4324\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4085\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3925\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3745\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3555\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3385\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3230\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3065\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2886\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2769\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2623\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2476\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2359\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2260\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2148\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2033\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1956\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1873\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1791\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1703\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1642\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1576\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1507\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1481\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1434\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1359\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1327\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1277\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1241\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1187\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1167\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1146\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1096\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1096\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1084\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1035\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0993\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0947\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0902\n",
      "4/4 [==============================] - 0s 1000us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:01,927] Trial 85 finished with value: 1.9088530605681968 and parameters: {'lr': 0.008581755849567359, 'alpha': 0.02017474108281178, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.3510\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.2067\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.8319\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.7220\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.7147\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.7814\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.9296\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1577\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4493\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.8048\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2136\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6767\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1878\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7408\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3322\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9612\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6209\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3103\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0278\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7705\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5352\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3195\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1223\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9415\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7791\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6275\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4910\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3695\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2525\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1509\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0588\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9740\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8966\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8243\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7553\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6926\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6387\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5880\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5411\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4999\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4607\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4286\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3971\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3707\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3385\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3130\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2881\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2675\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2471\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2315\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2164\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2026\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1926\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1838\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1823\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1762\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1744\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1505\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1432\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1368\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1247\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1191\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1076\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1042\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0993\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0963\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0923\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0912\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0867\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0826\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0797\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0816\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0780\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0781\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0777\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0715\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0682\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0684\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0672\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0677\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0650\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0664\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0640\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0629\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0632\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0643\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0619\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0660\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0679\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0625\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0600\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0578\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0572\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0547\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0611\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0579\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0579\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0555\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0538\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0528\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:04,096] Trial 86 finished with value: 1.876296759397036 and parameters: {'lr': 0.011110277986149534, 'alpha': 0.024261406496676973, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0441\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6491\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3106\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7283\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2479\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8216\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.4863\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2108\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9390\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6924\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4689\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2610\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0684\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8957\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7187\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5646\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4200\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2662\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1367\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0207\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8992\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8012\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6844\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5878\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5020\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4157\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3465\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2654\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2081\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1368\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0630\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0018\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9461\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8942\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8503\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8131\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7614\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7231\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6876\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6497\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6106\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5904\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5628\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5378\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5032\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4766\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4483\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4283\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4087\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4054\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3827\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3538\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3424\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3321\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3174\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2938\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2790\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2702\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2594\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2765\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2491\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2444\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2430\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2231\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2096\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2068\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2026\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1948\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1989\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1751\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1804\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1735\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1641\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1589\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1698\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1507\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1518\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1511\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1521\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1335\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1376\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1559\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1638\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1568\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1959\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1753\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1420\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1556\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1806\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2745\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2362\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2107\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1425\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1286\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1164\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1152\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1096\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1035\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1101\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1022\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:06,197] Trial 87 finished with value: 2.3522943618652983 and parameters: {'lr': 0.018416449156888764, 'alpha': 0.010115273494956436, 'activation': 'sigmoid', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.6224\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5724\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.0780\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5959\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2577\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9579\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6781\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4093\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1577\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9202\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6926\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4739\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2657\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0649\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8717\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6874\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5087\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3374\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1735\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0168\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8643\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7179\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5769\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.4409\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3124\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1860\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0657\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9521\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8390\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7334\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6303\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5311\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4354\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3443\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2555\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1714\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0902\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0132\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9371\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8644\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7947\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7280\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6623\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6007\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5413\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4840\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4287\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3764\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3248\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2764\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2289\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1829\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1407\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0982\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0578\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0194\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9816\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9447\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9119\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8799\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8455\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8167\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7856\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7583\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7307\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7058\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6796\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6575\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6345\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6111\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5916\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5697\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5496\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5285\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5092\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4902\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4726\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4561\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4396\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4237\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4077\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3945\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3810\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3679\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3582\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3461\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3327\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3226\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3108\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3010\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2893\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2804\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2706\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2618\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2552\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2471\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2376\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2312\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2221\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2156\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:08,319] Trial 88 finished with value: 1.815602878789126 and parameters: {'lr': 0.008539531313631443, 'alpha': 0.013961220404542339, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1278\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5523\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2175\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9239\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7727\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6769\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.6086\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5635\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5265\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4952\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4704\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4498\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4328\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4173\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4040\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3917\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3813\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3710\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3620\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3537\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3462\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3393\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3323\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3261\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3202\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3142\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3089\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3037\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2988\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2940\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2891\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2846\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2800\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2756\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2716\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2674\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2633\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.2593\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2555\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2516\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2478\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2443\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2404\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2368\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2333\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2297\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2263\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2228\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2194\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2160\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2126\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2094\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2060\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2028\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1995\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1963\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1931\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1899\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1867\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1836\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1804\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1772\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1741\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1710\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1679\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1648\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1618\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1587\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1557\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1527\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1496\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1467\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1436\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1406\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1376\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1346\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1317\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1287\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1257\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1228\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1199\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1169\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1141\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1111\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1082\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1053\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1024\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0996\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0966\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0938\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0909\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0880\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.0852\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0823\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0795\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0766\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0738\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0710\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0681\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0653\n",
      "4/4 [==============================] - 0s 1000us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:10,436] Trial 89 finished with value: 3.1430040321695363 and parameters: {'lr': 0.000285295565368326, 'alpha': 0.007414871033854456, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.2743\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.3898\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0637\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6412\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3021\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0089\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7233\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4649\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2184\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9845\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7597\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5449\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3387\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1409\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9498\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7666\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5896\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4198\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2564\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1005\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9491\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.8029\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6624\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5264\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3975\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2716\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1511\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0369\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.9235\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8173\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7141\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6142\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.5172\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4256\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3362\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2507\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1686\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0907\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0140\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9399\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8693\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8013\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7347\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6722\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6112\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5525\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4960\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4425\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3894\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3402\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2914\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2433\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2001\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1566\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1153\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0753\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0360\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9986\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9643\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9314\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8962\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8663\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8345\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8054\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7756\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7500\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7245\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6992\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6740\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6515\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6303\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6082\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5882\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5668\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5467\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5248\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5068\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4893\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4729\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4558\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4395\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4250\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4108\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3967\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3871\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3737\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3596\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3479\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3359\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3249\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3128\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3030\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2931\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2838\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2761\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2670\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2579\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2415\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2347\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:12,585] Trial 90 finished with value: 1.895565274762255 and parameters: {'lr': 0.008340467653707682, 'alpha': 0.013937444611989128, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.0237\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.1986\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.5385\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.9639\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.3993\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.8894\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4347\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0076\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5962\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2098\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8392\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4858\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1501\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8292\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5225\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.2311\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9515\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6849\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4312\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1895\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9571\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7358\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5244\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3222\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1304\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.9458\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7705\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6037\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4430\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2904\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1443\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0051\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8713\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7448\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6228\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5075\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3967\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2925\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1911\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0938\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0015\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9138\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8286\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7488\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6720\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5991\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5294\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4628\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3984\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3382\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2795\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2228\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1719\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1201\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0719\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0258\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9806\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9390\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8998\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8627\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8247\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7910\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7571\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7270\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6963\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6680\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6406\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6153\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5904\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5664\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5447\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5217\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4972\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4758\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4558\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4365\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4171\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4012\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3848\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3688\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3534\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3401\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3268\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3138\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3045\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2926\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2797\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2693\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2587\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2496\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2395\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2317\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2225\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2140\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2070\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1991\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1913\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1854\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1777\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1728\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:14,715] Trial 91 finished with value: 1.880960266684912 and parameters: {'lr': 0.006828466933177667, 'alpha': 0.020557573287446843, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.0477\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.4035\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.7752\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.1639\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.6768\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.3397\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.1398\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.0564\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0927\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2338\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4662\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7807\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1705\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6246\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1363\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7023\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3133\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9657\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.6564\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3809\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1349\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9141\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7167\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5400\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3833\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2423\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1191\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0112\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9085\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8203\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7458\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6735\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6110\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5475\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4957\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4466\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4066\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3735\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3388\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3104\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2830\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2643\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2461\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2351\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2088\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1890\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1748\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1627\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1488\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1389\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1313\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1232\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1188\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1137\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1124\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1141\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1214\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1066\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1032\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1021\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0904\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0871\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0782\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0763\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0732\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0721\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0697\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0713\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0727\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0714\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0767\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0697\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0694\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0674\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0672\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0640\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0611\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0605\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0592\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0604\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0581\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0607\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0589\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0564\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0608\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0655\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0636\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0623\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0677\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0639\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0639\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0629\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0645\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0623\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0695\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0698\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0728\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0639\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0602\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0568\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:16,906] Trial 92 finished with value: 1.905345241997888 and parameters: {'lr': 0.010669462110863857, 'alpha': 0.03093744498459813, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.0960\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.8533\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.8785\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.8763\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.9243\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.0699\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2688\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5340\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8514\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2205\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6345\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0908\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.5871\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1195\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6852\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2828\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9082\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5609\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2388\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9409\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6629\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4047\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1653\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9422\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7369\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5443\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3663\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2035\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0476\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9064\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7742\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6518\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5364\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4313\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3320\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2411\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1564\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0794\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0051\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9380\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8746\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8185\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7650\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7221\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6690\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6278\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5842\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5470\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5097\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4767\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4476\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4177\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3928\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3699\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3457\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3265\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3070\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2857\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2727\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2616\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2443\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2344\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2210\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2133\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2021\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1918\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1838\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1724\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1631\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1536\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1475\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1384\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1310\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1229\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1186\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1133\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1065\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1027\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0998\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0954\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0913\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0904\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0858\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0821\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0825\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0815\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0761\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0753\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0734\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0706\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0688\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0672\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0653\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0653\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0672\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0662\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0625\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0624\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0626\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0608\n",
      "4/4 [==============================] - 0s 1000us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:19,094] Trial 93 finished with value: 1.846589944786391 and parameters: {'lr': 0.008603311929851948, 'alpha': 0.025809729357058647, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.7830\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.8942\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.8565\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.7940\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.8361\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.9465\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.1302\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.3741\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6770\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0358\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4424\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8944\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3877\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9183\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4837\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0824\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7092\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3645\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0460\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7520\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4786\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2253\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9906\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7720\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5728\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3850\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2121\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0552\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9052\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7701\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6443\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5278\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4164\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3156\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2216\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1355\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0560\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9851\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9158\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8519\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7926\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7406\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6883\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6457\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5982\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5579\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5188\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4847\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4522\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4258\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3970\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3700\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3477\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3253\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3046\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2872\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2698\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2393\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2294\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2127\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2051\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1916\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1854\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1777\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1710\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1632\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1593\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1510\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1412\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1348\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1301\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1199\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1130\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1076\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1040\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0959\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0956\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0922\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0877\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0862\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0832\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0806\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0770\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0770\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0791\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0735\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0759\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0725\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0711\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0684\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0678\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0655\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0640\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0699\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0704\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0680\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0657\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0648\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0607\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:21,202] Trial 94 finished with value: 1.826500537834437 and parameters: {'lr': 0.008958388502742425, 'alpha': 0.025655139884194735, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1727\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3481\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0128\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6672\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3933\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1278\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8870\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6525\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.4309\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2204\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0184\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8256\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6396\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4601\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.2873\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1232\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9636\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8115\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6662\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5264\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3937\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2622\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1365\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0159\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9020\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7882\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6819\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5833\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4823\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3898\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.3013\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2161\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1289\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0503\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9688\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8920\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8212\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7559\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6864\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6231\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.5605\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5049\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4500\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3975\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3394\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2936\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2442\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1962\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1513\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1061\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0653\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0257\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9917\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9566\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9175\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8953\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8799\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8388\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7997\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7784\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7428\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7220\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6877\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6623\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6348\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6129\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5904\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5693\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5482\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5293\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5085\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4899\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4736\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4566\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4407\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4230\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4081\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3946\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3818\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3702\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3567\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3467\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3360\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3229\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3166\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3093\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2964\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2838\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2777\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2690\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2608\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2524\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2476\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2466\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2382\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2305\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2347\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2236\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2090\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2038\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:23,313] Trial 95 finished with value: 2.2191851535248786 and parameters: {'lr': 0.009806420690363532, 'alpha': 0.012295703691669067, 'activation': 'tanh', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.9775\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.8279\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.1102\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.4428\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.8485\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.2565\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.7271\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.2157\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.7350\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2762\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.8396\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4219\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.0238\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6423\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2771\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9287\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5940\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2741\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9684\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6758\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3949\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1260\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8687\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6221\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3869\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1604\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9444\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7381\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5393\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3496\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1676\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.9935\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8265\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.6670\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.5137\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3677\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2275\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0940\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.9644\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8411\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7227\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6100\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5014\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3980\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2988\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2039\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1128\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0251\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9416\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8618\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7850\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7112\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6417\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5735\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5098\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.4480\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3878\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3320\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2785\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2270\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1760\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1287\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0828\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0402\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9971\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9578\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9190\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8816\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8467\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8131\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7812\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7498\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7193\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6904\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6640\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6371\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6117\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5890\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5653\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5437\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5217\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5022\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4833\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4645\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4488\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4312\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4138\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3984\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3836\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3693\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3545\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3420\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3289\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3178\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3064\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2956\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2841\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2746\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2636\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2555\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:25,433] Trial 96 finished with value: 1.917697895304525 and parameters: {'lr': 0.005257397860900887, 'alpha': 0.02511773505779283, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8576\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.3367\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8634\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5131\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1976\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9275\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.6803\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4533\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2351\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0270\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8238\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6282\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4401\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2571\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0796\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9084\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7409\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5788\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.4223\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2702\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1215\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9770\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8369\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.7006\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5694\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.4402\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3159\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.1958\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.0770\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.9635\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.8524\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.7442\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6388\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.5374\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4377\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.3420\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2486\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.1582\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0693\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.9837\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.8999\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8192\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7402\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6641\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5901\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5177\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4480\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3796\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3133\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.2492\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1863\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.1250\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.0669\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0091\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9542\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9000\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8461\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7959\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7472\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6995\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6509\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6055\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5609\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5192\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4760\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4366\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3966\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3575\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3204\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2845\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2493\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2152\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1807\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1484\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1183\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0866\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0565\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0289\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0005\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9735\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9463\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9211\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8967\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8720\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8512\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8274\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8041\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7818\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7606\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7403\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7192\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7007\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6812\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6639\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6467\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6299\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6121\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5965\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5795\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5648\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:27,590] Trial 97 finished with value: 1.9698508721034247 and parameters: {'lr': 0.005808939063589194, 'alpha': 0.015219313775476187, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.6481\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.3431\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.9743\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7219\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6094\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5532\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5052\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4600\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4367\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4185\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4007\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3841\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3691\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3538\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3390\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3266\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3123\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2991\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2866\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2748\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2620\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2496\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2367\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2241\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2130\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1998\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1879\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1787\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1650\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1544\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1424\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1310\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1195\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1090\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0975\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0872\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0765\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0669\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0556\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0450\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0343\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0244\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0138\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0044\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9947\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9849\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9752\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9655\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.9557\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9467\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9374\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9274\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9193\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9099\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9019\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8933\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8833\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8750\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.8678\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8597\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8496\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8418\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8329\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8263\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8169\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8098\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8016\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7933\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7856\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7780\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7710\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7636\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7555\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7477\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7414\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7333\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7257\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7195\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7119\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7056\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6980\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6915\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6854\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6780\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6730\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6660\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6587\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6522\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6458\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6397\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6330\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6271\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6209\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6153\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6099\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6045\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5978\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5928\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5855\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5803\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:29,702] Trial 98 finished with value: 2.109682740285708 and parameters: {'lr': 0.012197085565606811, 'alpha': 0.002569986158176706, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3835\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1251\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8678\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5790\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4836\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4325\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4062\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3810\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3690\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3619\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3548\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3525\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3499\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3483\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3461\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3461\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3436\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3423\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3420\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3426\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3419\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3406\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3386\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3371\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3374\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3352\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3346\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3350\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3328\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3333\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3330\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3315\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3292\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3288\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3273\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3269\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3261\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3266\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3259\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3237\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3233\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3222\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3217\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3199\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3196\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3194\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3178\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3169\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3157\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3151\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3152\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3141\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3129\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3120\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3121\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3124\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3114\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3086\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3097\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3087\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3059\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3075\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3047\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3048\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3030\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3029\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3019\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3013\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3005\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2988\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2984\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2983\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2965\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2956\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2957\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2944\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2933\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2935\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2918\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2918\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2906\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2900\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2898\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2889\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2896\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2886\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2874\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2864\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2848\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2844\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2834\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2824\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2815\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2811\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2810\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2811\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2799\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2802\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2773\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2769\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:08:31,808] Trial 99 finished with value: 2.1499222717567448 and parameters: {'lr': 0.014747864311141374, 'alpha': 0.0005741013677108224, 'activation': 'relu', 'n1': 256, 'n2': 128}. Best is trial 84 with value: 1.8050954250882123.\n"
     ]
    }
   ],
   "source": [
    "def objective_mlp_fp(trial):\n",
    "    lr = trial.suggest_float('lr', 1e-4, 2e-2, log=True)\n",
    "    alpha = trial.suggest_float('alpha', 1e-4, 5e-2, log=True)\n",
    "    act = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid', 'gelu'])\n",
    "    n1 = trial.suggest_int('n1', 128, 384, step=64)\n",
    "    n2 = trial.suggest_int('n2', 128, 384, step=64)\n",
    "\n",
    "    model = MLP(\n",
    "        engine='tensorflow',\n",
    "        nfeatures=X_train_fp_scaled.shape[1], \n",
    "        nneurons=[n1, n2],\n",
    "        activations=[act, act], \n",
    "        learning_rate=lr, \n",
    "        alpha=alpha,\n",
    "        nepochs=100, \n",
    "        batch_size=64, \n",
    "        loss='mean_squared_error', \n",
    "        is_regression=True\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train_fp_scaled, y_train_scaled)\n",
    "    preds_scaled = model.predict(X_test_fp_scaled).reshape(-1, 1)\n",
    "    preds = yscaler.inverse_transform(preds_scaled)\n",
    "    y_test_inv = yscaler.inverse_transform(y_test_scaled)\n",
    "    metrics = regression_metrics(y_test_inv, preds)\n",
    "    return metrics['MAE'][0]\n",
    "\n",
    "study_mlp_fp = optuna.create_study(direction='minimize')\n",
    "study_mlp_fp.optimize(objective_mlp_fp, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5cac4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_mlp_cm(trial):\n",
    "#     lr = trial.suggest_float('lr', 1e-6, 1e-2, log=True)\n",
    "#     alpha = trial.suggest_float('alpha', 1e-6, 1e-1, log=True)\n",
    "#     act = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid', 'gelu'])\n",
    "#     n1 = trial.suggest_int('n1', 64, 256, step=64)\n",
    "#     n2 = trial.suggest_int('n2', 64, 256, step=64)\n",
    "\n",
    "#     model = MLP(engine='tensorflow', nfeatures=X_train_cm_scaled.shape[1], nneurons=[n1, n2],\n",
    "#                 activations=[act, act], learning_rate=lr, alpha=alpha,\n",
    "#                 nepochs=100, batch_size=64, loss='mean_squared_error', is_regression=True)\n",
    "\n",
    "#     model.fit(X_train_cm_scaled, y_train_scaled)\n",
    "#     preds_scaled = model.predict(X_test_cm_scaled).reshape(-1, 1)\n",
    "#     preds = yscaler.inverse_transform(preds_scaled)\n",
    "#     y_test_inv = yscaler.inverse_transform(y_test_scaled)\n",
    "#     metrics = regression_metrics(y_test_inv, preds)\n",
    "#     return metrics['MAE'][0]\n",
    "\n",
    "# study_mlp_cm = optuna.create_study(direction='minimize')\n",
    "# study_mlp_cm.optimize(objective_mlp_cm, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d47ff58",
   "metadata": {},
   "source": [
    "## Retrain Models with Best Parameters Found in Respective Optuna Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2132a2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Tuned Kernel Ridge (RDKit FP):\n",
      "        MAE      RMSE  r_squared\n",
      "0  1.906799  2.569051   0.677546\n"
     ]
    }
   ],
   "source": [
    "best_krr = study_krr.best_params\n",
    "\n",
    "# build final model using best params\n",
    "if best_krr['kernel'] == 'rbf':\n",
    "    final_krr = KernelRidge(alpha=best_krr['alpha'], kernel='rbf', gamma=best_krr['gamma'])\n",
    "else:\n",
    "    final_krr = KernelRidge(alpha=best_krr['alpha'], kernel=best_krr['kernel'])\n",
    "\n",
    "# train on scaled data\n",
    "final_krr.fit(X_train_fp_scaled, y_train_scaled)\n",
    "# predict on test set (scaled)\n",
    "final_preds_krr_scaled = final_krr.predict(X_test_fp_scaled).reshape(-1, 1)\n",
    "# inverse transform both predictions and gt\n",
    "final_preds_krr = yscaler.inverse_transform(final_preds_krr_scaled)\n",
    "y_test_krr = yscaler.inverse_transform(y_test_scaled)\n",
    "# eval\n",
    "final_metrics_krr = regression_metrics(y_test_krr, final_preds_krr)\n",
    "print(\"Final Tuned Kernel Ridge (RDKit FP):\")\n",
    "print(final_metrics_krr[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d89540fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Tuned Random Forest (RDKit FP):\n",
      "        MAE      RMSE  r_squared\n",
      "0  1.739076  2.422393    0.71331\n"
     ]
    }
   ],
   "source": [
    "# rebuild and retrain the RFR model\n",
    "best_rfr = study_rfr.best_params\n",
    "final_rfr = RandomForestRegressor(n_estimators=best_rfr['n_estimators'], max_depth=best_rfr['max_depth'], random_state=42)\n",
    "final_rfr.fit(X_train_fp_unscaled, y_train_unscaled)\n",
    "\n",
    "# predict on test set\n",
    "final_preds_rfr = final_rfr.predict(X_test_fp_unscaled)\n",
    "\n",
    "# eval using unscaled targets\n",
    "final_metrics_rfr = regression_metrics(y_test_unscaled, final_preds_rfr)\n",
    "print(\"Final Tuned Random Forest (RDKit FP):\")\n",
    "print(final_metrics_rfr[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "68ac4ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.4007\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.9067\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.7918\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.9003\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.1801\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.5536\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.9716\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.4465\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.9442\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.4781\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.0414\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6303\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2463\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8840\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5447\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.2248\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9230\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6392\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.3727\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.1226\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.8857\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.6626\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.4524\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.2533\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3.0690\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.8919\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.7264\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.5738\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.4245\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2886\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.1579\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2.0351\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.9190\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.8106\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.7073\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.6115\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.5213\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.4380\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3572\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2802\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2089\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1425\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0769\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.0192\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9614\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9087\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8574\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8109\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7654\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7244\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6863\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6478\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6179\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5814\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5501\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5218\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4944\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4685\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4476\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4260\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4019\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3850\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3636\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3479\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3310\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3146\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2995\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2860\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2716\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2563\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2448\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2342\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2199\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2102\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2009\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1936\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1813\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1733\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1658\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1582\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1504\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1448\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1390\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1332\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1306\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1268\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1197\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1170\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1131\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1086\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1027\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1002\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0969\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0966\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0963\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0920\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0908\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0872\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0835\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Final Tuned MLP (RDKit FP):\n",
      "        MAE      RMSE  r_squared\n",
      "0  1.843407  2.498902   0.694915\n"
     ]
    }
   ],
   "source": [
    "best_fp = study_mlp_fp.best_params\n",
    "final_mlp_fp = MLP(\n",
    "    engine='tensorflow', \n",
    "    nfeatures=X_train_fp_scaled.shape[1], \n",
    "    nneurons=[best_fp['n1'], best_fp['n2']], \n",
    "    activations=[best_fp['activation'], best_fp['activation']], \n",
    "    learning_rate=best_fp['lr'], \n",
    "    alpha=best_fp['alpha'], \n",
    "    nepochs=100, \n",
    "    batch_size=64, \n",
    "    loss='mean_squared_error', \n",
    "    is_regression=True\n",
    "    )\n",
    "\n",
    "# train on scaled data\n",
    "final_mlp_fp.fit(X_train_fp_scaled, y_train_scaled)\n",
    "\n",
    "# predict and inverse transform\n",
    "final_preds_fp_scaled = final_mlp_fp.predict(X_test_fp_scaled).reshape(-1, 1)\n",
    "final_preds_inv_fp = yscaler.inverse_transform(final_preds_fp_scaled)\n",
    "y_test_inv_fp = yscaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "# eval in eV\n",
    "final_metrics_fp = regression_metrics(y_test_inv_fp, final_preds_inv_fp)\n",
    "print(\"Final Tuned MLP (RDKit FP):\")\n",
    "print(final_metrics_fp[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "406d6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_cm = study_mlp_cm.best_params\n",
    "# final_mlp_cm = MLP(engine='tensorflow', nfeatures=X_train_cm_scaled.shape[1], nneurons=[best_cm['n1'], best_cm['n2']], activations=[best_cm['activation'], best_cm['activation']], learning_rate=best_cm['lr'], alpha=best_cm['alpha'], nepochs=100, batch_size=64, loss='mean_squared_error', is_regression=True)\n",
    "\n",
    "# # train on scaled data\n",
    "# final_mlp_cm.fit(X_train_cm_scaled, y_train_scaled)\n",
    "\n",
    "# # predict and inverse transform\n",
    "# final_preds_cm_scaled = final_mlp_cm.predict(X_test_cm_scaled).reshape(-1, 1)\n",
    "# final_preds_inv_cm = yscaler.inverse_transform(final_preds_cm_scaled)\n",
    "# y_test_inv_cm = yscaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "# # eval in eV\n",
    "# final_metrics_cm = regression_metrics(y_test_inv_cm, final_preds_inv_cm)\n",
    "# print(\"Final Tuned MLP (Coulomb Matrix):\")\n",
    "# print(final_metrics_cm[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e3c54319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHvCAYAAACFRmzmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAAD8W0lEQVR4nOzdd1xV9f8H8Nfh3gtc9pANAu6taM7MmZgbV5ZZastylPk1y7Ic1c+0UtMyW4oNR0uzLBBTNFNzb8GBE8SBDIHL5Y7z+4PuVeReuBcud8Dr+Xjw0Hvm+3y4wDnv+/m8P4IoiiKIiIiIiIiIiIisyMnWARARERERERERUe3DpBQREREREREREVkdk1JERERERERERGR1TEoREREREREREZHVMSlFRERERERERERWx6QUERERERERERFZHZNSRERERERERERkdUxKERERERERERGR1TEpRUREREREREREVsekFBERERERERERWR2TUkRERLVEfHw8BEFAfHy8Vc/bo0cPCIJg1XPayuTJkxEQEID8/Hxbh2IWW703zLVq1SpIJBIcP37c1qEQERGRBTApRURE5OAuXrwIQRDK/XIU915LWFgYNBqNwe2OHz+u365Jkyal1ukSLO+//36F5xs3blyZtvLy8kL79u2xePFiqFQqk2M/c+YMPv/8c8yYMQMeHh5l4rn3Sy6Xo1GjRpgyZQoyMzMNHi8qKqrUPi4uLggICECHDh0wadIk7Nq1y2gshtpFZ82aNZDJZKhTpw72799f7jUJgoAePXpUfPH3MPf9eP86qVSKkJAQxMXFYefOnaW2ffLJJxEdHY3p06ebFRMRERHZJ6mtAyAiIiLLqF+/PsaMGWN0/dChQ9GpUyeEhIRYMarKkUqlyMjIQGJiIvr3719m/ddffw2pVAq1Wm2R8z3zzDMIDw+HVqvF1atX8csvv2DatGnYvn07Nm3aZNIx5s6dCxcXF7z44osG1/fu3Rtdu3YFANy6dQvbtm3DJ598go0bN+LQoUMICAgos49EIsGsWbMAAGq1GtnZ2Th+/Dg+//xzLF++HIMGDcLq1avh6+trUozLly/H5MmTERoaii1btqBZs2YAque9UdH78V7+/v6YPHkyAEChUODo0aP49ddfsWnTJvzwww8YMWIEgJL3xdSpUzFlyhTs2rVL355ERETkoEQiIiJyaBcuXBABiH379rV1KAZ1795dNPWWQ3ct3bp1E729vcXhw4eX2UapVIp16tQRBw8eLAIQGzduXGr9qlWrRADi/PnzKzzf2LFjRQDinj17Si1PT08XAwMDRQBicnJyhce5efOm6OLiIo4ZM6bMOmPxaDQasX///iIA8e233y6zX2RkpOji4mLwfBcvXhR79+4tAhC7d+8uajSaUusNtcu7774rAhAbNGggXrx4scJr0h2ne/fuJm2rY+770VCsoiiKX375pQhAjIqKKrX85s2bolQqFZ944gmz4iIiIiL7w+F7REREtYSxukG6IVo3b97E008/jcDAQMjlcnTq1AnJyclljnPw4EFMnjwZLVq0gLe3N+RyOVq2bIn333/frOFu5ZHL5Rg1ahR+++033Lp1q9S6TZs24datWxg/frxFzmVIaGgohg0bBgAVDnEDgLVr10KpVGLkyJEmn8PJyQnjxo0DUNKm5oiMjMRvv/2GZs2aYceOHfjpp5+MbiuKIv73v/9h1qxZaN26NXbt2oXIyMhS29z/3khOTtYPs9uxY0ep4XXWqjv19NNPw93dHRcvXiz1HqhTpw569uyJn376yeFqdxEREVFpTEoRERERcnJy8OCDD+LYsWN44oknMGzYMBw4cAB9+/bFiRMnSm375ZdfYsOGDWjZsiUmTJiAZ555BqIoYubMmXjssccsFtPTTz+N4uJifP/996WWr1y5EoGBgRg4cKDFzlVVf/31FwCgU6dOZu0niiKAkmFp5pLL5fraSuvXrze4jUajwbPPPotFixaha9eu2LFjB4KCgio8dlRUFGbPng2gJAE2e/Zs/VebNm3MjrWydO1zv86dO0OpVOKff/6xWixERERkeawpRUREVEOcO3cOc+bMKbP8kUceqTBZcvToUUycOBHLli2Dk1PJZ1a9evXCs88+i08++QQrVqzQbztz5kx8+umnkEgk+mWiKOLZZ5/FypUr8c8//+DBBx+s8vV07NgRzZs3x8qVK/Hyyy8DANLT07FlyxZMnTq1UokcU2VkZOCXX34BALRv377C7Xfv3o2wsDAEBgaafA6NRoOVK1cCQKVrI3Xv3h2A4d5cxcXFePTRR/HLL7+gX79++PnnnyGXy006blRUFObMmYO5c+fq/2+uqrwfgZLkY2FhIaKiolCnTp1S69q1awegpN379u1rdmxERERkH5iUIiIiqiHOnz+PuXPnllnu4+NTYRLA3d0dCxYs0CekAGDs2LF44YUXyiQ87h/6BZQMAZw0aRJWrlyJrVu3WiQpBQDjx4/H9OnTcfDgQbRr1w7x8fHQaDR4+umnLXJ8na+++goJCQkQRRFXrlzBL7/8gtzcXAwePFif+DGmuLgYN2/eRNu2bcvdbuvWrSgqKgIAZGVlISkpCampqejUqZPR4ugVCQ0NBYAyQxwB4MKFC7hw4QIiIyOxceNGODs7V+oclWXO+/HWrVv6BFZRURGOHDmCxMREODk54cMPPyxzDF1vr6tXr1o+cCIiIrIaJqWIiIhqiL59+yIhIaFS+zZs2BAeHh6llkmlUgQFBSEnJ6fU8uLiYnzyySdYt24dUlJSkJ+fX2qYVUZGRqViMOTJJ5/EzJkzsXLlSn1SqmPHjvpZ4yzl66+/1v/f09MTTZo0wejRo/UzwpUnKysLACqcAe+vv/7SD/PT6dy5M7Zt2wZXV9dKRG18eBtQkrDy9fXFyZMnMWnSJHzxxRf6OlHWYM77MSsrS5/AkkgkqFOnDuLi4jBt2jQ89NBDZbb38/MDYDgZR0RERI6DSSkiIiKCt7e3weVSqRQajabUshEjRuC3335Do0aNMGrUKAQGBkImkyEnJwcff/wxlEqlxeIKDAxE//79sXbtWgwePBjnzp3T11GypD179phdD0pHNyROoVCUu938+fPx+uuvQ6vV4uLFi5gzZw6+/fZbPPfcc/j2228rde5r164BAAICAsqs8/T0xPbt29G7d2989dVX0Gg0+Oqrr0r1hrMXjRs3RkpKisnb69razc2tukIiIiIiK2BSioiIiEy2f/9+/Pbbb+jbty82b95cqq7U3r178fHHH1v8nE8//TR+/fVXPPPMM5DL5Xj88cctfo6q8PHxgUwmw+3bt03a3snJCfXq1cPq1atx6dIlfPfddxg+fDji4uLMPrdudkRjda8CAgKwbds29O7dG6tWrYIoivj666/tMjFlDl1bG0rGERERkeNw7DsSIiIisqrz588DAAYMGFAqIQUAf//9d7Wcs3///ggODkZ6ejqGDx8OLy+vajlPVbRo0QIXL16ESqUyeR9BEPDxxx9DEATMnDmzTI+0iigUCnz00UcAUG6irk6dOti2bRvatGmD+Ph4jB8/Hlqt1qRzODk5mR2XNaSmpgIAWrZsaeNIiIiIqCqYlCIiIiKT6Yqc79q1q9TykydPYv78+dVyTqlUik2bNmHDhg147733quUcVdW9e3cUFRXh+PHjZu3Xpk0bxMXFISUlBWvWrDF5v0uXLmHQoEE4deoUevbsiWHDhpW7vb+/P/766y+0bdsW33zzDcaOHWtSYsrPz88ui4n/+++/AFBhEXoiIiKybxy+R0RERCbr0KEDOnTogB9++AHXrl1Dp06dcPnyZWzatAkDBgzATz/9VC3nbd++vdEhasb8+OOPRusUjR49GrGxsZYIDQAQFxeHJUuWYOvWrRXOwne/OXPmYOPGjZg3bx4ef/xxSKV3b8/UarV+VjqNRoPs7GwcP34c//zzDzQaDYYMGYL4+HiTCpj7+flh69atiI2NxXfffQdRFLF69eoyPd7u1atXL/zwww8YMWIEYmJiIJFIMGDAAJv2UBJFEX/99ReaNm2KRo0a2SwOIiIiqjompYiIiMhkEokEv//+O15//XUkJCRg//79aNiwIT788EP069ev2pJSlXHo0CEcOnTI4Lo2bdpYNCnVvXt3NGnSBN999x1mzJhh1r6tWrXCsGHD8PPPP+Obb77B008/rV+n0Wj0s9I5OzvDy8sL0dHRmDBhAkaPHo0HH3zQrHP5+vrqE1Pff/89tFptuUXWdTXCtm3bhg0bNkCr1SI4ONimSamdO3fi8uXLWLJkic1iICIiIssQxPLmEiYiIiIik3zxxReYMGEC9u7di44dO9o6nBrrqaeewu+//460tDT4+PjYOhwiIiKqAialiIiIiCxAo9GgZcuWiIqKwh9//GHrcGqkc+fOoUmTJvjggw/wyiuv2DocIiIiqiIWOiciIiKyAIlEglWrVqFjx47Iz8+3dTg10tWrVzF79mxMmjTJ1qEQERGRBbCnFBERERERERERWR17ShERERERERERkdUxKUVERERERERERFbHpBQREREREREREVkdk1JERERERERERGR1TEoREREREREREZHVMSlFRERERERERERWx6QUERERERERERFZHZNSRERERERERERkdUxKERERERERERGR1TEpRUREREREREREVsekFBERERERERERWR2TUkREREREREREZHVMShERERERERERkdUxKUVERERERERERFbHpBQREREREREREVkdk1JERERERERERGR1TEoREREREREREZHVMSlFRERERERERERWx6QUERERERERERFZHZNSRGQX4uPjIQiC/ksqlSI8PBzjx49Henq6Rc8VFRWFcePG6V9nZGRgzpw5OHLkiEXPY+o1JScnQxAEJCcnm32O3bt3Y86cOcjJybFc4ERERDWcob/RISEheOyxx3D27NlqO++cOXMgCIJJ295/v2LreCrSo0cPtGjRwuC6W7duQRAEzJkzR7+ssvc/y5cvR3x8fOUDJSK7IrV1AERE91q1ahWaNGkChUKBnTt3Yv78+dixYweOHz8Od3d3i5xjw4YN8PLy0r/OyMjA3LlzERUVhTZt2ljkHPeqzmvavXs35s6di3HjxsHHx8cyARMREdUSur/RRUVF+Oeff/Dee+9h+/btSElJga+vr8XP9+yzz+KRRx6x+HEdUdu2bbFnzx40a9bMrP2WL1+OOnXqVHvCjoisg0kpIrIrLVq0wAMPPAAA6NmzJzQaDd555x1s3LgRTzzxRJWOrVAoIJfLERMTY4lQTVad10RERESVd+/f6B49ekCj0WD27NnYuHEjxo8fb/HzhYeHIzw83OLHdUReXl7o1KmTrcMwW2FhIdzc3GwdBlGNweF7RGTXdDcrly5dAgDMnTsXHTt2hJ+fH7y8vNC2bVt8/fXXEEWx1H5RUVEYOHAgfvnlF8TExMDV1RVz587Vr9N9upacnIz27dsDAMaPH6/vxj9nzhx8++23EAQBe/bsKRPXvHnzIJPJkJGRUeVrMmbTpk3o3Lkz3Nzc4OnpiT59+pSKZc6cOXj11VcBANHR0frYKzMMkIiIiKBPUF2/fr3U8gMHDmDw4MHw8/ODq6srYmJi8MMPP5TaprCwENOnT0d0dDRcXV3h5+eHBx54AGvXrtVvY2i4nEqlwowZMxAcHAw3Nzd07doV+/btKxObsaF2uqGIFy9e1C9bv349YmNjERISArlcjqZNm+L1119HQUFBhW2wbds29OjRA/7+/pDL5ahbty6GDx+OwsLCCvc1h6Hhe2lpaXjssccQGhoKFxcXBAUFoXfv3voSC1FRUTh58iR27Nihv++JiorS73/58mWMGTMGgYGBcHFxQdOmTfHRRx9Bq9WWOvfVq1cxYsQIeHp6wsfHB0888QT2798PQRBKDQ0cN24cPDw8cPz4ccTGxsLT0xO9e/cGACQlJWHIkCEIDw+Hq6srGjRogAkTJuDWrVulzqX7vh07dgwjR46Et7c3/Pz8MG3aNKjVaqSmpuKRRx6Bp6cnoqKisHDhQou2M5G9Y08pIrJr586dAwAEBAQAAC5evIgJEyagbt26AIC9e/diypQpSE9Px9tvv11q30OHDuH06dOYNWsWoqOjDQ6Va9u2LVatWoXx48dj1qxZGDBgAICSTzIDAwMxY8YMfPrpp+jcubN+H7Vajc8//xxDhw5FaGhola/JkDVr1uCJJ55AbGws1q5dC6VSiYULF6JHjx7466+/0LVrVzz77LO4ffs2li1bhl9++QUhISEAYHY3eCIiIipx4cIFAECjRo30y7Zv345HHnkEHTt2xIoVK+Dt7Y1169Zh1KhRKCws1H/QNW3aNHz77bd49913ERMTg4KCApw4cQJZWVnlnvO5557DN998g+nTp6NPnz44ceIEhg0bhjt37lT6Os6ePYv+/ftj6tSpcHd3R0pKChYsWIB9+/Zh27ZtRve7ePEiBgwYgIceeggrV66Ej48P0tPTkZCQgOLiYpN6CKnV6jLLNBqNSXH3798fGo0GCxcuRN26dXHr1i3s3r1bXztzw4YNGDFiBLy9vbF8+XIAgIuLCwDg5s2b6NKlC4qLi/HOO+8gKioKv//+O6ZPn47z58/rty8oKEDPnj1x+/ZtLFiwAA0aNEBCQgJGjRplMKbi4mIMHjwYEyZMwOuvv66/vvPnz6Nz58549tln4e3tjYsXL2LRokXo2rUrjh8/DplMVuo4jz76KMaMGYMJEyYgKSkJCxcuhEqlwtatWzFx4kRMnz4da9aswWuvvYYGDRpg2LBhJrUZkcMTiYjswKpVq0QA4t69e0WVSiXeuXNH/P3338WAgADR09NTzMzMLLOPRqMRVSqVOG/ePNHf31/UarX6dZGRkaJEIhFTU1PL7BcZGSmOHTtW/3r//v0iAHHVqlVltp09e7bo7OwsXr9+Xb9s/fr1IgBxx44dFrmm7du3iwDE7du3668rNDRUbNmypajRaPTHu3PnjhgYGCh26dJFv+yDDz4QAYgXLlwoNxYiIiK6y9Df6ISEBDE4OFjs1q2bqFKp9Ns2adJEjImJKbVMFEVx4MCBYkhIiP5vdYsWLcS4uLhyzzt79mzx3kew06dPiwDEV155pdR233//vQig1P3K/fvefy3G7gW0Wq2oUqnEHTt2iADEo0ePGj3mTz/9JAIQjxw5Uu51GNK9e3cRQLlfs2fP1m9///3PrVu3RADikiVLyj1P8+bNxe7du5dZ/vrrr4sAxH///bfU8hdffFEUBEF/T/jpp5+KAMQ///yz1HYTJkwocz84duxYEYC4cuXKcmPStfGlS5dEAOKvv/6qX6dr448++qjUPm3atBEBiL/88ot+mUqlEgMCAsRhw4aVez6imoTD94jIrnTq1AkymQyenp4YOHAggoOD8eeffyIoKAhASZfyhx9+GN7e3pBIJJDJZHj77beRlZWFGzdulDpWq1atSn3SWRkvvvgiAODLL7/UL/vkk0/QsmVLdOvWzSLXdL/U1FRkZGTgySefhJPT3V/THh4eGD58OPbu3WvxLvRERES10b1/ox955BH4+vri119/hVRaMqDk3LlzSElJ0deAVKvV+q/+/fvj2rVrSE1NBQB06NABf/75J15//XUkJydDoVBUeP7t27cDQJkak48++qg+hspIS0vD6NGjERwcrL9f6t69OwDg9OnTRvdr06YNnJ2d8fzzz2P16tVIS0sz67z169fH/v37y3xt3bq1wn39/PxQv359fPDBB1i0aBEOHz5cZthdebZt24ZmzZqhQ4cOpZaPGzcOoijqe4jt2LFD//2+1+OPP2702MOHDy+z7MaNG3jhhRcQEREBqVQKmUyGyMhIAIbbeODAgaVeN23aFIIgoF+/fvplUqkUDRo0qLDEA1FNwuF7RGRXvvnmGzRt2hRSqRRBQUH6IWkAsG/fPsTGxqJHjx748ssvER4eDmdnZ2zcuBHvvfdemZu/e/etrKCgIIwaNQqff/45Xn/9dZw8eRJ///03Pv/8c4tckyG6bv6GtgsNDYVWq0V2djaLbBIREVWR7m/0nTt3sH79enz++ed4/PHH8eeffwK4W1tq+vTpmD59usFj6GoILV26FOHh4Vi/fj0WLFgAV1dX9O3bFx988AEaNmxocF/d3/zg4OBSy6VSKfz9/St1Tfn5+XjooYfg6uqKd999F40aNYKbmxuuXLmCYcOGlZssq1+/PrZu3YqFCxdi0qRJKCgoQL169fDSSy/h5ZdfrvDcrq6u+rpc97q/zpIhgiDgr7/+wrx587Bw4UL873//g5+fH5544gm899578PT0LHf/rKysUvWldHSlFnRtnZWVZfCDQWMfFrq5uZWatRkAtFotYmNjkZGRgbfeegstW7aEu7s7tFotOnXqZLCN/fz8Sr12dnaGm5sbXF1dyyzPy8szfqFENQyTUkRkV5o2bWrwZgYA1q1bB5lMht9//73UH/CNGzca3N5QMdDKePnll/Htt9/i119/RUJCgr4gpqnKuyZDdDeh165dK7MuIyMDTk5O1TJNNRERUW1z799o3Qy5X331FX766SeMGDECderUAQDMnDnTaI2fxo0bAwDc3d0xd+5czJ07F9evX9f3mho0aBBSUlIM7qv7m5+ZmYmwsDD9crVaXaYWle7eR6lU6usoAWUTPtu2bUNGRgaSk5P1vaMA6OsyVeShhx7CQw89BI1GgwMHDmDZsmWYOnUqgoKC8Nhjj5l0jMqKjIzE119/DQA4c+YMfvjhB8yZMwfFxcVYsWJFufv6+/sbvXcCoP9e+vv7Gywkn5mZafC4hu4nT5w4gaNHjyI+Ph5jx47VL9fVDSUi03H4HhE5DEEQIJVKIZFI9MsUCgW+/fbbKh1Xd2Nn7JPDdu3aoUuXLliwYAG+//57jBs3zmDRdEtp3LgxwsLCsGbNmlKzChYUFODnn3/Wz8hnSuxERERkuoULF8LX1xdvv/02tFotGjdujIYNG+Lo0aN44IEHDH4Z6sETFBSEcePG4fHHH0dqaqrRYfc9evQAAHz//fellv/www9lCobregEdO3as1PLffvut1GtdEuXexBUAs3p5A4BEIkHHjh3x6aefAiiZQMaaGjVqhFmzZqFly5alzu3i4mLwvqd37944depUmTi/+eYbCIKAnj17AgC6d++OO3fu6HvD6axbt87k2CzVxkTEnlJE5EAGDBiARYsWYfTo0Xj++eeRlZWFDz/8sMwNgbnq168PuVyO77//Hk2bNoWHhwdCQ0NLzaz38ssvY9SoURAEARMnTqzqpZTLyckJCxcuxBNPPIGBAwdiwoQJUCqV+OCDD5CTk4P3339fv23Lli0BAB9//DHGjh0LmUyGxo0bV9jFnYiIiMry9fXFzJkzMWPGDKxZswZjxozB559/jn79+qFv374YN24cwsLCcPv2bZw+fRqHDh3Cjz/+CADo2LEjBg4ciFatWsHX1xenT5/Gt99+W+rDpPs1bdoUY8aMwZIlSyCTyfDwww/jxIkT+PDDD8sMGevfvz/8/PzwzDPPYN68eZBKpYiPj8eVK1dKbdelSxf4+vrihRdewOzZsyGTyfD999/j6NGjFV7/ihUrsG3bNgwYMAB169ZFUVERVq5cCQB4+OGHK9OkJjt27BgmT56MkSNHomHDhnB2dsa2bdtw7NgxvP766/rtWrZsiXXr1mH9+vWoV68eXF1d0bJlS7zyyiv45ptvMGDAAMybNw+RkZHYvHkzli9fjhdffFFfZ3Ts2LFYvHgxxowZg3fffRcNGjTAn3/+icTERAAoVc/TmCZNmqB+/fp4/fXXIYoi/Pz88NtvvyEpKal6GoeoBmNPKSJyGL169cLKlStx/PhxDBo0CG+++SZGjBhR6kalMtzc3LBy5UpkZWUhNjYW7du3xxdffFFqm7i4OLi4uKBv375G60JY0ujRo7Fx40ZkZWVh1KhRGD9+PLy8vLB9+3Z07dpVv12PHj0wc+ZM/Pbbb+jatSvat2+PgwcPVnt8RERENdWUKVNQt25dzJs3DxqNBj179sS+ffvg4+ODqVOn4uGHH8aLL76IrVu3lkrU9OrVC5s2bcL48eMRGxuLhQsX4qmnnirTk+l+X3/9NaZNm4b4+HgMHjwYP/zwA37++ecyQ/W9vLyQkJAAT09PjBkzBi+88AJatGiBN998s9R2/v7+2Lx5M9zc3DBmzBg8/fTT8PDwwPr16yu89jZt2kCtVmP27Nno168fnnzySdy8eRObNm1CbGysGa1ovuDgYNSvXx/Lly/HiBEjMGTIEPz222/46KOPMG/ePP12c+fORffu3fHcc8+hQ4cOGDRoEAAgICAAu3fvRq9evTBz5kwMHDgQiYmJWLhwIZYtW6bf393dHdu2bUOPHj0wY8YMDB8+HJcvX8by5csBAD4+PhXGKpPJ8Ntvv6FRo0aYMGECHn/8cdy4ccOkgu5EVJog3js2hIiIDPrtt98wePBgbN68Gf3797d1OERERERkQf/3f/+HWbNm4fLlywgPD7d1OES1BpNSRETlOHXqFC5duoSXX34Z7u7uOHTokMUKqBMRERGR9X3yyScASobhqVQqbNu2DUuXLsWoUaPwzTff2Dg6otqFNaWIiMoxceJE/PPPP2jbti1Wr17NhBQRERGRg3Nzc8PixYtx8eJFKJVK1K1bF6+99hpmzZpl69CIah32lCIiIiIiIiIiIqtjoXMiIiIiIiIiIrI6JqWIiIiIiIiIiMjqmJQiIiIiIiIiIiKrY6FzA7RaLTIyMuDp6cmixkRERGSQKIq4c+cOQkND4eRUez7n430SERERVcTU+yQmpQzIyMhARESErcMgIiIiB3DlyhWEh4fbOgyr4X0SERERmaqi+yQmpQzw9PQEUNJ4Xl5eUKlU2LJlC2JjYyGTyWwcXc3FdrYOtrP1sK2tg+1sHWznsvLy8hAREaG/b6gt7r9PohL8GTGM7WIY28U4to1hbBfD2C7G2bptTL1PYlLKAF1XdC8vL31Sys3NDV5eXnyjVyO2s3Wwna2HbW0dbGfrYDsbV9uGsN1/n0Ql+DNiGNvFMLaLcWwbw9guhrFdjLOXtqnoPqn2FEAgIiIiIiIiIiK7waQUERERERERERFZHZNSRERERERERERkdUxKERERERERERGR1TEpRUREREREREREVsekFBERERERERERWR2TUkREREREREREZHVMShERERERERERkdUxKUVERERERERERFbHpBQREREREREREVkdk1JERERERERERGR1TEoREREREREREZHVMSlFRERERERERERWx6QUERFRTbB/P1BcbOsoiIiIiIhMJrV1AERERFRFiYnAkCFA377Ajz8Czs62joiIqFa6efMmcnNzK9xOo9EAANLS0iCRSAAA3t7eCAgIqNb4iIjsDZNSREREjkyXkFIqASd2gCYispWbN2+iQYOGyMurOCkll8uxdu1axMTEQKFQAAC8vLxx7txZJqaIqFZhUoqIiMhRabXArFklCam4OGD9evaSIiKykdzcXOTl5eKFBfHwDQwtd1sJRAAK/G/5RmggIPtGBla8Ng65ublMShFRrcKkFBERkaNycgI2bwYWLgT+7/+YkCIisgO+gaEICIssdxtB1ACKM/APjYAoSKwUGRGR/WE/fyIiIkeTkXH3/4GBwIcfMiFFRERERA6HSSkiIiJHkpAANGgAfPWVrSMhIiIiIqoSJqWIiIgcRUJCSe0ohQL4809AFG0dERERERFRpTEpRURE5Ah0CSmlEhg6FFi7FhAEW0dFRERERFRpTEoRERHZu/sTUuvWsYYUERERETk8JqWIiIjsGRNSRERERFRDMSlFRERkz/bsYUKKiIiIiGokqa0DICIionLMmQM0bgyMGMGEFBERERHVKOwpRUREZG/27AEKC0v+LwjA6NFMSBERERFRjcOkFBERkT1JSAB69gSGDAEUCltHQ0RERERUbZiUIiIishf3FjX38AAkEltHRERERERUbZiUIiIisgf3JqTi4oD16zlkj4iIiIhqNBY6JyIisrUqJqTyi9RIz1GgoFgND2cpQn3k8HDln3giIiIism827Sk1f/58tG/fHp6enggMDERcXBxSU1NLbSMIgsGvDz74wOhx4+PjDe5TVFRU3ZdERERknsTEKiWkrmYX4seDV/DH8WvYkXoTm49fw48Hr+BqdmG1hUz2a+fOnRg0aBBCQ0MhCAI2btxYZpvTp09j8ODB8Pb2hqenJzp16oTLly9bP1giIiKq9WyalNqxYwcmTZqEvXv3IikpCWq1GrGxsSgoKNBvc+3atVJfK1euhCAIGD58eLnH9vLyKrOvq6trdV8SERGRefz9AVfXSveQSjp1HTmFqlLLcwpVSDp1HflFagsHS/auoKAArVu3xieffGJw/fnz59G1a1c0adIEycnJOHr0KN566y3eIxEREZFN2LRvf0JCQqnXq1atQmBgIA4ePIhu3boBAIKDg0tt8+uvv6Jnz56oV69euccWBKHMvkRERHbngQeAvXuBevXMriGVnqMok5DSySlUIT1HgcbBnpaIkhxEv3790K9fP6Pr33zzTfTv3x8LFy7UL6vonoqIiIiouthVofPc3FwAgJ+fn8H1169fx+bNm/HMM89UeKz8/HxERkYiPDwcAwcOxOHDhy0aKxERUaVt2QLs23f3dZMmlSpqXlBcfk+owgrWU+2i1WqxefNmNGrUCH379kVgYCA6duxocIgfERERkTXYTRVUURQxbdo0dO3aFS1atDC4zerVq+Hp6Ylhw4aVe6wmTZogPj4eLVu2RF5eHj7++GM8+OCDOHr0KBo2bFhme6VSCaVSqX+dl5cHAFCpVPov3WuqPmxn62A7Ww/b2jocrZ2FLVsgGT4ccHGBeudOoFmzSh/L1QkQRI3R9S5OlmsXR2tna3C0trhx4wby8/Px/vvv491338WCBQuQkJCAYcOGYfv27ejevbvB/Sq6T6IS/BkxrDa1i0ajgVwuhwRiub+bgbu/u3X/SiBCLpdDo9HUirYqT216z5iD7WIY28U4W7eNqecVRFEUqzkWk0yaNAmbN2/Grl27EB4ebnCbJk2aoE+fPli2bJlZx9ZqtWjbti26deuGpUuXllk/Z84czJ07t8zyNWvWwM3NzaxzERERGRNw+DA6/t//QaJS4VrHjtg/fTpEmczWYVElFRYWYvTo0cjNzYWXl5etwylDEARs2LABcXFxAICMjAyEhYXh8ccfx5o1a/TbDR48GO7u7li7dq3B4/A+iYiIiMxl6n2SXfSUmjJlCjZt2oSdO3caTUj9/fffSE1Nxfr1680+vpOTE9q3b4+zZ88aXD9z5kxMmzZN/zovLw8RERGIjY2Fl5cXVCoVkpKS0KdPH8j48FBt2M7WwXa2Hra1dThKOwtbtkDy/vsQVCpoBw9GnTVr0K8SQ/bul5GjwLaUG8hV3P00ylsuQ68mgQj1kVf5+DqO0s7WpOsx5Cjq1KkDqVSKZvf1zmvatCl27dpldL+K7pOoBH9GDKtN7ZKWloaYmBj8b/lG+IdGlLutIGoQVXQeF13rQxQkyMq4go8mxuHw4cO1vs5bbXrPmIPtYhjbxThbt42p90k2TUqJoogpU6Zgw4YNSE5ORnR0tNFtv/76a7Rr1w6tW7eu1HmOHDmCli1bGlzv4uICFxeXMstlMlmpb979r6l6sJ2tg+1sPWxr67Drdk5MBIYPB5RKIC4OTuvXw8kCCSkAiAyQYaSnG9JzFCgsVsPNWYowHzk8XKvnT7xdt7OVOVo7ODs7o3379khNTS21/MyZM4iMjDS6n6n3SVSC7WJYbWgXiUQChUIBDQSIgsSkfURBAlGQQAMBCoUCEomkxreTqWrDe6Yy2C6GsV2Ms1XbmHpOmyalJk2ahDVr1uDXX3+Fp6cnMjMzAQDe3t6Qy+9+upuXl4cff/wRH330kcHjPPXUUwgLC8P8+fMBAHPnzkWnTp3QsGFD5OXlYenSpThy5Ag+/fTT6r8oIiKie+3dCwwZok9IYf36ShU1L4+Hq5Sz7BGAkolezp07p3994cIFHDlyBH5+fqhbty5effVVjBo1Ct26dUPPnj2RkJCA3377DcnJybYLmoiIiGotmyalPvvsMwBAjx49Si1ftWoVxo0bp3+9bt06iKKIxx9/3OBxLl++DCenuxMJ5uTk4Pnnn0dmZia8vb0RExODnTt3okOHDha/BiIionK1aQP06AHI5dWSkCK614EDB9CzZ0/9a92wu7FjxyI+Ph5Dhw7FihUrMH/+fLz00kto3Lgxfv75Z3Tt2tVWIRMREVEtZvPhe6Z4/vnn8fzzzxtdf/+ne4sXL8bixYurEhoREZFluLoCGzcCTk5MSFG169GjR4X3V08//TSefvppK0VEREREZJxTxZsQERGRWRISgFmzAF1ywNWVCSkiIiIiovvYxex7RERENUZCQkntKKUSaNwYePJJW0dERERERGSX2FOKiIjIUu5NSA0dCowaZeuIiIiIiIjsFpNSRERElnB/QmrdOg7ZIyIiIiIqB5NSREREVcWEFBERERGR2ZiUIiIiqorMTGDYMCakiIiIiIjMxELnREREVREcDHz6KbB5M7BmDRNSREREREQmYk8pIiKiytBq7/5//Hjgxx+ZkCIiIiIiMgOTUkREROZKSADatweuX7+7TBBsFw8RERERkQNiUoqIiMgcuqLmhw4BCxfaOhoiIiIiIofFpBQREZGp7p1lLy4OmD/f1hERERERETksJqWIiIhMcX9Cav161pAiIiIiIqoCJqWIiIgqwoQUEREREZHFMSlFRERUHrUamDaNCSkiIiIiIguT2joAIiIiuyaVAn/+CXz4IfDRR0xIERHZsZs3byI3N7fS+3t7eyMgIMCCERERUXmYlCIiIjLk1i2gTp2S/0dGAsuW2TYeIiIq182bN9GgQUPk5VU+KeXl5Y1z584yMUVEZCVMShEREd0vIQEYORKIjweGD7d1NEREZILc3Fzk5eXihQXx8A0MNXv/7BsZWPHaOOTm5jIpRURkJUxKERER3eveoubr1wPDhgGCYOuoiIjIRL6BoQgIi7R1GEREZAIWOiciItK5f5a9775jQoqIiIiIqJowKUVERASUTUhxlj0iIiIiomrFpBQRERETUkREREREVsekFBER0R9/MCFFRERERGRlLHRORES0ZAnQogUwbhwTUkREREREVsKeUkREVDsdPAioVCX/d3ICnn+eCSkiIiIiIitiUoqIiGqfxETgwQeBxx+/m5giIiIiIiKrYlKKiIhql8REYMiQkhpSGg0giraOiIiIiIioVmJSioiIao97E1Isak5EREREZFNMShERUe3AhBQRERERkV1hUoqIiGo+JqSIiIiIiOwOk1JERFTzOTuXzLDHhBQRERERkd2Q2joAIiKiatezJ/DPP0Dz5kxIERERERHZCfaUIiKimmnLFuDkybuvY2KYkKIab+fOnRg0aBBCQ0MhCAI2btxodNsJEyZAEAQsWbLEavERERER3YtJKSIiqnkSEoDBg4FevYC0NFtHQ2Q1BQUFaN26NT755JNyt9u4cSP+/fdfhIaGWikyIiIiorI4fI+IiGqWhISS2lFKJfDgg0B4uK0jIrKafv36oV+/fuVuk56ejsmTJyMxMREDBgywUmREREREZdm0p9T8+fPRvn17eHp6IjAwEHFxcUhNTS21zbhx4yAIQqmvTp06VXjsn3/+Gc2aNYOLiwuaNWuGDRs2VNdlEBGRvbg3ITV0KLBuHYfsEd1Dq9XiySefxKuvvormzZvbOhwiIiKq5WzaU2rHjh2YNGkS2rdvD7VajTfffBOxsbE4deoU3N3d9ds98sgjWLVqlf61cwUPGHv27MGoUaPwzjvvYOjQodiwYQMeffRR7Nq1Cx07dqy26yEiItsREhOBESOYkCIqx4IFCyCVSvHSSy+ZvI9SqYRSqdS/zsvLAwCoVCqoVCqLx+iodG3BNinNmu2i0Wggl8shgQhB1Ji9vwQi5HI5NBpNpeI15/y69bp/q3rumoQ/S4axXQxjuxhn67Yx9bw2TUolJCSUer1q1SoEBgbi4MGD6Natm365i4sLgoODTT7ukiVL0KdPH8ycORMAMHPmTOzYsQNLlizB2rVrLRM8ERHZDb9TpyCZO5cJKaJyHDx4EB9//DEOHToEQRBM3m/+/PmYO3dumeVbtmyBm5ubJUOsEZKSkmwdgl2yVruU3OsrAMUZs/eN9i3ZPyUlBSkpKVY5f1TReYudu6bhz5JhbBfD2C7G2aptCgsLTdrOrmpK5ebmAgD8/PxKLU9OTkZgYCB8fHzQvXt3vPfeewgMDDR6nD179uCVV14ptaxv376cXYaIqIbKi4qC2KYNhNBQJqSIjPj7779x48YN1K1bV79Mo9Hgf//7H5YsWYKLFy8a3G/mzJmYNm2a/nVeXh4iIiIQGxsLLy+v6g7bYahUKiQlJaFPnz6QyWS2DsduWLNd0tLSEBMTg/8t3wj/0Aiz98/KuIKPJsbh8OHDqFevXrWeXxA1iCo6j4uu9SEKkiqfuybhz5JhbBfD2C7G2bptdD2rK2I3SSlRFDFt2jR07doVLVq00C/v168fRo4cicjISFy4cAFvvfUWevXqhYMHD8LFxcXgsTIzMxEUFFRqWVBQEDIzMw1uX1G3dFt3e6st2M7WwXa2Hra1dahUKqjd3FD066+QeXgAggCwzS2O7+eyHK0tnnzySTz88MOllvXt2xdPPvkkxo8fb3Q/FxcXg/dcMpmMDwAGsF0Ms0a7SCQSKBQKaCBAFCRm76+BAIVCAYlEUqlYK3N+UZBAFCRVPndNxJ8lw9guhrFdjLNV25h6TrtJSk2ePBnHjh3Drl27Si0fNWqU/v8tWrTAAw88gMjISGzevBnDhg0zerz7u6WLomi0q7qp3dLZJdA62M7WwXa2HrZ19Qg8dAiely/jfFwcACBp717bBlRL8P18l6nd0q0pPz8f586d07++cOECjhw5Aj8/P9StWxf+/v6ltpfJZAgODkbjxo2tHSoRERGRfSSlpkyZgk2bNmHnzp0Ir2Dq7pCQEERGRuLs2bNGtwkODi7TK+rGjRtlek/pVNQt3dbd3moLtrN1sJ2th21dfYTEREgWLICgVKJhbCwS3NzYztWM7+eyTO2Wbk0HDhxAz5499a919zdjx45FfHy8jaIiIiIiMsymSSlRFDFlyhRs2LABycnJiI6OrnCfrKwsXLlyBSEhIUa36dy5M5KSkkrVldqyZQu6dOlicHtTu6WzS6B1sJ2tg+1sPWxrC0tIuDvLXlwcnAYPBrZuZTtbCdv5Lntshx49ekAURZO3N1ZHioiIiMgabJqUmjRpEtasWYNff/0Vnp6e+t5N3t7ekMvlyM/Px5w5czB8+HCEhITg4sWLeOONN1CnTh0MHTpUf5ynnnoKYWFhmD9/PgDg5ZdfRrdu3bBgwQIMGTIEv/76K7Zu3VpmaCARETmYhAQgLk6fkML69SU1pIiIiIiIyOE42fLkn332GXJzc9GjRw+EhITov9avXw+gpFjg8ePHMWTIEDRq1Ahjx45Fo0aNsGfPHnh6euqPc/nyZVy7dk3/ukuXLli3bh1WrVqFVq1aIT4+HuvXr0fHjh2tfo1ERGQhhhJSnGWPiIiIiMhh2Xz4XnnkcjkSExMrPE5ycnKZZSNGjMCIESMqGxoREdmTS5eYkCIiIiIiqmHsotA5ERFRuSIjgffeA3btYkKKiIiIiKiGsOnwPSIionLd26P2f/8Dfv6ZCSkiIiIiohqiSkmpK1eu4OrVq5aKhYiI6K6EBKBHDyAn5+4yJ36WQkRERERUU5h9d69Wq/HWW2/B29sbUVFRiIyMhLe3N2bNmgWVSlUdMRIRUW2jK2q+cyfwwQe2joaIiIiIiKqB2TWlJk+ejA0bNmDhwoXo3LkzAGDPnj2YM2cObt26hRUrVlg8SCIiqkXun2Vv9mxbR0RERERERNXA7KTU2rVrsW7dOvTr10+/rFWrVqhbty4ee+wxJqWIiKjy7k9Isag5EREREVGNZfbwPVdXV0RFRZVZHhUVBWc+OBARUWUxIUVEREREVKuYnZSaNGkS3nnnHSiVSv0ypVKJ9957D5MnT7ZocEREVEsUFwMvvsiEFBERERFRLWL28L3Dhw/jr7/+Qnh4OFq3bg0AOHr0KIqLi9G7d28MGzZMv+0vv/xiuUiJiKjmcnYG/vwTWLQI+OQTJqSIiIiIiGoBs5NSPj4+GD58eKllERERFguIiIhqkdxcwNu75P9NmgBffGHbeIiIiIiIyGrMTkqtWrWqOuIgIqLaJjERePxx4IcfgIcftnU0RERERERkZWbXlCIiIqqyxERgyBAgOxtYudLW0RARERERkQ2Y3VMKAH766Sf88MMPuHz5MoqLi0utO3TokEUCIyKiGkqXkNIVNY+Pt3VERERERERkA2b3lFq6dCnGjx+PwMBAHD58GB06dIC/vz/S0tLQr1+/6oiRiIhqivsTUpxlj4iIiIio1jI7KbV8+XJ88cUX+OSTT+Ds7IwZM2YgKSkJL730EnJzc6sjRiIiqgmYkCIiIiIionuYnZS6fPkyunTpAgCQy+W4c+cOAODJJ5/E2rVrLRsdERHVHGvWMCFFRERERER6ZielgoODkZWVBQCIjIzE3r17AQAXLlyAKIqWjY6IiGqOr78GFi9mQoqIiIiIiABUIinVq1cv/PbbbwCAZ555Bq+88gr69OmDUaNGYejQoRYPkIiIHNixY4BWW/J/qRSYOpUJKSIiIiIiAlCJ2fe++OILaP97wHjhhRfg5+eHXbt2YdCgQXjhhRcsHiARETmohISSoXpjxgBffAE4mf05CBERERER1WBmJ6WcnJzgdM+DxaOPPopHH33UokEREZGD0yWklErg9m1Ao2FSioiIiIiISjErKZWXlwcvLy8AwB9//AG1Wq1fJ5FIMGDAAMtGR0REjufehNTQocC6dYBMZuuoiIiIiIjIzpiclPr999/x1ltv4fDhwwCAUaNGoaCgQL9eEASsX78eI0aMsHyURETkGAwlpFhDioiIiIiIDDB5LMUXX3yByZMnl1p27tw5aLVaaLVazJ8/HytXrrR4gERE5CCYkCIiIiIiIjOYnJQ6duwYWrdubXR9v379cODAAYsERUREVZdfpEZq5h0cupyNM5l3kF+krninqigqKqkdxYQUERERERGZwOThe5mZmfD399e/3r59OyIiIvSvPTw8kJuba9noiIioUq5mFyLp1HXkFKr0y3zcZOjTLAjhvm7Vc9K4OGDHDuCBB5iQIiIiIiKiCpncU8rPzw/nz5/Xv37ggQcgu6dw7dmzZ+Hn52fZ6IiIyGz5ReoyCSkAyClUIenUdbN6TFXY22rrVuDixbuvu3RhQoqIiIiIiExick+pbt26YenSpXj44YcNrl+6dCm6detmscCIiKhy0nMUZRJSOjmFKqTnKNA42LPC41TY20pXQyo4GPjnHyAszFKXQEREREREtYDJPaVee+01bNmyBSNHjsT+/fuRm5uL3Nxc7Nu3D8OHD8fWrVvx2muvVWesRERkgoLi8ntCFVawHqi4t5Vi0+a7Rc1jYoCAgKqETEREREREtZDJSamYmBisX78eycnJ6NSpE/z8/ODn54fOnTtjx44dWLduHdq2bVudsRIRkQncncvvBOtWwXqg/N5W3jv+gsujw0sSUnFxwPr1HLJHZCd27tyJQYMGITQ0FIIgYOPGjfp1KpUKr732Glq2bAl3d3eEhobiqaeeQkZGhu0CJiIiolrN5OF7ADBkyBD06dMHiYmJOHv2LACgYcOGiI2Nhbu7e7UESERE5gnzkcPHTWYwqeTjJkOYj7zCYxjrbRW5fycGz5kEJ1UxE1JEdqigoACtW7fG+PHjMXz48FLrCgsLcejQIbz11lto3bo1srOzMXXqVAwePJgzKBMREZFNmJWUAgA3NzcMHTq0OmIhIrKq/CI10nMUKChWw8NZilAfOTxczf61aHcxeLhK0adZkNF6UKYc31Bvq/Cj/2LwnEmQqopx55GB8GRCisju9OvXD/369TO4ztvbG0lJSaWWLVu2DB06dMDly5dRt25da4RIREREpGfdpy8iIjtRYRFvB48h3NcNI9tFID1HgcJiNdycpQgzI+FlqLdVVt0GyAmLQkFEFPyZkCKqEXJzcyEIAnx8fIxuo1QqoVQq9a/z8vIAlAwHVKkMD/OtjXRtwTYpzZrtotFoIJfLIYEIQdSYvb8EIuRyOTQaTaXiNef8uvW6f6t67pqEP0uGsV0MY7sYZ+u2MfW8TEoRUa1TURHvke0iqr3HlDVi8HCVmjTLnrF97+9tpfD1x5ZP16Jbu3rw8LJO4o6Iqk9RURFef/11jB49Gl5eXka3mz9/PubOnVtm+ZYtW+Dmxt8F97u/NxqVsFa7rF27FoACUJwxe99o35L9U1JSkJKSYpXzRxWdt9i5axr+LBnGdjGM7WKcrdqmsLDQpO2YlCKiWqe8It45hSqk5ygqncxxpBgqEu7rhseyTiH33EVcGzHa7N5WRGS/VCoVHnvsMWi1WixfvrzcbWfOnIlp06bpX+fl5SEiIgKxsbHlJrNqG5VKhaSkJPTp0wcymczW4dgNa7ZLWloaYmJi8L/lG+EfGmH2/lkZV/DRxDgcPnwY9erVq9bzC6IGUUXncdG1PkRBUuVz1yT8WTKM7WIY28U4W7eNrmd1RWz6ZDF//nz88ssvSElJgVwuR5cuXbBgwQI0btwYQEkjzpo1C3/88QfS0tLg7e2Nhx9+GO+//z5CQ0ONHjc+Ph7jx48vs1yhUMDV1bXaroeIHIOxIt46hRWsrykxVCghAfJHh0OuVCK4eQOgb19bR0REFqBSqfDoo4/iwoUL2LZtW4WJJRcXF7i4uJRZLpPJ+ABgANvFMGu0i0QigUKhgAYCREFi9v4aCFAoFJBIJJWKtTLnFwUJREFS5XPXRPxZMoztYhjbxThbtY2p5zQ7KZWeno6ff/4ZZ86cgSAIaNSoEYYNG4awsDCzg9yxYwcmTZqE9u3bQ61W480330RsbCxOnToFd3f3Ks0S4+XlhdTU1FLLmJAiIsBwEe97uVWwvqbEUK6EhJLZ9ZRKYOhQoGdP28ZDRBahS0idPXsW27dvh7+/v61DIiIiolrMrKee5cuXY9q0aSguLoa3tzdEUUReXh5effVVLFq0CBMnTjTr5AkJCaVer1q1CoGBgTh48CC6detWpVliBEFAcHCwWfEQUe1gqIi3jo+bDGE+8loRg1H3J6TWrWNRcyIHkZ+fj3PnzulfX7hwAUeOHIGfnx9CQ0MxYsQIHDp0CL///js0Gg0yMzMBAH5+fnDmzzkRERFZmZOpG27evBkvvfQSJk+ejPT0dGRnZyMnJwfp6emYOHEiXn75Zfzxxx9VCiY3NxdAyY1RedtUNEsMUHJTFhkZifDwcAwcOBCHDx+uUmxEVHPoinj7uJXuUqqb+c4aNZPsIQaDmJAicmgHDhxATEwMYmJiAADTpk1DTEwM3n77bVy9ehWbNm3C1atX0aZNG4SEhOi/du/ebePIiYiIqDYy+aln4cKFeP311/Huu++WWh4SEoJFixbBzc0NCxYsQP/+/SsViCiKmDZtGrp27YoWLVoY3MbUWWKaNGmC+Ph4tGzZEnl5efj444/x4IMP4ujRo2jYsGGZ7Sua6tjWUynWFmxn62A7lwjykGFo62Bk5BRBoVJDLpMi1McV7i5Si7VNRW1tjRjMcuYMpHFxEJRKaIcMgebbbwFBAOz8vcL3tHWwncuyx7bo0aMHRFE0ur68dURERETWZnJS6vDhw/jiiy+Mrn/yySfx8ccfVzqQyZMn49ixY9i1a5fB9ebMEtOpUyd06tRJ//rBBx9E27ZtsWzZMixdurTM9qZOdcxpJq2D7WwdbOeyzlbTcc1p6+qKwSSiiKaDBsHj6lUcGDMG4tattozGbHxPWwfb+S5TpzomIiIiIsNMTkpptdpyq6fLZLJKf/o2ZcoUbNq0CTt37kR4eHiZ9ebOEnM/JycntG/fHmfPGn7cq2iqY1tPpVhbsJ2tw17auUCpRkZOEQpVarjLpAj5r4eQLWXkKLAt5QZyFXd7P3jLZejVJBChlajxpGvrlh0ewo5zt/XHdRKASH93XMkugFpzd3tD56pKTBk5CiSn3oCrTILjV3Nx+HIOijUa+LjJEO7jhh5NAlEvwB2HLt5GtqJktr8Lz86Fj7MTerYINfuaLd1+prKX93RNx3Yuy9SpjomIiIjIMJOfAJs3b45ff/0Vr7zyisH1GzduRPPmzc06uSiKmDJlCjZs2IDk5GRER0eX2cYSs8SIoogjR46gZcuWBtebOtUxp5m0Drazddiyna9mFyLp1PVSRb51tZTCfd3K2bN65BepcTW7EJuOZiBPoYKnqwzO0pKSezlFWmw7k4WR7SIqXedpx7nbyCnSAv9ND+3n4Yy/Um4ht0iFZiHeRs+VX6TGtjNZpfY1NSbdvjKJFIeu5OLw5VwUFJdkwIrz1RCFYmw5dRN9riSh9+Y12PDaIqhd5YAAZKth9jVXJVZL4e8O62A738V2ICIiIqoakwudT5w4EW+++SaWL18OtVqtX65Wq/Hpp59i1qxZePHFF806+aRJk/Ddd99hzZo18PT0RGZmJjIzM6FQKPTHHjFiBA4cOIDvv/9eP0tMZmYmiouL9cd56qmnMHPmTP3ruXPnIjExEWlpaThy5AieeeYZHDlyBC+88IJZ8RGR5eUXqcskpAAgp1CFpFPXkV+kNrJn9biaXYgfD17B/ou3cfBSNs7eyMepa7nIK7obX06hCuk5ikqf496eQwAgArh+R4kilRZ3isq2g+5c6TkKg7PzmRKTbl8RwI07ShQU3/N7WytCqdbAf3cyBs6eiPB/tqHtz6vMOr6x81UmViIiIiIiqp1M/th67NixOH78OCZPnoyZM2eifv36AIDz588jPz8fL730EsaNG2fWyT/77DMAJUU577Vq1SqMGzdOP0sMALRp06bUNtu3b9fvd/nyZTg53c2v5eTk4Pnnn0dmZia8vb0RExODnTt3okOHDmbFR0SWZ0ryonGwp1ViuTdB5nnP0MEilRZpN/NL9WIqLLZcskyp0ur/r9Joy6zXnauggnOWF5NuX6VKC7Wm7NDqNif/xayv34RUXYyLD/XBgUefNev4xs5XmViJiIiIiKh2MmssxYcffogRI0Zg7dq1+vpM3bp1w2OPPVaqsLipKqpBFRUVZVKdquTk5FKvFy9ejMWLF5sdDxFVP3tKXtybIHORle44quvF5O9RMrTXzdlyQ8/uPZdMUrbDqu5c7hWcs7yYdPu6yJwglQil1nU4sx9vfTsbzupinOncG/vf/QRaZdljmHPNVYmViIiIiIhqJ7OfEu6f2Y6IyBzWTl7kF6mRnqNAQbEaHs5ShPrI9fWarmYX4lquAs4SJ/jKZQjydMH1O3ezM7peTD5uMoRVoVC3t1xWUmvpPwKAIE8X5BaV1K+6173nCvORw8dNZrBnWUUx6fbVaER4y2VwljihoFiNLucO4v3v3oazWoUj7Xpg11tL4CNzBpTFpfY395qrEisREREREdVOJj/9Xb582aTt6tatW+lgiKjms2bywlhB9Q7Rfjhw8TacBAGXskqmdM/IVaB7owAcvJitT0zJJE76AuxVKdLdq0lgSRHw/+LIKihG72ZBuHy79Ox795/Lw1WKPs2CjBaFLy8mD1cpOkT74dvdlxDh5wa1RosT569j1k8fwEWtwtEHeuDy8pXoFeaH/Rdvl9q3MtdclViJiIiIiKh2Mvkp4d6Z8XRD6gRBKLVMEARoNJoy+xIR6VgreWGsoPqNPCXi/7mI1uHeAKDvHVWg1GDHmZvo1iAAkv+Gu7WJ8EV0HfcqxxTqI8fIdhFIz1GgsFgNN2epPvl2/7L7zxXu62Zw34piyi9S48DF22gR5gVBAKLruOGRFiE4HLkajX5aDenHy9Er1BcerlLU9XM3+/iGVDZWIiIiIiKqnUx+UhAEAeHh4Rg3bhwGDRoEqZQPGURUOdZIXhgrqH6nSIXLtwvRKtwbtwuK8WCDOvjn3C19YurczXzUD/RAn2ZBCPd1s1g8Hq5SgwXcTSnq7uFa0j66YYgZOQr9MERj0nMUuF1Qcv1SRSHU8v+uJbQRTrz0Hvq7uZbqkWWp4vKWPBYREREREdVsJj8BXr16FatXr0Z8fDxWrFiBMWPG4JlnnkHTpk2rMz4iqqGqO3lhrKB68X91opRqLbQikJlXhA7RfhD/W9YoyANt6/rZVe8eY8MQy0uc6a4/cv9O9P3gdWya+xkym7bWr+dseEREREREZGtlp30yIjg4GK+99hpOnz6Nn376CdnZ2ejYsSM6deqEL7/8Elpt2WnNiYhsxVhBdef/ZrtzkZb8qxWBm/nFuJVfjDtFagR5Va7HVn6RGqmZd3DocjbOZN5BfpFlkj7GhiHmFKqQdOq60fO4O0sRuX8nBs+ZBPecLLT6fW2p9ZwNj4iIiIiIbM3kpNS9unbtiq+//hpnz56Fm5sbXnjhBeTk5Fg4NCKiytMVVL+fp6sMdf3cIBjYp7KF1q9mF+LHg1fwx/Fr2JF6E5uPX8OPB68gI0dRichLMzYMEShJTKUbOUfdfTsxeO4kSFXFOPtgH2ydOk+/jrPhERERERGRPahUUmr37t149tln0ahRI+Tn5+PTTz+Fj4+PhUMjIqo8XUH1+xNTgV4uGPdgFDT/TdigU9lC6+X1ZNqWcqNywd/D2DBEHYPD8BISIH90OKTFxbjULRZ/vLEIWpkzAM6GR0RERERE9sPkp5Jr167hm2++wapVq5CdnY0nnngCu3fvRvPmzaszPiKiSiuvoHqkhWacK68nU65CBb8qXoOxYYg6ZYbhJSQAcXGAUgkMHQr/+O/wSKGGs+EREREREZHdMfnJJDIyEqGhoRg7diwGDx4MmUwGjUaDY8eOldquVatWFg+SiKiyjBVUt1Sh9Yp6MlWVbhhiTqEKxWot7hSpUKzRwlnihLp+bmWH4a1YoU9IYd06eDg7o7FXtYZIRERERERUKSYnpdRqNS5fvox33nkH7777LgBAvG/4iyAI0Gg0lo2QiMiOVdSTqap0wxA3HL6KI1dyUaQqmVQiyNMFQd6uyFEUl+75tG4dsGgRMH064OxcrbERERERERFVhclPUxcuXKjOOIiIHNK9PZnu5y2XAVWvdQ4fuTPCfdzg09QZSrUWLlInCAAu3irA7YJiPOpZCPeWzQBBAFxdgTfeqPpJiYiIiIiIqplZw/eIiKg0XU+m+4ud+7jJ0KuRP47sPlnlc6TnKHAxq1D/+s4967x3/AX5vMnAyy8D779fkpgiIiIiIiJyACYnpXbu3Glwube3Nxo0aAB3d3eLBUVE5EiMFVR3kYg4YoHjG6tbFbl/JwbPmQQnVTFw5gyg0QBSFjEnIiIiIiLHYPLTS48ePYyuk0gkePHFF/HRRx9BJpMZ3Y6IqKYyVDhdpTI8K5+5DNWt0iWkpKpi3HlkIDzXr2dCioiIiIiIHIrJTzDZ2dkGl+fk5GDfvn149dVXERwcjDdYy4SIyKLur1t1b0Lq0kOx8F+/nkXNiYiIiIjI4ZiclPL29ja6PDIyEs7OznjjjTeYlCIisrB761Z57/irVEJK8uN6eHi52TpEIiIiIiIis1lsrEfr1q1x6dIlSx2OiIjuoatbdeegGhK1CnceGQj/9UxIERERERGR47JYUiojIwOBgYGWOhwRkVXlF6mRnqNAQbEaHs5ShPrI4eFqXzWaPFyl8HjpBaBpfXh2725wyJ4jXEdl1NTrIiIiIiKqzSxyR3/jxg3MmjULvXr1ssThiIis6mp2IZJOXdfXbAIAHzcZ+jQLQrivHfRE2r4daNYMCAoqed2nj8HN7P46KqmmXhcRERERUW1nclIqJiYGgiCUWZ6bm4urV6+iadOmWLdunUWDIyKqbvlF6jIJDwDIKVQh6dR1jGwXYdseOQkJQFwcUK8esHMnUKeOwc3s/joqyZTrcpHYKDgiIiIiIqoSk59Q4uLiDC738vJCkyZNEBsbC4mETwZE5FjScxRlEh46OYUqpOco0DjY08pR/UeXkFIqgSZNAC8vo5va9XVUgSnXVc/f1cpRERFRbXI1uxDHr+Yi7VYBrmYXolgtQqPVwlsuQ4SfG+oFuKN9lJ+twyQickgmJ6Vmz55d7vrTp09jwIABSEtLq3JQRETWUlCsLnd9YQXrq829CamhQ4F16wzWkNKx2+uoopp6XUREZN/SbuZj/YEr2Hb6Bs7eyK9we4mTgCZ1XODZdiCKNaIVIiQiqhksNpajuLiYs+8RkcNxdy7/16BbBeurpQC3mQkpoOrXoWNvBcUtdV1ERESmcIlogdcTruDA1VT9MicBaB7qjQaBHoj0d4OrTAInAcgqKMbVbAVOZeThwq0CnLxRBL8+L2DDORWaFd5Ax2g/uLvw7xQRUXn4W5KIarUwHzl83GQGh4j5uMkQ5iM3um+1FOD+6y+zE1JA1a5Dxx4Lipt2XfxEmoiIquZ2QTGSr6gQPPp9HLhaCEEAejUOxJCYMHRvGABvN1m5+1/NLsTanSexeNN+OAdE4nh6LlIy89C2ri/aRfpCJnGy0pUQETkW/nYkolrNw1WKPs2C4HPfzaYuGWOsl1BFBbjziyo5rKxhQyA01KyEVFWuQ6farqeKqnpdRLXNzp07MWjQIISGhkIQBGzcuLHUelEUMWfOHISGhkIul6NHjx44efKkbYIlsgNarYh9F27j+38vIaNAhKjVYFBTH+yY3hNfj2uPwa1DK0xIAUC4rxuGNvfFtZWT0DtCimAvV6g0Iv69cBtr911GZl6RFa6GiMjx8G6eiGq9cF83jGwXgfQcBQqL1XBzliKsgmFr1VaAu25d4J9/AH9/kxNSOpW5Dh17LpRelesiqm0KCgrQunVrjB8/HsOHDy+zfuHChVi0aBHi4+PRqFEjvPvuu+jTpw9SU1Ph6el4kyEQVUV2YTEST2biep4SABDqLmD/kol4+d9k1PWvfA/hIHcnNG8YjnM38rHjzE1kF6rww4Er6BTtj/ZRvgZnNCciqq1MvqP39S3/F6hazWKzROS4PFylZiVdLFqAOzERyM8HdA+QISGm73sfc69Dx94Lilf2uohqm379+qFfv34G14miiCVLluDNN9/EsGHDAACrV69GUFAQ1qxZgwkTJlgzVCKbSruVj8ST11Gs1sJF6oQejQPgp76NPbfTLXJ8QRDQMMgTEX5u2J5yA2du5GNPWhZu5SvRp1kQnDlehYgIgBlJqSVLllRjGEREjsViBbgTE4EhQwC1GkhOBrp2rXpwlcCC4kQ134ULF5CZmYnY2Fj9MhcXF3Tv3h27d+82mpRSKpVQKpX613l5eQAAlUoFlcpwD8vaSNcWbJPSzG2XW7du6d9j5rpy5QrkcjkkECGIGoPbiKKI/ZdysDstGwAQ4u2CAS2C4OEiRVbGbcjlcmg0mkp9HzUaTZnzy6VA/xaBqJvhim2pt3D2Rj5yFcWIaxkIAPrtJBAhl8tx8eJFaDSGY6+Il5cX6tSpU6l97Ql/lgxjuxjGdjHO1m1j6nlNfsoYO3ZspYMhIqppLFKAW5eQUipLipt36FAtsZrCEoXSici+ZWZmAgCCgoJKLQ8KCip3BuX58+dj7ty5ZZZv2bIFbm62mQTBniUlJdk6BLtkrXZZu3YtAAWgOFNmnVYEfrrghN3XS7opdQ3SYmhUAaTaNEABRPuW7J+SkoKUlBSLnj/aF2jWDFiZKsGNO8XYcOgKQpoBUTivX7927VoUFBRU+tw1DX+WDGO7GMZ2Mc5WbVNYWGjSdlX66HvixImYN29ejcjIExGZQ1eA29hsdR6u0vI/Hbg/IbV+vdk1pCzJlOtxBPlFaqTnKFBQrIaHsxShrD1FVMb95RhEUSy3RMPMmTMxbdo0/eu8vDxEREQgNjYWXl5e1Rano1GpVEhKSkKfPn0gk1VcGLu2MKdd0tLSEBMTg6fnfQbfOuYPZb+Uegw/ffw2nn1/Neo1aVFqnVorIvHkDZy9WQAA6NnIH63DvXHlnm2yMq7go4lxOHz4MOrVq2f2+XXx/2/5RviHRpRZ7yQHRniq8PORa7hZpMbSExLExYTB290V547+i5WzJ+LR6QsRUa+R2efOvnUNK99+sdKx2xP+LBnGdjGM7WKcrdvG1F6vVbpT/+677zB9+nQmpYioVqp0AW47S0jpOHpB8YwcBbadyTKYVAv3ZW8OouDgYAAlPaZC7qldd+PGjTK9p+7l4uICFxeXMstlMhkfAAxguxhmSrtIJBIoFAp41QmFX1ik2ee4eT0DCoUCGhEQBYl+uUYr4o8T15B2qwASQUDf5kFoGORZpj+zBgIUCgUkEkmlvoe6+DUQSp3/Xt7uEoxsF44Nh9ORXajCT0dvYGS7cKi1gEKhgLt/EPzCosw+d1Vjt0f8WTKM7WIY28U4W7WNqeesUok9UaxgaAoRkRXlF6mRmnkHhy5n40zmHeQXVX9xbl0B7pi6vmgc7FlxAufECbtMSOmYfT12ZFvKjTLDD3MKVUg6dd0q7wUiexcdHY3g4OBS3fiLi4uxY8cOdOnSxYaREVUfjVbEn7qElJOAQa1D0DDIthNneLrKMCImBIGuIu4UqbHxcAaKtZyRj4hqJ8d52iAiKsfV7EKjQ8/sqpdMs2bA+PFAZqbdJaQcXa5CBRj4ZDqnUIX0HAVn7yO7Vq9ePezfvx/+/v6llufk5KBt27ZIS0sz6Tj5+fk4d+6c/vWFCxdw5MgR+Pn5oW7dupg6dSr+7//+Dw0bNkTDhg3xf//3f3Bzc8Po0aMtej1E9kAURWw5lYnzN/9LSLUKQaS/u63DAgC4u0jxYjMNPjrpgtuFxTgo8YYgLdsjkYiopqtSUurOnTuWioOIqNLyi9RlElLA3V4yI9tF2E+PHycn4NNPAY0GYBdjqyksZk8psm/GZtxSKpVITzd9ivoDBw6gZ8+e+te6WlBjx45FfHw8ZsyYAYVCgYkTJyI7OxsdO3bEli1b4OnJpC3VLKIoYufZWzhzPR9OAjCgpf0kpHT8XIBhbULww6EM5Kpk8B/wCjgQhYhqm0oN38vJycGBAwdw8OBB5OTkVPrk8+fPR/v27eHp6YnAwEDExcUhNTW11DaiKGLOnDkIDQ2FXC5Hjx49cPLkyQqP/fPPP6NZs2ZwcXFBs2bNsGHDhkrHSUT2LT1HYXDWOOBuLxlbEhITgSeeAHSFz52cmJCyMjdnO0lKEt1n06ZN2LRpEwAgMTFR/3rTpk3YsGED3nnnHURFRZl8vB49ekAUxTJf8fHxAEqKnM+ZMwfXrl1DUVERduzYgRYtWpR/UCIHdOhyDo5cyQEA9GkWhOg69pWQ0vFzd8bAlqEQIMK9SVecyef9ARHVLmbdpV+8eBGTJk1CYmKivp6UIAh45JFH8Mknn5h10wQAO3bswKRJk9C+fXuo1Wq8+eabiI2NxalTp+DuXvKHY+HChVi0aBHi4+PRqFEjvPvuu+jTpw9SU1ONfqq3Z88ejBo1Cu+88w6GDh2KDRs24NFHH8WuXbvQsWNHs2IkIvtXUEEvGFv2kgk8dAiSBQtKaki1awfcM4MVWZa3XIacIm2Z5T5uMoT5yG0QEVHF4uLiAJTcT40dO7bUOplMhqioKHz00Uc2iIzIcV1TSHDo2i0AwEMN6qBJsH3PEhnmK0dztzs4UeiFc/kynLl+B41sXPeKiMhaTE5KXblyBZ06dYJMJsM777yDpk2bQhRFnD59Gp999hk6d+6M/fv3Izw83OSTJyQklHq9atUqBAYG4uDBg+jWrRtEUcSSJUvw5ptvYtiwYQCA1atXIygoCGvWrMGECRMMHnfJkiXo06cPZs6cCaBkKuMdO3ZgyZIlWLt2rcnxEZFjcK+gF4yteskIiYnoMH8+BJUKGDoUmDzZJnHUFr2aBBqdfc9uhm8S3UerLUmkRkdHY//+/ZzRmKiKZIH1cDS3pF5j63BvtI30tXFEpgl3UeKf5F/g3WEYtp6+jgAPF/i6s+4kEdV8Jt+lz549G40bN0ZiYiJcXV31y4cOHYpXXnkFjzzyCGbPno2vv/660sHk5uYCAPz8/ACUFOfMzMxEbGysfhsXFxd0794du3fvNpqU2rNnD1555ZVSy/r27YslS5YY3F6pVEKpVOpf5+XlAQBUKpX+S/eaqg/b2TpqYjsHeUjh4+pUUuj6Pt5yGYI8pFa/XiExEZIRIyCoVFAPHgzx228BQbg7hI8sRve9DXCXYmjrYGTkFEGhUkMukyLUxxXuLtb//tdENfF3R1VZsi0uXLhgsWMR1VZKrYDA4bOgEQXU9XNDt4YBtg7JLDnJ8YjuOgS3iyXYfPwaRrWPgExSpcnSiYjsnslJqYSEBPzwww+lElI6crkc77zzDh577LFKByKKIqZNm4auXbvqaxtkZmYCAIKCgkptGxQUhEuXLhk9VmZmpsF9dMe73/z58zF37twyy7ds2QI3t7uzdt07hTJVH7azddS0dvb776sMBbDjr4rr0FlS4KFD+h5SGZ064cCTT0LcutWqMdRGht7TZ20QR01X0353VEVhYaFFj/fXX3/hr7/+wo0bN/Q9qHRWrlxp0XMR1TRarYijBV6QejnDXaJF/xbBcHISbB2WeUQtYnyU2JPjiayCYmxPvYHYZsG2joqIqFqZnJTKysoqt2ZUvXr1kJWVVelAJk+ejGPHjmHXrl1l1glC6T8ooiiWWVaVfWbOnKmfnQYo6SkVERGB2NhYeHl5QaVSISkpCX369IGMxYmrDdvZOmpyOxco1QZ7yVhVXh6k48fre0gdePJJPNy/f41ra3tSk9/T9oTtXJauZ7UlzJ07F/PmzcMDDzyAkJCQCu9ziKi0fy/cxm21M7TFCjwQBrjIJLYOqVJcJUC/FsH45VA6Tl+7gyh/d9aXIqIazeSntdDQUJw8edJozagTJ04gJCSkUkFMmTIFmzZtws6dO0sdPzi45JOBzMzMUse+ceNGmZ5Q9woODi7TK6q8fVxcXODi4lJmuUwmK3Xjff9rqh5sZ+uoie3sI5PBx8PGBa39/YGffwa++griF19A3Lq1Rra1PWI7Wwfb+S5LtsOKFSsQHx+PJ5980mLHJKotLmUVYN/F2wCArIRP4DFhko0jqppwXze0j/LDvou3sS3lBoK9XeHlyt+7RFQzmTxIeciQIXj11Vdx8+bNMutu3LiB1157TT+DjKlEUcTkyZPxyy+/YNu2bYiOji61Pjo6GsHBwaWGChQXF2PHjh3o0qWL0eN27ty5zPCCLVu2lLsPEVGV3FOXDj17At9/DzizQCkRmaa4uJj3KUSVkF+kRuLJ6wCACGcFCk/vsHFEltEh2g/BXq5QqrXYcvI6tP/NfE5EVNOYnJSaPXs2ioqKUL9+fUycOBFLly7F0qVL8cILL6BBgwZQKBR4++23zTr5pEmT8N1332HNmjXw9PREZmYmMjMzoVAoAJQMwZs6dSr+7//+Dxs2bMCJEycwbtw4uLm5YfTo0frjPPXUU/qZ9gDg5ZdfxpYtW7BgwQKkpKRgwYIF2Lp1K6ZOnWpWfEREhuQXqZGaeQeHLmfjTOYdKDZtBho1Ak5WrnbV/cfLL1JbOGIisnfPPvss1qxZY+swiByKViviz5PXoFBpEODhgiZu+bYOyWIkTgL6Ng+CTCIgPUeBI1dybB0SEVG1MHn4nq+vL/7991+88cYbWLduHXJycgAAPj4+GD16NN577z39rHmm+uyzzwAAPXr0KLV81apVGDduHABgxowZUCgUmDhxIrKzs9GxY0ds2bIFnp53x1ZfvnwZTk5382tdunTBunXrMGvWLLz11luoX78+1q9fj44dO5oVHxHRvfKL1EjJzMXpa3cAQYAgivBI3op6cyYBxcXAkiXAl1+adcyr2YVIOnUdOYV3Z/HycZOhT7MghPu6lbOndeQXqZGeo0BBsRoezlKE+sjh4WrlOl1EtUBRURG++OILbN26Fa1atSozNHDRokU2iozIfu1Jy0JGThGcJU7o3zIY11Ou2joki/Jxc0a3hgH4K+UGdp/PQnQdd/i6sRc2EdUsZj1Z+Pr64rPPPsPy5cv1w/gCAgIqXYxTNKEbqiAImDNnDubMmWN0m+Tk5DLLRowYgREjRlQqLiKi+13NLsQfx67hn/O3UKQqmRWrR9pBPL5kOiTFxbj0UCz8P/oYHmYcM79IXSYhBQA5hSoknbqOke0ibJoAsveEmTHVmUhjko6qy7Fjx9CmTRsAJXU678Wi50RlXb5diAOXsgEADzcNhI+bM67bOKbq0DzUC2du3MGV2woknbqOEe3C4cTfCURUg1TqTloQBAQGBlo6FiIiu6RLHl2+XahPSLU+uRfPfPY6JOpiXO4Wi42vfYRHCjVo7GX6cdNzFGUSUjo5hSqk5yjQONg2M+7Ye8LMmOpMpDlqko4cw/bt220dApHDUKo0SDpVkoJqGeaNhjV4djpBEPBwkyB8/+9lXMstwtErOYip62vrsIiILMbkJ4pevXqZtN22bdsqHQwRkT3SJY+KNXcTUtM/ex3O6mLsa90NZ99ZBq0SKCw2rxZUQQXbm3s8S7LnhJkxBcrqS6Q5apKOiKgmSj5zE/lKNbzlMjzUsI6tw6l2XnIZujaog22pN7AnLQsNAj3gydn4iKiGMPkOOjk5GZGRkRgwYACngiaiWkWXPHKWOAGiiMFbvtMnpJY89w56CVIAarg5m5eUcK9ge3OPZ0n2nDAzJiOnqNoSaY6YpCPH0rNnz3KH6fFDP6IS527kIyXzDgTgv0LgJs/b5NBahHnhdGYeruUWYeeZWxjQKsTWIRERWYTJTzzvv/8+4uPj8eOPP+KJJ57A008/jRYtWlRnbERUwzlKfR5d8sjTVQZXZwk+fGEBBiV9j5/7j4dGKoOL1AkSNxnCfOQmHzO/SA1RFKHSaJGnUMHTVQZn6d0bax8zj2dp9pwwM6ZQVX2JNEdM0pFj0dWT0lGpVDhy5AhOnDiBsWPH2iYoIjtToFRjW8oNAMADUb4I8bbd30lrEwQBPRsHYu3+yzh3Mx8XbhUguo67rcMiIqoyk58qZsyYgRkzZmDPnj1YuXIlHnzwQTRu3BhPP/00Ro8eDS8vMwqpEFGt50j1ecJ85AjPvY6r3kGoF+CBNAA/DH4eABDk6QIPFynaR/uZnFDTXXueQoVIPzf8c+4WrmQXol6AB7xcZfp2sGWCLsxHDh83mcHeQbZOmBnjJqu+RJojJunIsSxevNjg8jlz5iA/v+ZMc09UWaIoYlvKDShUGtTxcEbHaH9bh2R1AZ4uiInwwaHLOUhOvYFw38ha01OMiGous3+Lde7cGV9++SWuXbuGSZMmYeXKlQgNDUVeXl51xEdENVBF9Xnyi+yr14lH8laMGNMHD/3yNbxcZWgW4o2GgR5oF+mLp7tGo0+zYJMTafdeu1YEMvOK0CHaDw83DUK9Ou54uGkQRraLsHlizsNVij7NguDjVnq4tj0kzIwJ9XEtE69OVRNpuiRddRybqDxjxozBypUrbR0Gkc2dvnYHabcKIBEExDYLhsSpds5A1zHaHx4uUuQVqbH/4m1bh0NEVGWVfqo4dOgQduzYgdOnT6NFixasM0VEJnOo+jwJCUBcHASlEm0yzsCzeRAK1Vq4OUsRVonhhvdfu1YEbuYX6187S53sJuET7uuGke0ikJ6jQGGxutLXbC3uLiWJNGM98KoSty5JVx3HJirPnj174OrqauswiGzqTpEKO87cBAB0queHAE8XG0dkO85SJ/RoHIDfj13DwUvZaBzkCX+P2tseROT4zLqLzsjIQHx8POLj45GXl4cxY8bg33//RbNmzaorPiKqgRymPs9/CSkolcDQoZCuW4fGzs5VOqTDXPt/PFyl9pMgNEF1JtIcLUlHjmXYsGGlXouiiGvXruHAgQN46623bBQVke2JoojtqTdRrNEi2MsVbSN9bR2SzdWr447oOu64cKsA21NvYnjbMFuHRERUaSbfSffv3x/bt29HbGwsPvjgAwwYMABSKW/Eich8DlGf576EFNatA6qYkAIc5NodXHUm0hwtSUeOw9vbu9RrJycnNG7cGPPmzUNsbKyNoiKyvbM3Sop6OwnAw00D4VTOLJW1hSAI6NEoAFduFyI9R4HUzDuofRW2iKimMPnpJyEhASEhIbh8+TLmzp2LuXPnGtzu0KFDFguOiGomuy+iXU0JKcABrp2IbGLVqlW2DoHI7hSpNEhOLRm21z7Kj8PU7uEll6FDtB92n8/CP+ez0D+SyToickwmJ6Vmz55dnXEQUS1i9/V5UlOrJSEF2P+15xepkZ6jQEGxGh7OUoRyeBqRVR08eBCnT5+GIAho1qwZYmJibB0Skc38ffYWFCoN/Nyd8UAUh+3dLybCByfSc5FXpMapLM7CR0SOiUkpIrIYcxIadl2f5+WXgagooF8/iyakdOz12q9mFxpNltl6NkCimu7GjRt47LHHkJycDB8fH4iiiNzcXPTs2RPr1q1DQECArUMksqrLtwtx6lrJ7N4PNw2E1IlJl/tJJU7o2qAO/jiRidO3tZB48vcEETmeSj0BHTt2DGfOnIEgCGjYsCFatWpl6biIyMFUJqFhV/V5/v4baNkS8PEpeT1kSLWezq6uHSUJxfu/f0DJbIhJp65jZLsImyfNiGqyKVOmIC8vDydPnkTTpk0BAKdOncLYsWPx0ksvYe3atTaOkMh61FoRf52+DgBoHe6NEG8ObTemQaAHwnzkSM9RwLf7WFuHQ0RkNrM+cti3bx9atmyJmJgYPProoxg5ciRiYmLQqlUr7N+/v7piJCI7V1FCI7/IvmaUKyMxEejTB+jbF8jLq5ZT5BepkZp5B4cuZ+NM5h2z26Sq+1ckPUdhsM4VUPJ9TM9RWPR8RFRaQkICPvvsM31CCgCaNWuGTz/9FH/++acNIyOyvuO3NMgrUsPDRYou9evYOhy7JggCujUsaSP35j1w6jr/XhORYzH5Y+9Tp06hd+/eaNq0Kb777js0bdoUoiji9OnTWLx4MXr37o29e/eiWbNm1RkvEdkhUxIa9tQrqJTExJJeUUolEBoKuLpa/BQZOQpsO5NV6WFx1hhWV1BcfpKrsIL1lsbaVlTbaLVayGSyMstlMhm0Wq0NIiKyDVmdSKTcLnnP92oSCGcph+1VJNDLFfW8nZCWq8XyvTcwsLMIJycWPicix2Dyb/nZs2ejT58++Pfff/H444+jTZs2iImJwejRo7Fv3z707t0bc+bMqcZQiWq26u4JU53sLaFhsnsTUnFxwPr11VJDalvKjUr3IrNWLzR35/ITPm4VrLekq9mF+PHgFfxx/Bp2pN7E5uPX8OPBK7iaXWi1GIisrVevXnj55ZeRkZGhX5aeno5XXnkFvXv3tmFkRNYjiiL8YidCBNAgwAPRddxtHZLDaB0ggVZZiJSbRdh0NKPiHYiI7ITJSank5GS88cYbEISyWXdBEPDGG29g+/btFg2OqLZw9Idwe0pomMxKCSkAyFVUflictYbVhfnI4eNWtpcGUNIrK8zHOvU8HH4oKFElffLJJ7hz5w6ioqJQv359NGjQANHR0bhz5w6WLVtm6/CIrCLpbB5cI5pDKgDdGnHYnjnkUgG5e38EALz/Z4r9fiBIRHQfk5NSd+7cQVBQkNH1wcHBuHPnjkWCIqpNasJDuL0kNEyWlGS1hFRFKrpptFYvNA9XKfo0CyrzfdQNE7TW0DnWtqLaKiIiAocOHcLmzZsxdepUvPTSS/jjjz9w8OBBhIeH2zo8omqXW6jCF/tuAgBa1JHA09XwfQUZl7d/I4I9ZMjMK8Kqfy7aOhwiIpOYnJSKiorCvn37jK7/999/ERkZaZGgiGqTmvAQbi8JDZNFRAC+vjZPSAEV9yKzZi+0cF83jGwXgf4tQ9CjcQD6twzByHYRFqtbZQqHHQpKVEnbtm1Ds2bNkPffJAt9+vTBlClT8NJLL6F9+/Zo3rw5/v77bxtHSVT9PtySipwiDYpvXUZjP9aRqhSNCuMfKOlhtiL5PG4XFNs4ICKiipn8G3/UqFGYNm0aTpw4UWbd8ePHMX36dDz22GMWDY6oNqgpD+H2kNAwWZMmwJ49VktIecsr34vM2r3QPFylaBzsiZi6vmgc7Gn1hKJDDgUlqoIlS5bgueeeg5eXV5l13t7emDBhAhYtWmSDyIis5/jVXHz37yUAwO0tn0FioFwImaZnfU80C/HCHaUan24/Z+twiIgqZHJSaubMmQgPD0ebNm3Qr18/TJs2DdOmTcMjjzyCmJgYhIaGYubMmdUZK1GNVJMewm2d0ChXQkLJsD2dqCir9ZDq1SSw0r3IHK4XWhU53FBQoio6evQoHnnkEaPrY2NjcfDgQYueU61WY9asWYiOjoZcLke9evUwb948zvJHNqHVipj16wmIItCrvieUV47bOiSH5iQIeL1fEwDAt3su4cptx6hPSkS1l8lPM66urti+fTsWL16MtWvXYseOHQCARo0a4d1338Urr7wCFxeXaguUqKbSPYQbGsLHh3ALSUgoGaonCMCuXUC7dlY9faiPHCPbRSA9R4HCYjXcnKUI85GbnFDS9UKr7P7Wll+kRnqOAgXFang4SxFqRqy6JNz9ddZqahKO6Pr165DJjNfOkUqluHnzpkXPuWDBAqxYsQKrV69G8+bNceDAAYwfPx7e3t54+eWXLXouooqs238FR6/kwNNFigkdA7HK1gHVAN0aBaBrgzrYde4WFiWdweJRbWwdEhGRUWbd3Ts7O+O1117Da6+9Vl3xENU6Nf0hvCoJCovQJaSUSmDoUKBlS+ud+x66XmS22t9armYXGn0vmzqU09GScERVERYWhuPHj6NBgwYG1x87dgwhISEWPeeePXswZMgQDBgwAEBJ3dC1a9fiwIEDFj0PUUVuFxRjYWIKAOCVPo3g76axcUQ1x2uPNMGuT3Zh45F0PPtQNJqHets6JCIig1hFkMgOOFQ9JjNczS7Ejwev4I/j17Aj9SY2H7+GHw9ewdVsK3Ulvz8htW6dTYua13SWnEnSroeCEllQ//798fbbb6OoqKjMOoVCgdmzZ2PgwIEWPWfXrl3x119/4cyZMwBKhhDu2rUL/fv3t+h5iCqy4M8U5BSq0DTEC0915oRJltQy3BuDWodCFIEFCam2DoeIyCiT7/J9fX0hmFB08Pbt21UKiKi2cpSeMKaqKEExsl1E9SYamJCyOlNmkqxJ73EiS5g1axZ++eUXNGrUCJMnT0bjxo0hCAJOnz6NTz/9FBqNBm+++aZFz/naa68hNzcXTZo0gUQigUajwXvvvYfHH3/c4PZKpRJKpVL/WjdToEqlgkpl+Ge+NtK1BdukNGPtcvhyDtYfuAIAmDOwCUStBhqNBnK5HBKIEETze01JnVCyv4BK7S+BCLlcDo1GU6nvoznx69br/q2O2Kf2qoeEE9ew88xN7EjJRJf6/mYf1xb4s2QY28Uwtotxtm4bU89r8hPhkiVL9P8XRREvvvgi5s2bh8DAQLODI6Kaz6YJikOHmJCygZoykySRNQUFBWH37t148cUXMXPmTIiiCAAQBAF9+/bF8uXLERQUZNFzrl+/Ht999x3WrFmD5s2b48iRI5g6dSpCQ0MxduzYMtvPnz8fc+fOLbN8y5YtcHNz7B691SHp3kk1SO/edtGIwEfHJAAEdAzQIvPEbvzx3wTfa9euBaAAFGfMPkd0I1/0Wbu25EVl9vctOX9KSgpSUlLM3h8wP/6oovMl566m2DsHOGFnphNm/XgA01pq4ORAExvyZ8kwtothbBfjbNU2hYWmjY4xOSl1/03KlClTMHz4cNSrV8+8yIioWplaw6lAqcb1rKJqq/Vk0wRFq1bAkCGASsWElBXVpJkkiawpMjISf/zxB7Kzs3Hu3DmIooiGDRvC19e3Ws736quv4vXXX8djjz0GAGjZsiUuXbqE+fPnG0xKzZw5E9OmTdO/zsvLQ0REBGJjY+Hl5VUtMToilUqFpKQk9OnTp9zi9bWNoXb5Zu9lpBemwFsuxcfPdIW/e8nf6bS0NMTExOB/yzfCPzTC7HOdO/ovVs6eiGffX416TVqYvX9WxhV8NDEOhw8frtQzjjnxC6IGUUXncdG1PkRBUm2xd8xXovfiXbhSoIFQty36tww2+9jWxp8lw9guhrFdjLN12+h6VleETwhENYg5RaY3HE5HTpG2wu0qy6YJCqkU+P57QKtFvtYJ6Zl3bFdo3Uw2LwxfBZxJkqhqfH190b59+2o/T2FhIZycSpcVlUgk0Gq1Brd3cXExOMOyTCbjA4ABbBfDdO1y844SS7aeAwC82rcJgn3c9dtIJBIoFApoIEAUJGafQ60tqcWmEVGp/TUQoFAoIJFIKvU9rEz8oiCBKEiqLfZgXxme71Yfi7eeweK/zqF/qzA4Sx2jrDB/lgxjuxjGdjHOVm1j6jkd4zcSEVXI1CLTBcqSf3MVVS9GXR5dgsKQaklQJCQAkyYBuocqqRRXC9S2LbRuJpsXhq8i3UyS93/fa8pMkkQ1xaBBg/Dee+9h8+bNuHjxIjZs2IBFixZh6NChtg6NaoH5f5zGHaUarcK98XiHurYOp1Z49qFo1PFwwaWsQqzbf9nW4RARlcKkFFENYUoNJwDIyCk7w5Oh7arKqgkKXVHz5cuBzz8HYNmZ4KzB0eI1pqbOJElUkyxbtgwjRozAxIkT0bRpU0yfPh0TJkzAO++8Y+vQqIb7Ny0LvxxOhyAA7wxpAYkjFThyYO4uUrz8cEMAwNK/ziJf6Rj3FERUO5j8VHhvLQEAKC4uxnvvvQdvb+9SyxctWmSZyIjILKbWcCpUWa/Wky5BkZ6jQGGxGm7OUoRZejjavbPsxcUBzzwDwPFmgnO0eMtT02aSJKppPD09sWTJklKT2BBVN5VGi7d+Lalm/niHumgd4WPbgGqZx9pHYOWuC7hwqwBf/Z2GqQ83snVIREQAzEhKHT58uNTrLl26IC0trdQyQeCnHUS2YmoNJzeZdWs9VWuC4v6E1Pr1+qLmjjYTnKPFS0REZI5v9l7Gmev58HN3xoy+jW0dTq0jkzhhemxjTFpzCF/uTMOTnSLh71G2VhwRkbWZ/PS5ffv26oyDiKrI1CLToT6uOGvkGA5VjLqchBTgeDPBOVq8NYEjF5UnInIkOUpg2bbzAIDXH2kCHzfOimsL/VoEo2WYN46n5+KT7ecwe1BzW4dERMSaUkQ1hak1nNxdSv71ljtwMeqsLODRR40mpAAbFFqvIkeL19E5elF5IiJHsvGSEwqKNWhb1wcj2oXbOpxay8lJwIxHSnqpfb/3Mq7c5t88IrI9myaldu7ciUGDBiE0NBSCIGDjxo2l1guCYPDrgw8+MHrM+Ph4g/sUFRkv7kxUU5hTZHpoTJjjFqP29we++QYYOdJgQgpwvJngHC1eR1ZTisoTETmCf85n4XCWE5wE4J24FnBicXObeqhhAB5s4I9ijRZLthrrO09EZD02fcopKChA69atMX78eAwfPrzM+mvXrpV6/eeff+KZZ54xuO29vLy8kJqaWmqZq6tr1QMmcgCm1nByd5GisYeD9b5RqwHpf7+24uJKvsphlULrFuRo8TqqmlRUnojIninVGsz97TQAYEzHumge6l3BHmQNM/o2wZBz/+CXw1fxfLd6/JtHRDZl0yedfv36oV+/fkbXBwcHl3r966+/omfPnqhXr165xxUEocy+ROTgEhKAqVNL/o2KMnk3SxZat0YNIs5cV/1YVJ6IyDq++vsCLmQVwksmYmrv+rYOh/7TOsIH/VsG44/jmfggMRVfjX3A1iERUS3mMB+/X79+HZs3b8bq1asr3DY/Px+RkZHQaDRo06YN3nnnHcTExBjdXqlUQqlU6l/n5eUBAFQqlf5L95qqD9vZOhyxnYXEREhGjICgVEKzYAG0S5daPYaMHAW2pdxAruJuu3nLZejVJBChRuo9OWJb21qBUo2MnCIUqtRwl0kR4uOqr4NmjLnt7OoECKLG6HoXJ37PDOH7uSy2BZFxV24XYtm2kuFhQyK18HQ1XDeRbON/sY2RePI6tp6+jgMXb+OBKD9bh0REtZRJSaljx46ZfMBWrVpVOpjyrF69Gp6enhg2bFi52zVp0gTx8fFo2bIl8vLy8PHHH+PBBx/E0aNH0bBhQ4P7zJ8/H3Pnzi2zfMuWLXBzu1tjJykpqWoXQSZhO1uHo7Rz4KFD6DB/PgSVChmdOuFA794Q//jDJrH4/felpwCO7D6JIxXs5yhtbY/OmLGtOe0cXc66swfPGJ2hkvh+vldhIYsEExkz7/dTKFJp0THaF+3q3LR1OHSf+gEeGNkuHOv2X8GChBT8MKEzBIH1vojI+kxKSrVp0waCIEAUxQp/WWk0xj99roqVK1fiiSeeqLA2VKdOndCpUyf96wcffBBt27bFsmXLsNRI74qZM2di2rRp+td5eXmIiIhAbGwsvLy8oFKpkJSUhD59+kAm46c81YXtbLrK9CbRcaR2FhITIVmwAIJKBe2QIQj4/nv0M1DUvLqdvZ6PLacyja6PbRaMhkEeZZY7UlvbWoFSjQ2H00v1RNPxlsswNCbM6Hu8Mu1cmZ5vtR3fz2XpelYTUWnbUq4j6dR1SJ0EzB7YFGcPMCllj15+uCE2HE7H/ovZSE69iZ5NAm0dEhHVQiY9xV64cEH//8OHD2P69Ol49dVX0blzZwDAnj178NFHH2HhwoXVEuTff/+N1NRUrF+/3ux9nZyc0L59e5w9a/xzbxcXF7i4uJRZLpPJSt143/+aqgfbuXxXswvLzBymm6HNnNnz7L6dExKAESMApRIYOhRO69bByQYJKQAo0gKiIDG6XqlFuW1p921tB65nFSGnSAsYaOecIi2u56srLMxvTjtHBsgw0tONReUrge/nu9gORGUVqTSYvekkAOCZrtFoGOjB3qd2KsRbjnFdovD5zjQsSEhB90YBnB2RiKzOpLvvyMhI/f9HjhyJpUuXon///vplrVq1QkREBN566y3EVTAbVmV8/fXXaNeuHVq3bm32vqIo4siRI2jZsqXF4yKytoqmsh/ZLqJmPFRrtcBbb+kTUli3DrgvIWWNouM67s7lH9etgvU1maW+D7YoPs6i8kRElrc8+Tyu3FYgxNsVL/VuCEC0dUhUjhd71MeafZeRknkHm45mIC4mzNYhEVEtY/aTw/HjxxEdXbYaR3R0NE6dOmXWsfLz83Hu3Dn96wsXLuDIkSPw8/ND3bp1AZR0jf/xxx/x0UcfGTzGU089hbCwMMyfPx8AMHfuXHTq1AkNGzZEXl4eli5diiNHjuDTTz81KzYie1RbprLPL9Yic9UP8Fy2CLlvzkGo1gn3Do6zVG8xU4X5yOHjJjPY9j5uMoTV0uFelvw+MPFHROT40m7mY0XyeQDAWwObwd1FygkB7JyPmzNe6F4fHySm4qOkVPRvGQJnqZOtwyKiWsTs3zhNmzbFu+++i6KiIv0ypVKJd999F02bNjXrWAcOHEBMTIx+Zrxp06YhJiYGb7/9tn6bdevWQRRFPP744waPcfnyZVy7dk3/OicnB88//zyaNm2K2NhYpKenY+fOnejQoYNZsRHZoxo/lf21a7iaXYgfD17Bb9dUWDNiCjanZuHHg1dwNbukoHBFvcXyi6qnR02fZkHwcSs9VEeXgKkRvdPMZOnvgy7xZ0htTvwRETkKURTx9q8nUazRonujAPRrEWzrkMhE4x+MQoCnC67cVmDtvsu2DoeIahmzn6RWrFiBQYMGISIiQj+c7ujRoxAEAb///rtZx+rRowdEsfwuvc8//zyef/55o+uTk5NLvV68eDEWL15sVhxEjqJG9yZJTIQ4dCguvvQ2cnqXnmXz3uGJtuotFu7rpj8/axBZvteeLvFnrOdVbW1nIiJH8fuxa9h17hacpU6YO7g5Z3JzIG7OUrzcuyFmbTyBZdvOYkS7cJMn0CEiqiqzf9t06NABFy5cwHfffYeUlBSIoohRo0Zh9OjRcHd3r44Yieg/NXYYWWIiMGQIBKUSQf9sB3oNBe67mdUlOmzZW4w1iO6qju8DE39ERI7pTpEK7/xeUsZjUo8GiKrj2M8Ely5dsup+9mBU+wh8+XcaLmUV4utdF/6rB0ZEVP0qdafv5uZWbu8lIqq88gpH18jeJP8lpKBUIqfvAPzx0vtlElI6hcXqmt1bzIFU1/eBiT8iIsezKOkMbtxRIsrfDRO617N1OJVWmJcDQMDDDz9cpeMUFRVaJB5rkkmc8L/Yxnhp7WF8sTMNYzpFws/dNrMeE1HtUqmnhm+//Raff/450tLSsGfPHkRGRmLx4sWoV68ehgwZYukYiWoNUwpH16jeJPckpBAXh+sffwVtapbRzXXXWiN7izkYfh+IiAgATqTnYvXuiwCAeUNawFUmsW1AVVCkKAAg4ok3l6JugyZm73/x1GGs/eA1KJXFlg/OCga2DMHnO87jZEYePt1+Dm8NbGbrkIioFjC70Plnn32GadOmoV+/fsjOzoZGowEA+Pr6YsmSJZaOj6jWMKdwtK43SUxdXzQO9qwRCSmsX4+wQO8Ki13bY9Hx/CI1UjPv4NDlbJzJvFMtxdbtjT1+H4iIyLq0WhGzNp6AVgQGtApBt0YBtg7JIrwDghEQFmn2l5d/oK1DrxInJwEzHilJxn275xLScxQ2joiIagOznxqWLVuGL7/8EnFxcXj//ff1yx944AFMnz7dosER1Sa2KuBtM3v2lEpIwdkZHoBJwxOro7dYecMmy1Ne77YgD8MJtpqiRvXaIyIis63bfwVHruTAw0WKt9mrpkbo1rAOOtfzx560LCxJOoMPRra2dUhEVMOZ/eRw4cIFxMTElFnu4uKCgoICiwRFVBvZsoC3qSqbuDFo9mygUSNgxAjA+W7NAlMTHZasPWTKsElDKurdNrR1zZ8OmzWgiIhqp6x8JRYkpAAAXunTCEFerjaOiCxBEATMeKQxhi7fjZ8PXcXz3eqhYRD/zhNR9TF7+F50dDSOHDlSZvmff/6JZs34CQlRZdl7Ae+r2YX48eAV/HH8Gnak3sTm49fw48EruJptRjHPPXuAwv+2FwRg9OhSCSkdaw5PNGfY5P0q6t2WkVNk0ViJiIjsxfw/U5CrUKFpiBfGdo60dThkQTF1fdG3eRC0IvBBYqqtwyGiGs7spNSrr76KSZMmYf369RBFEfv27cN7772HN954A6+++mp1xEhUK+gKRxti68LRVUnc6CUkAD17ltSRUthPjQJThk0aU1HvNoXK9r3biIiILG3fhdv46eBVAMC7cS0glZj9SEF27tW+jeEkAFtOXcehy9m2DoeIajCz/4KMHz8es2fPxowZM1BYWIjRo0djxYoV+Pjjj/HYY49VR4xEtYI9F46uSuIGQElCKi6upIaUhwcgsZ+ZeaoybLKi3m1yGWsrERFRzVKs1mLWxuMAgMc7RKBdpK+NI6Lq0CDQEyPahQMAFvyZAlEUbRwREdVUlXpieu655/Dcc8/h1q1b0Gq1CAx07JkmiOyFvRaOrlK9q3sTUvcUNbcXVRk2qevdZihh5+MmQ6iPK85WOUIiIiL78cXO8zhzPR/+7s6Y0beJrcOhajT14UbYeCQD/164jR1nbqJHYz7zEZHlmd1TqlevXsjJyQEA1KlTR5+QysvLQ69evSwaHFFtZM16SqaqdOLGzhNSQNWGTVbUu83dxfbfOyIiIktJu5mPpdvOAQDeHtQMvu729TedLCvUR66vF7YgIRVaLXtLEZHlmZ2USk5ORnFxcZnlRUVF+Pvvvy0SFBHZl0olbhIT7T4hBVR92KSud1v/liHo0TgA/VuGYGS7iHJn7SMiInI0oijijQ3HUazWolujAAxuHWrrkMgKJvZoAE8XKU5fy8NvxzJsHQ4R1UAmf4x/7Ngx/f9PnTqFzMxM/WuNRoOEhASEhYVZNjoisgu6xM39xc7LTdz4+wNyOdCvn90mpHSqOmxS17uNiIiopvrx4FXsTbsNV5kT3otrAUEQbB0SWYGvuzMmdK+HD7ecwYdbUvFIi2C4SO2nNigROT6Tk1Jt2rSBIAgQBMHgMD25XI5ly5ZZNDgish9mJ24eeADYsweoV8+uE1I6TCwREREZditfifc2nwYATOvTCBF+7A1cmzzdNRrf7r2EK7cVWL37Ip7vVt/WIRFRDWJyUurChQsQRRH16tXDvn37EBAQoF/n7OyMwMBASOxoRi0isrwKEzeJiYCPD9CxY8nrJiyAWhX5RWqk5yhQUKyGh7MUoXZQ9J6IiGqfd34/hVyFCs1CvPD0g9G2DoeszM1Ziv/FNsaMn45h2bZzGNEuAn6sJ0ZEFmLy001kZEmRO61WW23BEDkiJg7+oytq7uIC7NmD/HqN2C5VcDW70OhwSdarIiIia0lOvYFfj2TASQDeH94SUonZJWmpBhjeNhyr/rmI09fysPSvs5gzuLmtQyKiGsLsJ8T58+cjKCgITz/9dKnlK1euxM2bN/Haa69ZLDgie8fEwX/unWWvXz9c9Q9F0sErbJdKyi9Sl3lfAUBOoQpJp65jZLsIJviIiKjaFRarMWvjCQDAuC7RaBXuY9uAyGYkTgJmDWiKJ776F9/tvYSnOkeiXoCHrcMiohrA7I86Pv/8czQxMCSnefPmWLFihUWCInIEFSUO8ovUNorMyu5NSMXFIX/190g6l812qYL0HEWZ9tPJKVQhPUdh5YiIqKZJT0/HmDFj4O/vDzc3N7Rp0wYHDx60dVhkZz7eehZXsxUI85Hjf7GNbB0O2diDDeqgV5NAqLUi5v+ZYutwiKiGMDsplZmZiZCQkDLLAwICcO3aNYsEReQIyksc5ClUuJpdiNTMOzh0ORtnMu/UzGTMfQkprF+P9EINEypVVFBc/nulsIL1RETlyc7OxoMPPgiZTIY///wTp06dwkcffQQfHx9bh0Z25ER6Lr7adQEA8E5cc7i7sIcuAW/0bwKJk4CkU9exNy3L1uEQUQ1g9l+XiIgI/PPPP4iOLl3k8J9//kFoaKjFAiOyd8YSB04CEOzlik1HMyC7p+5CjRu+tndvmYQUnJ1RUFxQ7m5MqFTM3bn8X81uFawnIirPggULEBERgVWrVumXRUVF2S4gsjtqjRYzfzkOjVbEgFYh6NUkyNYhkZ1oEOiJxztE4Lu9l/Hu5lPYNKkrnJwEW4dFRA7M7J5Szz77LKZOnYpVq1bh0qVLuHTpElauXIlXXnkFzz33XHXESGSXjCUO/N2d8c+5W8hT1PDha23aAD16lEpIAUyoWEKYjxw+bjKD63zcZAjzkVs5IiKqSTZt2oQHHngAI0eORGBgIGJiYvDll1/aOiyyI1/tuoDj6bnwcpVi9qBmtg6H7MzUhxvBw0WKE+l52HA43dbhEJGDM/vpcMaMGbh9+zYmTpyI4uJiAICrqytee+01zJw50+IBEtkrXeLg/qFqIoDcIhXCDPSI0g1faxzsaaUoq5GrK7BxI+DkpE9IAcbbBWBCxVQerlL0aRZktIg+i5wTUVWkpaXhs88+w7Rp0/DGG29g3759eOmll+Di4oKnnnqqzPZKpRJKpVL/Oi8vDwCgUqmgUhkerl0b6dqiqm1y69YtfRtXhpeXF+rUqVPp/c/fLMCipDMAgDf7N4avq8Ssa7o/fo1GAwA4e/YsJBJJufteuXIFcrkcEogQRI3ZsUudULK/ALvfX7de929Vzy2BCLlcDo1GU+n3oDnvvUdbeGPlwSy89/sJ1HO5AzeZE1QqFWQywx+qmUL33rXUz1JNw3YxjO1inK3bxtTzCqIoipU5QX5+Pk6fPg25XI6GDRvCxcWlMoexS3l5efD29kZubi68vLygUqnwxx9/oH///lX6RUvlc8R2NjT7Xh0PZ6TdKoCXq+Fr6NE4ADF1fa0VYhlVaueEBODvv4F33wUE4121OSthiaq+p/OL1EjPUaCwWA03ZynCfORMSBngiL87HBHbuaz77xcchbOzMx544AHs3r1bv+yll17C/v37sWfPnjLbz5kzB3Pnzi2zfM2aNXBzqz2/02sDrQh8fEKCi/kCmvpoMaGJtrw/91SLqbXA/CMS3FIKeDhUi0GRWluHRER2prCwEKNHj67wPqnSTzceHh5o3759ZXcnqhHCfd0wsl1EqcRBYbEaqZl3cC1XAWeJEzxdZXCW3h0p67DD1+4tat64MWDg03QdQ+1S2YSKLjFTUKyGh7MUobUoMePhKq0ZveqIyK6EhISgWbPSQ7KaNm2Kn3/+2eD2M2fOxLRp0/Sv8/LyEBERgdjYWIdKxlU3lUqFpKQk9OnTp9KJ27S0NMTExODpeZ/Bt07ZiYUqkn3rGla+/SIOHz6MevXqmb1//J5LuJifCncXCVY88xBCzezdbCh+J4ho61uEQ9mu0KL8DNel1GP46eO38ez7q1GvSQuz4z939F+snD3RIfYXRA2iis7jomt9iIKkyufO+v/27js8qmpr4PBv+qT3ThJKQu9NERVQiqAIIgLXAoh6P7uIXhXLBWxcC4JdUQGxgQ0rIrFQLXRFemhJIA3Sk+lzvj9ChoR0UiZlvc+TB+bMKWt2TmbOrLP32ieTWXDn+PP+3Z/Pudc92Mm6Ew5+SVWTe/II378ym0kPPk90+9rP1Fj63I2Ojq7z31JLVB/vMS2RtEvl3N02Ne15WaNvdhMmTGDZsmX4+voyYcKEKtf98ssva3RgIVqK0omDlOwi9p7MJc9kIz2/eKiDUaemfYg3vkZd8x2+Vjohdc01FIyfyIm0fFeiKMBLT1ahtVziqK4JFelxJYQQ9W/w4MEcOHCgzLKDBw8SGxtb4foGg6HCHvE6nU6+AFSgLu2i0WgwmUz4BkcSGFXx76MqDlSYTCY0Gk2tYzh+upAFCYcAeHRMF2JDap9wrCh+leIA00ECImNQVFUP38tMP4nJZMKhUO26FbE7aXbbKyoNikpT52PX5XcP53fuBSgKR0wnScoq4rguGpPJhFdQGIFRbWt9/Iril/eYikm7VEzapXLuapuaHrNGSSk/Pz9UZ/ru+vn5nX9UQrRgBWY7CXvTyTPZGBwXzObEU6TnWzDbnBzJLGBwh+DmWQ/onIRUyptLSNh9NlGUZ7ZRYLbTPzaAtDwzTqV+Ekcl7XlubaqSgvHX9Ytufm0phBBNwP33389FF13Es88+y6RJk9iyZQuLFy9m8eLF7g5NuInTqfDwF39jtjkZ1D6Ifw2IcXdIohlQqVQM6RjCR38eJ8NmwNiur7tDEkI0QzX6Rld6yuDS/xdCnHUix+RKoKTlmRnYLhAFsNidGLRqBrQNbH69e87tIbXswzIJKau9OOFmtjmx2BwMbBdIZoG1XhJHpdvzXC2qYLwQQjSyAQMGsGrVKmbPns2TTz5Ju3btWLRoETfccIO7QxNu8vGWJP44koWHTsNz1/ZErZZCUqJmAr309Grjz87kHAIvvw3neVUrFkK0ZtLNQIh6Umi1u/7vVCCzwOp6nA+YbLWfScWt0tJgwgRXQooVKziRZSmTKMo32zDbigtbpudbKH0dUtfEUen2rEhRNc8LIYSo3FVXXcVVV13l7jBEE3Aix8T81fsAeOiKTsQENbMbaMLtLmgXyJ6ULAiK5lihldpXlBJCtGY1Skr16dPHNXyvOjt27KhTQEI0V17VFDBv6gXOyxcUD8b7jTfgu+/g449Br6fQWlhmG6uj7EwrFnvZx3VJHDX39hRCCCGaOkVRmP3lbgqtDvrHBjBtUFt3hySaIYNOQ7xHIXuKfDhUoGOwxY6XQa7ThBA1U6N3i/Hjx7v+bzabeeONN+jatSuDBg0C4I8//mDPnj3ceeedDRKkEM1BlL8H/p66CoecNfUC52UKijudoFYX14UaN4k206ZRMh/0uYkivUZd5rFBqya/1OO6JI6ac3sKIYQQzcHn21PYcDATvVbNcxNl2J44f230ZnYcToOIeDYnnmJkt3B3hySEaCZq9I1xzpw5rv/feuut3HvvvTz11FPl1klOTq7f6IRoRryNWkZ0Dat0trimWpS7dEHx2G0bGbxkIV89s5gcgsvVhTo3UeRj1GHUqTHbnIT5GMpM9FzXxFFzbU8hhBCiOUjLNfPUd3sBmDWiIx1CvN0ckWjOVCrISniTiKkL2JeWT9dI3+ZXS1UI4Ra1/lb32WefsW3btnLLb7zxRvr378+SJUvqJTAhmqM2AZ5c1y+aEzkmiqx2PPVaovw96pxAKT+0ru77LFFSUDx220aunnMnWpuV/p++y4b/e6RcXahzE0V6rZr2Id4UmO30jvbnYFo+JruDCF8jQzqG1DnGhmpPIYQQojVTlOLZ9vLMdnq18ePWi9u5OyTRAlhTDxLjaSepSMev+zO5/oIYNNL7TghRjVp/s/Pw8GDTpk3Ex8eXWb5p0yaMRmO9BSZEc+Vt1NbrrHBlhtadUdJbqD7uQBVa7WUSUokXDWfTjFmu58+tC1VRokirgZ1J2UQHeWLQqlEBGw8VDweoa4z13Z5CCCFEa7diazLrzwzbWzCpF9pzhuMLcb46+9jItBnJKrKyIymbAW0D3R2SEKKJq3VSaubMmdxxxx1s376dCy+8ECiuKbVkyRL++9//1nuAQrRmpYfWlZZTZCs3tO58hWxeR89SCanvH1uIU6d3PV9RXajSiaICs53Ptie7YixdU6q+YhRCCCFE/UjOKuLpM8P2HhrVibhQufEj6o9ODZfEB7N2bzpbjmbRKcwHXw+du8MSQjRhtb4t8sgjj7B8+XJ27tzJvffey7333svOnTtZtmwZjzzySK32tWHDBsaOHUtkZCQqlYqvvvqqzPPTp09HpVKV+SlJhFXliy++oGvXrhgMBrp27cqqVatqFZcQTUXJ0LqKlAytq5Mff6TNzf+qNCFVk7pQDR6jEEIIIeqF06nw4Gd/UWh1MLBtIDcPlmF7ov51Dvehjb8HdqfCuoOZ7g5HCNHEnVdf3UmTJrF582aysrLIyspi8+bNTJo0qdb7KSwspFevXrz22muVrnPFFVeQmprq+lm9enWV+/z999+ZPHkyN910E3/99Rc33XQTkyZN4s8//6x1fEK4W+E5Q+fOde7Qulqx22HWLFQWC6Yrx7L5mdfKJaRqUlC8QWMUQgghRL1Z9tsx/jyahadewwvX9ZR6P6JBqFQqhnUORa2Co6cKOZxZ4O6QhBBN2HmNqcnJyeHzzz/nyJEjPPjggwQGBrJjxw7CwsKIioqq8X5Gjx7N6NGjq1zHYDAQHl7zKUUXLVrEiBEjmD17NgCzZ89m/fr1LFq0iE8++aTG+xGiKfCqYOhcaRUNrasxrRZ++AFeeAGPBQu41qk+r4LiDRqjEEIIIerF4cwCnluzH4DZY7oQG+Tl5ohESxbopadvTADbjmez7kAm0QGe6LVSu0wIUV6tvy3+/fffDB8+HD8/P44dO8att95KYGAgq1at4vjx4yxfvrxeA1y3bh2hoaH4+/szZMgQnnnmGUJDQytd//fff+f+++8vs2zUqFEsWrSo0m0sFgsWi8X1OC8vDwCbzeb6KXksGk5Tb+dCi52TOWaKbHa8dFoi/I14GRo24RLmrcXfqCbXVL5N/Dx0hHlra91ettTU4n9tNoiIgJdeAsCgUWgfVHqyAqVG+26IGFuKpn5OtxTSzo1D2rk8aQvRXNgdTh787C8sdieXxAdz4wUx7g5JtAID2wVyMD2fPLOd3w+fZkinEHeHJIRogmr9jXrWrFlMnz6d559/Hh+fs4URR48ezfXXX1+vwY0ePZrrrruO2NhYjh49yhNPPMFll13G9u3bMRgMFW6TlpZGWFhYmWVhYWGkpaVVepz58+czb968csvXrl2Lp+fZmcMSEhLO85WI2mgu7XywkY4TeOanHBOs/3lPrfYVumMHA55/noh776U+W7k+Y2yJmss53dxJOzcOaeezioqK3B2CEDWyeOMRdibl4GPQ8ty1PVGpZNieaHg6jZrLOofy1a6T7ErJIT7Mm8hqapUKIVqfWieltm7dyttvv11ueVRUVJWJn/MxefJk1/+7d+9O//79iY2N5fvvv2fChAmVbnfuB62iKFV++M6ePZtZs2a5Hufl5REdHc3IkSPx9fXFZrORkJDAiBEj0Olk9oiG0lTbudBiZ9XOE5X2BLqmT1SD95gq6aVlstnx0GmJPI9eWqoff0Tz3HOoLBaiNm2i+3//i06vr37DRoyxpWmq53RLI+3cOKSdyyvpWS1EU7Y/LY+FCcW30uZc3U2SAqJRxQZ50TXCl72pefy0L53rB8ag1cgwPiHEWbX+xmg0Giu8CDtw4AAhIQ3bJTMiIoLY2FgOHTpU6Trh4eHlkmMZGRnlek+VZjAYKux5pdPpylx4n/tYNIym1s7pp83kmJ2g0pR7LsfsJL3ATifvhr3A89fp8K/LMdasgYkTwWLBefXVbL/pJkbr9fXaziUxFpjtnMgxcSTLjLdeS2QNa1PVp5IYCq12t8VQWlM7p1sqaefGIe18lrSDaOqsdiezVv6FzaEwvEsY1/atee1XIerLJfHBHDtdSHaRja3HshnUIcjdIQkhmpBaf0sbN24cTz75JJ9++ilQ3CspKSmJRx55hGuvvbbeAyzt9OnTJCcnExERUek6gwYNIiEhoUxdqbVr13LRRRc1aGyi5WqM2eUaNImyZg2MHw8WC4wfj+PDD1F++ql+9n2OlOwiEvamk1N0tldZySx+bQI8q9iyZcUghBBCNAUv/3yQval5BHjqeHZC9xoN28vMzCQ3N/e8jnf8+PHz2k7Un/P9HTTk786o0zCsUyjf705l2/Es4kK9CfGpuBSLEKL1qfW33hdffJExY8YQGhqKyWRiyJAhpKWlMWjQIJ555pla7augoIDExETX46NHj7Jr1y4CAwMJDAxk7ty5XHvttURERHDs2DEeffRRgoODueaaa1zbTJ06laioKObPnw/Afffdx6WXXspzzz3HuHHj+Prrr/npp5/YtGlTbV+qEEDDzy7XoEmUcxJSrFwJDVRHosBsL/c6AHKKbCTsTee6ftEN3lupKcQghBBCNAXbjmXx5rrDADxzTQ9CfYzVbFGckIqLiycv7/ySUiXMZqm31tiK8nIAFcOHD6/TfhrqdxcX6k1ciDeJmQX8tC+dyf2jUaultpkQ4jySUr6+vmzatIlffvmFHTt24HQ66du373m9AW7bto1hw4a5HpfUdZo2bRpvvvkmu3fvZvny5eTk5BAREcGwYcNYuXJlmQLrSUlJqNVnxyVfdNFFrFixgscff5wnnniCDh06sHLlSi644IJaxycEQJS/B/6eunKJDihOHkXVsjZD6V5RXjoN6w9mUmh1lFnnfJIoFfa2+uGHsgkpvR4aaLaoEzmmCtsIil/PiRwTncJ9Kny+JcUghBBCuFuh1cH9X+7CqcCEvlGM6VH5KIPScnNzycvL5fbnlhEQGlnr4x7bu5NPXngYi8Va621F3ZhNhYDCDY+9Qkxc51pv3xi/u6GdQkjOLiIj38KO5Gz6x1Y4TY4QopWpVVLKbrdjNBrZtWsXl112GZdddlmdDj506FAURan0+R9//LHafaxbt67csokTJzJx4sS6hCaEi7dRy4iuYZX2ZqpNz5tze0UFe+vZfPgU7UO88TWWrU1SmyRKpb2t/vsMbbp1g+nTixNSDagxhjk2hxiEEEIId3vjjwySs0xE+Xsw9+putd4+IDSSkKjYWm+XlX6i1tuI+uUXEt5kf3deBi2XxoeQsC+dP45k0S7IiyBvGcYnRGtXq6kPtFotsbGxOByO6lcWogVpE+DJdf2iGdMjgqGdQhjTI4Lr+kXXanhdRUPLLDYnZpuTI5kFWO3OctvUJIly7n7DDu5GbbcV97ban0nB1BkNnpCChh/m2FxiEEIIIdzJo+MgfjyYh0oFCyf3LnfTSwh36hLhQ2yQJw6nwtq96TiclXdQEEK0DrWej/Pxxx9n9uzZZGVlNUQ8QjRZ3kYtncJ96BMTQKdwn2p7SBWY7RxIy2dHUjYH0/JJyS4iz1R2aJlBV/wnaLY5yTeXH3ZWkyRK6SFrsVs3MOn+6xnz7CxXYupEjqmmL7FOSoY5VuR8hjk21xiEEEIIdzHZFYJG3Q3A/13agYHtZHiUaFpUKhUjuoRh1KrJyLew5Zh8pxSitat1t4FXXnmFxMREIiMjiY2NxcvLq8zzO3bsqLfghGiuKhpOZ3M4iQ30JC3PTMlNIRUQ5mMgPd+CzVG2p1RNkyglQ9Zit27g6rl3obVZUTmdcGZobGMNWavPYY7NOQYhhBDCHRRF4Y9UOxpPPzoEGZg1oqO7QxKiQl4GLcM6h/LDP2lsPVY8jE/j7qCEEG5T629o48aNq9F0skK0VpXNAJdnsrE58RQD2wWSWVBcRPJ0oZXBccFsTjyFTnO242Jtkiheem2ZhFTiRcP5/rGFOHXFQ/Yac8hayTDHEzkmiqx2PPVaovw9GjUZ1BRiEEIIIRrb7hO5pBYqKHYrs4e2Ra+t9YAIIRpNxzAfDmcWcDC9gB/3pjGyjbsjEkK4S62/pc2dO7cBwhCi6alwNrsaJDYqmwHOx6gjObuI0iPnnQqk5ZkZ1jmUuFAf7E5nrZMoYb+tI27e3WhsVpIuHcmWp14FK6C4Z8hayTBHd2oKMQghhBCNJbvQysZDp4r/v24ZbW9f4OaIhKjesE6hruvmXZmSRBWitarxX39RURF33XUXUVFRhIaGcv3113Pq1KmGjE0It0nJLuKz7cms3p3K+gOZfL87lc+2J5OSXVTttpXNAKfXqmkf4l1uua+HjgHtAunRxq/G9apKZH7+Nd7/mojGamF7nyE8Mukx/kgpINzXSKCXDFkTQgghWjqHU2HNnjTsToVwTxX52791d0hC1IhRp2FElzAADmY7Mbbt7d6AhBBuUeNvq3PmzGHZsmXccMMNGI1GPvnkE+644w4+++yzhoxPiEZX2fC7nCIbCXvTua5fdJWJnqpmgPM16ugdHYBeq67z0LICs51tqYUMV6lJvGg4vzy8gPYOFTaHkxM5Jm4a1JYQn8afZvd8e5gJIYQQova2HM0iI9+CQavmwggNfyKzmYnmIzbIi55Rfvx9IpegMfeTY7IT6+6ghBCNqsbfFL/88kvee+89pkyZAsCNN97I4MGDcTgcaDRSmk60HJUNvwNcs9lVNTSsZAa4ivbh76mjXbBXvSRpTuSY2N+5P6cXfsLp2Di0Oj1BpZ7PKrQ2elKqogLvJfWx2gR4NmosQgghREuXmmti65nZyy7rHIqnXWYyE83PxfHBHMvMJc8niOc3pPFBx/buDkkI0YhqPHwvOTmZSy65xPV44MCBaLVaTp482SCBCeEulQ2/K1HdbHYlM8D5e+rKLK+3GeDWroU9e1xxZsZ1dRU1r02c9a26HmYF5saNRwghhGjJrHYnP+5JRwE6h/vQMUxqKYrmSadRc3GkFsVuZUtyIe//keTukIQQjajGSSmHw4FeX/aLr1arxW6XL5qiZalq+J3V7sRsc7AjKZuDafmVJlpKZoAb3T2cfrH+9Gzjx4DYQPw9iv+GCsx2DqTlV7ufctasgauvhmHD8E9NrnLVxpx1D2rWw0wIIYQQ9WPDoUxyTTZ8jFqGdgpxdzhC1Im/UU3Wz+8A8MLagyQXuDkgIUSjqfG3VkVRmD59OgbD2eFAZrOZ22+/HS8vL9eyL7/8sn4jFKKRVTb8Ls9so8Bs53BGAZkFVqDs0LRzaylp1Sp+P3K61H5y8ffUMbBdINuOZZFVWMshbmvWwPjxYLHAxRcT2iUO/93leyaV7K+xZ92raw8zIYQQQtTMkcwC9pzMA2Bk1zAMWimlIZq/gl0/MO7fD7PpeAHLDmm4wWInQKerfkMhRLNW46TUtGnTyi278cYb6zUYIWqjoQpqlwy/Kz0UzWp3UmC20zvan4Np+ZjsDvQaNVa7k4S96QzrHMKv+zNd64d46/krJRdvoxZf49kP04w8C8s2H6NXG78yx6y2iHrphNQ118CKFXjr9eXihHocJlhLVfUwg8bvuSWEEEK0RIUWOz/tywCgb4y/1GwULcoDl4Zz5JsUTuaamfPNPl7+Vx9UKpW7wxJCNKAaf0tcunRpQ8YhRK00dEHtkuF3J3JMFFntmG0Odp/I5ef96RRaHK71jDo1hRYvfAyaMrEoQFJWEUadmq4Rfui1xSNl8802ThVY8NBr8DFoMeg1qBSF04XWyouoV5CQ4sxQ2nPjrMtsfnVVXYH3xu65JYQQQrQ0iqLw0750TDYHwd56BnUIqn4jIZoRH4OGl67rwfXvbuGbv1O5OD6ESQOi3R2WEKIB1bimlBBNRWMV1PY2aukU7kOfmADUKhWbE0+VSUgBmG1O9qflYbWXnX7ZYnO6ns83n43TbHdwItvE3tR8Eval891fJ9lyNItwXyNqVQVD3DZtqjQhVVGcncJ93JKQKomjQQu8CyGEEK3c7hO5HDtdhEalYlS3cLRquZQXLU+/2ABGRxdfSz/x9T/8cyLXzREJIRqSfJKJZscdBbWdioL5TKLpXAUWOwZd2T+l0o9tjuLtrHYnSaeLKLTa0WnOdkNOz7ewOfEUQV768kPcevaEvn0rTUg1lPMtxF7Sc2tMjwiGdgphTI8IrusXLUMLhBCiiZo/fz4qlYqZM2e6OxRRjdMFFjYcOgXARXFBBHsbqtlCiOZreJTC0I7BWOxO7vhoOzlFVneHJIRoINJ1QbhdSW2ofJMZKK6V4F9FUUN3FNT21GsJ8zGQnm8p91yUvwecM9RdBa71dZqzQ/ecCkQHelJ4TpInPd+CQacuP8TN1xd+/BEMhkZLSNV1aGRJzy0hhBBN29atW1m8eDE9e/Z0dyiiGnankzV70nA4FWICPekT7e/ukIRoUGoVvDixBxPe+pOkrCLuX7mL96YNQK2W+lJCtDTSU0q4VUp2EZ9tT2b17lQ2nbn7t2rnCVKyiyrdxh0FtY06DYPjggnzKXtXMszHwNBOoShK2eF7pwutDI4LJibQE58zhc6tDieRfkbG9IjgRG7Z3lxGnZowX2PxELcff4QXXzz7pI9Po/aQaoyhkUIIIdyroKCAG264gXfeeYeAgAB3hyOq8VviaU4VWPHQaRjZNUwKP4tWwc9Dx5s39sWgVfPrgUxe/SXR3SEJIRqA9JQSblNZAiTXVPVMdO4oqB3l78EfR04xsF0gCmCxOzFo1agovns5uG0I+eazs+85leL6UXdd1gGLTXEVSz+cUcDpQiudwnzJN9uwOZzoNGp8jDrCfD2KE1LjxhXXkGrfHiZMqPfXUpWaDI2UXlBCCNH83XXXXVx55ZUMHz6cp59+usp1LRYLFsvZnsJ5eXkA2Gw2bLaKPzNao5K2qEubOBwOPDw80KCgUorrWB4/XcTO5BwARnQJxluvAsVR4fYaFDw8PDh27BgOR8XrVCU5Obnc8WtDq6Z4exWu7c/9t7bb1/X4TXX7c9ulOcXeENuXPnet1uKheocOHUKr0XDPhcG8uCmDRT8dJEiVz4A2XhXuw9fXl+Dg4Fofu7moj/eYlkjapXLubpuaHlelnNvFQ5CXl4efnx+5ubn4+vpis9lYvXo1Y8aMQVfFsDJROwfS8lm9O9X1WKU4aGc6yFGPjigqDWN6RFSaAGno2feqO6bVXlzA3NdDx8iuYcSFFsdZ1Sx4BWY7n21PJiPPQr7ZhtXhRH8mIRXqa2BK1j6M101AZbGQM+pKMha/T2SoX70XCK/qfN6RlM36A5mVbju0Uwh9YuSOek3Je0fjkHZuHNLO5Z17vdBcrFixgmeeeYatW7diNBoZOnQovXv3ZtGiRRWuP3fuXObNm1du+ccff4ynp9QMbEj5NnjuLw35NhWXhDmZ2L7i+pZCtHQrj6j5LV2Np0bhgZ4Ogo3ujkgIUZ2ioiKuv/76aq+TpKeUcJu61IYqKahdVRKovpUcc39aLvtS8wEjKmD9wUx2JucwomtYlb2IvI1aBrYLZNnmYyRlnR2eGBPoyeTsvRhnXI/KYiHxouH8cN//CDiRjyGjkDBfI+G+HkQ28OuD8kMjS5JvJQk0meVHCCGat+TkZO677z7Wrl2L0Vizb3WzZ89m1qxZrsd5eXlER0czcuTIZpWMa2g2m42EhARGjBhx3onbI0eO0KdPHx544ysCI9rwzcF08m1FBHnp6NU5iqOaqj+HE//6kyVz7mTSg88T3b5jrY9//MDffP7yf7n1f+/TvnP3Wm9fcvzS26sUB23Nhzlm7ICi0tR6+7oev6luf267NKfYG3L7SQ8+T2z7ePoGmNmRbcR5pnBrWz+FA7kOTpvhlT1arojVoi81cVD2qVSW/PcOdu7cSfv27Wt9/OagPt5jWiJpl8q5u21KelZXR5JSwm3qWhuqdEHtkmLphVY73nptgyZwdp/Iq7TmUmVDDkti3HYsi15t/OjZxs81BDDyj/VEP/J/qKxnElKPLyQ0yJfNiadIz7dg1KnpGuFHqK+hQXuCQdmhkXlmG0cyC1yzDob5GEjMyCfASyez6QkhRDO1fft2MjIy6Nevn2uZw+Fgw4YNvPbaa1gsFjSasokDg8GAwVB+pjedTidfACpQl3bRaDSYTCYcqPjrRAFHTxehUasY1S0CjVZHdcMb7E4wmUx4BYURGNW21sfPTD9ZfHyFahNIVR2/ou0VlabafVa1fV2P31S3L2mX5hh7Q2zvFRRGQGQMmA4SEBlTZl/jQ+2s2JpErsXBn1l6xvaKRH2mvpoDFSaTCY1G0+Lfl+S9t2LSLpVzV9vU9JiSlBJuU1+1oRpzKF9dai6dyDGRVXjOthknGfLI/6GxWtg78DISHltIkL+3KyEFYLYV91bSa9XVJr7qytuoZUTXMFb/ncqu5OwyCanBccEkZRWRY7I1aAxCCCEazuWXX87u3bvLLLv55pvp3LkzDz/8cLmElHCPHIuTjceLJ4AZ3CGIEJ/ySUEhWhtvg5arekby+fYUjp0uYnPiKS6JD3F3WEKIOpJvlcJtShIg5yaU/DyKE0o1SXpUN1tcfSdP6jLksKJt80Mj2XTzLNru2caHdz5LiE6PAq6EVAmbozg51BjFxtsEeHJpxxA89JoyBd3T8sw4FSl4LoQQzZmPjw/du5cdWuPl5UVQUFC55cJNNDp+O+nA4VSIDfKkd7S/uyMSoskI9zUysmsYP/yTxo6kHIK8DHSNlGHEQjRnkpQSblW6NlSByUzK3we5pk8U/t416yXVELPFVTYUsMBsx2Jz4GPQYtBrUCkKpwutOEv1pa9qyGGZ4YqKAme6G++8djrJN92KOqMQAIutfBFTXakaElUlvupLkc3BqYLimU/yK3q+EWIQQgghWqOAodPJsSh46DSM6BKGSqWqfiMhWpGOYT6cLrCy5VgWP+9Px99ThwzaEqL5kqSUcLuS2lA2m5GUv8HLUPPT8nx6LlVVf6qioYCBXjr6tw1ky9EsMvIs7E3NxWxzuoa0lfQgqm7IYclwRb/1PzNwxWK+mfcGFu/iOzveHnpiAhUKrQ4MurJFTI264hn6SlRXa6s+1LXelxBCiOZj3bp17g5BnLEluQDf/uMAGNE1rFbXREK0Jhe2DySr0EpiZgHf/Z3K8GiZjEeI5ko+6USzVtvkSVX1p/w99BUOBdSoVCzbfIxgbwN6rZr2Id4cySwgPd/C5sRTDGwXiM2pVDvk0Nuo5aqTfxEw7y60Viv9PnuP326+H39PHQPaBQKQsDcdFcU1nEqKnLcP8UavVbtirWmtrbqor3pfQgghhKiZzHwLz69PA6BjgJp2wV5ujkiIpkulUjGyWxh5221k5Fv4NdmB2kOG8QnRHElSSjRrtUmeVFd/alD7oAr3owBJWUUYtGqCvA34GnV0jfAj32zD5nDSIdSbHlH+1deuWrOGkJumgNVK/hVXYXhyLmO8PYkq1VOrZChjx3AfNh06hc2hlElI1bTWVl1VVu+rMWMQQgghWgunU+E/n/9FjtmBNfMYfTrGuzskIZo8nUbN1b0i+XRbMnlmO6ET/4vZXr4MhhCiaZNvlqJZq03ypLr6U+n55gqfK6nxVFJsHEB/JkEFYNRpapSQYvx4sFhg/Hh8Vq6kt15f4espqYHVNcKPEzkmiqx2PPXaMsmr0qoajlgXpet9VReDEEIIIc7fe5uOsu5AJjqNipPfvIDmknfcHZIQzYKXQcv43lGs2HIcIjvzzC+pfBAfh1Yjw/mEaC7k26Vo9mqaPKmu/pTdoVS4vKTGk66SD7dq6yudk5Bi5UqoICF1rtIJqspUNRyxTYBntceojxiEEEIIcf7+Ss7huTX7AbjzwlBmPXvczREJ0bwEeOkZ0kbL2iNF/J5UwNxv9/DUuO4ySYAQzYSkkEWLUJI86RMTQKdwnwp785xbf0qtghBvPcHe+uIZ9bQa2gZ5oj7n80sFxAR6lik2XqLa+kpWK9xxR7UJqQKznQNp+exIyuZgWj4F5upnt6tuOGJN9iGEEEII98kz27jnk53YnQpjeoRzVWc/d4ckRLMU4qkm89sXUAEf/pHEG+sOuzskIUQNSU8p0WqUrj+lVkG4r5HNiadcBcVPF1rxNKhpG+zFsVOFOM90nHIoCtMHt2XL0aza11fS6+GHH+Cll+C11ypMSJ1vb6fqhiOeyDFJLychhBCiiVIUhUe/3E1SVhFR/h7Mn9CTzBPSS0qI82U6+Dt3Dgrl9d8zeOHHA/gatdw0qK27wxJCVEOSUqLVKF1/SqdWlUlIlcxwZ3dAeq6ZUd3CMdkcZYYCxgZ6VTlEsHRtJ19zIeEx4cXPd+4MixdXGFN1vZ2u6xddadKruuGIRdU8L4QQQgj3Wbk1me/+TkWrVvHq9X3w89CR6e6ghGjmrukWgNrDl1d/SeSJr/fgbdRyTZ827g5LCFEFGb4nWpWS+lMdQr3x0GuID/Wma4QfvqWG5hVaHahUqnJDAasaIpiSXcRn25NZvTuVYx9+QZt+3dj81sekZBdVGU9NejtV5tzhiOeqttaVEEIIIdziYHo+c7/dA8CDozrRNybAzREJ0XLMGtGR6Re1BeDBz/7mxz1p7g1ICFElSUqJVsfbqMWg0xDu50GQtwG9tvyfQW16GZXu7RS7dQNXz70LY0EeMd98Vm1tp7r0dioZjliRamtdCSGEEMItTFYHd3+8A7PNySXxwfz7kvbuDkmIFkWlUvHfq7oysV8bHE6Fez7eyaZDp9wdlhCiEm5NSm3YsIGxY8cSGRmJSqXiq6++cj1ns9l4+OGH6dGjB15eXkRGRjJ16lROnjxZ5T6XLVuGSqUq92M2mxv41YjmpKpeRla7E7PNUeOi4yW9nUoSUlqblUODR7D2wfkN2tupZDjiuYmpGtW6EkIIIYRbPPndXg6mFxDiY+ClSb1RnzvDihCiztRqFf+b0IPR3cOxOpzctnwb245luTssIUQF3PqttbCwkF69enHzzTdz7bXXlnmuqKiIHTt28MQTT9CrVy+ys7OZOXMmV199Ndu2batyv76+vhw4cKDMMqPRWO/xi6aldE0nb72WyHNqPpVWuuh5aXlmGwVmO4czCsgssALVFx032xz0/Od3hpxJSCVePIJP7n0Og0qLnpr1dqpoCF9NejuVDEesqtaVEEIIIZqG7/4+ySdbklCpYOGk3oT4GNwdkhAtllajZtGU3hQu386Gg5lMW7KF92cMpH/bQHeHJoQoxa3fXEePHs3o0aMrfM7Pz4+EhIQyy1599VUGDhxIUlISMTExle5XpVIRHh5er7GKpi0lu4jVf6eSlFWE1eFEr1ETE+jJmJ4RFSaTShc9L0kIWe1OCsx2+scGkJZ3tmddRUXHSxJguSYrRz74nAlP3YvWbmV9t4t5a9Jj9PXxYF9aHrFBXjXq7VTZ7Hs1SS6V1LoSQgghRNN19FQhs7/YDcCdQztwcXywmyMSouUzaDW8fWM/bl2+lc2Jp5l6JjE1QBJTQjQZzao7RW5uLiqVCn9//yrXKygoIDY2FofDQe/evXnqqafo06dP4wQpGl2B2c6qnSlsO5aN2eYEQKNWodeqWH8gg7hQHwI89QR46TldYCE934zdoRDuY+SqnpFkFVopstox2xwcziggKauIXJPNldzyMepcw/A6hfuQkl3kmsFvy9Esrl/7LXq7lV+7DmbulMewF9qxH82ic7gPBWY7gV76KuOX3k5CCCFEy2a2Objjw+3kW4pvft0/vKO7QxKi1fDQa3h36gBuW76NTYmnmLZkC8tuHsjAdpKYEqIpaDbfes1mM4888gjXX389vr6+la7XuXNnli1bRo8ePcjLy+Pll19m8ODB/PXXX8THx1e4jcViwWKxuB7n5eUBxXWtSn5KHouGcz7tXGix83dKDkUmC93DvSmw2EnKLqJ7hA+7knM4lWei0GzF4VTQaVT4GnX8sCcNq92JUaemT4w/Y3tG0T7Cm79Scjiamc+x0wWu5BaAUaembZA3BSYzOQVaEv45Sa7JRpCXntMFJpZNf4SDEe35oM9o0GjRqxRO5RcRFh9IiLeBzNwi/I1Vl28zaKB9UOkhpkqDnW9yPjceaevGIe3cOKSdy5O2EJXJzMwkNzfX9fiFDansT8vH36jhwYsCOXb0SIXbHT9+vLFCFKJV8dBreHdaf25bvo2Nh04xfekWlkwfwIXtg1zrnPt3W1t+fn6EhITUR7hCtCrNIills9mYMmUKTqeTN954o8p1L7zwQi688ELX48GDB9O3b19effVVXnnllQq3mT9/PvPmzSu3fO3atXh6nh36de5wQtEwzqedO5f8Rw19gwBrBnFhZ5blli2OPz6o1IOCDHb9dpBdZx72BHpWdNNEySDl7yOkAIFA22PHyIuJYVLYmWTTjZdzN1bAenabwgNQCIfS4VCtX1HDk/O58UhbNw5p58Yh7XxWUVGRu0MQTdCpU6fo3LkLeXnFX269e44gaPR9KE4HB5Y+yqB5f1e7D7NZzi0h6ptRp+GdqWcTUzcv3co7U/tzcXwwmZmZxMXFu/5uz4evrx+JiYckMSVELTX5pJTNZmPSpEkcPXqUX375pcpeUhVRq9UMGDCAQ4cqTwvMnj2bWbNmuR7n5eURHR3NyJEj8fX1xWazkZCQwIgRI9DpdJXuR9RNbdq50GJn1c4T5JpsJGcVsSMp2/Vc90g/YgI92Jh4mpPZRYT6emB3OskqKO4NN6hDMIFeehIzCwDoEOLNlAExKCi8u+EIGQWWcscL9TZw66XtMdkcJH+8ikvmPcSxkVcz96p78TDoXPvyM+rwOjPs7opu4ZwutDKyazjxYd710kb1Qc7nxiNt3TiknRuHtHN5JT2rhSgtLy+PvLxcbn9uGfiG8+NxO04Feofq6TbvxSq3PbZ3J5+88DAWi7XK9YQQ56ckMfV/H2xn/cFMZizbyiv/6k2csdD1dxsQGlnr/WZnnOSth6eTm5srSSkhaqlJJ6VKElKHDh3i119/JSgoqPqNzqEoCrt27aJHjx6VrmMwGDAYys9+otPpylx4n/tYNIyatHP6aTM5ZieoNBgNenRaHYVnZrlLybXSIzqQo6dN6DQaFJUas92JxalyPR8V5I1dKe7lZHGAxQmg4sK4UDYnniI9/2xiKszHwIVxwdicKsJ+30jvefegtVnRF+QT7Kkn16FyHd+hUmNX1IT5GFCpNfh5GYkJ9kGna3p/anI+Nx5p68Yh7dw4pJ3PknYQVfEKiiAhBZwKtA3yZEjPSFQqVZXbZKWfaKTohGi9jDoNi6f2Y+aKXfzwTxp3frSD+y8uniQrIDSSkKhYN0coROvi1m/KBQUFJCYmuh4fPXqUXbt2ERgYSGRkJBMnTmTHjh189913OBwO0tLSAAgMDESvLy4ePXXqVKKiopg/fz4A8+bN48ILLyQ+Pp68vDxeeeUVdu3axeuvv974L1A0mJIEFICvUUf7EC+OZBZSaLWjVoPJ7kCnURPgpUerUaFWn70IVKvB5lBcj3UatWuGvLQ8MwPbBaIAFrsTg1aN6szyixK30WbG9ahsVg4NHsGah19kUJAvvx0+hSbQk5O5JgxaDWE+BgbHBWO2O2o8g54QQgghWpY/Uu3kmhR8jVpGdQuvNiElhGg8Bq2G167vy6Nf7mbltmQWbEzDd+A17g5LiFbJrd+Wt23bxrBhw1yPS4bQTZs2jblz5/LNN98A0Lt37zLb/frrrwwdOhSApKQk1OqzRaRzcnL497//TVpaGn5+fvTp04cNGzYwcODAhn0xolF56c+eunqtmgh/DxQFrA4nOo2KUB8DAZ56fIxatGo1RariJJVRp8FmL14HiouYxwR6EuXvAYCvh47MgrNd5vPP/Nvt799o89jtqCwWTFddzW8PPI/dpiItz8yAtoEYdWqCfYwYtMUJLqNOIzPoCSGEEK2U78AJpBQoaFQqxvSIwKjTuDskIcQ5NGoV/7u2B/5eOt5ef4SAYbewK8PO8EhFkshCNCK3fmMeOnQoiqJU+nxVz5VYt25dmccLFy5k4cKFdQ1NNHFR/h74e+rIKSqe+cjXqMMYoiHfbMPXQ0ePCD96RftzIscEgI+HFi+DBh+jDpvDSaHZjlGnpn/bAMb0jHAlj0Z0DSNhb7prv1CckBpxJiHFNdfgsWIF1zrVnMgxUWS146nXSgJKCCGEEADsTjPhP2QaAJd2DCbM11jNFkIId1GpVMwe3QVHUR7vbj3F3iwnzr3pXN4lFK266tmzhRD1Q75Fi2bJ26gtl0DSa9V0CPVmRNcw2gR4cvewOL7adYL0PDM6jRoPvQYPnZqB7YKw2p2E+BhpF+xVJpnUJsCT6/pFcyLHhMlqx6jTYEzSg8NB/uixqJZ9iLdejzfQKdzHTa9eCCGEEE1RrhUWrUtDpdbQ1ldNjyg/d4ckhKiBKb2CeO7JJwgefR/70/LJN9u5qqf0chSiMUhSSjRbpRNIFfVYah/qzb8v7VDrHk3eRi2dwn1IyS4qTnrF9CfixQ9Jj++G7+50V9JLCCGEEKKExebgvQMaskwOrJnHGdgxToYACdGMFPydwMR/P8Dmk05O5Jj4dFsyV/eKxN9T7+7QhGjRpE+iaNZKEkh9YgLoFO5TLuFU3fOVMa3+kd9/3u7qhZXatQ9OnZ6cIhsJe9MpMNur2YMQQgghWgtFUfjvt/s4XqDCW68m88un0aolISVEcxPhpea6/m3wNmjJLrLx6bYUUnNN7g5LiBZNklKiVSsw2zmQls+OpGwOpuUXJ5vWrMEwYRwj7/4XXqfSy22TU2Rz1aoSQgghhFj22zG+3HkSFQqPDg3HnpPq7pCEEOcp2NvA5AHRhPoYMNkcfLHjBHtO5ro7LCFaLBm+J1ot1/C8Coqaqy0WMjt0weQXUOG2Rda69ZQqMNs5kWOi0GrHW68lUgqlCyGEEM3Sb4mnePr7fQCMi3XSL0qG+AvR3HkbtEzs14Yf96RxOLOQn/ZlkJFv4dL4EDTSC1KIeiXfgkWrVGC2l0tIxW7dwGVz70Jls5J/xVV8f898nLqKx5B76s//T6eiZJi/p05qVQkhhBDNTHJWEXd+vAOHU2F8rwiGeiS7OyQhRD3RadRc2SOCLUez+ONoFn+n5HKqwMKY7hF4GeRrtBD1RYbviVbpRI6pXELq6rl3obVZSbxoOMdfX4Kvn1eF2/p76ojy9ziv41aUDAOkVpUQQgjRzOSbbdy2fBs5RTZ6tfHjqXFdkbrmQrQsKpWKC9oHMbZXBHqNmpM5ZlZsTSYt1+zu0IRoMSQpJVqlwlLD79r89WeZhNT3jy3EptUyomsY/p66MtuV9Gg636F25ybDSpNaVUIIIUTzYHc4ufvjnexPyyfEx8BbN/WTqeOFaMHaB3szZUA0AZ46Cix2PtuezI7j2SiK4u7QhGj2pN+haJW8Sg2/Ox0bT05UW3IiY/j+sYU4dXo89VraBHhyXb9oTuSYKLLa8dRriapj7afCampR1bVWlRBCCCEalqIo/PebPaw/mImHTsN70/oT4eeBzVbxTSchRMsQ4KVn8oBoft6XwaGMAjYmniIpu4iRXcPcHZoQzZokpUSrFOXvgb+njpwiGyb/QD57cTlWDy+cOn2Z4XneRi2dwn3q7bhe1dSiqkutKiGEEEI0vHc2HuHjP5NQqeDlKb3p2cbf3SEJIRqJQathdPdwok/msf5gJsdPF/Hxn0lcGCZjd4U4XzJ8T7RK3ut+Ytz2Na7heWbfAFdCqi7D86pTkgyrSF1qVQkhhBA1NX/+fAYMGICPjw+hoaGMHz+eAwcOuDusZuGH3ak8u3o/AI9f2ZWR3cLdHJEQorGpVCp6RPkxZUA0gZ56Cq0Ofk624z9kOla7093hCdHsSFJKtD5r1sD48QTedydTsvYxpkcEQzuFMKZHBNf1i27QGfC8jQ1Tq0oIIYSoqfXr13PXXXfxxx9/kJCQgN1uZ+TIkRQWFro7tCZtZ1I2M1fuAmDaoFhmDG7r1niEEO4V7G1gysBoukX6AuB34UTu+Oo4f6fkuDcwIZoZ+QYsWpczCSksFhg/Ho9Rw+mk1zdqCA1Rq0oIIYSoqTVr1pR5vHTpUkJDQ9m+fTuXXnqpm6Jq2hIz8pmxbCsWu5PLOofyxFVdUclUe0K0ejqNmuFdwghSFfHrgUyOE8A1b/zGnUM7cM9l8ei10gdEiOrIX4loPc5JSLFyJTRyQqpESa2qPjEBdAr3kYSUEEIIt8nNzQUgMDDQzZE0TSdyTNz03hayi2z0auPHq//qg1Yjl9BCiLPa+Kg5+d6dDGvvg8Op8OoviVz92iZ2JGW7OzQhmjz5JixarAKznRM5JgqtdkI3/UrUjOtRNYGElBBCCNFUKIrCrFmzuPjii+nevXuF61gsFiwWi+txXl4eADabrcFmnDt16pTrOOfD19eX4ODgOsdxusDCje9uJTXXTIcQLxbf2Ae9WqnwdZcsczgceHh4oEFBpThqdTytmuJtVdR626a6/bn/Nvbxm+r257ZLc4q9sbavzX40KHh4eOBwOM7rfakuf7clxzeo7DwyJIxrBrRjzrf72J+Wz4Q3fmNy/ygeGBFPgGfl3z1q8p7ncBTHdejQITQaTZnn6vqe5+733Locv6Rd3DkDal3ir6/Pq4qUtIm72qamx1UpiqI0cCzNTl5eHn5+fuTm5uLr64vNZmP16tWMGTMGna7iItWi7urSzqUTUN56LVq1ivWHMskpsuF/4hg3/XssWpsV05Vj8fjy81adkJLzufFIWzcOaefGIe1c3rnXC83RXXfdxffff8+mTZto06ZNhevMnTuXefPmlVv+8ccf4+nZcHUY3c1kh9f2akgpVBGgV5jZ3YG/wd1RCSGagwIbfH1czZbM4l6VXlqFsTFOLghVUMvIX9FKFBUVcf3111d7nSQ9pUSzl5JdRMLedHKKijOxId56/krJxduoxdeoIycylu0TZxB0PJHND77AtU413m6OWQghhHC3e+65h2+++YYNGzZUmpACmD17NrNmzXI9zsvLIzo6mpEjRzZIMu7IkSP06dOHGU++SUBwRK23zz6VypL/3sHOnTtp3779ecVgtjmYsXwHKYXZBHrpWHHrQNoFe1W5jc1mIyEhgfj4ePr3788Db3xFUGR0rY6b+NefLJlzJ7f+733ad66451pz216lOGhrPswxYwcUlabW27s7/oba/tx2aU6xN/T2HTp1qfE5U+L0yWQW3Dn+vP/uS953zufvtrLjTwK2Hc9mzjf7OJhRwIojGvbb/HhkVEf6xQaUO3Z173lqFPoGmNmRbcTJ2cxWXd/z3P2eW9fj52WlclkHf+Lj44mPj6/19nVVl/jr4/OqKiWfSyNGjHDLDcWa9h6TpJRo1grM9jIJKQAFSMoqwqhV0TXSH71WzW/TZ6JyOlBsKk7kmOgU7uO+oIUQQgg3UhSFe+65h1WrVrFu3TratWtX5foGgwGDoXwXIZ1O1yAXuRqNBpPJhG9wJIFRsbXe3oEKk8mERqM5r/jMNgd3rdjJ1mPZ+Bi0LJ9xAR0j/Gq8fUn8DlQ1/kJdwu6keFuFWm/b1LdXVJpq99mU42+o7UvapTnG3tDb1+ScKVHXv/u6/N1WdfxBcaF8f18w7/92jIUJB9mVnMuUd7cysmsYD13RmbhQ7xq/56kUB5gOEhAZUybG+nrt7nrPrevxi53/8euqLvHXte1qqqE+r2ty3JqQKo2iWTuRYyqTkAKw2Jz02vMH9778AObcM9lZlQpFU5yDLbLaGztMIYQQosm46667+PDDD/n444/x8fEhLS2NtLQ0TCaTu0NzO7PNwW3Lt7HhYCYeOg3vTR9A96iaJ6SEEOJcOo2aWy9pzy8PDuVfA6NRq2Dt3nRGLdrA7C93c6pQvpuI1k2SUqLJKLQUvyH/lZLDwbR8CszVv0EXVpBgard9Iw+++Qj9dv/GhV8vL/e8p146CAohhGi93nzzTXJzcxk6dCgRERGun5UrV7o7NLcyWR3c8v5WNh46hadew7KbBzCwncxIKISoH2G+RuZP6MmPMy9leJcwHE6FT7YkcdOnRwgccTsFVin1LFon+XYumoSU7CIS/jlJILDp0CkUlQZ/Tx0juobRJqDyIqpe5ySYYrduYOi8u9DYrWzpPYTfrpmBf6nn/T11RPl7NMhrEEIIIZoDmeOmvCKrnVuWbeP3I6fx0mtYNmMgA9pKQkoIUf/iw3x4d1p/th7L4rkf9rPteDY+fa/i2yM2OpvS6B8bSKBX652USbQ+0lNKuF1JXahcU9lheDlFNhL2plfZYyrK3wN/z+KxqrFbN3D13LvQWK2kDB3Flw+9iKfP2YRWSZLL2yi5WCGEEEIUK7TYmbFsK78fOY23QcvyWyQhJYRoeAPaBvLZ7YN4cUw0pmM7UYB9qfl88MdxvvnrJMdOF8pNBNEqyLdz4XYldaEqmh01p8hWZWFyb6OWEV3D2LvkUy6bexdam5VDg0fwx9yXub17FBabQpHVjqdeS5S/hySkhBBCCOGSmW9hxrKt7D6Ri7dBy/szBpaZFUsIIRqSSqWid6QnGSuf4LY31pBYaODIqUKOnvnx89DRI8qPrpG+eMrXGNFCyakt3K6iulClVVaYvMBs50SOiaLcPIY9/wham5WcUVfhXLyMa0L9JAElhBBCiEodP13I1CVbOH66iEAvPUunD6BXtL+7wxJCtFLBHmq6xEWSVWhl94lc9qbmkWuysSnxFL8fOU37IE8uCVDhGeFEo639DIFCNFXyrV243bl1oc5VUWHylOwiEvamu2be2/ff1xmw+hM8lrxLpzD/hghTCCGEEC3E3yk53Lx0K6cLrUQHevD+zQNpH+Lt7rCEEIJALz1DOoZwUYcgDqTl83dKLpkFFg5lFnIoU4P+8HHiQn3oGOZNVIDUyhXNnySlhNuV1IXKLXSUe66iwuQlNagKTueCR3HNqPSOPfiuYw/8E7O5zs9bekkJIYQQokLrD2Zyx4fbKbI66Bbpy9KbBxDqY3R3WEIIUYZOo6Z7lB/dIn3JzLdwMD2Pw2k55Fhhb2oee1Pz0GlUhHmAd69RZBTYiHN30EKcByl0LtyupC6Un4euzPLKCpOfyDHht/5nZkwbTvi+XWWeK6lBJYQQQghRmqIoLNt8lBnLtlJkdTA4LogV/75QElJCiCZNpVIR6mvkkrgg5vR1MLFPBN2jfPHUa7A5FFIKFIKuuIfrVxzh8gXrePjzv/l0WzJHMgukULpoFqQ7iWgS2gR4ck2fKNb/vIdL4oPx9jBWWphcs3YNV58pat7r209I69K7zPOV1aASQgghROtktjl4/Kt/+Hx7CgDX9IniuWt7otfK/VkhRPOhVkGbAA+iAr25rJNCZoGFPUdOsmXXbjyju3I4s5DDmYWs3JYMQJCXnu5RfnSO8KFzuA+dw33pEOIt732iSZGklGgyvAzFp2PPNv7odLqKV1qzhvb/vgn1mVn2Eu5/qtwqFdWgEkIIIUTrlJpr4vYPtvNXSi5qFcwe3YVbL2mHSlXRvL9CCNE8qFQqQn2MqII1fPvRQ2zfvZ8stT/bjmez/XgWf6XkcrrQyvqDmaw/mOnaTqtWEenvQXSgBzGBnng4TXh2uZT0QieaQiteeg16rVreI0WjkW/vovlYswbGj0dtsXD80pGsfngBTp2+zCqla1CVzM5XaLXjrdcSWUnPKyGEEEK0TFuOZnHnR9s5VWDF31PHa//qy8Xxwe4OSwgh6p2vUUPfuDCGdw0DwGJ38M+JPPal5nEgLZ8DafnsS8sj32wnKauIpKwiNnMagJCrH+LnZDskHwdAo1LhcSY5ZdCqi//VFP9bvEyDVqNCr1FjynXgETeQHScKyddn46nX4qnX4KnX4GXQYpAEl6iGfEMXzcOZhBQWC1xzDZo3l+CbmO2afQ/K1qA6d3a+0s+3CfB0wwsQQgghRKNRqVm+4xQf7TqIw6nQOdyHd6b2JzpQrgGEEK2DQauhX2wA/WIDXMsURSE9z+JKSiVnFbHneDrf/bKZkLgeWJxqrHYnDkWhwGIHS82OFXrtf3nohxQgpdxzKlXxbOseZxJVpZNWnnoNDouJwJF3sjPDjq/lNLozyS9PvQavUutqNTLksKWSpJRoHt5+25WQYsUK2uj1XOfnzYkcE0VWO556rasGVcnsfKUTUlBcBD1hbzrX9YuWHlNCCCFEC1VgVQi74TmW7yjuAXBNnyieuaa7DO8XQrR6KpWKcD8j4X5GBrYLBCAxUc17t81mxtK1hETFYnc4KbI6MNkcWO1OLHbnmX/PPHYUP7Y5nNgdCoVFRSQl7qNTt57Y0VBkdVBktWO2OQFQFCiw2IuTXJXw6TOGfVlOyMqqdB29Vo3XmaSWr1GLr4cOX6MOCp2c9gCHU4q6N1fy6Syah08+gYUL4YEHQF88ZM/bqKVTuE+5VU/kmMolpEqUzM5X0XZCCCGEaL4URWF/Wj6/HLNhjOqCp07N/Gt7Mq53lLtDE0KIZkOrUeProcbXo5Iav+fIPHGc+XMeZOOhQ8TFxbmWO5wKJpuDIov9TKKqOFlV+t9Cq4Pkk+k8/9LLDLr6RrQePtgcxYmws+s6cDgVrGeSY9lFNk6cE0NCshb1rsNE+p+kXbAXcaHedAgp/okL9SbYWy9DCJswSUqJpmv/fujUqbjPp9EIs2fXaLPCambfk9n5hBBCiJalwGJn/YFMEjMLADAn7+HDB8dysSSkhBDCLTRqFd4GLd6GqlMOiYk2Htv8MX1unU5IVGi55xWlOCFVeCaZVWCxk2+2k2eykWe2k51fRJFdwaGoSMk2kZJtYuOhU2X24WvUEhfqTadwH7pG+NI10pfO4b6uibaEe7n1t7BhwwZeeOEFtm/fTmpqKqtWrWL8+PGu5xVFYd68eSxevJjs7GwuuOACXn/9dbp161blfr/44gueeOIJDh8+TIcOHXjmmWe45pprGvjViHpVUkPq3nvhueeKE1M15FVN93zpvi+EEEK0DE5FYXdKLr8dPo3V4USlgu5BGr57fjbh8ya4OzwhhBB1pFKpMOg0GHQaAr305Z7POnGMvv4mwmLj0AVEcDijgMOZBSRmFHA4s5Dk7CLyzHZ2JOWwIymn1H6hbZCXK0nVNcKXbpG+hPgYpFdVI3Prt/PCwkJ69erFzTffzLXXXlvu+eeff56XXnqJZcuW0bFjR55++mlGjBjBgQMH8PGpePjV77//zuTJk3nqqae45pprWLVqFZMmTWLTpk1ccMEFDf2SRD1Q/fgjTJxYXEPq0CFwOEBb81M1yt8Df09dhUP4Ss/OJ4QQQojmKzPfwi/7M0jLMwMQ5mvg8s5hkJfGd4qzbvvOzCQ3N7dW2zgcDgCSk5PrdGwhxPk7fvx4o25XH/upr2O7mzvbXq2CotOptPUx0D8Q+gfqoVMgEIjF7uRErpXjOVaOZllIzLJw+LSF00V2jp4q5OipQr7fneraV7C3np5t/OnZxo9e0f70auNfYTKsqajq86rkc+nIkSNoNJoK1/Hz8yMkJKTB4qsJtyalRo8ezejRoyt8TlEUFi1axGOPPcaECcV3ut5//33CwsL4+OOP+b//+78Kt1u0aBEjRoxg9pmhXrNnz2b9+vUsWrSITz75pGFeiKg3oTt2oHnuueKE1PjxsHJlrRJSUFxrakTXsEpn35Mi50IIIUTzZbI6+PPoaXafyMWpgF6j5qIOQfRo44dapSIzr277z8zMJC4unry82iWlPDw8+OSTT7j66qsBMJuL6haIEKLGivJyABXDhw+v037O9++2Po7fXN8z3N72+bkQoOfqq6/GZDLVeDu1px/60HboQ9vjGdWJzoOGczzLxKkCK7/sz+CX/RmudaMDPejVxp/e0f70bONP9yjfJjH6prrPq5LPpT59+lTaNr6+fiQmHnJrYsr9LVmJo0ePkpaWxsiRI13LDAYDQ4YM4bfffqs0KfX7779z//33l1k2atQoFi1a1JDhinqg+vFHBs6fj8pmO5uQ0p9fVrpNgCfX9YuucHY+IYQQQjQ/doeTnck5bDuWjdVR3BOqQ4gXQzuG1uvne25uLnl5udz+3DICQiNrvJ0GBTAx8b4n+eB//8FisdZbTEKIqplNhYDCDY+9Qkxc51pvf2zvTj554eHz/ruty/Hremx3c3fbW8yFgJ5JDz5PZPtOtd4+O+Mkbz08nY3Pz6BNbDv2pubxd3IOf6fksislhyOZhSRnmUjOMvHd38U9qtQq6BjmQ682/kQYrehC2uFUGn/2v+o+r0o+lx544ysclB+SWPLac3NzJSlVkbS0NADCwsLKLA8LC6uyi19aWlqF25TsryIWiwWLxeJ6nJdXfIvNZrO5fkoei4ah+vFHNBMnorLZsI8di/Lhh8UDfevQ5gYNtA8yllqiyO8Q5HxuRNLWjUPauXFIO5cnbdE4nIrCgbR8fjt82jWleIi3gYvjg4kJ9Gyw4waERhISFVvj9VWKA0wH8Q0MbrCYhBBV8wsJr9XfbYms9HPnc2u849fXsd3N3W3vGxx2XscvzajT0DcmgL4xAa5luSYbu1Ny+Sslh7+Sc/grJYf0PAv70/LZn5YPQOSMV/nsoI2w9GTCfY3FP35GvA3aRqlPVdnnVcnnUlBkNIqq4uF7TUGTTUqVOPeXqChKtb/Y2m4zf/585s2bV2752rVr8fQ8e7GTkJBQk5DFeYj+5Rf6WK2kXnABW6dORfnpJ3eH1OLJ+dx4pK0bh7Rz45B2PquoqHkOtWguHIrCnpO5bDuWTY6pOAHobdByUYcgOof7SCFaIYQQDc7PQ8fF8cFcHH/2hkNartmVpPr9YCrbj2aCwYuTOWZO5phd63nqNa4EVbivkVBfAwZt000OuUuTTUqFh4cDxT2fIiIiXMszMjLK9YQ6d7tze0VVt83s2bOZNWuW63FeXh7R0dGMHDkSX19fbDYbCQkJjBgxAp1Od74vSVRlzBgsI0aw1Wxm+Jgx0s4NSM7nxiNt3TiknRuHtHN5JT2rRf2y2J1497mSbw/bKLIX1/QwatX0iw2gd7Q/Wo3azREKIYRozcL9jIT7hTOqWziJcVri4y/nzrfWYDEGkJ5nIS3PzKkCC0VWB0dOFXLkVKFr20Avvas3lcHiBJV8pjXZpFS7du0IDw8nISGBPn36AGC1Wlm/fj3PPfdcpdsNGjSIhISEMnWl1q5dy0UXXVTpNgaDAYPBUG65Tqcrc+F97mNRz0aNQlm9Wtq5kUg7Nx5p68Yh7dw4pJ3Pknaof/+cyGXqyiMEjbyDInvxXeZ+MQF0j/JDr5ULdyGEEE2Rgp9BRUikH93OlHayOZxk5FtIzzWTllf8k2+2k1VoJavQyt7U4htb0fd/ysxvkxjU0Urv6AB6x/gT6WdsVb2B3ZqUKigoIDEx0fX46NGj7Nq1i8DAQGJiYpg5cybPPvss8fHxxMfH8+yzz+Lp6cn111/v2mbq1KlERUUxf/58AO677z4uvfRSnnvuOcaNG8fXX3/NTz/9xKZNmxr99QkhhBBCiJprH+KFUwF7bgYXdozggi5tpWeUEEKIZkenURPl70GUv4drWaHFTnre2SRVWo4Jm87IP+km/kk/ChwFINjbQO9of/rElMz454ePseXeCHNrUmrbtm0MGzbM9bhkCN20adNYtmwZDz30ECaTiTvvvJPs7GwuuOAC1q5di4+Pj2ubpKQk1OqzFysXXXQRK1as4PHHH+eJJ56gQ4cOrFy5kgsuuKDxXpgQQgghhKg1T72W50e3YdRFY+n43mpJSAkhhGgxvAxa2od40z7EG4CMlGO8+J9beHn5V5y06tmVnMP+1HxOFVj4aV86P+1LB4rn/+oQ4k3vaH/XT3yYtztfSr1ya1Jq6NChKFVMnahSqZg7dy5z586tdJ1169aVWzZx4kQmTpxYDxEKIYQQQojG1D7ICE67u8MQQgghGpRKpcJ+OoWRHf2Ii4sDwGxzsOdkLjuTctiVXPyTkm0iMaOAxIwCPt+eAoBOo6Ktv57AK+7hULYDh7eZYG99s7yZ02RrSgkhhBBCCCGEEEK0Fkadhn6xgfSLDXQtO1Vg4a/ks0mqv1NyyTXZOHTagk+vUWxNd7A1PRmVCoK89IT4GAj1MRLmoyOyGUz2J0kpIYQQQgghhBBCiCYo2NvA5V3CuLxLGACKopCSbeKn7QeY9dRCOo+YQo5Vjcnm4FSBlVMFVval5gPwGRoCvJIJ9TES4mMgxNtAiI8Bo67pZKskKSWEEEIIIYQQQgjRDKhUKqIDPbmknQ85Gz9g2IybCI6MocBiJzPfQnq+hcx8Cxl5ZgqtDrIKbWQV2tiflu/ah49Ri6/Wgd/FN3Ai10qcG1+PJKWEEEIIIYQQQgghmimVSoWPUYePUecqpK5SHATmHmS7rQ3pBVYyzySr8sx28s128gH/wf/iVJF76zhKUkoIIYQQQgghhBCihfHTQzs/T9qG+LiWWc4M8zuaksr6n36gw013ujFCaH6l2YUQQgghhBBCCCFErRl0GqICPOgUqOH0D6/gbXBvfSlJSgkhhBBCCCGEEEKIRidJKSGEEEKIVuaNN96gXbt2GI1G+vXrx8aNG90dkhBCCCFaIUlKCSGEEEK0IitXrmTmzJk89thj7Ny5k0suuYTRo0eTlJTk7tCEEEII0cpIUkoIIYQQohV56aWXuOWWW7j11lvp0qULixYtIjo6mjfffNPdoQkhhBCilZGklBBCCCFEK2G1Wtm+fTsjR44ss3zkyJH89ttvbopKCCGEEK2V1t0BNEWKogCQl5cHgM1mo6ioiLy8PHQ6nTtDa9GknRuHtHPjkbZuHNLOjUPaubyS64SS64bm4NSpUzgcDsLCwsosDwsLIy0trcJtLBYLFovF9Tg3NxeArKwsbDZbvceYm5uL0WjkVMph7KaCWm+fczodo9HInj17XLHWRkpKynkdX41CmL+FnIyTGI1Gsk8eI1Vfu0vtnPSU8962qW5f0i5pJ/fhRNXs4m+o7c9tl+YUe0Nvn6bX1PicaYjjN/b2Nd22sr8ld73n1Tb+Bts+4yRFbQzkpJ4gVXce27ux/Rr62NW9/5YcPzc3l9OnT9f6+NXJz88Hqr9OUinN6UqqkaSkpBAdHe3uMIQQQgjRDCQnJ9OmTRt3h1EjJ0+eJCoqit9++41Bgwa5lj/zzDN88MEH7N+/v9w2c+fOZd68eY0ZphBCCCFaiOquk6SnVAUiIyNJTk7Gx8cHlUpFXl4e0dHRJCcn4+vr6+7wWixp58Yh7dx4pK0bh7Rz45B2Lk9RFPLz84mMjHR3KDUWHByMRqMp1ysqIyOjXO+pErNnz2bWrFmux06nk6ysLIKCglCpataLoTWQv5GKSbtUTNqlctI2FZN2qZi0S+Xc3TY1vU6SpFQF1Gp1hZk8X19fOdEbgbRz45B2bjzS1o1D2rlxSDuX5efn5+4QakWv19OvXz8SEhK45pprXMsTEhIYN25chdsYDAYMBkOZZf7+/g0ZZrMmfyMVk3apmLRL5aRtKibtUjFpl8q5s21qcp0kSSkhhBBCiFZk1qxZ3HTTTfTv359BgwaxePFikpKSuP32290dmhBCCCFaGUlKCSGEEEK0IpMnT+b06dM8+eSTpKam0r17d1avXk1sbKy7QxNCCCFEKyNJqRowGAzMmTOnXNd1Ub+knRuHtHPjkbZuHNLOjUPauWW58847ufPOO90dRosifyMVk3apmLRL5aRtKibtUjFpl8o1l7aR2feEEEIIIYQQQgghRKNTuzsAIYQQQgghhBBCCNH6SFJKCCGEEEIIIYQQQjQ6SUoJIYQQQgghhBBCiEYnSalKzJ07F5VKVeYnPDzc3WG1CBs2bGDs2LFERkaiUqn46quvyjyvKApz584lMjISDw8Phg4dyp49e9wTbDNWXTtPnz693Dl+4YUXuifYZmz+/PkMGDAAHx8fQkNDGT9+PAcOHCizjpzTdVeTdpZzuu7efPNNevbsia+vL76+vgwaNIgffvjB9bycy0LUjsVioXfv3qhUKnbt2uXucNzq2LFj3HLLLbRr1w4PDw86dOjAnDlzsFqt7g7NLd544w3atWuH0WikX79+bNy40d0huVVNPudFcTupVCpmzpzp7lCahBMnTnDjjTcSFBSEp6cnvXv3Zvv27e4Oy63sdjuPP/646722ffv2PPnkkzidTneHVilJSlWhW7dupKamun52797t7pBahMLCQnr16sVrr71W4fPPP/88L730Eq+99hpbt24lPDycESNGkJ+f38iRNm/VtTPAFVdcUeYcX716dSNG2DKsX7+eu+66iz/++IOEhATsdjsjR46ksLDQtY6c03VXk3YGOafrqk2bNvzvf/9j27ZtbNu2jcsuu4xx48a5Ek9yLgtROw899BCRkZHuDqNJ2L9/P06nk7fffps9e/awcOFC3nrrLR599FF3h9boVq5cycyZM3nsscfYuXMnl1xyCaNHjyYpKcndoblNTT/nW7OtW7eyePFievbs6e5QmoTs7GwGDx6MTqfjhx9+YO/evSxYsAB/f393h+ZWzz33HG+99RavvfYa+/bt4/nnn+eFF17g1VdfdXdolVNEhebMmaP06tXL3WG0eICyatUq12On06mEh4cr//vf/1zLzGaz4ufnp7z11ltuiLBlOLedFUVRpk2bpowbN84t8bRkGRkZCqCsX79eURQ5pxvKue2sKHJON5SAgADl3XfflXNZiFpavXq10rlzZ2XPnj0KoOzcudPdITU5zz//vNKuXTt3h9HoBg4cqNx+++1llnXu3Fl55JFH3BRR01PR53xrlp+fr8THxysJCQnKkCFDlPvuu8/dIbndww8/rFx88cXuDqPJufLKK5UZM2aUWTZhwgTlxhtvdFNE1ZOeUlU4dOgQkZGRtGvXjilTpnDkyBF3h9TiHT16lLS0NEaOHOlaZjAYGDJkCL/99psbI2uZ1q1bR2hoKB07duS2224jIyPD3SE1e7m5uQAEBgYCck43lHPbuYSc0/XH4XCwYsUKCgsLGTRokJzLQtRCeno6t912Gx988AGenp7uDqfJys3NLfc+3tJZrVa2b99e5r0UYOTIkfJeWkpln/Ot1V133cWVV17J8OHD3R1Kk/HNN9/Qv39/rrvuOkJDQ+nTpw/vvPOOu8Nyu4svvpiff/6ZgwcPAvDXX3+xadMmxowZ4+bIKqd1dwBN1QUXXMDy5cvp2LEj6enpPP3001x00UXs2bOHoKAgd4fXYqWlpQEQFhZWZnlYWBjHjx93R0gt1ujRo7nuuuuIjY3l6NGjPPHEE1x22WVs374dg8Hg7vCaJUVRmDVrFhdffDHdu3cH5JxuCBW1M8g5XV92797NoEGDMJvNeHt7s2rVKrp27er6siTnshBVUxSF6dOnc/vtt9O/f3+OHTvm7pCapMOHD/Pqq6+yYMECd4fSqE6dOoXD4ajwvbTkmqG1q+xzvrVasWIFO3bsYOvWre4OpUk5cuQIb775JrNmzeLRRx9ly5Yt3HvvvRgMBqZOneru8Nzm4YcfJjc3l86dO6PRaHA4HDzzzDP861//cndolZKkVCVGjx7t+n+PHj0YNGgQHTp04P3332fWrFlujKx1UKlUZR4rilJumaibyZMnu/7fvXt3+vfvT2xsLN9//z0TJkxwY2TN1913383ff//Npk2byj0n53T9qayd5ZyuH506dWLXrl3k5OTwxRdfMG3aNNavX+96Xs5l0VrNnTuXefPmVbnO1q1b+e2338jLy2P27NmNFJl71bRd+vfv73p88uRJrrjiCq677jpuvfXWhg6xSZL30spVdT3V2iQnJ3Pfffexdu1ajEaju8NpUpxOJ/379+fZZ58FoE+fPuzZs4c333yzVSelVq5cyYcffsjHH39Mt27d2LVrFzNnziQyMpJp06a5O7wKSVKqhry8vOjRoweHDh1ydygtWskMh2lpaURERLiWZ2RklLujJOpXREQEsbGxco6fp3vuuYdvvvmGDRs20KZNG9dyOafrV2XtXBE5p8+PXq8nLi4OgP79+7N161ZefvllHn74YUDOZdF63X333UyZMqXKddq2bcvTTz/NH3/8Ua6HZv/+/bnhhht4//33GzLMRlfTdilx8uRJhg0bxqBBg1i8eHEDR9f0BAcHo9FoyvWKkvfSYrX5nG8Ntm/fTkZGBv369XMtczgcbNiwgddeew2LxYJGo3FjhO4TERFB165dyyzr0qULX3zxhZsiahr+85//8Mgjj7jel3v06MHx48eZP3++JKWaO4vFwr59+7jkkkvcHUqL1q5dO8LDw0lISKBPnz5A8dj79evX89xzz7k5upbt9OnTJCcnl/myKaqnKAr33HMPq1atYt26dbRr167M83JO14/q2rkick7XD0VRsFgsci6LVi84OJjg4OBq13vllVd4+umnXY9PnjzJqFGjWLlyJRdccEFDhugWNW0XKJ6+fdiwYfTr14+lS5eiVre+8rZ6vZ5+/fqRkJDANddc41qekJDAuHHj3BiZe53P53xrcPnll5ebAf7mm2+mc+fOPPzww602IQUwePBgDhw4UGbZwYMHiY2NdVNETUNRUVG591aNRoPT6XRTRNWTpFQlHnzwQcaOHUtMTAwZGRk8/fTT5OXlNdnsYnNSUFBAYmKi6/HRo0fZtWsXgYGBxMTEMHPmTJ599lni4+OJj4/n2WefxdPTk+uvv96NUTc/VbVzYGAgc+fO5dprryUiIoJjx47x6KOPEhwcXOYCSVTvrrvu4uOPP+brr7/Gx8fHdefTz88PDw8PVCqVnNP1oLp2LigokHO6Hjz66KOMHj2a6Oho8vPzWbFiBevWrWPNmjVyLgtRQzExMWUee3t7A9ChQ4dW3fPj5MmTDB06lJiYGF588UUyMzNdz5X0Km4tZs2axU033UT//v1dPcaSkpK4/fbb3R2a21T3Od9a+fj4lKur5eXlRVBQUKuvt3X//fdz0UUX8eyzzzJp0iS2bNnC4sWLW2UPzNLGjh3LM888Q0xMDN26dWPnzp289NJLzJgxw92hVc49k/41fZMnT1YiIiIUnU6nREZGKhMmTFD27Nnj7rBahF9//VUByv1MmzZNURRFcTqdypw5c5Tw8HDFYDAol156qbJ79273Bt0MVdXORUVFysiRI5WQkBBFp9MpMTExyrRp05SkpCR3h93sVNTGgLJ06VLXOnJO11117SzndP2YMWOGEhsbq+j1eiUkJES5/PLLlbVr17qel3NZiNo7evSoAig7d+50dyhutXTp0krfy1uj119/3fV+27dvX2X9+vXuDsmtanI9JYoNGTJEue+++9wdRpPw7bffKt27d1cMBoPSuXNnZfHixe4Oye3y8vKU++67T4mJiVGMRqPSvn175bHHHlMsFou7Q6uUSlEUpYHzXkIIIYQQQgghhBBClNH6BnILIYQQQgghhBBCCLeTpJQQQgghhBBCCCGEaHSSlBJCCCGEEEIIIYQQjU6SUkIIIYQQQgghhBCi0UlSSgghhBBCCCGEEEI0OklKCSGEEEIIIYQQQohGJ0kpIYQQQgghhBBCCNHoJCklhBBCCCGEEEIIIRqdJKWEEG6jUqn46quv3B2GEEIIIYQQQgg3kKSUEK3Ab7/9hkaj4Yorrqj1tm3btmXRokX1H1QNTJ8+nfHjx5dbvm7dOlQqFTk5Oa5lDoeDhQsX0rNnT4xGI/7+/owePZrNmzeX2XbZsmWoVCq6dOlSbr+ffvopKpWKtm3bllluMpmYM2cOnTp1wmAwEBwczMSJE9mzZ0+1r6GiWEvH4u/vX+F2/v7+LFu2zPVYpVKhUqn4448/yqxnsVgICgpCpVKxbt26Ms999913DB06FB8fHzw9PRkwYECZfVYlMTGRGTNmEBMTg8FgICoqissvv5yPPvoIu91eo30IIYQQLUl1N9OOHTuGSqVi165d9XrcmlyLWa1W4uLiyl33NFVVXQM1Vedelw4dOpSZM2c2ehznXlt+99139OnTB6fT2eixCFEfJCklRCuwZMkS7rnnHjZt2kRSUpK7w6l3iqIwZcoUnnzySe6991727dvH+vXriY6OZujQoeUuIL28vMjIyOD3338vs3zJkiXExMSUWWaxWBg+fDhLlizhqaee4uDBg6xevRqHw8EFF1xQLknUkKKjo1m6dGmZZatWrcLb27vcuq+++irjxo3joosu4s8//+Tvv/9mypQp3H777Tz44INVHmfLli307duXffv28frrr/PPP//w3XffMWPGDN56660aJeOEEEKIxjR9+nTXDRytVktMTAx33HEH2dnZ9XaM1NRURo8eXW/7q0+LFy8mNjaWwYMHl3vu3//+NxqNhhUrVtRqn1XdWGsqhg4d6vq9GwwGOnbsyLPPPovD4WjwY3/55Zc89dRTNVq3IdvyqquuQqVS8fHHH9f7voVoDJKUEqKFKyws5NNPP+WOO+7gqquuqrCnzDfffEP//v0xGo0EBwczYcIEoPiD/vjx49x///2uD3yAuXPn0rt37zL7WLRoUZkeRlu3bmXEiBEEBwfj5+fHkCFD2LFjR4O8xk8//ZTPP/+c5cuXc+utt9KuXTt69erF4sWLufrqq7n11lspLCx0ra/Varn++utZsmSJa1lKSgrr1q3j+uuvL/e6fv/9d7777jsmTZpEbGwsAwcO5IsvvqBLly7ccsstKIrSIK/rXNOmTWPFihWYTCbXsiVLljBt2rQy6yUnJ/PAAw8wc+ZMnn32Wbp27UpcXBwPPPAAL7zwAgsWLODPP/+s8BiKojB9+nQ6duzI5s2bGTt2LPHx8fTp04cbbriBjRs30rNnT9f6Dz/8MB07dsTT05P27dvzxBNPYLPZXM+XnCtvv/020dHReHp6ct111zXpC1whhBDN0xVXXEFqairHjh3j3Xff5dtvv+XOO++st/2Hh4djMBjqbX/16dVXX+XWW28tt7yoqIiVK1fyn//8h/fee88NkTW82267jdTUVA4cOMC9997L448/zosvvljhulartd6OGxgYiI+PT73try5uvvlmXn31VXeHIcR5kaSUEC3cypUr6dSpE506deLGG29k6dKlZZIo33//PRMmTODKK69k586d/Pzzz/Tv3x8ovgPUpk0bnnzySVJTU0lNTa3xcfPz85k2bRobN27kjz/+ID4+njFjxpCfn1/vr/Hjjz+mY8eOjB07ttxzDzzwAKdPnyYhIaHM8ltuuYWVK1dSVFQEFHcjv+KKKwgLCyu37xEjRtCrV68yy9VqNffffz979+7lr7/+qudXVLF+/frRrl07vvjiC6A4+bRhwwZuuummMut9/vnn2Gy2CntE/d///R/e3t588sknFR5j165d7Nu3jwcffBC1uuKPiJLkJICPjw/Lli1j7969vPzyy7zzzjssXLiwzPqJiYl8+umnfPvtt6xZs4Zdu3Zx11131eq1CyGEENUxGAyEh4fTpk0bRo4cyeTJk1m7dm2ZdZYuXUqXLl0wGo107tyZN954w/Wc1Wrl7rvvJiIiAqPRSNu2bZk/f77r+XOH723ZsoU+ffpgNBrp378/O3fuLHOsioaoffXVV2U+Rw8fPsy4ceMICwvD29ubAQMG8NNPP9Xqde/YsYPExESuvPLKcs999tlndO3aldmzZ7N582aOHTtW5nmLxcJDDz1EdHQ0BoOB+Ph43nvvPY4dO8awYcMACAgIQKVSMX36dKDi4YS9e/dm7ty5rscvvfQSPXr0wMvLi+joaO68804KCgpq9bpqytPTk/DwcNq2bcvdd9/N5Zdf7vo9lQy5mz9/PpGRkXTs2BGAEydOMHnyZAICAggKCmLcuHFl2sbhcDBr1iz8/f0JCgrioYceKncT8tzhe+fTloqi8Pzzz9O+fXs8PDzo1asXn3/+eZnjrF69mo4dO+Lh4cGwYcPK/Q4Brr76arZs2cKRI0fq1phCuIEkpYRo4d577z1uvPFGoPgOYkFBAT///LPr+WeeeYYpU6Ywb948unTpQq9evXj00UeB4jtAGo0GHx8fwsPDCQ8Pr/FxL7vsMm688Ua6dOlCly5dePvttykqKmL9+vW1iv+7777D29u7zM+5XecPHjxYYY0owLX84MGDZZb37t2bDh068Pnnn6MoCsuWLWPGjBnltj+ffTekm2++2dXDa+nSpYwZM4aQkJAy6xw8eBA/Pz8iIiLKba/X62nfvn2lMZcs79Spk2tZRkZGmfYvfQH/+OOPc9FFF9G2bVvGjh3LAw88wKefflpmn2azmffff5/evXtz6aWX8uqrr7JixQrS0tLOrxGEEEKIahw5coQ1a9ag0+lcy9555x0ee+wxnnnmGfbt28ezzz7LE088wfvvvw/AK6+8wjfffMOnn37KgQMH+PDDD8vVmSxRWFjIVVddRadOndi+fTtz586tdnh8RQoKChgzZgw//fQTO3fuZNSoUYwdO7ZW5RY2bNhAx44d8fX1LfdcyXWgn58fY8aMKVcGYOrUqaxYsYJXXnmFffv28dZbb+Ht7U10dLTrJtiBAwdITU3l5ZdfrnFMarWaV155hX/++Yf333+fX375hYceeqjG29eFh4dHmV7bP//8M/v27SMhIYHvvvuOoqIihg0bhre3Nxs2bGDTpk14e3tzxRVXuHpSLViwgCVLlvDee++xadMmsrKyWLVqVZXHPZ+2fPzxx1m6dClvvvkme/bs4f777+fGG290XS8nJyczYcIExowZw65du7j11lt55JFHyh07NjaW0NBQNm7cWC9tKERj0ro7ACFEwzlw4ABbtmzhyy+/BIqHrU2ePJklS5YwfPhwoLhnzG233Vbvx87IyOC///0vv/zyC+np6TgcDoqKimpd02rYsGG8+eabZZb9+eefrkRbTZW+K1lixowZLF26lJiYGNdF4WuvvVbjfZbcMSvZd7du3Th+/DgAl1xyCT/88EOtYqyJG2+8kUceeYQjR46wbNkyXnnllVrvQ1GUCtujtNLPBwUFuYq2Dh06tEzX988//5xFixaRmJhIQUEBdru93EVxTEwMbdq0cT0eNGgQTqeTAwcO1CrRKYQQQlSl5EaWw+HAbDYDxT12Sjz11FMsWLDAVaagXbt27N27l7fffptp06aRlJREfHw8F198MSqVitjY2EqP9dFHH+FwOFiyZAmenp5069aNlJQU7rjjjlrF3KtXrzK9sZ9++mlWrVrFN998w913312jfRw7dozIyMhyyw8dOsQff/zhug688cYbuffee5kzZw5qtZqDBw/y6aefkpCQ4LoubN++vWv7wMBAAEJDQ2tdlLx0D6J27drx1FNPcccdd5S5sVXfnE4na9eu5ccffyxzfC8vL9599130ej1QXPpArVbz7rvvuq53li5dir+/P+vWrWPkyJEsWrSI2bNnc+211wLw1ltv8eOPP1Z67PNpy8LCQl566SV++eUXBg0a5Npm06ZNvP322wwZMoQ333yT9u3bs3DhQlQqFZ06dWL37t0899xz5WKIioqqsBeVEE2dJKWEaMHee+897HY7UVFRrmWKoqDT6cjOziYgIAAPD49a71etVpfrwlz6jhQUd5fOzMxk0aJFxMbGYjAYGDRoUK3H8nt5eREXF1dmWUpKSpnHHTt2ZO/evRVuv2/fPgDi4+PLPXfDDTfw0EMPMXfuXKZOnYpWW/4tsap979+/v8y+V69e7WqHmrSrr68vBQUFOBwONBqNa7nD4aCgoAA/P79y2wQFBXHVVVdxyy23YDabGT16dLkhkR07diQ3N5eTJ0+Wu0i1Wq0cOXKEyy67rMKYSl7L/v37XXXDNBqN63dQuo3++OMPVy+7UaNG4efnx4oVK1iwYEGVr7vkArC6xJgQQghRGyU3soqKinj33Xc5ePAg99xzDwCZmZkkJydzyy23lLkZZ7fbXZ+306dPZ8SIEXTq1IkrrriCq666ipEjR1Z4rH379tGrVy88PT1dy0oSC7VRWFjIvHnz+O677zh58iR2ux2TyVSrm3gmkwmj0Vhu+XvvvceoUaMIDg4GYMyYMdxyyy389NNPjBw5kl27dqHRaBgyZEit467Or7/+yrPPPsvevXvJy8vDbrdjNpspLCzEy8ur2u1Hjx7t6vUTGxtb5SQrb7zxBu+++67rGvOmm25izpw5rud79OjhSkgBbN++ncTExHL1oMxmM4cPHyY3N5fU1NQyv0+tVkv//v0rrSN6Pm25d+9ezGYzI0aMKLPcarXSp08foPg8u/DCC8tcM1V2nnl4eLjKUgjRnMjwPSFaKLvdzvLly1mwYAG7du1y/fz111/Exsby0UcfAdCzZ88yw/nOpdfry81gEhISQlpaWpkP5nOnP964cSP33nsvY8aMoVu3bhgMBk6dOlV/L7CUKVOmcOjQIb799ttyzy1YsICgoKByH/hQfNfq6quvZv369RUO3SvZ908//VSubpTT6WThwoV07drVdYczNjaWuLg44uLiyiQCK9O5c2ccDke5GhQ7duzA4XCUGUJX2owZM1i3bh1Tp04tk8wqce2116LVaitMDr311lsUFhbyr3/9q8J99+nTh86dO/Piiy9WO7Xw5s2biY2N5bHHHqN///7Ex8e7eoqVlpSUxMmTJ12Pf//9d9RqtauugxBCCFEfSm5k9ezZk1deeQWLxcK8efMAXJ9p77zzTpnron/++cc1k27fvn05evQoTz31FCaTiUmTJjFx4sQKj1WTSU5qchPvP//5D1988QXPPPMMGzduZNeuXfTo0aNWN/GCg4PLzTLocDhYvnw533//PVqtFq1Wi6enJ1lZWa6C5+dzY7Imr+v48eOMGTOG7t2788UXX7B9+3Zef/31cutV5d1333X9jlavXl3lujfccAO7du3i8OHDmEwm3nvvvTLJwnOTYE6nk379+pU5D3bt2sXBgwfLTXhTU+fTliXn5Pfff18mjr1797rqStVmMp2srKxyJR2EaA6kp5QQLdR3331HdnY2t9xyS7keNxMnTuS9997j7rvvZs6cOVx++eV06NCBKVOmYLfb+eGHH1zj/tu2bcuGDRuYMmUKBoOB4OBghg4dSmZmJs8//zwTJ05kzZo1/PDDD2WGbcXFxfHBBx/Qv39/8vLy+M9//nPeFz/VmTJlCp999hnTpk3jhRde4PLLLycvL4/XX3+db775hs8++6zSu3LLli3jjTfeICgoqMLn77//fr7++mvGjh3LggULuOCCC0hPT+fZZ59l3759/PTTTzXq8bN79+5yd+R69+7N6NGjmTFjBi+99BIdOnTg8OHDzJo1i9GjR9O1a9cK93XFFVeQmZlZYe0IKB4u9/zzz/Pggw9iNBq56aab0Ol0fP311zz66KM88MADXHDBBRVuq1KpWLp0KSNGjGDw4MHMnj2bLl26YLPZ2LBhA5mZma5EWFxcHElJSaxYsYIBAwbw/fffV1hvwWg0Mm3aNF588UXy8vK49957mTRpkgzdE0II0aDmzJnD6NGjueOOO4iMjCQqKoojR45www03VLqNr68vkydPZvLkyUycOJErrriCrKws1/CrEl27duWDDz7AZDK5rm9KklslQkJCyM/PL9M7qKKbeNOnT+eaa64BimtM1XYIVp8+fXjzzTfLDM9fvXo1+fn57Ny5s8wNrP3793PDDTdw+vRpevTogdPpZP369a4hZ6WV9C6q6OZk6clv8vLyOHr0qOvxtm3bsNvtLFiwwDVpyrn1JqtTk5t7Jfz8/Mr1qq9K3759WblyJaGhoZVeS0VERPDHH39w6aWXAsU3e7dv307fvn0rXP982rJr164YDAaSkpIq7WHVtWvXMsX1ofx5Bmd7eZX0sBKiWVGEEC3SVVddpYwZM6bC57Zv364Ayvbt2xVFUZQvvvhC6d27t6LX65Xg4GBlwoQJrnV///13pWfPnorBYFBKv2W8+eabSnR0tOLl5aVMnTpVeeaZZ5TY2FjX8zt27FD69++vGAwGJT4+Xvnss8+U2NhYZeHCha51AGXVqlWVvoZp06Yp48aNK7f8119/VQAlOzvbtcxmsykvvvii0q1bN8VgMCi+vr7KqFGjlI0bN5bZdunSpYqfn1+lx1y4cGGZ16EoilJYWKg8/vjjSlxcnKLT6ZTAwEDl2muvVXbv3l3pfs6NtaIfRVGU3Nxc5f7771fi4uIUo9GoxMXFKTNnzlRycnLK7KeqtsrOzlYA5ddffy2z/Ouvv1YuueQSxcvLSzEajUq/fv2UJUuWVBuzoijKgQMHlGnTpilt2rRRtFqt4ufnp1x66aXK22+/rdhsNtd6//nPf5SgoCDF29tbmTx5srJw4cIy7TtnzhylV69eyhtvvKFERkYqRqNRmTBhgpKVlVWjOIQQQoiaqOyaoV+/fspdd92lKIqivPPOO4qHh4eyaNEi5cCBA8rff/+tLFmyRFmwYIGiKIry0ksvKZ988omyb98+5cCBA8ott9yihIeHKw6HQ1GUsp/F+fn5SnBwsPKvf/1L2bNnj/L9998rcXFxCqDs3LlTURRFOX36tOLl5aXce++9yqFDh5SPPvpIiYyMLHM9NX78eKV3797Kzp07lV27diljx45VfHx8lPvuu8+1zrnXT+c6deqUotfry1yXjBs3Tpk8eXK5dZ1OpxIVFaUsWrRIURRFmT59uhIdHa2sWrVKOXLkiPLrr78qK1euVBRFUVJSUhSVSqUsW7ZMycjIUPLz8xVFUZRHHnlECQ8PVzZs2KDs3r1bGT9+vOLt7a3MmTNHURRF2blzpwIoixYtUg4fPqwsX75ciYqKKnPtVt31WE0NGTKkTFudq6LzorCwUImPj1eGDh2qbNiwQTly5Iiybt065d5771WSk5MVRVGU//3vf0pAQIDy5ZdfKvv27VNuu+02xcfHp8y+zj32+bTlY489pgQFBSnLli1TEhMTlR07diivvfaasmzZMkVRFOX48eOKXq9X7r//fmX//v3KRx99pISHh5e7Dv71118Vb29vpbCw8PwbUwg3kaSUEEKIBlOSlBJCCCEaUmVJqY8++kjR6/VKUlKS63HJjbiAgADl0ksvVb788ktFURRl8eLFSu/evRUvLy/F19dXufzyy5UdO3a49nXuDaLff/9d6dWrl6LX65XevXsrX3zxRZmklKIoyqpVq1w3nq666ipl8eLFZZJSR48eVYYNG6Z4eHgo0dHRymuvvVYu2VFdUkpRFGXKlCnKI488oiiKoqSlpSlarVb59NNPK1z3nnvuUXr06KEoiqKYTCbl/vvvVyIiIhS9Xq/ExcWVuYH15JNPKuHh4YpKpVKmTZumKErxDbVJkyYpvr6+SnR0tLJs2TKlV69erqSUohQn+CIiIhQPDw9l1KhRyvLly5tMUkpRFCU1NVWZOnWqEhwcrBgMBqV9+/bKbbfdpuTm5iqKUnyz87777lN8fX0Vf39/ZdasWcrUqVOrTEqdT1s6nU7l5ZdfVjp16qTodDolJCREGTVqlLJ+/XrXdt9++60SFxenGAwG5ZJLLlGWLFlSLin173//W/m///u/WrWdEE2FSlFqMVBVCCGEqIW5c+fy1VdflRuuIIQQQoj6s3v3boYPH15hAW/RsmVmZtK5c2e2bdtGu3bt3B2OELUmhc6FEEIIIYQQohnr0aMHzz//fK3rUYnm7+jRo7zxxhuSkBLNlvSUEkIIIYQQQgghhBCNTnpKCSGEEEIIIYQQQohGJ0kpIYQQQgghhBBCCNHoJCklhBBCCCGEEEIIIRqdJKWEEEIIIYQQQgghRKOTpJQQQgghhBBCCCGEaHSSlBJCCCGEEEIIIYQQjU6SUkIIIYQQQgghhBCi0UlSSgghhBBCCCGEEEI0OklKCSGEEEIIIYQQQohG9/8fAg0zUR/TXQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_regression_results(y_test_krr, final_preds_krr, title=\"Final Kernel Ridge (RDKit FP)\", save_dir=\"plots\")\n",
    "plot_regression_results(y_test_unscaled, final_preds_rfr, title=\"Final Random Forest (RDKit FP)\", save_dir=\"plots\")\n",
    "plot_regression_results(y_test_inv_fp, final_preds_inv_fp, title=\"Final MLP (RDKit FP)\", save_dir=\"plots\")\n",
    "# plot_regression_results(y_test_inv_cm, final_preds_inv_cm, title=\"Final MLP (Coulomb Matrix)\", save_dir=\"plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4771aa",
   "metadata": {},
   "source": [
    "\n",
    "| Model Type             | Featurization        |   MAE |  RMSE |   R² | Notes             |\n",
    "|------------------------|----------------------|-------|-------|------|-------------------|\n",
    "| MLP (Tuned)          | RDKit Fingerprints   | 0.426 | 0.574 | 0.798 | Strong performance across all metrics   |\n",
    "| KRR (Tuned)          | RDKit Fingerprints   | 0.454 | 0.593 | 0.784 | Good overall, slightly lower R² than MLP|\n",
    "| RF (Tuned)           | RDKit Fingerprints   | 0.423| 0.583 | 0.791  | Top MAE, but slightly higher R²/RMSE    |\n",
    "| MLP (Tuned)          | Coulomb Matrix       | 0.636 | 0.819 | 0.588 | Significantly worse than FP models      |\n",
    "| MLP (Untuned Baseline) | RDKit Fingerprints | 0.467 | 0.609 | 0.772 | Reasonable baseline performance         |\n",
    "| KRR (Untuned Baseline) | RDKit Fingerprints | 0.519 | 0.668 | 0.726 | Noticeable drop from tuned KRR          |\n",
    "| RF (Untuned Baseline) | RDKit Fingerprints  | 0.426| 0.587 | 0.788  | Surprisingly strong untuned performance |\n",
    "| MLP (Untuned Baseline) | Coulomb Matrix     | 0.663 | 0.847 | 0.559 | Confirms Coulomb Matrix as weak         |\n",
    "\n",
    "Save best model and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bbd2c2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_Rg/best_mlp_fp_model_keras\\assets\n"
     ]
    }
   ],
   "source": [
    "# create a save directory\n",
    "os.makedirs(\"saved_models_Rg\", exist_ok=True)\n",
    "\n",
    "# save the final trained MLP model (Keras backend)\n",
    "final_mlp_fp.model.save(\"saved_models_Rg/best_mlp_fp_model_keras\")\n",
    "\n",
    "# save the X and Y scalers\n",
    "joblib.dump(xscaler_fp, \"saved_models_Rg/xscaler_fp.pkl\")\n",
    "joblib.dump(yscaler, \"saved_models_Rg/yscaler.pkl\")\n",
    "\n",
    "# save evaluation metrics\n",
    "final_metrics_fp.to_csv(\"saved_models_Rg/best_mlp_fp_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c226234",
   "metadata": {},
   "source": [
    "If you wanted to reload these later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "136af5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # load model and scalersand metrics\n",
    "# mlp_model = load_model(\"saved_models/best_mlp_fp_model_keras\")\n",
    "# xscaler_fp = joblib.load(\"saved_models/xscaler_fp.pkl\")\n",
    "# yscaler = joblib.load(\"saved_models/yscaler.pkl\")\n",
    "# metrics_df = pd.read_csv(\"saved_models/best_mlp_fp_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39535b4",
   "metadata": {},
   "source": [
    "# Training a Baseline GNN with ChemML\n",
    "ChemML's `tensorise_molecules` generates its own graph. Its important to note this graph is not the official graph from PCQM4Mv2. It may miss out on features OGB uses like formal charge, aromatacity flags, atomic chirality, and explicit hydrogens. However, tensorise_molecules is a good choice for quick prototyping and it handles graph generation and tensor formatting in a numpy-friendly way which was easier for me to understand. Final training will use smiles2graph for compatability with OGB splits and better feature representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ccf8404c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorising molecules in batches of 3000 ...\n",
      "613/613 [==================================================] - 3s 6ms/step\n",
      "Merging batch tensors ...    [DONE]\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 1212.5925 - val_loss: 718.6636\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 166.0625 - val_loss: 171.6432\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 106.5689 - val_loss: 7.2758\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 23.5615 - val_loss: 44.3803\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 12.9392 - val_loss: 11.7245\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8.5996 - val_loss: 1.5196\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3.2210 - val_loss: 4.3959\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.8914 - val_loss: 2.7033\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.9880 - val_loss: 1.2545\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2351 - val_loss: 1.4835\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1248 - val_loss: 1.3878\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1021 - val_loss: 1.2279\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0935 - val_loss: 1.2375\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0644 - val_loss: 1.2830\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0540 - val_loss: 1.2237\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0059 - val_loss: 1.2219\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0206 - val_loss: 1.1968\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9902 - val_loss: 1.1905\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9799 - val_loss: 1.1885\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9634 - val_loss: 1.1740\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9583 - val_loss: 1.1720\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9743 - val_loss: 1.1700\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9578 - val_loss: 1.1629\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9342 - val_loss: 1.1690\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9219 - val_loss: 1.1583\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9247 - val_loss: 1.1692\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9124 - val_loss: 1.1546\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9032 - val_loss: 1.1613\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8981 - val_loss: 1.1677\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8850 - val_loss: 1.1649\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8796 - val_loss: 1.1690\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8738 - val_loss: 1.1663\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8640 - val_loss: 1.1659\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8605 - val_loss: 1.1732\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8596 - val_loss: 1.1753\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8472 - val_loss: 1.1707\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8464 - val_loss: 1.1699\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8453 - val_loss: 1.1905\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8400 - val_loss: 1.1670\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8247 - val_loss: 1.1720\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8174 - val_loss: 1.1645\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8155 - val_loss: 1.1555\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8056 - val_loss: 1.1570\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8009 - val_loss: 1.1568\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8033 - val_loss: 1.1598\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7924 - val_loss: 1.1564\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7937 - val_loss: 1.1488\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7759 - val_loss: 1.1639\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7805 - val_loss: 1.1622\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.7699 - val_loss: 1.1534\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7655 - val_loss: 1.1613\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7604 - val_loss: 1.1513\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7549 - val_loss: 1.1516\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7630 - val_loss: 1.1484\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7711 - val_loss: 1.1780\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7482 - val_loss: 1.1528\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7423 - val_loss: 1.1506\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7533 - val_loss: 1.1969\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.7401 - val_loss: 1.1579\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7431 - val_loss: 1.2146\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.7280 - val_loss: 1.1528\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7263 - val_loss: 1.1654\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7228 - val_loss: 1.1525\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7361 - val_loss: 1.2320\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7345 - val_loss: 1.1503\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.7045 - val_loss: 1.2128\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.7126 - val_loss: 1.1367\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.7060 - val_loss: 1.1450\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6936 - val_loss: 1.1362\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6909 - val_loss: 1.1414\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6965 - val_loss: 1.1376\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6848 - val_loss: 1.1248\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6869 - val_loss: 1.1233\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6924 - val_loss: 1.1227\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6843 - val_loss: 1.1367\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6769 - val_loss: 1.1160\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.6785 - val_loss: 1.1131\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6664 - val_loss: 1.1220\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6672 - val_loss: 1.1265\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6838 - val_loss: 1.1200\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6718 - val_loss: 1.1165\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6516 - val_loss: 1.1575\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6705 - val_loss: 1.1088\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6576 - val_loss: 1.1155\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6657 - val_loss: 1.1718\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.6631 - val_loss: 1.1122\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6491 - val_loss: 1.1092\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6584 - val_loss: 1.1748\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6495 - val_loss: 1.1257\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6562 - val_loss: 1.1181\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6539 - val_loss: 1.1712\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6585 - val_loss: 1.1082\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6492 - val_loss: 1.1320\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.6626 - val_loss: 1.1933\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6597 - val_loss: 1.1196\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6327 - val_loss: 1.1239\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6266 - val_loss: 1.1103\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6249 - val_loss: 1.1130\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6272 - val_loss: 1.1098\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6714 - val_loss: 1.1232\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "\n",
      "GNN Model Results:\n",
      "        MAE      RMSE  r_squared\n",
      "0  3.006363  3.645833   0.322397\n"
     ]
    }
   ],
   "source": [
    "# tensorize molecules\n",
    "X_atoms, X_bonds, X_edges = tensorise_molecules(valid_mol_objs)\n",
    "y = df_clean['Rg'].values.reshape(-1, 1)\n",
    "\n",
    "# train test split (80/20)\n",
    "split = int(0.8 * len(y))\n",
    "X_atoms_train, X_atoms_test = X_atoms[:split], X_atoms[split:]\n",
    "X_bonds_train, X_bonds_test = X_bonds[:split], X_bonds[split:]\n",
    "X_edges_train, X_edges_test = X_edges[:split], X_edges[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# scale target\n",
    "yscaler = StandardScaler()\n",
    "y_train_scaled = yscaler.fit_transform(y_train)\n",
    "\n",
    "# model input shapes\n",
    "max_atoms = X_atoms.shape[1]\n",
    "max_degree = X_bonds.shape[2]\n",
    "num_atom_features = X_atoms.shape[-1]\n",
    "num_bond_features = X_bonds.shape[-1]\n",
    "\n",
    "# input layers\n",
    "atoms_input = Input(shape=(max_atoms, num_atom_features), name=\"atom_inputs\")\n",
    "bonds_input = Input(shape=(max_atoms, max_degree, num_bond_features), name=\"bond_inputs\")\n",
    "edges_input = Input(shape=(max_atoms, max_degree), name=\"edge_inputs\", dtype=\"int32\")\n",
    "\n",
    "# GNN layers\n",
    "conv1 = NeuralGraphHidden(8, activation='relu')([atoms_input, bonds_input, edges_input])\n",
    "conv2 = NeuralGraphHidden(8, activation='relu')([conv1, bonds_input, edges_input])\n",
    "\n",
    "fp1 = NeuralGraphOutput(128, activation='relu')([atoms_input, bonds_input, edges_input])\n",
    "fp2 = NeuralGraphOutput(128, activation='relu')([conv1, bonds_input, edges_input])\n",
    "fp3 = NeuralGraphOutput(128, activation='relu')([conv2, bonds_input, edges_input])\n",
    "\n",
    "# fingerprint aggregation\n",
    "fingerprint = Add()([fp1, fp2, fp3])\n",
    "\n",
    "# dense layers\n",
    "dense1 = Dense(128, activation='relu')(fingerprint)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "output = Dense(1, activation='linear')(dense2)\n",
    "\n",
    "# model compilation\n",
    "model = Model(inputs=[atoms_input, bonds_input, edges_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# model training\n",
    "model.fit([X_atoms_train, X_bonds_train, X_edges_train], y_train_scaled, epochs=100, batch_size=64, verbose=1, validation_split=0.1)\n",
    "\n",
    "# preds and eval\n",
    "y_pred = model.predict([X_atoms_test, X_bonds_test, X_edges_test])\n",
    "y_pred = yscaler.inverse_transform(y_pred)\n",
    "metrics = regression_metrics(y_test, y_pred)\n",
    "print(\"\\nGNN Model Results:\")\n",
    "print(metrics[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a97ae357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "plot_regression_results(y_test, y_pred, title=\"GNN Model Untuned(ChemML)\", save_dir=\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d2fb6dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:09:27,411] A new study created in memory with name: no-name-d11d1ef8-cd4a-4151-a199-48bb3059a524\n",
      "[I 2025-09-04 21:09:35,034] Trial 0 finished with value: 0.6815736889839172 and parameters: {'conv_width': 16, 'fp_length': 160, 'n1': 192, 'n2': 64, 'lr': 0.00443736565770757, 'alpha': 5.617342228935627e-07}. Best is trial 0 with value: 0.6815736889839172.\n",
      "[I 2025-09-04 21:09:41,949] Trial 1 finished with value: 0.7887053489685059 and parameters: {'conv_width': 32, 'fp_length': 96, 'n1': 128, 'n2': 96, 'lr': 2.1221192660420185e-05, 'alpha': 2.6652848376141498e-06}. Best is trial 0 with value: 0.6815736889839172.\n",
      "[I 2025-09-04 21:09:48,755] Trial 2 finished with value: 0.6089875102043152 and parameters: {'conv_width': 16, 'fp_length': 128, 'n1': 128, 'n2': 64, 'lr': 0.0009744279410292105, 'alpha': 2.6646951824271946e-05}. Best is trial 2 with value: 0.6089875102043152.\n",
      "[I 2025-09-04 21:09:55,956] Trial 3 finished with value: 1.0359008312225342 and parameters: {'conv_width': 16, 'fp_length': 128, 'n1': 160, 'n2': 64, 'lr': 1.7744499481720197e-05, 'alpha': 2.264061348429816e-06}. Best is trial 2 with value: 0.6089875102043152.\n",
      "[I 2025-09-04 21:10:08,067] Trial 4 finished with value: 0.6970009207725525 and parameters: {'conv_width': 32, 'fp_length': 96, 'n1': 192, 'n2': 64, 'lr': 0.004638296838906989, 'alpha': 2.2185550293331918e-07}. Best is trial 2 with value: 0.6089875102043152.\n",
      "[I 2025-09-04 21:10:30,476] Trial 5 finished with value: 0.7171813249588013 and parameters: {'conv_width': 16, 'fp_length': 96, 'n1': 160, 'n2': 64, 'lr': 0.000125498307348537, 'alpha': 1.8120347965359696e-07}. Best is trial 2 with value: 0.6089875102043152.\n",
      "[I 2025-09-04 21:10:38,983] Trial 6 finished with value: 0.7946706414222717 and parameters: {'conv_width': 8, 'fp_length': 128, 'n1': 160, 'n2': 64, 'lr': 0.0035259916658171766, 'alpha': 3.2398058670831817e-06}. Best is trial 2 with value: 0.6089875102043152.\n",
      "[I 2025-09-04 21:10:46,154] Trial 7 finished with value: 0.6617838144302368 and parameters: {'conv_width': 16, 'fp_length': 128, 'n1': 128, 'n2': 96, 'lr': 0.0025465393725712525, 'alpha': 3.0453647859674597e-05}. Best is trial 2 with value: 0.6089875102043152.\n",
      "[I 2025-09-04 21:10:53,058] Trial 8 finished with value: 0.6046004295349121 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.0031085266258499985, 'alpha': 9.923167487970681e-08}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:11:00,722] Trial 9 finished with value: 0.7708513736724854 and parameters: {'conv_width': 16, 'fp_length': 96, 'n1': 160, 'n2': 64, 'lr': 0.00030674914326573135, 'alpha': 1.1915531847444471e-08}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:11:07,829] Trial 10 finished with value: 0.6451406478881836 and parameters: {'conv_width': 32, 'fp_length': 160, 'n1': 192, 'n2': 96, 'lr': 0.0008214206404035956, 'alpha': 1.0513869755997286e-08}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:11:14,875] Trial 11 finished with value: 0.7348687052726746 and parameters: {'conv_width': 8, 'fp_length': 128, 'n1': 128, 'n2': 64, 'lr': 0.0010276678007221792, 'alpha': 9.103515147700775e-05}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:11:21,788] Trial 12 finished with value: 0.6531068682670593 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 128, 'n2': 64, 'lr': 0.00908860086233947, 'alpha': 4.847289658515308e-08}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:11:28,635] Trial 13 finished with value: 0.6078091263771057 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.0009153476341640747, 'alpha': 1.2716105776118297e-05}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:11:35,498] Trial 14 finished with value: 0.6516814827919006 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 96, 'lr': 0.00015391065467347355, 'alpha': 1.3394919278488293e-05}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:11:42,794] Trial 15 finished with value: 0.6060627698898315 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.001381135496541911, 'alpha': 8.651065266880382e-06}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:11:50,069] Trial 16 finished with value: 0.6676430106163025 and parameters: {'conv_width': 32, 'fp_length': 160, 'n1': 192, 'n2': 64, 'lr': 0.0019110042527533094, 'alpha': 3.891943895195516e-08}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:11:57,608] Trial 17 finished with value: 0.7299230098724365 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 96, 'lr': 0.009526370402277598, 'alpha': 7.981985046317903e-07}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:12:05,002] Trial 18 finished with value: 0.767455518245697 and parameters: {'conv_width': 8, 'fp_length': 128, 'n1': 160, 'n2': 64, 'lr': 0.0002750594332114791, 'alpha': 1.6848401191010085e-07}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:12:12,181] Trial 19 finished with value: 0.6741869449615479 and parameters: {'conv_width': 32, 'fp_length': 160, 'n1': 192, 'n2': 64, 'lr': 0.0017780412866001993, 'alpha': 5.116428010524844e-08}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:12:19,114] Trial 20 finished with value: 0.6420897841453552 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 96, 'lr': 0.00041684038693533975, 'alpha': 5.449944275746259e-06}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:12:26,575] Trial 21 finished with value: 0.6433561444282532 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.0009708575281721525, 'alpha': 9.70927674368912e-06}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:12:33,820] Trial 22 finished with value: 0.641981840133667 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.0005119420397636718, 'alpha': 6.64477866723514e-05}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:12:41,140] Trial 23 finished with value: 0.6587417125701904 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 160, 'n2': 64, 'lr': 7.100024202665339e-05, 'alpha': 1.4129593944176729e-05}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:12:48,193] Trial 24 finished with value: 0.6301843523979187 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.0015743714156032811, 'alpha': 1.2225344468206156e-06}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:12:55,239] Trial 25 finished with value: 0.7588546276092529 and parameters: {'conv_width': 8, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.005449217875774881, 'alpha': 7.178227752076053e-06}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:13:02,963] Trial 26 finished with value: 0.6500750780105591 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 160, 'n2': 64, 'lr': 0.0006061260488728275, 'alpha': 3.649800710850728e-05}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:13:10,330] Trial 27 finished with value: 0.6712186932563782 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.0024745983164833284, 'alpha': 4.796981050313246e-07}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:13:17,570] Trial 28 finished with value: 0.6840843558311462 and parameters: {'conv_width': 32, 'fp_length': 96, 'n1': 192, 'n2': 64, 'lr': 0.0012940790466933205, 'alpha': 4.378989036382223e-06}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:13:24,997] Trial 29 finished with value: 0.7840716242790222 and parameters: {'conv_width': 8, 'fp_length': 160, 'n1': 192, 'n2': 64, 'lr': 0.003443653335261637, 'alpha': 1.2793997921767881e-06}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:13:32,418] Trial 30 finished with value: 0.6880255937576294 and parameters: {'conv_width': 32, 'fp_length': 160, 'n1': 192, 'n2': 64, 'lr': 0.006306743152921041, 'alpha': 1.69085722104713e-05}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:13:39,155] Trial 31 finished with value: 0.6434189081192017 and parameters: {'conv_width': 16, 'fp_length': 128, 'n1': 128, 'n2': 64, 'lr': 0.0007257006562493554, 'alpha': 2.3857949047421422e-05}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:13:45,912] Trial 32 finished with value: 0.6091161966323853 and parameters: {'conv_width': 16, 'fp_length': 128, 'n1': 160, 'n2': 64, 'lr': 0.0012399351370251392, 'alpha': 8.375217213186262e-06}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:13:53,072] Trial 33 finished with value: 0.7543115615844727 and parameters: {'conv_width': 16, 'fp_length': 128, 'n1': 128, 'n2': 64, 'lr': 0.00020954699271823283, 'alpha': 3.464420584169567e-05}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:13:59,843] Trial 34 finished with value: 0.8997491598129272 and parameters: {'conv_width': 16, 'fp_length': 128, 'n1': 160, 'n2': 64, 'lr': 1.1184876691041753e-05, 'alpha': 5.4408642330909305e-05}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:14:07,374] Trial 35 finished with value: 0.7178683876991272 and parameters: {'conv_width': 16, 'fp_length': 96, 'n1': 160, 'n2': 64, 'lr': 0.0024606637859151126, 'alpha': 2.214135522133454e-06}. Best is trial 8 with value: 0.6046004295349121.\n",
      "[I 2025-09-04 21:14:14,662] Trial 36 finished with value: 0.6021361947059631 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.0004877178140583043, 'alpha': 9.228706453418102e-08}. Best is trial 36 with value: 0.6021361947059631.\n",
      "[I 2025-09-04 21:14:21,753] Trial 37 finished with value: 0.9068041443824768 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 4.397487088920819e-05, 'alpha': 3.405640221518881e-07}. Best is trial 36 with value: 0.6021361947059631.\n",
      "[I 2025-09-04 21:14:28,889] Trial 38 finished with value: 0.7450543642044067 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.00043684733844740923, 'alpha': 1.1348903124719907e-07}. Best is trial 36 with value: 0.6021361947059631.\n",
      "[I 2025-09-04 21:14:35,995] Trial 39 finished with value: 0.6338549256324768 and parameters: {'conv_width': 32, 'fp_length': 96, 'n1': 192, 'n2': 96, 'lr': 0.003598716547227118, 'alpha': 2.2935905443854973e-08}. Best is trial 36 with value: 0.6021361947059631.\n",
      "[I 2025-09-04 21:14:43,325] Trial 40 finished with value: 0.77923583984375 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 8.625088416437168e-05, 'alpha': 1.0605165405659692e-07}. Best is trial 36 with value: 0.6021361947059631.\n",
      "[I 2025-09-04 21:14:50,265] Trial 41 finished with value: 0.7134038805961609 and parameters: {'conv_width': 16, 'fp_length': 128, 'n1': 128, 'n2': 64, 'lr': 0.000728115674533103, 'alpha': 2.5795218652005364e-06}. Best is trial 36 with value: 0.6021361947059631.\n",
      "[I 2025-09-04 21:14:57,699] Trial 42 finished with value: 0.6353152394294739 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.0003772993561283973, 'alpha': 8.762373981220405e-08}. Best is trial 36 with value: 0.6021361947059631.\n",
      "[I 2025-09-04 21:15:05,237] Trial 43 finished with value: 0.6349210739135742 and parameters: {'conv_width': 16, 'fp_length': 128, 'n1': 160, 'n2': 64, 'lr': 0.0011782873369176321, 'alpha': 2.605433593491954e-07}. Best is trial 36 with value: 0.6021361947059631.\n",
      "[I 2025-09-04 21:15:12,257] Trial 44 finished with value: 0.6189049482345581 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.00023649896741525088, 'alpha': 1.9728891424727015e-05}. Best is trial 36 with value: 0.6021361947059631.\n",
      "[I 2025-09-04 21:15:19,520] Trial 45 finished with value: 0.7724565863609314 and parameters: {'conv_width': 8, 'fp_length': 96, 'n1': 160, 'n2': 64, 'lr': 0.0005751392248391462, 'alpha': 1.7449973015353407e-08}. Best is trial 36 with value: 0.6021361947059631.\n",
      "[I 2025-09-04 21:15:26,787] Trial 46 finished with value: 0.6142918467521667 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.001918349654987398, 'alpha': 9.286552519634022e-05}. Best is trial 36 with value: 0.6021361947059631.\n",
      "[I 2025-09-04 21:15:33,532] Trial 47 finished with value: 0.6713241934776306 and parameters: {'conv_width': 16, 'fp_length': 160, 'n1': 128, 'n2': 64, 'lr': 0.0009440208924045405, 'alpha': 5.855296329262599e-07}. Best is trial 36 with value: 0.6021361947059631.\n",
      "[I 2025-09-04 21:15:40,495] Trial 48 finished with value: 0.6515449285507202 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 96, 'lr': 0.0037531351334745547, 'alpha': 7.397465691615467e-08}. Best is trial 36 with value: 0.6021361947059631.\n",
      "[I 2025-09-04 21:15:47,324] Trial 49 finished with value: 0.6663808226585388 and parameters: {'conv_width': 32, 'fp_length': 128, 'n1': 192, 'n2': 64, 'lr': 0.00017449306263794213, 'alpha': 3.510263625625284e-08}. Best is trial 36 with value: 0.6021361947059631.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "\n",
    "def objective_gnn(trial):\n",
    "    conv_width = trial.suggest_categorical('conv_width', [8, 16, 32])\n",
    "    fp_length = trial.suggest_categorical('fp_length', [96, 128, 160])\n",
    "    n1 = trial.suggest_int('n1', 128, 192, step=32)\n",
    "    n2 = trial.suggest_int('n2', 64, 96, step=32)\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    alpha = trial.suggest_float('alpha', 1e-8, 1e-4, log=True)\n",
    "    activation = 'relu'\n",
    "\n",
    "    # model definition\n",
    "    atoms_input = Input(shape=(max_atoms, num_atom_features), name=\"atom_inputs\")\n",
    "    bonds_input = Input(shape=(max_atoms, max_degree, num_bond_features), name=\"bond_inputs\")\n",
    "    edges_input = Input(shape=(max_atoms, max_degree), name=\"edge_inputs\", dtype=\"int32\")\n",
    "\n",
    "    conv1 = NeuralGraphHidden(conv_width, activation=activation)([atoms_input, bonds_input, edges_input])\n",
    "    conv2 = NeuralGraphHidden(conv_width, activation=activation)([conv1, bonds_input, edges_input])\n",
    "\n",
    "    fp1 = NeuralGraphOutput(fp_length, activation=activation)([atoms_input, bonds_input, edges_input])\n",
    "    fp2 = NeuralGraphOutput(fp_length, activation=activation)([conv1, bonds_input, edges_input])\n",
    "    fp3 = NeuralGraphOutput(fp_length, activation=activation)([conv2, bonds_input, edges_input])\n",
    "    fingerprint = Add()([fp1, fp2, fp3])\n",
    "\n",
    "    dense1 = Dense(n1, activation=activation, kernel_regularizer=regularizers.l2(alpha))(fingerprint)\n",
    "    dense2 = Dense(n2, activation=activation, kernel_regularizer=regularizers.l2(alpha))(dense1)\n",
    "    output = Dense(1, activation='linear')(dense2)\n",
    "\n",
    "    model = Model(inputs=[atoms_input, bonds_input, edges_input], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mean_squared_error', metrics=[MeanAbsoluteError()])\n",
    "\n",
    "    history = model.fit([X_atoms_train, X_bonds_train, X_edges_train], y_train_scaled, epochs=100, batch_size=64, verbose=0, validation_split=0.2)\n",
    "\n",
    "    # return best validation MAE\n",
    "    val_mae = min(history.history[\"val_mean_absolute_error\"])  \n",
    "    return val_mae\n",
    "\n",
    "study_gnn = optuna.create_study(direction='minimize')  \n",
    "study_gnn.optimize(objective_gnn, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf7a39",
   "metadata": {},
   "source": [
    "## Retraining ChemML GNN with Best Parameter Found in Optuna Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1b42b9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 1s 24ms/step - loss: 230.2461\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 48.4497\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 18.8996\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.3738\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.2484\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.8585\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.6595\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.2595\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.1501\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.9684\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.9382\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.9229\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8936\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8654\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8837\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.9378\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.9041\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.9022\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8270\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.7917\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7685\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7686\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8115\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7739\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7538\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7646\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7316\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.7167\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6961\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7304\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7202\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6810\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7093\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7707\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6845\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.6606\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6449\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6494\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6267\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6576\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7268\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7620\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6395\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6858\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.7666\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6805\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5872\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5924\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5705\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6341\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6357\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5857\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5660\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5753\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5658\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5583\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5445\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5546\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5940\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5456\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5403\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5742\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5786\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5731\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5420\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5190\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6098\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5838\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5694\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5170\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5242\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4912\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6016\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5625\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5434\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4994\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5359\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5448\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6287\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5640\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5605\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5067\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5078\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4776\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4702\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4717\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5165\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5582\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5444\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4759\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4666\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4658\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5205\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5711\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5101\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4528\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4521\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5006\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5260\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5071\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4217\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4257\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4689\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4358\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4326\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4306\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4304\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4239\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4294\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4488\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4222\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4078\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4215\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4213\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4571\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4247\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4024\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4788\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4485\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4324\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5988\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5709\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5542\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6198\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7182\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5876\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5214\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5170\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4950\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5383\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6193\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6239\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5818\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4376\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3938\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4723\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4098\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4049\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3871\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4022\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3911\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3749\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3796\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4013\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4664\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3883\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4501\n",
      "Epoch 148/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5057\n",
      "Epoch 149/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5299\n",
      "Epoch 150/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5863\n",
      "Epoch 151/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5114\n",
      "Epoch 152/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5217\n",
      "Epoch 153/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4287\n",
      "Epoch 154/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4652\n",
      "Epoch 155/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4605\n",
      "Epoch 156/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4134\n",
      "Epoch 157/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4124\n",
      "Epoch 158/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3912\n",
      "Epoch 159/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3893\n",
      "Epoch 160/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3633\n",
      "Epoch 161/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3675\n",
      "Epoch 162/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3565\n",
      "Epoch 163/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3663\n",
      "Epoch 164/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4051\n",
      "Epoch 165/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4957\n",
      "Epoch 166/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4004\n",
      "Epoch 167/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3777\n",
      "Epoch 168/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3565\n",
      "Epoch 169/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4314\n",
      "Epoch 170/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3534\n",
      "Epoch 171/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3374\n",
      "Epoch 172/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3611\n",
      "Epoch 173/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3647\n",
      "Epoch 174/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3569\n",
      "Epoch 175/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3808\n",
      "Epoch 176/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3533\n",
      "Epoch 177/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3280\n",
      "Epoch 178/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4238\n",
      "Epoch 179/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4240\n",
      "Epoch 180/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4203\n",
      "Epoch 181/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4979\n",
      "Epoch 182/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4898\n",
      "Epoch 183/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6949\n",
      "Epoch 184/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7804\n",
      "Epoch 185/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5221\n",
      "Epoch 186/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4035\n",
      "Epoch 187/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3806\n",
      "Epoch 188/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3599\n",
      "Epoch 189/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3790\n",
      "Epoch 190/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5176\n",
      "Epoch 191/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7397\n",
      "Epoch 192/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.0116\n",
      "Epoch 193/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7980\n",
      "Epoch 194/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6646\n",
      "Epoch 195/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7908\n",
      "Epoch 196/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8398\n",
      "Epoch 197/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5652\n",
      "Epoch 198/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4068\n",
      "Epoch 199/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3862\n",
      "Epoch 200/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3367\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "\n",
      "Final Tuned GNN Results:\n",
      "        MAE      RMSE  r_squared\n",
      "0  2.404061  3.190796   0.480985\n"
     ]
    }
   ],
   "source": [
    "params = study_gnn.best_params\n",
    "\n",
    "# redefine and compile using best params\n",
    "atoms_input = Input(shape=(max_atoms, num_atom_features), name=\"atom_inputs\")\n",
    "bonds_input = Input(shape=(max_atoms, max_degree, num_bond_features), name=\"bond_inputs\")\n",
    "edges_input = Input(shape=(max_atoms, max_degree), name=\"edge_inputs\", dtype=\"int32\")\n",
    "\n",
    "conv1 = NeuralGraphHidden(params['conv_width'], activation='relu')([atoms_input, bonds_input, edges_input])\n",
    "conv2 = NeuralGraphHidden(params['conv_width'], activation='relu')([conv1, bonds_input, edges_input])\n",
    "\n",
    "fp1 = NeuralGraphOutput(params['fp_length'], activation='relu')([atoms_input, bonds_input, edges_input])\n",
    "fp2 = NeuralGraphOutput(params['fp_length'],activation='relu')([conv1, bonds_input, edges_input])\n",
    "fp3 = NeuralGraphOutput(params['fp_length'], activation='relu')([conv2, bonds_input, edges_input])\n",
    "fingerprint = Add()([fp1, fp2, fp3])\n",
    "\n",
    "dense1 = Dense(params['n1'], activation='relu', kernel_regularizer=regularizers.l2(params['alpha']))(fingerprint)\n",
    "dense2 = Dense(params['n2'], activation='relu', kernel_regularizer=regularizers.l2(params['alpha']))(dense1)\n",
    "output = Dense(1, activation='linear')(dense2)\n",
    "\n",
    "final_gnn = Model(inputs=[atoms_input, bonds_input, edges_input], outputs=output)\n",
    "final_gnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=params['lr']), loss='mean_squared_error')\n",
    "\n",
    "final_gnn.fit([X_atoms_train, X_bonds_train, X_edges_train], y_train_scaled, epochs=200, batch_size=64, verbose=1)\n",
    "\n",
    "# final eval\n",
    "y_pred_final = final_gnn.predict([X_atoms_test, X_bonds_test, X_edges_test])\n",
    "y_pred_final = yscaler.inverse_transform(y_pred_final)\n",
    "final_metrics = regression_metrics(y_test, y_pred_final)\n",
    "print(\"\\nFinal Tuned GNN Results:\")\n",
    "print(final_metrics[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c3bf3a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_regression_results(y_test, y_pred_final, title=\"Tuned ChemML GNN\", save_dir=\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "64159a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_Rg/gnn_tensorise_molecules_model\\gnn_tensorise_molecules_model_tf\\assets\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# make a directory for this specific model\n",
    "save_dir = \"saved_models_Rg/gnn_tensorise_molecules_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 1. save the trained GNN model\n",
    "final_gnn.save(os.path.join(save_dir, \"gnn_tensorise_molecules_model_tf\"), save_format=\"tf\")\n",
    "\n",
    "# 2. save the y target scaler\n",
    "with open(os.path.join(save_dir, \"gnn_tensorise_molecules_target_scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(yscaler, f)\n",
    "\n",
    "# 3. save the final metrics\n",
    "final_metrics.to_csv(os.path.join(save_dir, \"gnn_tensorise_molecules_metrics.csv\"), index=False)\n",
    "\n",
    "# 4. save predictions\n",
    "pred_df = pd.DataFrame({\"true_gap\": y_test.flatten(), \"predicted_gap\": y_pred_final.flatten()})\n",
    "pred_df.to_csv(os.path.join(save_dir, \"gnn_tensorise_molecules_predictions.csv\"), index=False)\n",
    "\n",
    "# 5. save the best hyperparameters\n",
    "with open(os.path.join(save_dir, \"gnn_tensorise_molecules_best_params.json\"), \"w\") as f:\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77f7ec",
   "metadata": {},
   "source": [
    "## ChemML GNN Model Results\n",
    "| Model Type             | Featurization        |   MAE |  RMSE |   R² | Notes             |\n",
    "|------------------------|----------------------|-------|-------|------|-------------------|\n",
    "| GNN (Tuned)            | tensorise_molecules Graph   | 0.302 | 0.411 | 0.900 | Best performance across all metrics   |\n",
    "| GNN (Untuned)          | tensorise_molecules Graph   | 0.400 | 0.519 | 0.841 | Good overall|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42db218",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Model Training\n",
    "\n",
    "Having explored different molecular graph representations and model architectures, I am now moving to training what is expected to be the best-performing model using the full dataset. The earlier GNN model was based on `tensorise_molecules` (ChemML) graphs and had strong performance with a **mean absolute error (MAE) around 0.30**. These graphs are based on RDKit's internal descriptors and do not reflect the original PCQM4Mv2 graph structure used in the Open Graph Benchmark (OGB). Therefore, I will shift focus to the `smiles2graph` representation provided by OGB, which aligns more directly with the benchmark's evaluation setup and top-performing models on the leaderboard.\n",
    "\n",
    "\n",
    "| Source                         | Atom/Bond Features                                                 | Format                                          | Customizable?     | Alignment with PCQM4Mv2?  |\n",
    "| ------------------------------ | ------------------------------------------------------------------ | ----------------------------------------------- | ----------------- | ---------------------- |\n",
    "| `tensorise_molecules` (ChemML) | RDKit-based descriptors (ex: atom number, degree, hybridization) | NumPy tensors (`X_atoms`, `X_bonds`, `X_edges`) | Limited           |  Not aligned          |\n",
    "| `smiles2graph` (OGB / PyG)     | Predefined categorical features from PCQM4Mv2                      | PyTorch Geometric `Data` objects                |  Highly flexible |  Matches OGB standard |\n",
    "\n",
    "By using `smiles2graph`, we:\n",
    "\n",
    "* Use OGB-standard graph construction and feature encoding for fair comparisons with leaderboard models\n",
    "* Include learnable AtomEncoder and BondEncoder embeddings from `ogb.graphproppred.mol_encoder`, which improve model expressiveness\n",
    "* Maintain compatibility with PyTorch Geometric, DGL, and OGB tools\n",
    "\n",
    "I will also concatenate GNN-derived embeddings with SMILES-based RDKit descriptors, feeding this hybrid representation into MLP head. This allows you to combine structural and cheminformatics perspectives for improved prediction accuracy. With this setup, I aim to improve upon the MAE of \\~0.30 achieved earlier and push closer toward state-of-the-art performance.\n",
    "\n",
    "\n",
    "## Step 1: Load PyG-Compatible Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cf8037c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1+cu118\n",
      "CUDA available?  True\n",
      "Device count: 1\n",
      "GPU Name: NVIDIA GeForce RTX 3070 Ti\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "def check_cuda():\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA available? \", torch.cuda.is_available())\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Device count:\", torch.cuda.device_count())\n",
    "        print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "        print(\"Current device:\", torch.cuda.current_device())\n",
    "    else:\n",
    "        print(\"Running on CPU\")\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "52c907b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load OGB dataset \n",
    "df_Rg = pd.read_csv('cleaned_Rg_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a988df2",
   "metadata": {},
   "source": [
    "#  Step 2: Extract SMILES from Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a684ddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 613 molecules.\n"
     ]
    }
   ],
   "source": [
    "# 2. Extract SMILES and FFV targets\n",
    "# Your `df_ffv` already contains the SMILES and FFV columns.\n",
    "smiles_list = df_Rg['SMILES'].tolist()\n",
    "ffv_list = df_Rg['Rg'].tolist()\n",
    "\n",
    "num_mols = len(smiles_list)\n",
    "print(f\"Loaded {num_mols} molecules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7aa26be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of RDKit features: (613, 9)\n"
     ]
    }
   ],
   "source": [
    "def compute_rdkit_features(smiles):\n",
    "    cleaned_smiles = canonicalize_polymer_smiles(smiles)\n",
    "    mol = Chem.MolFromSmiles(cleaned_smiles)\n",
    "    if mol is None:\n",
    "        return [np.nan] * 9  # Update the number of NaNs to match new features\n",
    "\n",
    "    # Check for empty molecule\n",
    "    if mol.GetNumAtoms() == 0:\n",
    "        return [np.nan] * 9\n",
    "\n",
    "    # Add features that capture size, shape, and interactions\n",
    "    return [\n",
    "        Descriptors.MolWt(mol),\n",
    "        Descriptors.NumRotatableBonds(mol),\n",
    "        Descriptors.TPSA(mol),\n",
    "        Descriptors.NumHAcceptors(mol),\n",
    "        Descriptors.NumHDonors(mol),\n",
    "        Descriptors.RingCount(mol),\n",
    "        Descriptors.FractionCSP3(mol),  # New: Fraction of sp3 hybridized carbons\n",
    "        Descriptors.MolLogP(mol),      # New: Octanol-water partition coefficient\n",
    "        Descriptors.NumSaturatedRings(mol) # New: Number of saturated rings\n",
    "    ]\n",
    "\n",
    "rdkit_features = np.array([compute_rdkit_features(smi) for smi in smiles_list])\n",
    "print(f\"Shape of RDKit features: {rdkit_features.shape}\") # Should be (N, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7f6d257f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 613 molecules with valid RDKit features.\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with NaN values (failed RDKit featurization)\n",
    "valid_indices = ~np.isnan(rdkit_features).any(axis=1)\n",
    "rdkit_features = rdkit_features[valid_indices]\n",
    "smiles_list = np.array(smiles_list)[valid_indices].tolist()\n",
    "ffv_list = np.array(ffv_list)[valid_indices].tolist()\n",
    "\n",
    "print(f\"Kept {len(smiles_list)} molecules with valid RDKit features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c75372",
   "metadata": {},
   "source": [
    "# Step 4: attach RDKit features to PyG data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "755a330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[644, 9], edge_index=[2, 1296], edge_attr=[1296, 3], y=[32], rdkit_feats=[288], batch=[644], ptr=[33])\n",
      "Batch's node features shape: torch.Size([644, 9])\n",
      "Batch's RDKit features shape: torch.Size([288])\n",
      "Batch's targets shape: torch.Size([32])\n",
      "Batch's 'batch' attribute shape: torch.Size([644])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning:\n",
      "\n",
      "'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "# Assuming your previous cells have loaded and processed the data into these lists:\n",
    "# smiles_list: list of SMILES strings\n",
    "# rdkit_features: numpy array of RDKit features (N, 6)\n",
    "# ffv_list: list of FFV values (N,)\n",
    "\n",
    "# 1. Create a list of PyG Data objects\n",
    "rdkit_features_tensor = torch.tensor(rdkit_features, dtype=torch.float32)\n",
    "ffv_targets_tensor = torch.tensor(ffv_list, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "data_list = []\n",
    "for i in range(len(smiles_list)):\n",
    "    # smiles2graph returns a dictionary\n",
    "    graph_dict = smiles2graph(smiles_list[i])\n",
    "    \n",
    "    # Create the Data object from the dictionary keys, converting to tensors\n",
    "    # Convert node and edge features to LongTensor \n",
    "    data = Data(\n",
    "        x=torch.tensor(graph_dict['node_feat'], dtype=torch.long),\n",
    "        edge_index=torch.tensor(graph_dict['edge_index'], dtype=torch.long),\n",
    "        edge_attr=torch.tensor(graph_dict['edge_feat'], dtype=torch.long),\n",
    "        rdkit_feats=rdkit_features_tensor[i],\n",
    "        y=ffv_targets_tensor[i]\n",
    "    )\n",
    "    data_list.append(data)\n",
    "\n",
    "# 2. Split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Create PyG DataLoaders\n",
    "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# 4. Verification\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    print(\"Batch's node features shape:\", batch.x.shape)\n",
    "    print(\"Batch's RDKit features shape:\", batch.rdkit_feats.shape)\n",
    "    print(\"Batch's targets shape:\", batch.y.shape)\n",
    "    print(\"Batch's 'batch' attribute shape:\", batch.batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2984411",
   "metadata": {},
   "source": [
    "## Step 5: Define the Hybrid GNN Model\n",
    "\n",
    "The final architecture uses both structural and cheminformatics data by combining GNN-learned graph embeddings with SMILES-derived RDKit descriptors. This Hybrid GNN model uses `smiles2graph` for graph construction and augments it with RDKit-based molecular features for improved prediction accuracy.\n",
    "\n",
    "### Model Components:\n",
    "\n",
    "* **AtomEncoder / BondEncoder**\n",
    "  Transforms categorical atom and bond features (provided by OGB) into learnable embeddings using the encoders from `ogb.graphproppred.mol_encoder`. These provide a strong foundation for expressive graph learning.\n",
    "\n",
    "* **GINEConv Layers (x2)**\n",
    "  I use two stacked GINEConv layers (Graph Isomorphism Network with Edge features). These layers perform neighborhood aggregation based on edge attributes, allowing the model to capture localized chemical environments.\n",
    "\n",
    "* **Global Mean Pooling**\n",
    "  After message passing, node level embeddings are aggregated into a fixed size graph level representation using `global_mean_pool`.\n",
    "\n",
    "* **Concatenation with RDKit Descriptors**\n",
    "  The pooled GNN embedding is concatenated with external RDKit descriptors, which capture global molecular properties not easily inferred from graph data alone.\n",
    "\n",
    "* **MLP Prediction Head**\n",
    "  A multilayer perceptron processes the combined feature vector with ReLU activations, dropout regularization, and linear layers to predict the HOMO–LUMO gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0ccbf07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridGNN(Module):\n",
    "    def __init__(self, gnn_dim, rdkit_dim, hidden_dim, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.gnn_dim = gnn_dim\n",
    "        self.rdkit_dim = rdkit_dim\n",
    "\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=gnn_dim)\n",
    "        self.bond_encoder = BondEncoder(emb_dim=gnn_dim)\n",
    "\n",
    "        self.conv1 = GINEConv(Sequential(Linear(gnn_dim, gnn_dim), ReLU(), Linear(gnn_dim, gnn_dim)))\n",
    "        self.conv2 = GINEConv(Sequential(Linear(gnn_dim, gnn_dim), ReLU(), Linear(gnn_dim, gnn_dim)))\n",
    "        self.pool = global_mean_pool\n",
    "\n",
    "        self.mlp = Sequential(Linear(gnn_dim + rdkit_dim, hidden_dim), ReLU(), \n",
    "                              Dropout(dropout_rate),\n",
    "                              Linear(hidden_dim, hidden_dim // 2), ReLU(), \n",
    "                              Dropout(dropout_rate),\n",
    "                              Linear(hidden_dim // 2, 1))\n",
    "\n",
    "    def forward(self, data):\n",
    "        # encode atoms and bonds\n",
    "        x = self.atom_encoder(data.x)\n",
    "        edge_attr = self.bond_encoder(data.edge_attr)\n",
    "\n",
    "        # GNN convolutions\n",
    "        x = self.conv1(x, data.edge_index, edge_attr)\n",
    "        x = self.conv2(x, data.edge_index, edge_attr)\n",
    "        x = self.pool(x, data.batch)\n",
    "\n",
    "        # handle RDKit features\n",
    "        rdkit_feats = getattr(data, 'rdkit_feats', None)\n",
    "        if rdkit_feats is not None:\n",
    "            # Reshape the RDKit features tensor to be (batch_size, rdkit_dim)\n",
    "            # The number of samples in the batch is given by x.shape[0] after pooling\n",
    "            reshaped_rdkit_feats = rdkit_feats.view(x.shape[0], self.rdkit_dim)\n",
    "            \n",
    "            # The check for shape mismatch is now more accurate\n",
    "            if x.shape[0] != reshaped_rdkit_feats.shape[0]:\n",
    "                raise ValueError(f\"Shape mismatch: GNN output ({x.shape[0]}) vs rdkit_feats ({reshaped_rdkit_feats.shape[0]})\")\n",
    "            \n",
    "            x = torch.cat([x, reshaped_rdkit_feats], dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"RDKit features not found in the data object\")\n",
    "\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252f39e",
   "metadata": {},
   "source": [
    "# Step 7: training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b42e8f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████| 16/16 [00:00<00:00, 67.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 101.0811 | Val Loss: 66.2718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|██████████| 16/16 [00:00<00:00, 139.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Train Loss: 68.3164 | Val Loss: 40.7643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: 100%|██████████| 16/16 [00:00<00:00, 218.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | Train Loss: 42.5605 | Val Loss: 22.8594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: 100%|██████████| 16/16 [00:00<00:00, 170.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | Train Loss: 30.2962 | Val Loss: 20.8899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: 100%|██████████| 16/16 [00:00<00:00, 226.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | Train Loss: 24.8070 | Val Loss: 20.9678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06: 100%|██████████| 16/16 [00:00<00:00, 232.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | Train Loss: 30.0150 | Val Loss: 22.0801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07: 100%|██████████| 16/16 [00:00<00:00, 208.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | Train Loss: 27.5740 | Val Loss: 21.2447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08: 100%|██████████| 16/16 [00:00<00:00, 210.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | Train Loss: 25.1097 | Val Loss: 21.9212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09: 100%|██████████| 16/16 [00:00<00:00, 182.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | Train Loss: 25.4864 | Val Loss: 20.1976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 16/16 [00:00<00:00, 225.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 27.6539 | Val Loss: 18.9335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 16/16 [00:00<00:00, 224.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 22.5605 | Val Loss: 17.6137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 16/16 [00:00<00:00, 204.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 21.3973 | Val Loss: 16.3129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 16/16 [00:00<00:00, 227.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 21.6120 | Val Loss: 18.3926"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 16/16 [00:00<00:00, 180.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 20.3974 | Val Loss: 15.1971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 16/16 [00:00<00:00, 226.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 20.3565 | Val Loss: 15.1256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 16/16 [00:00<00:00, 222.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss: 19.6178 | Val Loss: 17.8259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 16/16 [00:00<00:00, 218.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Loss: 18.9069 | Val Loss: 14.7946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 16/16 [00:00<00:00, 222.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss: 19.2337 | Val Loss: 14.6481"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 16/16 [00:00<00:00, 171.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Loss: 19.9497 | Val Loss: 13.8760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 16/16 [00:00<00:00, 217.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Loss: 17.9302 | Val Loss: 14.0850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 16/16 [00:00<00:00, 201.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Train Loss: 18.0963 | Val Loss: 13.2694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 16/16 [00:00<00:00, 210.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | Train Loss: 17.4319 | Val Loss: 12.9366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 16/16 [00:00<00:00, 216.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Train Loss: 18.2476 | Val Loss: 13.3825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 16/16 [00:00<00:00, 173.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | Train Loss: 18.4804 | Val Loss: 14.5292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 16/16 [00:00<00:00, 207.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Train Loss: 15.6979 | Val Loss: 17.2945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 16/16 [00:00<00:00, 204.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | Train Loss: 20.6087 | Val Loss: 13.2415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 16/16 [00:00<00:00, 203.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | Train Loss: 18.5020 | Val Loss: 12.9054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 16/16 [00:00<00:00, 212.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 | Train Loss: 17.3079 | Val Loss: 14.4876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 16/16 [00:00<00:00, 177.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 | Train Loss: 21.5480 | Val Loss: 18.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 16/16 [00:00<00:00, 205.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 | Train Loss: 20.2565 | Val Loss: 13.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 16/16 [00:00<00:00, 201.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 | Train Loss: 18.6552 | Val Loss: 22.4951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 16/16 [00:00<00:00, 209.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 | Train Loss: 19.6733 | Val Loss: 18.5225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 16/16 [00:00<00:00, 206.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 | Train Loss: 18.0136 | Val Loss: 12.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 16/16 [00:00<00:00, 173.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 | Train Loss: 18.7336 | Val Loss: 15.5598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 16/16 [00:00<00:00, 196.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 | Train Loss: 17.4747 | Val Loss: 16.2807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 16/16 [00:00<00:00, 202.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 | Train Loss: 17.9633 | Val Loss: 14.0451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 16/16 [00:00<00:00, 201.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 | Train Loss: 16.6454 | Val Loss: 14.9464\n",
      "Early stopping triggered at epoch 37\n",
      "\n",
      "GNN Evaluation:\n",
      "        MAE      RMSE  r_squared\n",
      "0  2.868629  3.592413   0.369485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattg\\AppData\\Local\\Temp\\ipykernel_16204\\2343932047.py:57: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model = HybridGNN(gnn_dim=128, rdkit_dim=rdkit_features.shape[1], hidden_dim=256)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            preds.append(pred.cpu())\n",
    "            targets.append(batch.y.view(-1, 1).cpu())\n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    loss = F.mse_loss(preds, targets)\n",
    "    return loss.item(), preds, targets\n",
    "\n",
    "# training loop\n",
    "for epoch in range(1, 101): # long since early stopping\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch:02d}\"):\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        loss = F.mse_loss(pred, batch.y.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    val_loss, val_preds, val_targets = evaluate(model, valid_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        save_dir = \"saved_models/gnn_smiles2graph_model\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"hybridgnn_untuned.pt\"))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# final eval on val set\n",
    "model.load_state_dict(torch.load(os.path.join(save_dir, \"hybridgnn_untuned.pt\")))\n",
    "model.eval()\n",
    "_, final_preds, final_targets = evaluate(model, valid_loader)\n",
    "metrics = regression_metrics(final_targets.numpy(), final_preds.numpy())\n",
    "print(\"\\nGNN Evaluation:\")\n",
    "print(metrics[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34585261",
   "metadata": {},
   "source": [
    "# Step 8: Optuna tuning of Hybrid GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "85485072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridGNN(Module):\n",
    "    def __init__(self, gnn_dim, rdkit_dim, hidden_dim, dropout_rate=0.2, activation='ReLU'):\n",
    "        super().__init__()\n",
    "        act_map = {'ReLU': torch.nn.ReLU(), 'ELU': torch.nn.ELU(), 'GELU': torch.nn.GELU(), 'LeakyReLU': torch.nn.LeakyReLU(), 'PReLU': torch.nn.PReLU(), 'Swish': torch.nn.SiLU()}\n",
    "        act_fn = act_map[activation]\n",
    "        self.gnn_dim = gnn_dim\n",
    "        self.rdkit_dim = rdkit_dim\n",
    "\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=gnn_dim)\n",
    "        self.bond_encoder = BondEncoder(emb_dim=gnn_dim)\n",
    "\n",
    "        self.conv1 = GINEConv(Sequential(Linear(gnn_dim, gnn_dim), act_fn, Linear(gnn_dim, gnn_dim)))\n",
    "        self.conv2 = GINEConv(Sequential(Linear(gnn_dim, gnn_dim), act_fn, Linear(gnn_dim, gnn_dim)))\n",
    "        self.pool = global_mean_pool\n",
    "\n",
    "        self.mlp = Sequential(Linear(gnn_dim + rdkit_dim, hidden_dim), act_fn, \n",
    "                              Dropout(dropout_rate), \n",
    "                              Linear(hidden_dim, hidden_dim // 2), act_fn, \n",
    "                              Dropout(dropout_rate), \n",
    "                              Linear(hidden_dim // 2, 1))\n",
    "\n",
    "    def forward(self, data):\n",
    "        # encode atoms and bonds\n",
    "        x = self.atom_encoder(data.x)\n",
    "        edge_attr = self.bond_encoder(data.edge_attr)\n",
    "\n",
    "        # GNN convolutions\n",
    "        x = self.conv1(x, data.edge_index, edge_attr)\n",
    "        x = self.conv2(x, data.edge_index, edge_attr)\n",
    "        x = self.pool(x, data.batch)\n",
    "\n",
    "        # handle RDKit features\n",
    "        rdkit_feats = getattr(data, 'rdkit_feats', None)\n",
    "        if rdkit_feats is not None:\n",
    "            # Reshape the RDKit features tensor to be (batch_size, rdkit_dim)\n",
    "            # The number of samples in the batch is given by x.shape[0] after pooling\n",
    "            reshaped_rdkit_feats = rdkit_feats.view(x.shape[0], self.rdkit_dim)\n",
    "            \n",
    "            # The check for shape mismatch is now more accurate\n",
    "            if x.shape[0] != reshaped_rdkit_feats.shape[0]:\n",
    "                raise ValueError(f\"Shape mismatch: GNN output ({x.shape[0]}) vs rdkit_feats ({reshaped_rdkit_feats.shape[0]})\")\n",
    "            \n",
    "            x = torch.cat([x, reshaped_rdkit_feats], dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"RDKit features not found in the data object\")\n",
    "\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f1fc3",
   "metadata": {},
   "source": [
    "Multiple rounds of tuning have suggested to refine my search space to ReLU, GELU, and Swish activation functions and Adam and AdamW optimizers. Therefore, I have commented out unused parameters like momentum for SGD and the unused optimizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f95a50c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:18,760] A new study created in RDB with name: final_2d_gnn_study_Rg_2\n",
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning:\n",
      "\n",
      "'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "\n",
      "[I 2025-09-04 21:19:19,097] Trial 0 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 | Epoch 01 | NaN loss detected so pruning trial\n",
      "Trial 1 | Epoch 01 | Train Loss: 228.6592 | Val Loss: 196.4705 | Optimizer: Adam\n",
      "Trial 1 | Epoch 02 | Train Loss: 141.6233 | Val Loss: 110.6457 | Optimizer: Adam\n",
      "Trial 1 | Epoch 03 | Train Loss: 75.5416 | Val Loss: 49.1656 | Optimizer: Adam\n",
      "Trial 1 | Epoch 04 | Train Loss: 49.4620 | Val Loss: 46.6548 | Optimizer: Adam\n",
      "Trial 1 | Epoch 05 | Train Loss: 62.5982 | Val Loss: 43.6512 | Optimizer: Adam\n",
      "Trial 1 | Epoch 06 | Train Loss: 50.4084 | Val Loss: 50.7238 | Optimizer: Adam\n",
      "Trial 1 | Epoch 07 | Train Loss: 51.5748 | Val Loss: 45.5888 | Optimizer: Adam\n",
      "Trial 1 | Epoch 08 | Train Loss: 44.9642 | Val Loss: 35.3212 | Optimizer: Adam\n",
      "Trial 1 | Epoch 09 | Train Loss: 40.4832 | Val Loss: 32.4114 | Optimizer: Adam\n",
      "Trial 1 | Epoch 10 | Train Loss: 41.0902 | Val Loss: 34.0826 | Optimizer: Adam\n",
      "Trial 1 | Epoch 11 | Train Loss: 37.9806 | Val Loss: 31.1625 | Optimizer: Adam\n",
      "Trial 1 | Epoch 12 | Train Loss: 32.0278 | Val Loss: 27.5335 | Optimizer: Adam\n",
      "Trial 1 | Epoch 13 | Train Loss: 31.0925 | Val Loss: 29.8587 | Optimizer: Adam\n",
      "Trial 1 | Epoch 14 | Train Loss: 31.3905 | Val Loss: 26.8112 | Optimizer: Adam\n",
      "Trial 1 | Epoch 15 | Train Loss: 32.3369 | Val Loss: 26.6164 | Optimizer: Adam\n",
      "Trial 1 | Epoch 16 | Train Loss: 29.7269 | Val Loss: 26.1825 | Optimizer: Adam\n",
      "Trial 1 | Epoch 17 | Train Loss: 28.5850 | Val Loss: 26.3588 | Optimizer: Adam\n",
      "Trial 1 | Epoch 18 | Train Loss: 29.5607 | Val Loss: 26.5082 | Optimizer: Adam\n",
      "Trial 1 | Epoch 19 | Train Loss: 26.0140 | Val Loss: 24.1871 | Optimizer: Adam\n",
      "Trial 1 | Epoch 20 | Train Loss: 26.9000 | Val Loss: 26.8334 | Optimizer: Adam\n",
      "Trial 1 | Epoch 21 | Train Loss: 25.7626 | Val Loss: 23.0526 | Optimizer: Adam\n",
      "Trial 1 | Epoch 22 | Train Loss: 25.8304 | Val Loss: 23.5637 | Optimizer: Adam\n",
      "Trial 1 | Epoch 23 | Train Loss: 29.2181 | Val Loss: 25.7299 | Optimizer: Adam\n",
      "Trial 1 | Epoch 24 | Train Loss: 25.3319 | Val Loss: 22.9106 | Optimizer: Adam\n",
      "Trial 1 | Epoch 25 | Train Loss: 27.3839 | Val Loss: 26.5037 | Optimizer: Adam\n",
      "Trial 1 | Epoch 26 | Train Loss: 26.7471 | Val Loss: 23.7837 | Optimizer: Adam\n",
      "Trial 1 | Epoch 27 | Train Loss: 27.9585 | Val Loss: 24.2945 | Optimizer: Adam\n",
      "Trial 1 | Epoch 28 | Train Loss: 24.6901 | Val Loss: 25.0829 | Optimizer: Adam\n",
      "Trial 1 | Epoch 29 | Train Loss: 24.6964 | Val Loss: 22.7092 | Optimizer: Adam\n",
      "Trial 1 | Epoch 30 | Train Loss: 22.8600 | Val Loss: 25.0535 | Optimizer: Adam\n",
      "Trial 1 | Epoch 31 | Train Loss: 25.8260 | Val Loss: 23.7666 | Optimizer: Adam\n",
      "Trial 1 | Epoch 32 | Train Loss: 23.8511 | Val Loss: 26.2582 | Optimizer: Adam\n",
      "Trial 1 | Epoch 33 | Train Loss: 24.8168 | Val Loss: 22.8987 | Optimizer: Adam\n",
      "Trial 1 | Epoch 34 | Train Loss: 24.3671 | Val Loss: 26.4078 | Optimizer: Adam\n",
      "Trial 1 | Epoch 35 | Train Loss: 25.4792 | Val Loss: 22.2584 | Optimizer: Adam\n",
      "Trial 1 | Epoch 36 | Train Loss: 23.9989 | Val Loss: 24.4708 | Optimizer: Adam\n",
      "Trial 1 | Epoch 37 | Train Loss: 23.8148 | Val Loss: 24.1931 | Optimizer: Adam\n",
      "Trial 1 | Epoch 38 | Train Loss: 22.4800 | Val Loss: 23.6201 | Optimizer: Adam\n",
      "Trial 1 | Epoch 39 | Train Loss: 22.7042 | Val Loss: 22.6967 | Optimizer: Adam\n",
      "Trial 1 | Epoch 40 | Train Loss: 24.1219 | Val Loss: 24.8935 | Optimizer: Adam\n",
      "Trial 1 | Epoch 41 | Train Loss: 24.3836 | Val Loss: 22.2741 | Optimizer: Adam\n",
      "Trial 1 | Epoch 42 | Train Loss: 24.5417 | Val Loss: 24.1415 | Optimizer: Adam\n",
      "Trial 1 | Epoch 43 | Train Loss: 23.5701 | Val Loss: 23.7357 | Optimizer: Adam\n",
      "Trial 1 | Epoch 44 | Train Loss: 22.4401 | Val Loss: 23.9650 | Optimizer: Adam\n",
      "Trial 1 | Epoch 45 | Train Loss: 23.6719 | Val Loss: 20.8164 | Optimizer: Adam\n",
      "Trial 1 | Epoch 46 | Train Loss: 22.7209 | Val Loss: 26.0101 | Optimizer: Adam\n",
      "Trial 1 | Epoch 47 | Train Loss: 21.7124 | Val Loss: 20.8095 | Optimizer: Adam\n",
      "Trial 1 | Epoch 48 | Train Loss: 22.5887 | Val Loss: 29.0535 | Optimizer: Adam\n",
      "Trial 1 | Epoch 49 | Train Loss: 23.7635 | Val Loss: 19.9284 | Optimizer: Adam\n",
      "Trial 1 | Epoch 50 | Train Loss: 23.4140 | Val Loss: 23.5389 | Optimizer: Adam\n",
      "Trial 1 | Epoch 51 | Train Loss: 22.3661 | Val Loss: 23.9443 | Optimizer: Adam\n",
      "Trial 1 | Epoch 52 | Train Loss: 21.9436 | Val Loss: 19.1302 | Optimizer: Adam\n",
      "Trial 1 | Epoch 53 | Train Loss: 23.7201 | Val Loss: 23.6106 | Optimizer: Adam\n",
      "Trial 1 | Epoch 54 | Train Loss: 22.2946 | Val Loss: 19.1252 | Optimizer: Adam\n",
      "Trial 1 | Epoch 55 | Train Loss: 22.2587 | Val Loss: 25.5305 | Optimizer: Adam\n",
      "Trial 1 | Epoch 56 | Train Loss: 20.2145 | Val Loss: 20.3939 | Optimizer: Adam\n",
      "Trial 1 | Epoch 57 | Train Loss: 22.8633 | Val Loss: 22.0471 | Optimizer: Adam\n",
      "Trial 1 | Epoch 58 | Train Loss: 22.3943 | Val Loss: 22.8500 | Optimizer: Adam\n",
      "Trial 1 | Epoch 59 | Train Loss: 21.5104 | Val Loss: 20.0730 | Optimizer: Adam\n",
      "Trial 1 | Epoch 60 | Train Loss: 19.2833 | Val Loss: 21.5969 | Optimizer: Adam\n",
      "Trial 1 | Epoch 61 | Train Loss: 20.3712 | Val Loss: 20.7884 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:22,425] Trial 1 finished with value: 19.12524386925426 and parameters: {'gnn_dim': 384, 'hidden_dim': 256, 'dropout_rate': 0.32279148171296423, 'lr': 0.00016821076514418284, 'activation': 'Swish', 'optimizer': 'Adam', 'weight_decay': 5.684834094844966e-06}. Best is trial 1 with value: 19.12524386925426.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 | Epoch 62 | Train Loss: 19.7043 | Val Loss: 20.7203 | Optimizer: Adam\n",
      "Trial 1 | Epoch 63 | Train Loss: 18.2053 | Val Loss: 20.7683 | Optimizer: Adam\n",
      "Trial 1 | Epoch 64 | Train Loss: 20.1813 | Val Loss: 22.6451 | Optimizer: Adam\n",
      "Trial 1 - Early stopping triggered at epoch 64\n",
      "Trial 2 | Epoch 01 | Train Loss: 149.1598 | Val Loss: 80.8559 | Optimizer: SGD\n",
      "Trial 2 | Epoch 02 | Train Loss: 79.6877 | Val Loss: 84.1163 | Optimizer: SGD\n",
      "Trial 2 | Epoch 03 | Train Loss: 78.5675 | Val Loss: 70.5913 | Optimizer: SGD\n",
      "Trial 2 | Epoch 04 | Train Loss: 71.6935 | Val Loss: 63.0041 | Optimizer: SGD\n",
      "Trial 2 | Epoch 05 | Train Loss: 67.4446 | Val Loss: 69.2160 | Optimizer: SGD\n",
      "Trial 2 | Epoch 06 | Train Loss: 67.5689 | Val Loss: 62.5684 | Optimizer: SGD\n",
      "Trial 2 | Epoch 07 | Train Loss: 62.5012 | Val Loss: 65.3156 | Optimizer: SGD\n",
      "Trial 2 | Epoch 08 | Train Loss: 59.0695 | Val Loss: 59.8691 | Optimizer: SGD\n",
      "Trial 2 | Epoch 09 | Train Loss: 59.8590 | Val Loss: 65.4415 | Optimizer: SGD\n",
      "Trial 2 | Epoch 10 | Train Loss: 62.6699 | Val Loss: 58.7825 | Optimizer: SGD\n",
      "Trial 2 | Epoch 11 | Train Loss: 64.5585 | Val Loss: 65.6600 | Optimizer: SGD\n",
      "Trial 2 | Epoch 12 | Train Loss: 61.1640 | Val Loss: 57.9932 | Optimizer: SGD\n",
      "Trial 2 | Epoch 13 | Train Loss: 61.6568 | Val Loss: 64.5566 | Optimizer: SGD\n",
      "Trial 2 | Epoch 14 | Train Loss: 61.9482 | Val Loss: 56.9447 | Optimizer: SGD\n",
      "Trial 2 | Epoch 15 | Train Loss: 63.6637 | Val Loss: 66.6567 | Optimizer: SGD\n",
      "Trial 2 | Epoch 16 | Train Loss: 62.8846 | Val Loss: 56.5003 | Optimizer: SGD\n",
      "Trial 2 | Epoch 17 | Train Loss: 60.0196 | Val Loss: 58.1733 | Optimizer: SGD\n",
      "Trial 2 | Epoch 18 | Train Loss: 58.7170 | Val Loss: 55.4298 | Optimizer: SGD\n",
      "Trial 2 | Epoch 19 | Train Loss: 62.2078 | Val Loss: 57.0466 | Optimizer: SGD\n",
      "Trial 2 | Epoch 20 | Train Loss: 55.2324 | Val Loss: 54.9656 | Optimizer: SGD\n",
      "Trial 2 | Epoch 21 | Train Loss: 55.6594 | Val Loss: 57.3881 | Optimizer: SGD\n",
      "Trial 2 | Epoch 22 | Train Loss: 56.3197 | Val Loss: 54.1193 | Optimizer: SGD\n",
      "Trial 2 | Epoch 23 | Train Loss: 61.0219 | Val Loss: 55.5427 | Optimizer: SGD\n",
      "Trial 2 | Epoch 24 | Train Loss: 56.0260 | Val Loss: 54.5439 | Optimizer: SGD\n",
      "Trial 2 | Epoch 25 | Train Loss: 56.4026 | Val Loss: 53.9705 | Optimizer: SGD\n",
      "Trial 2 | Epoch 26 | Train Loss: 55.7763 | Val Loss: 52.7559 | Optimizer: SGD\n",
      "Trial 2 | Epoch 27 | Train Loss: 54.7893 | Val Loss: 52.5769 | Optimizer: SGD\n",
      "Trial 2 | Epoch 28 | Train Loss: 54.3538 | Val Loss: 52.1687 | Optimizer: SGD\n",
      "Trial 2 | Epoch 29 | Train Loss: 56.3542 | Val Loss: 52.2881 | Optimizer: SGD\n",
      "Trial 2 | Epoch 30 | Train Loss: 59.6438 | Val Loss: 54.0882 | Optimizer: SGD\n",
      "Trial 2 | Epoch 31 | Train Loss: 53.4464 | Val Loss: 51.3686 | Optimizer: SGD\n",
      "Trial 2 | Epoch 32 | Train Loss: 49.5960 | Val Loss: 51.2978 | Optimizer: SGD\n",
      "Trial 2 | Epoch 33 | Train Loss: 51.3181 | Val Loss: 51.1156 | Optimizer: SGD\n",
      "Trial 2 | Epoch 34 | Train Loss: 55.4572 | Val Loss: 52.3595 | Optimizer: SGD\n",
      "Trial 2 | Epoch 35 | Train Loss: 55.1218 | Val Loss: 50.1379 | Optimizer: SGD\n",
      "Trial 2 | Epoch 36 | Train Loss: 49.1983 | Val Loss: 50.7603 | Optimizer: SGD\n",
      "Trial 2 | Epoch 37 | Train Loss: 53.3448 | Val Loss: 49.5287 | Optimizer: SGD\n",
      "Trial 2 | Epoch 38 | Train Loss: 54.2859 | Val Loss: 51.8765 | Optimizer: SGD\n",
      "Trial 2 | Epoch 39 | Train Loss: 50.3658 | Val Loss: 49.0781 | Optimizer: SGD\n",
      "Trial 2 | Epoch 40 | Train Loss: 48.7748 | Val Loss: 49.4911 | Optimizer: SGD\n",
      "Trial 2 | Epoch 41 | Train Loss: 53.0661 | Val Loss: 52.1154 | Optimizer: SGD\n",
      "Trial 2 | Epoch 42 | Train Loss: 51.8322 | Val Loss: 49.7106 | Optimizer: SGD\n",
      "Trial 2 | Epoch 43 | Train Loss: 52.6399 | Val Loss: 56.9089 | Optimizer: SGD\n",
      "Trial 2 | Epoch 44 | Train Loss: 50.8723 | Val Loss: 49.1233 | Optimizer: SGD\n",
      "Trial 2 | Epoch 45 | Train Loss: 52.7146 | Val Loss: 54.0680 | Optimizer: SGD\n",
      "Trial 2 | Epoch 46 | Train Loss: 47.9942 | Val Loss: 48.7154 | Optimizer: SGD\n",
      "Trial 2 | Epoch 47 | Train Loss: 49.2928 | Val Loss: 48.8614 | Optimizer: SGD\n",
      "Trial 2 | Epoch 48 | Train Loss: 49.7745 | Val Loss: 46.7909 | Optimizer: SGD\n",
      "Trial 2 | Epoch 49 | Train Loss: 47.4420 | Val Loss: 46.5330 | Optimizer: SGD\n",
      "Trial 2 | Epoch 50 | Train Loss: 47.4844 | Val Loss: 47.0284 | Optimizer: SGD\n",
      "Trial 2 | Epoch 51 | Train Loss: 46.8032 | Val Loss: 45.5636 | Optimizer: SGD\n",
      "Trial 2 | Epoch 52 | Train Loss: 48.3451 | Val Loss: 45.2092 | Optimizer: SGD\n",
      "Trial 2 | Epoch 53 | Train Loss: 48.9012 | Val Loss: 48.8947 | Optimizer: SGD\n",
      "Trial 2 | Epoch 54 | Train Loss: 46.3773 | Val Loss: 46.0590 | Optimizer: SGD\n",
      "Trial 2 | Epoch 55 | Train Loss: 46.5477 | Val Loss: 46.8394 | Optimizer: SGD\n",
      "Trial 2 | Epoch 56 | Train Loss: 46.8767 | Val Loss: 44.2742 | Optimizer: SGD\n",
      "Trial 2 | Epoch 57 | Train Loss: 45.1570 | Val Loss: 44.2649 | Optimizer: SGD\n",
      "Trial 2 | Epoch 58 | Train Loss: 45.1851 | Val Loss: 44.3319 | Optimizer: SGD\n",
      "Trial 2 | Epoch 59 | Train Loss: 44.9948 | Val Loss: 43.5763 | Optimizer: SGD\n",
      "Trial 2 | Epoch 60 | Train Loss: 43.9460 | Val Loss: 43.1686 | Optimizer: SGD\n",
      "Trial 2 | Epoch 61 | Train Loss: 44.5759 | Val Loss: 43.4444 | Optimizer: SGD\n",
      "Trial 2 | Epoch 62 | Train Loss: 46.2876 | Val Loss: 44.3751 | Optimizer: SGD\n",
      "Trial 2 | Epoch 63 | Train Loss: 44.8604 | Val Loss: 42.7305 | Optimizer: SGD\n",
      "Trial 2 | Epoch 64 | Train Loss: 41.6234 | Val Loss: 43.7659 | Optimizer: SGD\n",
      "Trial 2 | Epoch 65 | Train Loss: 40.8714 | Val Loss: 41.9333 | Optimizer: SGD\n",
      "Trial 2 | Epoch 66 | Train Loss: 46.2837 | Val Loss: 41.6866 | Optimizer: SGD\n",
      "Trial 2 | Epoch 67 | Train Loss: 43.8750 | Val Loss: 44.8913 | Optimizer: SGD\n",
      "Trial 2 | Epoch 68 | Train Loss: 47.1882 | Val Loss: 41.2650 | Optimizer: SGD\n",
      "Trial 2 | Epoch 69 | Train Loss: 43.7206 | Val Loss: 46.1390 | Optimizer: SGD\n",
      "Trial 2 | Epoch 70 | Train Loss: 44.6801 | Val Loss: 40.8391 | Optimizer: SGD\n",
      "Trial 2 | Epoch 71 | Train Loss: 42.0459 | Val Loss: 42.1211 | Optimizer: SGD\n",
      "Trial 2 | Epoch 72 | Train Loss: 43.4277 | Val Loss: 40.2962 | Optimizer: SGD\n",
      "Trial 2 | Epoch 73 | Train Loss: 40.8528 | Val Loss: 41.6051 | Optimizer: SGD\n",
      "Trial 2 | Epoch 74 | Train Loss: 41.4654 | Val Loss: 39.8171 | Optimizer: SGD\n",
      "Trial 2 | Epoch 75 | Train Loss: 39.9884 | Val Loss: 40.4854 | Optimizer: SGD\n",
      "Trial 2 | Epoch 76 | Train Loss: 42.0025 | Val Loss: 39.8945 | Optimizer: SGD\n",
      "Trial 2 | Epoch 77 | Train Loss: 39.7050 | Val Loss: 39.2796 | Optimizer: SGD\n",
      "Trial 2 | Epoch 78 | Train Loss: 37.2032 | Val Loss: 38.9791 | Optimizer: SGD\n",
      "Trial 2 | Epoch 79 | Train Loss: 43.9605 | Val Loss: 38.7515 | Optimizer: SGD\n",
      "Trial 2 | Epoch 80 | Train Loss: 40.1813 | Val Loss: 39.5065 | Optimizer: SGD\n",
      "Trial 2 | Epoch 81 | Train Loss: 40.7593 | Val Loss: 38.0985 | Optimizer: SGD\n",
      "Trial 2 | Epoch 82 | Train Loss: 37.6411 | Val Loss: 37.7350 | Optimizer: SGD\n",
      "Trial 2 | Epoch 83 | Train Loss: 40.1575 | Val Loss: 37.9982 | Optimizer: SGD\n",
      "Trial 2 | Epoch 84 | Train Loss: 39.9226 | Val Loss: 38.9707 | Optimizer: SGD\n",
      "Trial 2 | Epoch 85 | Train Loss: 38.8343 | Val Loss: 37.0781 | Optimizer: SGD\n",
      "Trial 2 | Epoch 86 | Train Loss: 41.4221 | Val Loss: 40.9490 | Optimizer: SGD\n",
      "Trial 2 | Epoch 87 | Train Loss: 37.4848 | Val Loss: 36.9854 | Optimizer: SGD\n",
      "Trial 2 | Epoch 88 | Train Loss: 38.1773 | Val Loss: 39.1796 | Optimizer: SGD\n",
      "Trial 2 | Epoch 89 | Train Loss: 38.9409 | Val Loss: 36.5603 | Optimizer: SGD\n",
      "Trial 2 | Epoch 90 | Train Loss: 36.0330 | Val Loss: 35.8276 | Optimizer: SGD\n",
      "Trial 2 | Epoch 91 | Train Loss: 38.5779 | Val Loss: 35.6570 | Optimizer: SGD\n",
      "Trial 2 | Epoch 92 | Train Loss: 35.8202 | Val Loss: 35.3261 | Optimizer: SGD\n",
      "Trial 2 | Epoch 93 | Train Loss: 37.4498 | Val Loss: 36.7363 | Optimizer: SGD\n",
      "Trial 2 | Epoch 94 | Train Loss: 34.1315 | Val Loss: 34.9190 | Optimizer: SGD\n",
      "Trial 2 | Epoch 95 | Train Loss: 33.2338 | Val Loss: 38.1816 | Optimizer: SGD\n",
      "Trial 2 | Epoch 96 | Train Loss: 34.9727 | Val Loss: 34.3951 | Optimizer: SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:29,912] Trial 2 finished with value: 34.07929231287018 and parameters: {'gnn_dim': 1024, 'hidden_dim': 384, 'dropout_rate': 0.3452449144029921, 'lr': 2.483462588772003e-05, 'activation': 'Swish', 'optimizer': 'SGD', 'momentum': 0.8694991766438684, 'weight_decay': 1.3332403849718848e-05}. Best is trial 1 with value: 19.12524386925426.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 | Epoch 97 | Train Loss: 36.2148 | Val Loss: 37.2463 | Optimizer: SGD\n",
      "Trial 2 | Epoch 98 | Train Loss: 34.3155 | Val Loss: 34.0793 | Optimizer: SGD\n",
      "Trial 2 | Epoch 99 | Train Loss: 35.2012 | Val Loss: 35.9620 | Optimizer: SGD\n",
      "Trial 3 | Epoch 01 | Train Loss: 184.4119 | Val Loss: 63.2193 | Optimizer: SGD\n",
      "Trial 3 | Epoch 02 | Train Loss: 79.9247 | Val Loss: 61.8296 | Optimizer: SGD\n",
      "Trial 3 | Epoch 03 | Train Loss: 67.6615 | Val Loss: 71.5300 | Optimizer: SGD\n",
      "Trial 3 | Epoch 04 | Train Loss: 67.9162 | Val Loss: 61.8442 | Optimizer: SGD\n",
      "Trial 3 | Epoch 05 | Train Loss: 66.0469 | Val Loss: 64.2398 | Optimizer: SGD\n",
      "Trial 3 | Epoch 06 | Train Loss: 65.9890 | Val Loss: 62.0930 | Optimizer: SGD\n",
      "Trial 3 | Epoch 07 | Train Loss: 63.4742 | Val Loss: 60.7473 | Optimizer: SGD\n",
      "Trial 3 | Epoch 08 | Train Loss: 63.7212 | Val Loss: 61.7962 | Optimizer: SGD\n",
      "Trial 3 | Epoch 09 | Train Loss: 61.4864 | Val Loss: 60.7988 | Optimizer: SGD\n",
      "Trial 3 | Epoch 10 | Train Loss: 66.9091 | Val Loss: 61.3965 | Optimizer: SGD\n",
      "Trial 3 | Epoch 11 | Train Loss: 61.4579 | Val Loss: 61.1624 | Optimizer: SGD\n",
      "Trial 3 | Epoch 12 | Train Loss: 58.8177 | Val Loss: 59.5306 | Optimizer: SGD\n",
      "Trial 3 | Epoch 13 | Train Loss: 63.7023 | Val Loss: 62.5715 | Optimizer: SGD\n",
      "Trial 3 | Epoch 14 | Train Loss: 61.2134 | Val Loss: 59.8221 | Optimizer: SGD\n",
      "Trial 3 | Epoch 15 | Train Loss: 60.0019 | Val Loss: 59.5825 | Optimizer: SGD\n",
      "Trial 3 | Epoch 16 | Train Loss: 57.6841 | Val Loss: 60.3535 | Optimizer: SGD\n",
      "Trial 3 | Epoch 17 | Train Loss: 61.5613 | Val Loss: 58.6030 | Optimizer: SGD\n",
      "Trial 3 | Epoch 18 | Train Loss: 59.5377 | Val Loss: 59.5962 | Optimizer: SGD\n",
      "Trial 3 | Epoch 19 | Train Loss: 58.8774 | Val Loss: 58.6219 | Optimizer: SGD\n",
      "Trial 3 | Epoch 20 | Train Loss: 60.5814 | Val Loss: 59.3253 | Optimizer: SGD\n",
      "Trial 3 | Epoch 21 | Train Loss: 58.4138 | Val Loss: 58.4387 | Optimizer: SGD\n",
      "Trial 3 | Epoch 22 | Train Loss: 56.7214 | Val Loss: 57.6378 | Optimizer: SGD\n",
      "Trial 3 | Epoch 23 | Train Loss: 56.7721 | Val Loss: 58.4131 | Optimizer: SGD\n",
      "Trial 3 | Epoch 24 | Train Loss: 58.9857 | Val Loss: 57.9139 | Optimizer: SGD\n",
      "Trial 3 | Epoch 25 | Train Loss: 59.2378 | Val Loss: 58.1644 | Optimizer: SGD\n",
      "Trial 3 | Epoch 26 | Train Loss: 60.2651 | Val Loss: 58.0049 | Optimizer: SGD\n",
      "Trial 3 | Epoch 27 | Train Loss: 57.8197 | Val Loss: 57.0853 | Optimizer: SGD\n",
      "Trial 3 | Epoch 28 | Train Loss: 65.2423 | Val Loss: 59.1637 | Optimizer: SGD\n",
      "Trial 3 | Epoch 29 | Train Loss: 55.5134 | Val Loss: 56.7245 | Optimizer: SGD\n",
      "Trial 3 | Epoch 30 | Train Loss: 57.4484 | Val Loss: 56.6089 | Optimizer: SGD\n",
      "Trial 3 | Epoch 31 | Train Loss: 62.0772 | Val Loss: 58.1967 | Optimizer: SGD\n",
      "Trial 3 | Epoch 32 | Train Loss: 59.4346 | Val Loss: 57.3678 | Optimizer: SGD\n",
      "Trial 3 | Epoch 33 | Train Loss: 59.2965 | Val Loss: 57.3274 | Optimizer: SGD\n",
      "Trial 3 | Epoch 34 | Train Loss: 55.9537 | Val Loss: 56.1191 | Optimizer: SGD\n",
      "Trial 3 | Epoch 35 | Train Loss: 59.1489 | Val Loss: 56.6284 | Optimizer: SGD\n",
      "Trial 3 | Epoch 36 | Train Loss: 58.0218 | Val Loss: 56.9061 | Optimizer: SGD\n",
      "Trial 3 | Epoch 37 | Train Loss: 52.1322 | Val Loss: 55.7655 | Optimizer: SGD\n",
      "Trial 3 | Epoch 38 | Train Loss: 57.5132 | Val Loss: 55.9686 | Optimizer: SGD\n",
      "Trial 3 | Epoch 39 | Train Loss: 58.2761 | Val Loss: 56.6170 | Optimizer: SGD\n",
      "Trial 3 | Epoch 40 | Train Loss: 58.0310 | Val Loss: 55.8667 | Optimizer: SGD\n",
      "Trial 3 | Epoch 41 | Train Loss: 56.4605 | Val Loss: 55.4968 | Optimizer: SGD\n",
      "Trial 3 | Epoch 42 | Train Loss: 54.8238 | Val Loss: 56.4284 | Optimizer: SGD\n",
      "Trial 3 | Epoch 43 | Train Loss: 53.1397 | Val Loss: 55.0713 | Optimizer: SGD\n",
      "Trial 3 | Epoch 44 | Train Loss: 58.6235 | Val Loss: 56.2433 | Optimizer: SGD\n",
      "Trial 3 | Epoch 45 | Train Loss: 55.4496 | Val Loss: 55.5057 | Optimizer: SGD\n",
      "Trial 3 | Epoch 46 | Train Loss: 56.4021 | Val Loss: 54.7513 | Optimizer: SGD\n",
      "Trial 3 | Epoch 47 | Train Loss: 59.3624 | Val Loss: 55.5183 | Optimizer: SGD\n",
      "Trial 3 | Epoch 48 | Train Loss: 56.4871 | Val Loss: 54.6841 | Optimizer: SGD\n",
      "Trial 3 | Epoch 49 | Train Loss: 53.9971 | Val Loss: 54.5171 | Optimizer: SGD\n",
      "Trial 3 | Epoch 50 | Train Loss: 53.3004 | Val Loss: 54.3923 | Optimizer: SGD\n",
      "Trial 3 | Epoch 51 | Train Loss: 60.3521 | Val Loss: 55.5990 | Optimizer: SGD\n",
      "Trial 3 | Epoch 52 | Train Loss: 54.9669 | Val Loss: 55.3518 | Optimizer: SGD\n",
      "Trial 3 | Epoch 53 | Train Loss: 56.1087 | Val Loss: 54.0899 | Optimizer: SGD\n",
      "Trial 3 | Epoch 54 | Train Loss: 56.7354 | Val Loss: 54.0174 | Optimizer: SGD\n",
      "Trial 3 | Epoch 55 | Train Loss: 55.3951 | Val Loss: 54.2791 | Optimizer: SGD\n",
      "Trial 3 | Epoch 56 | Train Loss: 53.6088 | Val Loss: 53.8711 | Optimizer: SGD\n",
      "Trial 3 | Epoch 57 | Train Loss: 56.9043 | Val Loss: 56.1051 | Optimizer: SGD\n",
      "Trial 3 | Epoch 58 | Train Loss: 54.4339 | Val Loss: 53.7631 | Optimizer: SGD\n",
      "Trial 3 | Epoch 59 | Train Loss: 54.0880 | Val Loss: 53.9280 | Optimizer: SGD\n",
      "Trial 3 | Epoch 60 | Train Loss: 55.5542 | Val Loss: 53.6071 | Optimizer: SGD\n",
      "Trial 3 | Epoch 61 | Train Loss: 56.3537 | Val Loss: 53.7331 | Optimizer: SGD\n",
      "Trial 3 | Epoch 62 | Train Loss: 57.0506 | Val Loss: 54.6784 | Optimizer: SGD\n",
      "Trial 3 | Epoch 63 | Train Loss: 51.9583 | Val Loss: 52.9828 | Optimizer: SGD\n",
      "Trial 3 | Epoch 64 | Train Loss: 53.1683 | Val Loss: 53.1482 | Optimizer: SGD\n",
      "Trial 3 | Epoch 65 | Train Loss: 56.2604 | Val Loss: 52.9993 | Optimizer: SGD\n",
      "Trial 3 | Epoch 66 | Train Loss: 53.8483 | Val Loss: 52.8354 | Optimizer: SGD\n",
      "Trial 3 | Epoch 67 | Train Loss: 52.5903 | Val Loss: 52.7434 | Optimizer: SGD\n",
      "Trial 3 | Epoch 68 | Train Loss: 54.8986 | Val Loss: 52.9777 | Optimizer: SGD\n",
      "Trial 3 | Epoch 69 | Train Loss: 56.2121 | Val Loss: 53.6295 | Optimizer: SGD\n",
      "Trial 3 | Epoch 70 | Train Loss: 52.5956 | Val Loss: 52.5498 | Optimizer: SGD\n",
      "Trial 3 | Epoch 71 | Train Loss: 54.4844 | Val Loss: 53.8454 | Optimizer: SGD\n",
      "Trial 3 | Epoch 72 | Train Loss: 52.8845 | Val Loss: 52.0634 | Optimizer: SGD\n",
      "Trial 3 | Epoch 73 | Train Loss: 51.8454 | Val Loss: 51.9419 | Optimizer: SGD\n",
      "Trial 3 | Epoch 74 | Train Loss: 53.6344 | Val Loss: 52.3466 | Optimizer: SGD\n",
      "Trial 3 | Epoch 75 | Train Loss: 50.6697 | Val Loss: 51.7558 | Optimizer: SGD\n",
      "Trial 3 | Epoch 76 | Train Loss: 52.1337 | Val Loss: 51.6442 | Optimizer: SGD\n",
      "Trial 3 | Epoch 77 | Train Loss: 50.4156 | Val Loss: 51.5401 | Optimizer: SGD\n",
      "Trial 3 | Epoch 78 | Train Loss: 52.3568 | Val Loss: 51.7892 | Optimizer: SGD\n",
      "Trial 3 | Epoch 79 | Train Loss: 51.9439 | Val Loss: 51.4787 | Optimizer: SGD\n",
      "Trial 3 | Epoch 80 | Train Loss: 54.8626 | Val Loss: 52.5441 | Optimizer: SGD\n",
      "Trial 3 | Epoch 81 | Train Loss: 53.8688 | Val Loss: 51.4606 | Optimizer: SGD\n",
      "Trial 3 | Epoch 82 | Train Loss: 52.4015 | Val Loss: 51.7655 | Optimizer: SGD\n",
      "Trial 3 | Epoch 83 | Train Loss: 50.6256 | Val Loss: 50.8456 | Optimizer: SGD\n",
      "Trial 3 | Epoch 84 | Train Loss: 51.0174 | Val Loss: 51.2457 | Optimizer: SGD\n",
      "Trial 3 | Epoch 85 | Train Loss: 49.6717 | Val Loss: 50.9607 | Optimizer: SGD\n",
      "Trial 3 | Epoch 86 | Train Loss: 51.8207 | Val Loss: 50.6105 | Optimizer: SGD\n",
      "Trial 3 | Epoch 87 | Train Loss: 50.9608 | Val Loss: 50.7371 | Optimizer: SGD\n",
      "Trial 3 | Epoch 88 | Train Loss: 50.3343 | Val Loss: 50.3539 | Optimizer: SGD\n",
      "Trial 3 | Epoch 89 | Train Loss: 52.3894 | Val Loss: 50.5441 | Optimizer: SGD\n",
      "Trial 3 | Epoch 90 | Train Loss: 52.2171 | Val Loss: 51.1476 | Optimizer: SGD\n",
      "Trial 3 | Epoch 91 | Train Loss: 52.4785 | Val Loss: 50.5273 | Optimizer: SGD\n",
      "Trial 3 | Epoch 92 | Train Loss: 54.3571 | Val Loss: 50.0982 | Optimizer: SGD\n",
      "Trial 3 | Epoch 93 | Train Loss: 54.1770 | Val Loss: 49.7979 | Optimizer: SGD\n",
      "Trial 3 | Epoch 94 | Train Loss: 50.7895 | Val Loss: 51.7807 | Optimizer: SGD\n",
      "Trial 3 | Epoch 95 | Train Loss: 50.9761 | Val Loss: 49.7724 | Optimizer: SGD\n",
      "Trial 3 | Epoch 96 | Train Loss: 50.4583 | Val Loss: 49.8172 | Optimizer: SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:35,285] Trial 3 finished with value: 49.5200790777439 and parameters: {'gnn_dim': 512, 'hidden_dim': 384, 'dropout_rate': 0.3138445295264759, 'lr': 9.416965155849704e-06, 'activation': 'Swish', 'optimizer': 'SGD', 'momentum': 0.8488845013853628, 'weight_decay': 4.787105377488972e-05}. Best is trial 1 with value: 19.12524386925426.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 | Epoch 97 | Train Loss: 49.7374 | Val Loss: 49.5201 | Optimizer: SGD\n",
      "Trial 3 | Epoch 98 | Train Loss: 53.0369 | Val Loss: 50.0268 | Optimizer: SGD\n",
      "Trial 3 | Epoch 99 | Train Loss: 48.5025 | Val Loss: 49.5308 | Optimizer: SGD\n",
      "Trial 4 | Epoch 01 | Train Loss: 274.8849 | Val Loss: 250.6067 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 02 | Train Loss: 203.9048 | Val Loss: 187.0994 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 03 | Train Loss: 155.2983 | Val Loss: 137.3567 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 04 | Train Loss: 113.0121 | Val Loss: 101.2755 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 05 | Train Loss: 86.2718 | Val Loss: 77.1391 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 06 | Train Loss: 70.0123 | Val Loss: 62.3693 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 07 | Train Loss: 59.9127 | Val Loss: 55.0801 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 08 | Train Loss: 60.0359 | Val Loss: 52.3142 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 09 | Train Loss: 60.3515 | Val Loss: 50.6155 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 10 | Train Loss: 59.8246 | Val Loss: 48.3470 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 11 | Train Loss: 47.4036 | Val Loss: 45.6974 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 12 | Train Loss: 46.4926 | Val Loss: 42.9763 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 13 | Train Loss: 46.6873 | Val Loss: 40.4902 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 14 | Train Loss: 43.1708 | Val Loss: 37.8779 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 15 | Train Loss: 40.1484 | Val Loss: 35.5449 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 16 | Train Loss: 39.8572 | Val Loss: 33.5281 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 17 | Train Loss: 38.2553 | Val Loss: 32.4311 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 18 | Train Loss: 35.8649 | Val Loss: 30.8077 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 19 | Train Loss: 37.4075 | Val Loss: 30.1099 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 20 | Train Loss: 34.3992 | Val Loss: 29.4359 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 21 | Train Loss: 29.6911 | Val Loss: 27.9170 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 22 | Train Loss: 33.7626 | Val Loss: 27.4810 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 23 | Train Loss: 31.4896 | Val Loss: 27.7277 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 24 | Train Loss: 29.5250 | Val Loss: 26.5706 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 25 | Train Loss: 29.9641 | Val Loss: 25.6920 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 26 | Train Loss: 30.1595 | Val Loss: 25.8101 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 27 | Train Loss: 27.0368 | Val Loss: 25.1278 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 28 | Train Loss: 28.2082 | Val Loss: 25.0668 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 29 | Train Loss: 27.3594 | Val Loss: 24.7234 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 30 | Train Loss: 28.5038 | Val Loss: 24.7029 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 31 | Train Loss: 31.5311 | Val Loss: 25.3191 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 32 | Train Loss: 29.5206 | Val Loss: 24.4699 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 33 | Train Loss: 27.7077 | Val Loss: 24.0371 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 34 | Train Loss: 28.9845 | Val Loss: 23.9659 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 35 | Train Loss: 28.3939 | Val Loss: 24.6476 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 36 | Train Loss: 29.7639 | Val Loss: 23.9277 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 37 | Train Loss: 25.3473 | Val Loss: 23.7025 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 38 | Train Loss: 26.6142 | Val Loss: 24.4612 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 39 | Train Loss: 27.3504 | Val Loss: 24.8002 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 40 | Train Loss: 27.1802 | Val Loss: 23.5582 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 41 | Train Loss: 25.3976 | Val Loss: 23.4085 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 42 | Train Loss: 25.2089 | Val Loss: 24.4637 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 43 | Train Loss: 28.6375 | Val Loss: 23.2434 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 44 | Train Loss: 25.1128 | Val Loss: 23.1881 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 45 | Train Loss: 25.5255 | Val Loss: 23.0524 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 46 | Train Loss: 21.5012 | Val Loss: 23.2956 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 47 | Train Loss: 24.8880 | Val Loss: 22.6686 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 48 | Train Loss: 25.5915 | Val Loss: 23.2275 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 49 | Train Loss: 28.1693 | Val Loss: 24.3112 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 50 | Train Loss: 25.6562 | Val Loss: 22.5662 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 51 | Train Loss: 27.8184 | Val Loss: 22.8946 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 52 | Train Loss: 26.4409 | Val Loss: 23.9975 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 53 | Train Loss: 26.6916 | Val Loss: 23.1146 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 54 | Train Loss: 27.8271 | Val Loss: 22.5626 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 55 | Train Loss: 27.5669 | Val Loss: 22.4813 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 56 | Train Loss: 26.5829 | Val Loss: 22.7435 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 57 | Train Loss: 26.5006 | Val Loss: 22.9647 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 58 | Train Loss: 25.5526 | Val Loss: 22.5655 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 59 | Train Loss: 24.9181 | Val Loss: 22.7028 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 60 | Train Loss: 24.1971 | Val Loss: 22.5168 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 61 | Train Loss: 26.5197 | Val Loss: 22.1714 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 62 | Train Loss: 21.9949 | Val Loss: 22.4077 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 63 | Train Loss: 24.1920 | Val Loss: 22.1168 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 64 | Train Loss: 25.4178 | Val Loss: 21.8104 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 65 | Train Loss: 25.8073 | Val Loss: 22.0128 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 66 | Train Loss: 23.7500 | Val Loss: 22.2152 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 67 | Train Loss: 25.7461 | Val Loss: 22.0460 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 68 | Train Loss: 24.1905 | Val Loss: 21.8453 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 69 | Train Loss: 22.6056 | Val Loss: 21.2281 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 70 | Train Loss: 26.7220 | Val Loss: 23.3148 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 71 | Train Loss: 23.7353 | Val Loss: 21.6334 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 72 | Train Loss: 24.0247 | Val Loss: 21.7561 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 73 | Train Loss: 23.4325 | Val Loss: 22.4043 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 74 | Train Loss: 24.4011 | Val Loss: 21.6501 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 75 | Train Loss: 24.9280 | Val Loss: 21.7694 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 76 | Train Loss: 22.6077 | Val Loss: 22.7654 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 77 | Train Loss: 22.2205 | Val Loss: 21.4050 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:39,364] Trial 4 finished with value: 21.228072864253345 and parameters: {'gnn_dim': 384, 'hidden_dim': 512, 'dropout_rate': 0.3576019968562552, 'lr': 4.529522026341851e-05, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 4.047981477928513e-06}. Best is trial 1 with value: 19.12524386925426.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 | Epoch 78 | Train Loss: 21.8249 | Val Loss: 21.4467 | Optimizer: AdamW\n",
      "Trial 4 | Epoch 79 | Train Loss: 23.0991 | Val Loss: 22.1380 | Optimizer: AdamW\n",
      "Trial 4 - Early stopping triggered at epoch 79\n",
      "Trial 5 | Epoch 01 | Train Loss: 274.1851 | Val Loss: 286.3472 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 02 | Train Loss: 262.1599 | Val Loss: 268.4985 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 03 | Train Loss: 242.7676 | Val Loss: 251.4922 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 04 | Train Loss: 229.7648 | Val Loss: 235.2606 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 05 | Train Loss: 213.3568 | Val Loss: 219.8172 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 06 | Train Loss: 205.8055 | Val Loss: 205.0578 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 07 | Train Loss: 179.5683 | Val Loss: 191.1358 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 08 | Train Loss: 171.5837 | Val Loss: 178.0267 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 09 | Train Loss: 162.2439 | Val Loss: 165.6020 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 10 | Train Loss: 153.4418 | Val Loss: 153.8042 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 11 | Train Loss: 141.9758 | Val Loss: 142.8123 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 12 | Train Loss: 131.6783 | Val Loss: 132.3892 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 13 | Train Loss: 122.4468 | Val Loss: 122.3129 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 14 | Train Loss: 112.7177 | Val Loss: 112.6443 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 15 | Train Loss: 102.4348 | Val Loss: 103.4130 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 16 | Train Loss: 96.0186 | Val Loss: 94.4201 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 17 | Train Loss: 87.7870 | Val Loss: 85.7643 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 18 | Train Loss: 81.2017 | Val Loss: 77.3852 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 19 | Train Loss: 75.3615 | Val Loss: 69.4440 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 20 | Train Loss: 67.5435 | Val Loss: 62.0079 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 21 | Train Loss: 61.3553 | Val Loss: 55.4380 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 22 | Train Loss: 54.1254 | Val Loss: 49.6877 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 23 | Train Loss: 47.3364 | Val Loss: 44.8643 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 24 | Train Loss: 48.2909 | Val Loss: 41.6173 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 25 | Train Loss: 44.2898 | Val Loss: 39.5836 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 26 | Train Loss: 44.4901 | Val Loss: 38.2803 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 27 | Train Loss: 42.3730 | Val Loss: 37.5678 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 28 | Train Loss: 40.4784 | Val Loss: 37.0130 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 29 | Train Loss: 40.8389 | Val Loss: 36.5213 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 30 | Train Loss: 39.4197 | Val Loss: 36.0546 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 31 | Train Loss: 41.9637 | Val Loss: 35.6279 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 32 | Train Loss: 41.7323 | Val Loss: 35.1988 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 33 | Train Loss: 41.5564 | Val Loss: 34.8608 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 34 | Train Loss: 39.0015 | Val Loss: 34.5022 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 35 | Train Loss: 38.7227 | Val Loss: 34.0404 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 36 | Train Loss: 35.2137 | Val Loss: 33.5005 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 37 | Train Loss: 38.3661 | Val Loss: 33.0333 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 38 | Train Loss: 39.7785 | Val Loss: 32.6464 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 39 | Train Loss: 37.6772 | Val Loss: 32.2757 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 40 | Train Loss: 39.6543 | Val Loss: 32.0076 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 41 | Train Loss: 35.1751 | Val Loss: 31.6181 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 42 | Train Loss: 37.0983 | Val Loss: 31.2279 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 43 | Train Loss: 31.5972 | Val Loss: 30.8902 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 44 | Train Loss: 35.7615 | Val Loss: 30.5536 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 45 | Train Loss: 33.3166 | Val Loss: 30.1999 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 46 | Train Loss: 35.2931 | Val Loss: 29.9029 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 47 | Train Loss: 37.1988 | Val Loss: 29.6978 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 48 | Train Loss: 32.8566 | Val Loss: 29.5045 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 49 | Train Loss: 36.6461 | Val Loss: 29.2251 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 50 | Train Loss: 31.6970 | Val Loss: 28.9092 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 51 | Train Loss: 32.9054 | Val Loss: 28.6123 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 52 | Train Loss: 31.3667 | Val Loss: 28.3468 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 53 | Train Loss: 31.6841 | Val Loss: 28.0813 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 54 | Train Loss: 29.9770 | Val Loss: 27.8644 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 55 | Train Loss: 33.0756 | Val Loss: 27.7113 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 56 | Train Loss: 32.8179 | Val Loss: 27.4535 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 57 | Train Loss: 33.3328 | Val Loss: 27.3863 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 58 | Train Loss: 31.2042 | Val Loss: 27.2262 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 59 | Train Loss: 32.6621 | Val Loss: 27.1130 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 60 | Train Loss: 31.8106 | Val Loss: 26.8428 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 61 | Train Loss: 32.2200 | Val Loss: 26.7513 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 62 | Train Loss: 31.2220 | Val Loss: 26.6793 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 63 | Train Loss: 30.2669 | Val Loss: 26.5280 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 64 | Train Loss: 30.7005 | Val Loss: 26.1852 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 65 | Train Loss: 29.5838 | Val Loss: 25.9481 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 66 | Train Loss: 31.3837 | Val Loss: 25.8283 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 67 | Train Loss: 31.6819 | Val Loss: 25.7748 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 68 | Train Loss: 30.9467 | Val Loss: 25.7501 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 69 | Train Loss: 28.7995 | Val Loss: 25.6714 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 70 | Train Loss: 28.8531 | Val Loss: 25.3362 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 71 | Train Loss: 30.5451 | Val Loss: 25.2136 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 72 | Train Loss: 28.2730 | Val Loss: 25.1887 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 73 | Train Loss: 29.2846 | Val Loss: 25.1865 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 74 | Train Loss: 30.4555 | Val Loss: 25.1553 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 75 | Train Loss: 28.4882 | Val Loss: 25.1061 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 76 | Train Loss: 30.2408 | Val Loss: 25.0595 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 77 | Train Loss: 30.8237 | Val Loss: 25.0049 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 78 | Train Loss: 28.7291 | Val Loss: 24.8694 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 79 | Train Loss: 28.5892 | Val Loss: 24.7036 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 80 | Train Loss: 27.6104 | Val Loss: 24.6169 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 81 | Train Loss: 26.5727 | Val Loss: 24.6432 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 82 | Train Loss: 28.2599 | Val Loss: 24.4706 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 83 | Train Loss: 28.8304 | Val Loss: 24.3319 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 84 | Train Loss: 28.1511 | Val Loss: 24.3223 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 85 | Train Loss: 28.9478 | Val Loss: 24.2461 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 86 | Train Loss: 27.9684 | Val Loss: 24.2053 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 87 | Train Loss: 27.3153 | Val Loss: 24.1854 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 88 | Train Loss: 28.6194 | Val Loss: 24.3576 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 89 | Train Loss: 26.9734 | Val Loss: 24.3805 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 90 | Train Loss: 30.4135 | Val Loss: 24.1530 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 91 | Train Loss: 27.9535 | Val Loss: 24.0035 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 92 | Train Loss: 28.6702 | Val Loss: 23.9173 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 93 | Train Loss: 27.1423 | Val Loss: 23.9374 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 94 | Train Loss: 27.6189 | Val Loss: 23.9250 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 95 | Train Loss: 26.6775 | Val Loss: 24.1033 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 96 | Train Loss: 28.6880 | Val Loss: 24.0482 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:44,499] Trial 5 finished with value: 23.573560389076793 and parameters: {'gnn_dim': 384, 'hidden_dim': 384, 'dropout_rate': 0.32672051933279234, 'lr': 1.5108839485547171e-05, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 1.3791000051589796e-06}. Best is trial 1 with value: 19.12524386925426.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 | Epoch 97 | Train Loss: 27.4146 | Val Loss: 23.8159 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 98 | Train Loss: 27.2269 | Val Loss: 23.6783 | Optimizer: AdamW\n",
      "Trial 5 | Epoch 99 | Train Loss: 27.9951 | Val Loss: 23.5736 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:44,659] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 | Epoch 01 | Train Loss: 218.9954 | Val Loss: 195.2109 | Optimizer: RMSprop\n",
      "Trial 6 | Epoch 02 | Train Loss: 159.5498 | Val Loss: 145.2901 | Optimizer: RMSprop\n",
      "Trial 7 | Epoch 01 | Train Loss: 169597.3636 | Val Loss: 38450.7725 | Optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:44,806] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 | Epoch 01 | Train Loss: 146.1666 | Val Loss: 70.8574 | Optimizer: SGD\n",
      "Trial 8 | Epoch 02 | Train Loss: 91.9746 | Val Loss: 63.2584 | Optimizer: SGD\n",
      "Trial 8 | Epoch 03 | Train Loss: 78.7989 | Val Loss: 87.3274 | Optimizer: SGD\n",
      "Trial 8 | Epoch 04 | Train Loss: 72.6391 | Val Loss: 62.0426 | Optimizer: SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:45,261] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 | Epoch 05 | Train Loss: 65.3659 | Val Loss: 71.0057 | Optimizer: SGD\n",
      "Trial 8 | Epoch 06 | Train Loss: 65.6558 | Val Loss: 61.8529 | Optimizer: SGD\n",
      "Trial 8 | Epoch 07 | Train Loss: 61.0258 | Val Loss: 66.8584 | Optimizer: SGD\n",
      "Trial 9 | Epoch 01 | Train Loss: 286.0283 | Val Loss: 312.5605 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:45,363] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 | Epoch 01 | Train Loss: 247.0558 | Val Loss: 183.3896 | Optimizer: Adam\n",
      "Trial 10 | Epoch 02 | Train Loss: 137.0077 | Val Loss: 91.8933 | Optimizer: Adam\n",
      "Trial 10 | Epoch 03 | Train Loss: 65.4880 | Val Loss: 41.6377 | Optimizer: Adam\n",
      "Trial 10 | Epoch 04 | Train Loss: 48.9357 | Val Loss: 45.9949 | Optimizer: Adam\n",
      "Trial 10 | Epoch 05 | Train Loss: 50.7239 | Val Loss: 43.0677 | Optimizer: Adam\n",
      "Trial 10 | Epoch 06 | Train Loss: 45.1726 | Val Loss: 43.3766 | Optimizer: Adam\n",
      "Trial 10 | Epoch 07 | Train Loss: 42.3201 | Val Loss: 37.9757 | Optimizer: Adam\n",
      "Trial 10 | Epoch 08 | Train Loss: 43.2476 | Val Loss: 33.6710 | Optimizer: Adam\n",
      "Trial 10 | Epoch 09 | Train Loss: 36.8748 | Val Loss: 32.9591 | Optimizer: Adam\n",
      "Trial 10 | Epoch 10 | Train Loss: 33.4974 | Val Loss: 29.8157 | Optimizer: Adam\n",
      "Trial 10 | Epoch 11 | Train Loss: 33.3939 | Val Loss: 28.1789 | Optimizer: Adam\n",
      "Trial 10 | Epoch 12 | Train Loss: 29.6911 | Val Loss: 28.5306 | Optimizer: Adam\n",
      "Trial 10 | Epoch 13 | Train Loss: 28.3842 | Val Loss: 25.9315 | Optimizer: Adam\n",
      "Trial 10 | Epoch 14 | Train Loss: 28.6785 | Val Loss: 28.2926 | Optimizer: Adam\n",
      "Trial 10 | Epoch 15 | Train Loss: 27.7199 | Val Loss: 24.7737 | Optimizer: Adam\n",
      "Trial 10 | Epoch 16 | Train Loss: 27.5957 | Val Loss: 24.4309 | Optimizer: Adam\n",
      "Trial 10 | Epoch 17 | Train Loss: 26.2832 | Val Loss: 24.6324 | Optimizer: Adam\n",
      "Trial 10 | Epoch 18 | Train Loss: 25.5614 | Val Loss: 23.3359 | Optimizer: Adam\n",
      "Trial 10 | Epoch 19 | Train Loss: 27.7075 | Val Loss: 23.9620 | Optimizer: Adam\n",
      "Trial 10 | Epoch 20 | Train Loss: 24.5432 | Val Loss: 22.9045 | Optimizer: Adam\n",
      "Trial 10 | Epoch 21 | Train Loss: 23.6515 | Val Loss: 24.6576 | Optimizer: Adam\n",
      "Trial 10 | Epoch 22 | Train Loss: 26.0091 | Val Loss: 22.3968 | Optimizer: Adam\n",
      "Trial 10 | Epoch 23 | Train Loss: 25.4513 | Val Loss: 25.8254 | Optimizer: Adam\n",
      "Trial 10 | Epoch 24 | Train Loss: 26.3251 | Val Loss: 22.0111 | Optimizer: Adam\n",
      "Trial 10 | Epoch 25 | Train Loss: 26.5837 | Val Loss: 24.5243 | Optimizer: Adam\n",
      "Trial 10 | Epoch 26 | Train Loss: 25.5162 | Val Loss: 21.8528 | Optimizer: Adam\n",
      "Trial 10 | Epoch 27 | Train Loss: 23.5311 | Val Loss: 22.9429 | Optimizer: Adam\n",
      "Trial 10 | Epoch 28 | Train Loss: 27.8483 | Val Loss: 22.0377 | Optimizer: Adam\n",
      "Trial 10 | Epoch 29 | Train Loss: 22.9513 | Val Loss: 22.0304 | Optimizer: Adam\n",
      "Trial 10 | Epoch 30 | Train Loss: 25.1770 | Val Loss: 22.4196 | Optimizer: Adam\n",
      "Trial 10 | Epoch 31 | Train Loss: 21.6061 | Val Loss: 21.8976 | Optimizer: Adam\n",
      "Trial 10 | Epoch 32 | Train Loss: 23.4899 | Val Loss: 21.5678 | Optimizer: Adam\n",
      "Trial 10 | Epoch 33 | Train Loss: 25.1458 | Val Loss: 21.9515 | Optimizer: Adam\n",
      "Trial 10 | Epoch 34 | Train Loss: 23.8798 | Val Loss: 21.8629 | Optimizer: Adam\n",
      "Trial 10 | Epoch 35 | Train Loss: 21.0762 | Val Loss: 21.4083 | Optimizer: Adam\n",
      "Trial 10 | Epoch 36 | Train Loss: 22.0879 | Val Loss: 20.4630 | Optimizer: Adam\n",
      "Trial 10 | Epoch 37 | Train Loss: 23.2492 | Val Loss: 22.1691 | Optimizer: Adam\n",
      "Trial 10 | Epoch 38 | Train Loss: 21.0941 | Val Loss: 20.2305 | Optimizer: Adam\n",
      "Trial 10 | Epoch 39 | Train Loss: 22.8531 | Val Loss: 20.9551 | Optimizer: Adam\n",
      "Trial 10 | Epoch 40 | Train Loss: 23.3441 | Val Loss: 20.6570 | Optimizer: Adam\n",
      "Trial 10 | Epoch 41 | Train Loss: 21.9029 | Val Loss: 21.1386 | Optimizer: Adam\n",
      "Trial 10 | Epoch 42 | Train Loss: 22.1521 | Val Loss: 20.0609 | Optimizer: Adam\n",
      "Trial 10 | Epoch 43 | Train Loss: 20.7572 | Val Loss: 19.3085 | Optimizer: Adam\n",
      "Trial 10 | Epoch 44 | Train Loss: 22.8144 | Val Loss: 22.2578 | Optimizer: Adam\n",
      "Trial 10 | Epoch 45 | Train Loss: 22.2974 | Val Loss: 20.0500 | Optimizer: Adam\n",
      "Trial 10 | Epoch 46 | Train Loss: 21.4922 | Val Loss: 19.9362 | Optimizer: Adam\n",
      "Trial 10 | Epoch 47 | Train Loss: 22.1626 | Val Loss: 19.4754 | Optimizer: Adam\n",
      "Trial 10 | Epoch 48 | Train Loss: 20.6689 | Val Loss: 23.2267 | Optimizer: Adam\n",
      "Trial 10 | Epoch 49 | Train Loss: 23.5461 | Val Loss: 18.4669 | Optimizer: Adam\n",
      "Trial 10 | Epoch 50 | Train Loss: 21.3059 | Val Loss: 21.2689 | Optimizer: Adam\n",
      "Trial 10 | Epoch 51 | Train Loss: 21.4444 | Val Loss: 18.4802 | Optimizer: Adam\n",
      "Trial 10 | Epoch 52 | Train Loss: 22.0092 | Val Loss: 22.6475 | Optimizer: Adam\n",
      "Trial 10 | Epoch 53 | Train Loss: 21.0418 | Val Loss: 18.2965 | Optimizer: Adam\n",
      "Trial 10 | Epoch 54 | Train Loss: 20.7963 | Val Loss: 21.3135 | Optimizer: Adam\n",
      "Trial 10 | Epoch 55 | Train Loss: 19.2936 | Val Loss: 18.1524 | Optimizer: Adam\n",
      "Trial 10 | Epoch 56 | Train Loss: 22.7348 | Val Loss: 27.2957 | Optimizer: Adam\n",
      "Trial 10 | Epoch 57 | Train Loss: 23.0719 | Val Loss: 17.9070 | Optimizer: Adam\n",
      "Trial 10 | Epoch 58 | Train Loss: 22.3999 | Val Loss: 20.7508 | Optimizer: Adam\n",
      "Trial 10 | Epoch 59 | Train Loss: 18.9195 | Val Loss: 18.7306 | Optimizer: Adam\n",
      "Trial 10 | Epoch 60 | Train Loss: 19.8871 | Val Loss: 19.6539 | Optimizer: Adam\n",
      "Trial 10 | Epoch 61 | Train Loss: 20.0357 | Val Loss: 18.2866 | Optimizer: Adam\n",
      "Trial 10 | Epoch 62 | Train Loss: 20.4730 | Val Loss: 19.5770 | Optimizer: Adam\n",
      "Trial 10 | Epoch 63 | Train Loss: 18.8657 | Val Loss: 17.7327 | Optimizer: Adam\n",
      "Trial 10 | Epoch 64 | Train Loss: 20.9420 | Val Loss: 18.3314 | Optimizer: Adam\n",
      "Trial 10 | Epoch 65 | Train Loss: 19.0868 | Val Loss: 21.5110 | Optimizer: Adam\n",
      "Trial 10 | Epoch 66 | Train Loss: 22.7945 | Val Loss: 17.2551 | Optimizer: Adam\n",
      "Trial 10 | Epoch 67 | Train Loss: 21.8445 | Val Loss: 23.9700 | Optimizer: Adam\n",
      "Trial 10 | Epoch 68 | Train Loss: 19.8474 | Val Loss: 17.3454 | Optimizer: Adam\n",
      "Trial 10 | Epoch 69 | Train Loss: 17.8173 | Val Loss: 20.5003 | Optimizer: Adam\n",
      "Trial 10 | Epoch 70 | Train Loss: 19.2134 | Val Loss: 17.1920 | Optimizer: Adam\n",
      "Trial 10 | Epoch 71 | Train Loss: 18.2699 | Val Loss: 20.3448 | Optimizer: Adam\n",
      "Trial 10 | Epoch 72 | Train Loss: 19.0925 | Val Loss: 17.0943 | Optimizer: Adam\n",
      "Trial 10 | Epoch 73 | Train Loss: 17.5907 | Val Loss: 17.9896 | Optimizer: Adam\n",
      "Trial 10 | Epoch 74 | Train Loss: 16.1037 | Val Loss: 18.4878 | Optimizer: Adam\n",
      "Trial 10 | Epoch 75 | Train Loss: 18.8856 | Val Loss: 17.2936 | Optimizer: Adam\n",
      "Trial 10 | Epoch 76 | Train Loss: 16.9007 | Val Loss: 16.7610 | Optimizer: Adam\n",
      "Trial 10 | Epoch 77 | Train Loss: 17.5916 | Val Loss: 18.4044 | Optimizer: Adam\n",
      "Trial 10 | Epoch 78 | Train Loss: 18.5338 | Val Loss: 16.3324 | Optimizer: Adam\n",
      "Trial 10 | Epoch 79 | Train Loss: 17.0988 | Val Loss: 18.0811 | Optimizer: Adam\n",
      "Trial 10 | Epoch 80 | Train Loss: 16.2758 | Val Loss: 17.4072 | Optimizer: Adam\n",
      "Trial 10 | Epoch 81 | Train Loss: 17.6584 | Val Loss: 16.5339 | Optimizer: Adam\n",
      "Trial 10 | Epoch 82 | Train Loss: 15.9656 | Val Loss: 16.3291 | Optimizer: Adam\n",
      "Trial 10 | Epoch 83 | Train Loss: 17.2659 | Val Loss: 16.9280 | Optimizer: Adam\n",
      "Trial 10 | Epoch 84 | Train Loss: 18.0481 | Val Loss: 19.6906 | Optimizer: Adam\n",
      "Trial 10 | Epoch 85 | Train Loss: 16.7499 | Val Loss: 16.2020 | Optimizer: Adam\n",
      "Trial 10 | Epoch 86 | Train Loss: 18.2795 | Val Loss: 16.8756 | Optimizer: Adam\n",
      "Trial 10 | Epoch 87 | Train Loss: 17.2253 | Val Loss: 17.4761 | Optimizer: Adam\n",
      "Trial 10 | Epoch 88 | Train Loss: 15.4553 | Val Loss: 19.1663 | Optimizer: Adam\n",
      "Trial 10 | Epoch 89 | Train Loss: 17.0699 | Val Loss: 15.7913 | Optimizer: Adam\n",
      "Trial 10 | Epoch 90 | Train Loss: 17.8894 | Val Loss: 20.2862 | Optimizer: Adam\n",
      "Trial 10 | Epoch 91 | Train Loss: 17.1947 | Val Loss: 18.1085 | Optimizer: Adam\n",
      "Trial 10 | Epoch 92 | Train Loss: 16.0256 | Val Loss: 16.6436 | Optimizer: Adam\n",
      "Trial 10 | Epoch 93 | Train Loss: 17.5795 | Val Loss: 16.9605 | Optimizer: Adam\n",
      "Trial 10 | Epoch 94 | Train Loss: 16.6108 | Val Loss: 16.2507 | Optimizer: Adam\n",
      "Trial 10 | Epoch 95 | Train Loss: 16.7276 | Val Loss: 15.9943 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:50,461] Trial 10 finished with value: 15.720193653571897 and parameters: {'gnn_dim': 384, 'hidden_dim': 256, 'dropout_rate': 0.2514117597604075, 'lr': 0.00019778176533884577, 'activation': 'Swish', 'optimizer': 'Adam', 'weight_decay': 1.698272497464436e-06}. Best is trial 10 with value: 15.720193653571897.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 | Epoch 96 | Train Loss: 14.7269 | Val Loss: 19.2345 | Optimizer: Adam\n",
      "Trial 10 | Epoch 97 | Train Loss: 16.3123 | Val Loss: 15.7202 | Optimizer: Adam\n",
      "Trial 10 | Epoch 98 | Train Loss: 15.9641 | Val Loss: 16.6215 | Optimizer: Adam\n",
      "Trial 10 | Epoch 99 | Train Loss: 15.0904 | Val Loss: 19.3349 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:50,574] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 | Epoch 01 | Train Loss: 273.4700 | Val Loss: 227.2870 | Optimizer: Adam\n",
      "Trial 12 | Epoch 01 | Train Loss: 207.0187 | Val Loss: 157.1885 | Optimizer: Adam\n",
      "Trial 12 | Epoch 02 | Train Loss: 118.3893 | Val Loss: 86.2426 | Optimizer: Adam\n",
      "Trial 12 | Epoch 03 | Train Loss: 63.9547 | Val Loss: 43.2925 | Optimizer: Adam\n",
      "Trial 12 | Epoch 04 | Train Loss: 48.6820 | Val Loss: 47.7400 | Optimizer: Adam\n",
      "Trial 12 | Epoch 05 | Train Loss: 53.1459 | Val Loss: 43.1932 | Optimizer: Adam\n",
      "Trial 12 | Epoch 06 | Train Loss: 46.1559 | Val Loss: 44.3746 | Optimizer: Adam\n",
      "Trial 12 | Epoch 07 | Train Loss: 46.2907 | Val Loss: 39.5078 | Optimizer: Adam\n",
      "Trial 12 | Epoch 08 | Train Loss: 40.3251 | Val Loss: 34.1796 | Optimizer: Adam\n",
      "Trial 12 | Epoch 09 | Train Loss: 36.1865 | Val Loss: 32.2539 | Optimizer: Adam\n",
      "Trial 12 | Epoch 10 | Train Loss: 32.8348 | Val Loss: 30.1334 | Optimizer: Adam\n",
      "Trial 12 | Epoch 11 | Train Loss: 31.2848 | Val Loss: 28.0420 | Optimizer: Adam\n",
      "Trial 12 | Epoch 12 | Train Loss: 31.8948 | Val Loss: 27.8195 | Optimizer: Adam\n",
      "Trial 12 | Epoch 13 | Train Loss: 29.4025 | Val Loss: 25.7412 | Optimizer: Adam\n",
      "Trial 12 | Epoch 14 | Train Loss: 29.9136 | Val Loss: 25.2548 | Optimizer: Adam\n",
      "Trial 12 | Epoch 15 | Train Loss: 29.6676 | Val Loss: 26.4881 | Optimizer: Adam\n",
      "Trial 12 | Epoch 16 | Train Loss: 26.8094 | Val Loss: 24.0101 | Optimizer: Adam\n",
      "Trial 12 | Epoch 17 | Train Loss: 27.0066 | Val Loss: 24.2303 | Optimizer: Adam\n",
      "Trial 12 | Epoch 18 | Train Loss: 25.9603 | Val Loss: 24.2402 | Optimizer: Adam\n",
      "Trial 12 | Epoch 19 | Train Loss: 26.0858 | Val Loss: 23.1500 | Optimizer: Adam\n",
      "Trial 12 | Epoch 20 | Train Loss: 24.2203 | Val Loss: 23.8514 | Optimizer: Adam\n",
      "Trial 12 | Epoch 21 | Train Loss: 26.5567 | Val Loss: 23.2298 | Optimizer: Adam\n",
      "Trial 12 | Epoch 22 | Train Loss: 24.9606 | Val Loss: 22.6071 | Optimizer: Adam\n",
      "Trial 12 | Epoch 23 | Train Loss: 25.2804 | Val Loss: 22.1290 | Optimizer: Adam\n",
      "Trial 12 | Epoch 24 | Train Loss: 24.3695 | Val Loss: 23.0968 | Optimizer: Adam\n",
      "Trial 12 | Epoch 25 | Train Loss: 23.4962 | Val Loss: 22.5686 | Optimizer: Adam\n",
      "Trial 12 | Epoch 26 | Train Loss: 21.3659 | Val Loss: 21.6955 | Optimizer: Adam\n",
      "Trial 12 | Epoch 27 | Train Loss: 24.9960 | Val Loss: 23.0369 | Optimizer: Adam\n",
      "Trial 12 | Epoch 28 | Train Loss: 22.6636 | Val Loss: 21.1461 | Optimizer: Adam\n",
      "Trial 12 | Epoch 29 | Train Loss: 26.0215 | Val Loss: 24.4142 | Optimizer: Adam\n",
      "Trial 12 | Epoch 30 | Train Loss: 22.3748 | Val Loss: 20.8475 | Optimizer: Adam\n",
      "Trial 12 | Epoch 31 | Train Loss: 23.7671 | Val Loss: 22.5276 | Optimizer: Adam\n",
      "Trial 12 | Epoch 32 | Train Loss: 21.6255 | Val Loss: 20.7596 | Optimizer: Adam\n",
      "Trial 12 | Epoch 33 | Train Loss: 21.9987 | Val Loss: 23.0998 | Optimizer: Adam\n",
      "Trial 12 | Epoch 34 | Train Loss: 24.6381 | Val Loss: 20.2157 | Optimizer: Adam\n",
      "Trial 12 | Epoch 35 | Train Loss: 22.9052 | Val Loss: 23.0270 | Optimizer: Adam\n",
      "Trial 12 | Epoch 36 | Train Loss: 23.1295 | Val Loss: 19.8666 | Optimizer: Adam\n",
      "Trial 12 | Epoch 37 | Train Loss: 22.6788 | Val Loss: 22.4743 | Optimizer: Adam\n",
      "Trial 12 | Epoch 38 | Train Loss: 21.9182 | Val Loss: 19.6535 | Optimizer: Adam\n",
      "Trial 12 | Epoch 39 | Train Loss: 22.1147 | Val Loss: 22.6139 | Optimizer: Adam\n",
      "Trial 12 | Epoch 40 | Train Loss: 19.1973 | Val Loss: 19.2561 | Optimizer: Adam\n",
      "Trial 12 | Epoch 41 | Train Loss: 22.2998 | Val Loss: 20.5343 | Optimizer: Adam\n",
      "Trial 12 | Epoch 42 | Train Loss: 21.8960 | Val Loss: 22.1093 | Optimizer: Adam\n",
      "Trial 12 | Epoch 43 | Train Loss: 21.7895 | Val Loss: 19.0784 | Optimizer: Adam\n",
      "Trial 12 | Epoch 44 | Train Loss: 24.1225 | Val Loss: 22.4945 | Optimizer: Adam\n",
      "Trial 12 | Epoch 45 | Train Loss: 22.5520 | Val Loss: 18.6363 | Optimizer: Adam\n",
      "Trial 12 | Epoch 46 | Train Loss: 21.8258 | Val Loss: 24.7052 | Optimizer: Adam\n",
      "Trial 12 | Epoch 47 | Train Loss: 22.4729 | Val Loss: 18.6509 | Optimizer: Adam\n",
      "Trial 12 | Epoch 48 | Train Loss: 21.4955 | Val Loss: 22.0865 | Optimizer: Adam\n",
      "Trial 12 | Epoch 49 | Train Loss: 20.7690 | Val Loss: 18.4809 | Optimizer: Adam\n",
      "Trial 12 | Epoch 50 | Train Loss: 21.4248 | Val Loss: 21.9248 | Optimizer: Adam\n",
      "Trial 12 | Epoch 51 | Train Loss: 21.9314 | Val Loss: 18.8683 | Optimizer: Adam\n",
      "Trial 12 | Epoch 52 | Train Loss: 20.4770 | Val Loss: 19.7063 | Optimizer: Adam\n",
      "Trial 12 | Epoch 53 | Train Loss: 19.0099 | Val Loss: 18.1751 | Optimizer: Adam\n",
      "Trial 12 | Epoch 54 | Train Loss: 19.3133 | Val Loss: 21.1989 | Optimizer: Adam\n",
      "Trial 12 | Epoch 55 | Train Loss: 18.7503 | Val Loss: 18.2878 | Optimizer: Adam\n",
      "Trial 12 | Epoch 56 | Train Loss: 20.5462 | Val Loss: 21.0704 | Optimizer: Adam\n",
      "Trial 12 | Epoch 57 | Train Loss: 19.1657 | Val Loss: 17.5871 | Optimizer: Adam\n",
      "Trial 12 | Epoch 58 | Train Loss: 18.3428 | Val Loss: 20.1848 | Optimizer: Adam\n",
      "Trial 12 | Epoch 59 | Train Loss: 20.5249 | Val Loss: 17.3885 | Optimizer: Adam\n",
      "Trial 12 | Epoch 60 | Train Loss: 22.9056 | Val Loss: 19.6729 | Optimizer: Adam\n",
      "Trial 12 | Epoch 61 | Train Loss: 18.0598 | Val Loss: 17.6714 | Optimizer: Adam\n",
      "Trial 12 | Epoch 62 | Train Loss: 18.0148 | Val Loss: 17.8695 | Optimizer: Adam\n",
      "Trial 12 | Epoch 63 | Train Loss: 18.8678 | Val Loss: 19.1379 | Optimizer: Adam\n",
      "Trial 12 | Epoch 64 | Train Loss: 19.1907 | Val Loss: 17.2415 | Optimizer: Adam\n",
      "Trial 12 | Epoch 65 | Train Loss: 19.1805 | Val Loss: 20.0420 | Optimizer: Adam\n",
      "Trial 12 | Epoch 66 | Train Loss: 19.0544 | Val Loss: 16.9785 | Optimizer: Adam\n",
      "Trial 12 | Epoch 67 | Train Loss: 20.0873 | Val Loss: 17.7993 | Optimizer: Adam\n",
      "Trial 12 | Epoch 68 | Train Loss: 18.3946 | Val Loss: 20.4029 | Optimizer: Adam\n",
      "Trial 12 | Epoch 69 | Train Loss: 19.5045 | Val Loss: 17.2020 | Optimizer: Adam\n",
      "Trial 12 | Epoch 70 | Train Loss: 18.5672 | Val Loss: 17.4070 | Optimizer: Adam\n",
      "Trial 12 | Epoch 71 | Train Loss: 18.1831 | Val Loss: 16.8014 | Optimizer: Adam\n",
      "Trial 12 | Epoch 72 | Train Loss: 18.3135 | Val Loss: 16.8983 | Optimizer: Adam\n",
      "Trial 12 | Epoch 73 | Train Loss: 17.8649 | Val Loss: 20.0680 | Optimizer: Adam\n",
      "Trial 12 | Epoch 74 | Train Loss: 18.1909 | Val Loss: 16.3711 | Optimizer: Adam\n",
      "Trial 12 | Epoch 75 | Train Loss: 18.2861 | Val Loss: 16.5589 | Optimizer: Adam\n",
      "Trial 12 | Epoch 76 | Train Loss: 17.8290 | Val Loss: 18.7195 | Optimizer: Adam\n",
      "Trial 12 | Epoch 77 | Train Loss: 17.5102 | Val Loss: 16.8368 | Optimizer: Adam\n",
      "Trial 12 | Epoch 78 | Train Loss: 17.9601 | Val Loss: 16.5354 | Optimizer: Adam\n",
      "Trial 12 | Epoch 79 | Train Loss: 16.9927 | Val Loss: 19.5070 | Optimizer: Adam\n",
      "Trial 12 | Epoch 80 | Train Loss: 18.1717 | Val Loss: 16.1855 | Optimizer: Adam\n",
      "Trial 12 | Epoch 81 | Train Loss: 17.2859 | Val Loss: 22.3601 | Optimizer: Adam\n",
      "Trial 12 | Epoch 82 | Train Loss: 18.6268 | Val Loss: 16.6236 | Optimizer: Adam\n",
      "Trial 12 | Epoch 83 | Train Loss: 17.9550 | Val Loss: 16.7034 | Optimizer: Adam\n",
      "Trial 12 | Epoch 84 | Train Loss: 15.6497 | Val Loss: 17.8907 | Optimizer: Adam\n",
      "Trial 12 | Epoch 85 | Train Loss: 17.8326 | Val Loss: 16.7333 | Optimizer: Adam\n",
      "Trial 12 | Epoch 86 | Train Loss: 16.7822 | Val Loss: 18.0100 | Optimizer: Adam\n",
      "Trial 12 | Epoch 87 | Train Loss: 16.6081 | Val Loss: 17.2340 | Optimizer: Adam\n",
      "Trial 12 | Epoch 88 | Train Loss: 15.6614 | Val Loss: 16.4750 | Optimizer: Adam\n",
      "Trial 12 | Epoch 89 | Train Loss: 18.4835 | Val Loss: 23.2603 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:55,200] Trial 12 finished with value: 16.18551901685513 and parameters: {'gnn_dim': 384, 'hidden_dim': 256, 'dropout_rate': 0.2513605709326827, 'lr': 0.0001794071563571159, 'activation': 'Swish', 'optimizer': 'Adam', 'weight_decay': 2.494254836160617e-06}. Best is trial 10 with value: 15.720193653571897.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12 | Epoch 90 | Train Loss: 18.9459 | Val Loss: 16.9695 | Optimizer: Adam\n",
      "Trial 12 - Early stopping triggered at epoch 90\n",
      "Trial 13 | Epoch 01 | Train Loss: 184.8815 | Val Loss: 117.4734 | Optimizer: Adam\n",
      "Trial 13 | Epoch 02 | Train Loss: 75.4655 | Val Loss: 45.8400 | Optimizer: Adam\n",
      "Trial 13 | Epoch 03 | Train Loss: 55.6180 | Val Loss: 50.8444 | Optimizer: Adam\n",
      "Trial 13 | Epoch 04 | Train Loss: 53.4380 | Val Loss: 49.5472 | Optimizer: Adam\n",
      "Trial 13 | Epoch 05 | Train Loss: 52.4251 | Val Loss: 48.6252 | Optimizer: Adam\n",
      "Trial 13 | Epoch 06 | Train Loss: 46.5366 | Val Loss: 37.0161 | Optimizer: Adam\n",
      "Trial 13 | Epoch 07 | Train Loss: 38.1683 | Val Loss: 31.8221 | Optimizer: Adam\n",
      "Trial 13 | Epoch 08 | Train Loss: 34.2547 | Val Loss: 32.7620 | Optimizer: Adam\n",
      "Trial 13 | Epoch 09 | Train Loss: 28.7709 | Val Loss: 27.6866 | Optimizer: Adam\n",
      "Trial 13 | Epoch 10 | Train Loss: 30.5502 | Val Loss: 32.3006 | Optimizer: Adam\n",
      "Trial 13 | Epoch 11 | Train Loss: 27.3761 | Val Loss: 25.0522 | Optimizer: Adam\n",
      "Trial 13 | Epoch 12 | Train Loss: 29.6320 | Val Loss: 30.0700 | Optimizer: Adam\n",
      "Trial 13 | Epoch 13 | Train Loss: 28.4489 | Val Loss: 23.2048 | Optimizer: Adam\n",
      "Trial 13 | Epoch 14 | Train Loss: 26.2340 | Val Loss: 24.6499 | Optimizer: Adam\n",
      "Trial 13 | Epoch 15 | Train Loss: 24.0124 | Val Loss: 22.4410 | Optimizer: Adam\n",
      "Trial 13 | Epoch 16 | Train Loss: 24.9897 | Val Loss: 24.5722 | Optimizer: Adam\n",
      "Trial 13 | Epoch 17 | Train Loss: 23.5651 | Val Loss: 21.8694 | Optimizer: Adam\n",
      "Trial 13 | Epoch 18 | Train Loss: 23.3893 | Val Loss: 23.2762 | Optimizer: Adam\n",
      "Trial 13 | Epoch 19 | Train Loss: 24.1242 | Val Loss: 21.6961 | Optimizer: Adam\n",
      "Trial 13 | Epoch 20 | Train Loss: 23.7567 | Val Loss: 24.4405 | Optimizer: Adam\n",
      "Trial 13 | Epoch 21 | Train Loss: 23.7855 | Val Loss: 21.0759 | Optimizer: Adam\n",
      "Trial 13 | Epoch 22 | Train Loss: 24.9522 | Val Loss: 21.6170 | Optimizer: Adam\n",
      "Trial 13 | Epoch 23 | Train Loss: 24.7073 | Val Loss: 21.2111 | Optimizer: Adam\n",
      "Trial 13 | Epoch 24 | Train Loss: 21.8686 | Val Loss: 20.8294 | Optimizer: Adam\n",
      "Trial 13 | Epoch 25 | Train Loss: 22.3719 | Val Loss: 20.2288 | Optimizer: Adam\n",
      "Trial 13 | Epoch 26 | Train Loss: 24.8498 | Val Loss: 27.0174 | Optimizer: Adam\n",
      "Trial 13 | Epoch 27 | Train Loss: 26.4268 | Val Loss: 20.2563 | Optimizer: Adam\n",
      "Trial 13 | Epoch 28 | Train Loss: 23.1421 | Val Loss: 25.8118 | Optimizer: Adam\n",
      "Trial 13 | Epoch 29 | Train Loss: 25.2581 | Val Loss: 19.5796 | Optimizer: Adam\n",
      "Trial 13 | Epoch 30 | Train Loss: 21.3576 | Val Loss: 21.3609 | Optimizer: Adam\n",
      "Trial 13 | Epoch 31 | Train Loss: 21.6683 | Val Loss: 19.9724 | Optimizer: Adam\n",
      "Trial 13 | Epoch 32 | Train Loss: 20.5255 | Val Loss: 20.8494 | Optimizer: Adam\n",
      "Trial 13 | Epoch 33 | Train Loss: 21.5756 | Val Loss: 21.2051 | Optimizer: Adam\n",
      "Trial 13 | Epoch 34 | Train Loss: 22.1704 | Val Loss: 19.4227 | Optimizer: Adam\n",
      "Trial 13 | Epoch 35 | Train Loss: 19.8214 | Val Loss: 20.2554 | Optimizer: Adam\n",
      "Trial 13 | Epoch 36 | Train Loss: 19.9707 | Val Loss: 19.5545 | Optimizer: Adam\n",
      "Trial 13 | Epoch 37 | Train Loss: 19.5721 | Val Loss: 18.5130 | Optimizer: Adam\n",
      "Trial 13 | Epoch 38 | Train Loss: 20.0990 | Val Loss: 19.1353 | Optimizer: Adam\n",
      "Trial 13 | Epoch 39 | Train Loss: 20.0530 | Val Loss: 18.8817 | Optimizer: Adam\n",
      "Trial 13 | Epoch 40 | Train Loss: 18.4034 | Val Loss: 20.1053 | Optimizer: Adam\n",
      "Trial 13 | Epoch 41 | Train Loss: 19.9094 | Val Loss: 17.9525 | Optimizer: Adam\n",
      "Trial 13 | Epoch 42 | Train Loss: 18.3652 | Val Loss: 18.2624 | Optimizer: Adam\n",
      "Trial 13 | Epoch 43 | Train Loss: 17.9068 | Val Loss: 16.7875 | Optimizer: Adam\n",
      "Trial 13 | Epoch 44 | Train Loss: 19.1146 | Val Loss: 19.3260 | Optimizer: Adam\n",
      "Trial 13 | Epoch 45 | Train Loss: 18.7528 | Val Loss: 21.4437 | Optimizer: Adam\n",
      "Trial 13 | Epoch 46 | Train Loss: 20.3012 | Val Loss: 18.2700 | Optimizer: Adam\n",
      "Trial 13 | Epoch 47 | Train Loss: 18.3941 | Val Loss: 16.2687 | Optimizer: Adam\n",
      "Trial 13 | Epoch 48 | Train Loss: 19.9274 | Val Loss: 16.5611 | Optimizer: Adam\n",
      "Trial 13 | Epoch 49 | Train Loss: 16.8216 | Val Loss: 17.4880 | Optimizer: Adam\n",
      "Trial 13 | Epoch 50 | Train Loss: 18.7552 | Val Loss: 18.5188 | Optimizer: Adam\n",
      "Trial 13 | Epoch 51 | Train Loss: 17.5691 | Val Loss: 17.9548 | Optimizer: Adam\n",
      "Trial 13 | Epoch 52 | Train Loss: 17.5590 | Val Loss: 22.2752 | Optimizer: Adam\n",
      "Trial 13 | Epoch 53 | Train Loss: 18.6700 | Val Loss: 16.5983 | Optimizer: Adam\n",
      "Trial 13 | Epoch 54 | Train Loss: 17.6363 | Val Loss: 19.1803 | Optimizer: Adam\n",
      "Trial 13 | Epoch 55 | Train Loss: 16.1787 | Val Loss: 17.7818 | Optimizer: Adam\n",
      "Trial 13 | Epoch 56 | Train Loss: 17.1060 | Val Loss: 16.8593 | Optimizer: Adam\n",
      "Trial 13 | Epoch 57 | Train Loss: 16.5599 | Val Loss: 16.5384 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:19:58,121] Trial 13 finished with value: 16.268691442846283 and parameters: {'gnn_dim': 384, 'hidden_dim': 256, 'dropout_rate': 0.2539114663014676, 'lr': 0.00029119051158418147, 'activation': 'Swish', 'optimizer': 'Adam', 'weight_decay': 2.3133401851093756e-06}. Best is trial 10 with value: 15.720193653571897.\n",
      "[I 2025-09-04 21:19:58,233] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 - Early stopping triggered at epoch 57\n",
      "Trial 14 | Epoch 01 | Train Loss: 245.0472 | Val Loss: 249.3150 | Optimizer: Adam\n",
      "Trial 15 | Epoch 01 | Train Loss: 215.5168 | Val Loss: 108.8808 | Optimizer: Adam\n",
      "Trial 15 | Epoch 02 | Train Loss: 67.2764 | Val Loss: 28.1131 | Optimizer: Adam\n",
      "Trial 15 | Epoch 03 | Train Loss: 42.0054 | Val Loss: 57.2744 | Optimizer: Adam\n",
      "Trial 15 | Epoch 04 | Train Loss: 43.8464 | Val Loss: 30.6711 | Optimizer: Adam\n",
      "Trial 15 | Epoch 05 | Train Loss: 40.7647 | Val Loss: 30.4531 | Optimizer: Adam\n",
      "Trial 15 | Epoch 06 | Train Loss: 36.3582 | Val Loss: 38.3880 | Optimizer: Adam\n",
      "Trial 15 | Epoch 07 | Train Loss: 36.6181 | Val Loss: 27.9633 | Optimizer: Adam\n",
      "Trial 15 | Epoch 08 | Train Loss: 29.4481 | Val Loss: 27.6787 | Optimizer: Adam\n",
      "Trial 15 | Epoch 09 | Train Loss: 30.3460 | Val Loss: 27.4274 | Optimizer: Adam\n",
      "Trial 15 | Epoch 10 | Train Loss: 27.6890 | Val Loss: 27.2128 | Optimizer: Adam\n",
      "Trial 15 | Epoch 11 | Train Loss: 28.0304 | Val Loss: 24.8672 | Optimizer: Adam\n",
      "Trial 15 | Epoch 12 | Train Loss: 25.4551 | Val Loss: 27.2024 | Optimizer: Adam\n",
      "Trial 15 | Epoch 13 | Train Loss: 26.8658 | Val Loss: 24.0477 | Optimizer: Adam\n",
      "Trial 15 | Epoch 14 | Train Loss: 27.0634 | Val Loss: 23.1812 | Optimizer: Adam\n",
      "Trial 15 | Epoch 15 | Train Loss: 23.5255 | Val Loss: 23.4287 | Optimizer: Adam\n",
      "Trial 15 | Epoch 16 | Train Loss: 25.1247 | Val Loss: 25.6778 | Optimizer: Adam\n",
      "Trial 15 | Epoch 17 | Train Loss: 26.0352 | Val Loss: 22.5444 | Optimizer: Adam\n",
      "Trial 15 | Epoch 18 | Train Loss: 24.7338 | Val Loss: 22.3957 | Optimizer: Adam\n",
      "Trial 15 | Epoch 19 | Train Loss: 26.7108 | Val Loss: 25.9716 | Optimizer: Adam\n",
      "Trial 15 | Epoch 20 | Train Loss: 24.1001 | Val Loss: 23.4292 | Optimizer: Adam\n",
      "Trial 15 | Epoch 21 | Train Loss: 25.4622 | Val Loss: 21.6713 | Optimizer: Adam\n",
      "Trial 15 | Epoch 22 | Train Loss: 24.2890 | Val Loss: 21.6025 | Optimizer: Adam\n",
      "Trial 15 | Epoch 23 | Train Loss: 25.5080 | Val Loss: 20.2697 | Optimizer: Adam\n",
      "Trial 15 | Epoch 24 | Train Loss: 23.7557 | Val Loss: 20.7651 | Optimizer: Adam\n",
      "Trial 15 | Epoch 25 | Train Loss: 22.7050 | Val Loss: 20.9525 | Optimizer: Adam\n",
      "Trial 15 | Epoch 26 | Train Loss: 22.2271 | Val Loss: 19.6423 | Optimizer: Adam\n",
      "Trial 15 | Epoch 27 | Train Loss: 21.4283 | Val Loss: 22.1056 | Optimizer: Adam\n",
      "Trial 15 | Epoch 28 | Train Loss: 23.7226 | Val Loss: 26.5598 | Optimizer: Adam\n",
      "Trial 15 | Epoch 29 | Train Loss: 23.1636 | Val Loss: 22.1035 | Optimizer: Adam\n",
      "Trial 15 | Epoch 30 | Train Loss: 22.3907 | Val Loss: 24.6008 | Optimizer: Adam\n",
      "Trial 15 | Epoch 31 | Train Loss: 22.1892 | Val Loss: 19.0327 | Optimizer: Adam\n",
      "Trial 15 | Epoch 32 | Train Loss: 21.5953 | Val Loss: 18.7364 | Optimizer: Adam\n",
      "Trial 15 | Epoch 33 | Train Loss: 22.1772 | Val Loss: 27.7054 | Optimizer: Adam\n",
      "Trial 15 | Epoch 34 | Train Loss: 21.1432 | Val Loss: 22.6463 | Optimizer: Adam\n",
      "Trial 15 | Epoch 35 | Train Loss: 21.3198 | Val Loss: 19.0059 | Optimizer: Adam\n",
      "Trial 15 | Epoch 36 | Train Loss: 20.4852 | Val Loss: 18.6768 | Optimizer: Adam\n",
      "Trial 15 | Epoch 37 | Train Loss: 19.4946 | Val Loss: 17.5544 | Optimizer: Adam\n",
      "Trial 15 | Epoch 38 | Train Loss: 19.9682 | Val Loss: 26.7497 | Optimizer: Adam\n",
      "Trial 15 | Epoch 39 | Train Loss: 20.6220 | Val Loss: 24.9389 | Optimizer: Adam\n",
      "Trial 15 | Epoch 40 | Train Loss: 19.7445 | Val Loss: 21.8264 | Optimizer: Adam\n",
      "Trial 15 | Epoch 41 | Train Loss: 20.9085 | Val Loss: 19.2653 | Optimizer: Adam\n",
      "Trial 15 | Epoch 42 | Train Loss: 18.5444 | Val Loss: 18.8324 | Optimizer: Adam\n",
      "Trial 15 | Epoch 43 | Train Loss: 18.9059 | Val Loss: 18.2301 | Optimizer: Adam\n",
      "Trial 15 | Epoch 44 | Train Loss: 16.2467 | Val Loss: 21.6288 | Optimizer: Adam\n",
      "Trial 15 | Epoch 45 | Train Loss: 18.1892 | Val Loss: 20.9506 | Optimizer: Adam\n",
      "Trial 15 | Epoch 46 | Train Loss: 19.8966 | Val Loss: 18.8276 | Optimizer: Adam\n",
      "Trial 15 | Epoch 47 | Train Loss: 20.8809 | Val Loss: 16.5160 | Optimizer: Adam\n",
      "Trial 15 | Epoch 48 | Train Loss: 18.9973 | Val Loss: 16.3863 | Optimizer: Adam\n",
      "Trial 15 | Epoch 49 | Train Loss: 17.7001 | Val Loss: 16.6344 | Optimizer: Adam\n",
      "Trial 15 | Epoch 50 | Train Loss: 19.0460 | Val Loss: 21.4315 | Optimizer: Adam\n",
      "Trial 15 | Epoch 51 | Train Loss: 19.2972 | Val Loss: 22.8291 | Optimizer: Adam\n",
      "Trial 15 | Epoch 52 | Train Loss: 20.0075 | Val Loss: 19.2937 | Optimizer: Adam\n",
      "Trial 15 | Epoch 53 | Train Loss: 17.4090 | Val Loss: 19.7349 | Optimizer: Adam\n",
      "Trial 15 | Epoch 54 | Train Loss: 17.3879 | Val Loss: 16.1933 | Optimizer: Adam\n",
      "Trial 15 | Epoch 55 | Train Loss: 18.7173 | Val Loss: 16.1132 | Optimizer: Adam\n",
      "Trial 15 | Epoch 56 | Train Loss: 19.4000 | Val Loss: 16.6367 | Optimizer: Adam\n",
      "Trial 15 | Epoch 57 | Train Loss: 20.9627 | Val Loss: 17.3598 | Optimizer: Adam\n",
      "Trial 15 | Epoch 58 | Train Loss: 22.2514 | Val Loss: 17.5277 | Optimizer: Adam\n",
      "Trial 15 | Epoch 59 | Train Loss: 21.7938 | Val Loss: 28.1447 | Optimizer: Adam\n",
      "Trial 15 | Epoch 60 | Train Loss: 19.6484 | Val Loss: 27.8811 | Optimizer: Adam\n",
      "Trial 15 | Epoch 61 | Train Loss: 20.3491 | Val Loss: 18.2283 | Optimizer: Adam\n",
      "Trial 15 | Epoch 62 | Train Loss: 21.7910 | Val Loss: 16.4020 | Optimizer: Adam\n",
      "Trial 15 | Epoch 63 | Train Loss: 19.6702 | Val Loss: 20.0366 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:03,548] Trial 15 finished with value: 16.11321484945654 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.27724349003807336, 'lr': 0.0002580131746406671, 'activation': 'ReLU', 'optimizer': 'Adam', 'weight_decay': 2.615960502236875e-06}. Best is trial 10 with value: 15.720193653571897.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 | Epoch 64 | Train Loss: 17.5063 | Val Loss: 18.7457 | Optimizer: Adam\n",
      "Trial 15 | Epoch 65 | Train Loss: 15.6140 | Val Loss: 16.5408 | Optimizer: Adam\n",
      "Trial 15 - Early stopping triggered at epoch 65\n",
      "Trial 16 | Epoch 01 | Train Loss: 150.9295 | Val Loss: 48.3758 | Optimizer: Adam\n",
      "Trial 16 | Epoch 02 | Train Loss: 53.0746 | Val Loss: 52.2422 | Optimizer: Adam\n",
      "Trial 16 | Epoch 03 | Train Loss: 42.8017 | Val Loss: 34.4713 | Optimizer: Adam\n",
      "Trial 16 | Epoch 04 | Train Loss: 39.9177 | Val Loss: 36.1164 | Optimizer: Adam\n",
      "Trial 16 | Epoch 05 | Train Loss: 36.6691 | Val Loss: 28.8663 | Optimizer: Adam\n",
      "Trial 16 | Epoch 06 | Train Loss: 31.9182 | Val Loss: 30.3957 | Optimizer: Adam\n",
      "Trial 16 | Epoch 07 | Train Loss: 31.1314 | Val Loss: 29.7971 | Optimizer: Adam\n",
      "Trial 16 | Epoch 08 | Train Loss: 28.2577 | Val Loss: 25.6993 | Optimizer: Adam\n",
      "Trial 16 | Epoch 09 | Train Loss: 28.1363 | Val Loss: 25.1343 | Optimizer: Adam\n",
      "Trial 16 | Epoch 10 | Train Loss: 25.5245 | Val Loss: 31.8462 | Optimizer: Adam\n",
      "Trial 16 | Epoch 11 | Train Loss: 26.9245 | Val Loss: 25.9332 | Optimizer: Adam\n",
      "Trial 16 | Epoch 12 | Train Loss: 27.2584 | Val Loss: 23.5904 | Optimizer: Adam\n",
      "Trial 16 | Epoch 13 | Train Loss: 26.0256 | Val Loss: 22.5092 | Optimizer: Adam\n",
      "Trial 16 | Epoch 14 | Train Loss: 23.6330 | Val Loss: 27.4256 | Optimizer: Adam\n",
      "Trial 16 | Epoch 15 | Train Loss: 26.5681 | Val Loss: 26.4402 | Optimizer: Adam\n",
      "Trial 16 | Epoch 16 | Train Loss: 22.0207 | Val Loss: 22.4472 | Optimizer: Adam\n",
      "Trial 16 | Epoch 17 | Train Loss: 24.3305 | Val Loss: 21.3955 | Optimizer: Adam\n",
      "Trial 16 | Epoch 18 | Train Loss: 21.5725 | Val Loss: 21.3183 | Optimizer: Adam\n",
      "Trial 16 | Epoch 19 | Train Loss: 23.7267 | Val Loss: 19.1252 | Optimizer: Adam\n",
      "Trial 16 | Epoch 20 | Train Loss: 20.8899 | Val Loss: 19.4673 | Optimizer: Adam\n",
      "Trial 16 | Epoch 21 | Train Loss: 23.7255 | Val Loss: 18.4802 | Optimizer: Adam\n",
      "Trial 16 | Epoch 22 | Train Loss: 21.0012 | Val Loss: 19.7223 | Optimizer: Adam\n",
      "Trial 16 | Epoch 23 | Train Loss: 20.0556 | Val Loss: 17.5894 | Optimizer: Adam\n",
      "Trial 16 | Epoch 24 | Train Loss: 22.8922 | Val Loss: 17.7974 | Optimizer: Adam\n",
      "Trial 16 | Epoch 25 | Train Loss: 19.9506 | Val Loss: 17.6797 | Optimizer: Adam\n",
      "Trial 16 | Epoch 26 | Train Loss: 21.0527 | Val Loss: 17.0362 | Optimizer: Adam\n",
      "Trial 16 | Epoch 27 | Train Loss: 20.4821 | Val Loss: 17.9202 | Optimizer: Adam\n",
      "Trial 16 | Epoch 28 | Train Loss: 22.2279 | Val Loss: 17.8872 | Optimizer: Adam\n",
      "Trial 16 | Epoch 29 | Train Loss: 22.0784 | Val Loss: 17.2305 | Optimizer: Adam\n",
      "Trial 16 | Epoch 30 | Train Loss: 20.6046 | Val Loss: 24.2771 | Optimizer: Adam\n",
      "Trial 16 | Epoch 31 | Train Loss: 20.2030 | Val Loss: 18.9494 | Optimizer: Adam\n",
      "Trial 16 | Epoch 32 | Train Loss: 22.3652 | Val Loss: 19.1802 | Optimizer: Adam\n",
      "Trial 16 | Epoch 33 | Train Loss: 18.6105 | Val Loss: 17.9696 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:06,382] Trial 16 finished with value: 17.036176603984057 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.2750336719354182, 'lr': 0.00038829019374009174, 'activation': 'ReLU', 'optimizer': 'Adam', 'weight_decay': 1.0260527049464841e-06}. Best is trial 10 with value: 15.720193653571897.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 | Epoch 34 | Train Loss: 21.0741 | Val Loss: 24.9628 | Optimizer: Adam\n",
      "Trial 16 | Epoch 35 | Train Loss: 20.5053 | Val Loss: 20.8151 | Optimizer: Adam\n",
      "Trial 16 | Epoch 36 | Train Loss: 18.3694 | Val Loss: 17.7535 | Optimizer: Adam\n",
      "Trial 16 - Early stopping triggered at epoch 36\n",
      "Trial 17 | Epoch 01 | Train Loss: 210.9694 | Val Loss: 93.0766 | Optimizer: Adam\n",
      "Trial 17 | Epoch 02 | Train Loss: 59.7888 | Val Loss: 30.9824 | Optimizer: Adam\n",
      "Trial 17 | Epoch 03 | Train Loss: 41.1916 | Val Loss: 48.5540 | Optimizer: Adam\n",
      "Trial 17 | Epoch 04 | Train Loss: 37.3296 | Val Loss: 29.7875 | Optimizer: Adam\n",
      "Trial 17 | Epoch 05 | Train Loss: 34.8008 | Val Loss: 40.3234 | Optimizer: Adam\n",
      "Trial 17 | Epoch 06 | Train Loss: 35.7716 | Val Loss: 26.7342 | Optimizer: Adam\n",
      "Trial 17 | Epoch 07 | Train Loss: 29.8446 | Val Loss: 30.5115 | Optimizer: Adam\n",
      "Trial 17 | Epoch 08 | Train Loss: 31.4780 | Val Loss: 25.2862 | Optimizer: Adam\n",
      "Trial 17 | Epoch 09 | Train Loss: 29.0616 | Val Loss: 30.6298 | Optimizer: Adam\n",
      "Trial 17 | Epoch 10 | Train Loss: 27.7166 | Val Loss: 23.7187 | Optimizer: Adam\n",
      "Trial 17 | Epoch 11 | Train Loss: 27.2031 | Val Loss: 24.4457 | Optimizer: Adam\n",
      "Trial 17 | Epoch 12 | Train Loss: 24.3973 | Val Loss: 29.5049 | Optimizer: Adam\n",
      "Trial 17 | Epoch 13 | Train Loss: 25.5860 | Val Loss: 23.7846 | Optimizer: Adam\n",
      "Trial 17 | Epoch 14 | Train Loss: 25.7475 | Val Loss: 26.4963 | Optimizer: Adam\n",
      "Trial 17 | Epoch 15 | Train Loss: 22.7545 | Val Loss: 26.1199 | Optimizer: Adam\n",
      "Trial 17 | Epoch 16 | Train Loss: 24.6091 | Val Loss: 22.2836 | Optimizer: Adam\n",
      "Trial 17 | Epoch 17 | Train Loss: 29.1484 | Val Loss: 28.1568 | Optimizer: Adam\n",
      "Trial 17 | Epoch 18 | Train Loss: 23.4095 | Val Loss: 29.1337 | Optimizer: Adam\n",
      "Trial 17 | Epoch 19 | Train Loss: 22.8051 | Val Loss: 20.8126 | Optimizer: Adam\n",
      "Trial 17 | Epoch 20 | Train Loss: 24.3105 | Val Loss: 21.3639 | Optimizer: Adam\n",
      "Trial 17 | Epoch 21 | Train Loss: 22.9917 | Val Loss: 25.1521 | Optimizer: Adam\n",
      "Trial 17 | Epoch 22 | Train Loss: 22.0254 | Val Loss: 19.5450 | Optimizer: Adam\n",
      "Trial 17 | Epoch 23 | Train Loss: 23.3103 | Val Loss: 20.1727 | Optimizer: Adam\n",
      "Trial 17 | Epoch 24 | Train Loss: 25.2083 | Val Loss: 20.3439 | Optimizer: Adam\n",
      "Trial 17 | Epoch 25 | Train Loss: 22.8868 | Val Loss: 23.8142 | Optimizer: Adam\n",
      "Trial 17 | Epoch 26 | Train Loss: 24.0113 | Val Loss: 19.9607 | Optimizer: Adam\n",
      "Trial 17 | Epoch 27 | Train Loss: 23.2755 | Val Loss: 18.6340 | Optimizer: Adam\n",
      "Trial 17 | Epoch 28 | Train Loss: 23.7671 | Val Loss: 21.8250 | Optimizer: Adam\n",
      "Trial 17 | Epoch 29 | Train Loss: 19.7319 | Val Loss: 26.1895 | Optimizer: Adam\n",
      "Trial 17 | Epoch 30 | Train Loss: 22.0781 | Val Loss: 19.1946 | Optimizer: Adam\n",
      "Trial 17 | Epoch 31 | Train Loss: 21.2464 | Val Loss: 18.0306 | Optimizer: Adam\n",
      "Trial 17 | Epoch 32 | Train Loss: 20.4542 | Val Loss: 18.2346 | Optimizer: Adam\n",
      "Trial 17 | Epoch 33 | Train Loss: 21.2238 | Val Loss: 21.3134 | Optimizer: Adam\n",
      "Trial 17 | Epoch 34 | Train Loss: 22.1070 | Val Loss: 23.3746 | Optimizer: Adam\n",
      "Trial 17 | Epoch 35 | Train Loss: 20.4755 | Val Loss: 25.6212 | Optimizer: Adam\n",
      "Trial 17 | Epoch 36 | Train Loss: 20.9959 | Val Loss: 21.9461 | Optimizer: Adam\n",
      "Trial 17 | Epoch 37 | Train Loss: 18.4728 | Val Loss: 17.5590 | Optimizer: Adam\n",
      "Trial 17 | Epoch 38 | Train Loss: 19.0522 | Val Loss: 19.5790 | Optimizer: Adam\n",
      "Trial 17 | Epoch 39 | Train Loss: 19.3163 | Val Loss: 18.7498 | Optimizer: Adam\n",
      "Trial 17 | Epoch 40 | Train Loss: 18.9268 | Val Loss: 20.5292 | Optimizer: Adam\n",
      "Trial 17 | Epoch 41 | Train Loss: 18.9187 | Val Loss: 21.7516 | Optimizer: Adam\n",
      "Trial 17 | Epoch 42 | Train Loss: 18.0943 | Val Loss: 22.2473 | Optimizer: Adam\n",
      "Trial 17 | Epoch 43 | Train Loss: 18.5388 | Val Loss: 20.1412 | Optimizer: Adam\n",
      "Trial 17 | Epoch 44 | Train Loss: 17.8035 | Val Loss: 22.4957 | Optimizer: Adam\n",
      "Trial 17 | Epoch 45 | Train Loss: 19.2145 | Val Loss: 17.3887 | Optimizer: Adam\n",
      "Trial 17 | Epoch 46 | Train Loss: 18.7189 | Val Loss: 20.5744 | Optimizer: Adam\n",
      "Trial 17 | Epoch 47 | Train Loss: 19.1602 | Val Loss: 17.9669 | Optimizer: Adam\n",
      "Trial 17 | Epoch 48 | Train Loss: 17.6865 | Val Loss: 16.5285 | Optimizer: Adam\n",
      "Trial 17 | Epoch 49 | Train Loss: 16.4794 | Val Loss: 16.7941 | Optimizer: Adam\n",
      "Trial 17 | Epoch 50 | Train Loss: 18.5861 | Val Loss: 23.8278 | Optimizer: Adam\n",
      "Trial 17 | Epoch 51 | Train Loss: 18.3486 | Val Loss: 24.0677 | Optimizer: Adam\n",
      "Trial 17 | Epoch 52 | Train Loss: 17.4908 | Val Loss: 26.2913 | Optimizer: Adam\n",
      "Trial 17 | Epoch 53 | Train Loss: 21.2578 | Val Loss: 19.9923 | Optimizer: Adam\n",
      "Trial 17 | Epoch 54 | Train Loss: 19.3121 | Val Loss: 17.6619 | Optimizer: Adam\n",
      "Trial 17 | Epoch 55 | Train Loss: 18.9977 | Val Loss: 20.8592 | Optimizer: Adam\n",
      "Trial 17 | Epoch 56 | Train Loss: 19.0833 | Val Loss: 21.4527 | Optimizer: Adam\n",
      "Trial 17 | Epoch 57 | Train Loss: 16.7987 | Val Loss: 17.6660 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:10,889] Trial 17 finished with value: 16.528479723426386 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.298421958322783, 'lr': 0.00028655500652053083, 'activation': 'ReLU', 'optimizer': 'Adam', 'weight_decay': 3.515884716355651e-06}. Best is trial 10 with value: 15.720193653571897.\n",
      "[I 2025-09-04 21:20:11,046] Trial 18 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 | Epoch 58 | Train Loss: 16.8265 | Val Loss: 16.5670 | Optimizer: Adam\n",
      "Trial 17 - Early stopping triggered at epoch 58\n",
      "Trial 18 | Epoch 01 | Train Loss: 141771.2496 | Val Loss: 288.7286 | Optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:11,208] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19 | Epoch 01 | Train Loss: 250.2019 | Val Loss: 209.1544 | Optimizer: AdamW\n",
      "Trial 20 | Epoch 01 | Train Loss: 156.5231 | Val Loss: 107.9966 | Optimizer: Adam\n",
      "Trial 20 | Epoch 02 | Train Loss: 86.0294 | Val Loss: 63.7121 | Optimizer: Adam\n",
      "Trial 20 | Epoch 03 | Train Loss: 60.9684 | Val Loss: 45.3946 | Optimizer: Adam\n",
      "Trial 20 | Epoch 04 | Train Loss: 63.0370 | Val Loss: 67.2261 | Optimizer: Adam\n",
      "Trial 20 | Epoch 05 | Train Loss: 52.1695 | Val Loss: 39.8054 | Optimizer: Adam\n",
      "Trial 20 | Epoch 06 | Train Loss: 44.0596 | Val Loss: 45.1396 | Optimizer: Adam\n",
      "Trial 20 | Epoch 07 | Train Loss: 35.8604 | Val Loss: 29.4192 | Optimizer: Adam\n",
      "Trial 20 | Epoch 08 | Train Loss: 35.1341 | Val Loss: 25.6319 | Optimizer: Adam\n",
      "Trial 20 | Epoch 09 | Train Loss: 28.3035 | Val Loss: 26.4781 | Optimizer: Adam\n",
      "Trial 20 | Epoch 10 | Train Loss: 28.7056 | Val Loss: 24.6301 | Optimizer: Adam\n",
      "Trial 20 | Epoch 11 | Train Loss: 32.1531 | Val Loss: 51.3093 | Optimizer: Adam\n",
      "Trial 20 | Epoch 12 | Train Loss: 31.1062 | Val Loss: 32.9893 | Optimizer: Adam\n",
      "Trial 20 | Epoch 13 | Train Loss: 27.9730 | Val Loss: 22.0616 | Optimizer: Adam\n",
      "Trial 20 | Epoch 14 | Train Loss: 25.8981 | Val Loss: 20.5638 | Optimizer: Adam\n",
      "Trial 20 | Epoch 15 | Train Loss: 27.3238 | Val Loss: 29.2586 | Optimizer: Adam\n",
      "Trial 20 | Epoch 16 | Train Loss: 26.3427 | Val Loss: 25.8241 | Optimizer: Adam\n",
      "Trial 20 | Epoch 17 | Train Loss: 25.8291 | Val Loss: 22.8126 | Optimizer: Adam\n",
      "Trial 20 | Epoch 18 | Train Loss: 21.9896 | Val Loss: 19.6711 | Optimizer: Adam\n",
      "Trial 20 | Epoch 19 | Train Loss: 24.5041 | Val Loss: 19.0271 | Optimizer: Adam\n",
      "Trial 20 | Epoch 20 | Train Loss: 21.6983 | Val Loss: 17.4868 | Optimizer: Adam\n",
      "Trial 20 | Epoch 21 | Train Loss: 22.4295 | Val Loss: 26.5232 | Optimizer: Adam\n",
      "Trial 20 | Epoch 22 | Train Loss: 22.2640 | Val Loss: 29.9358 | Optimizer: Adam\n",
      "Trial 20 | Epoch 23 | Train Loss: 21.2857 | Val Loss: 26.0211 | Optimizer: Adam\n",
      "Trial 20 | Epoch 24 | Train Loss: 21.7708 | Val Loss: 20.0981 | Optimizer: Adam\n",
      "Trial 20 | Epoch 25 | Train Loss: 19.2978 | Val Loss: 20.7187 | Optimizer: Adam\n",
      "Trial 20 | Epoch 26 | Train Loss: 19.0676 | Val Loss: 18.9876 | Optimizer: Adam\n",
      "Trial 20 | Epoch 27 | Train Loss: 20.6350 | Val Loss: 16.5424 | Optimizer: Adam\n",
      "Trial 20 | Epoch 28 | Train Loss: 20.5135 | Val Loss: 15.1104 | Optimizer: Adam\n",
      "Trial 20 | Epoch 29 | Train Loss: 18.0828 | Val Loss: 20.3110 | Optimizer: Adam\n",
      "Trial 20 | Epoch 30 | Train Loss: 24.3909 | Val Loss: 20.1237 | Optimizer: Adam\n",
      "Trial 20 | Epoch 31 | Train Loss: 18.8029 | Val Loss: 17.8232 | Optimizer: Adam\n",
      "Trial 20 | Epoch 32 | Train Loss: 18.1822 | Val Loss: 16.9052 | Optimizer: Adam\n",
      "Trial 20 | Epoch 33 | Train Loss: 21.3741 | Val Loss: 18.2634 | Optimizer: Adam\n",
      "Trial 20 | Epoch 34 | Train Loss: 18.5377 | Val Loss: 26.1849 | Optimizer: Adam\n",
      "Trial 20 | Epoch 35 | Train Loss: 19.6179 | Val Loss: 24.3498 | Optimizer: Adam\n",
      "Trial 20 | Epoch 36 | Train Loss: 18.8729 | Val Loss: 15.4804 | Optimizer: Adam\n",
      "Trial 20 | Epoch 37 | Train Loss: 18.9579 | Val Loss: 15.8131 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:14,206] Trial 20 finished with value: 15.11039302794914 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3966604263049831, 'lr': 0.0009734992713466208, 'activation': 'ReLU', 'optimizer': 'Adam', 'weight_decay': 8.868914988497863e-05}. Best is trial 20 with value: 15.11039302794914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 | Epoch 38 | Train Loss: 21.0437 | Val Loss: 16.7235 | Optimizer: Adam\n",
      "Trial 20 - Early stopping triggered at epoch 38\n",
      "Trial 21 | Epoch 01 | Train Loss: 151.8766 | Val Loss: 55.2069 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:14,514] Trial 21 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 21 | Epoch 02 | Train Loss: 79.6528 | Val Loss: 87.0752 | Optimizer: Adam\n",
      "Trial 21 | Epoch 03 | Train Loss: 67.8496 | Val Loss: 50.7941 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:14,676] Trial 22 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 22 | Epoch 01 | Train Loss: 232.0043 | Val Loss: 145.0951 | Optimizer: Adam\n",
      "Trial 23 | Epoch 01 | Train Loss: 170.7448 | Val Loss: 43.5436 | Optimizer: Adam\n",
      "Trial 23 | Epoch 02 | Train Loss: 47.4616 | Val Loss: 37.1718 | Optimizer: Adam\n",
      "Trial 23 | Epoch 03 | Train Loss: 40.7803 | Val Loss: 31.4256 | Optimizer: Adam\n",
      "Trial 23 | Epoch 04 | Train Loss: 33.0563 | Val Loss: 27.4942 | Optimizer: Adam\n",
      "Trial 23 | Epoch 05 | Train Loss: 35.1646 | Val Loss: 29.9875 | Optimizer: Adam\n",
      "Trial 23 | Epoch 06 | Train Loss: 27.7564 | Val Loss: 24.4620 | Optimizer: Adam\n",
      "Trial 23 | Epoch 07 | Train Loss: 31.8599 | Val Loss: 28.9126 | Optimizer: Adam\n",
      "Trial 23 | Epoch 08 | Train Loss: 29.3441 | Val Loss: 29.6526 | Optimizer: Adam\n",
      "Trial 23 | Epoch 09 | Train Loss: 29.3545 | Val Loss: 24.7704 | Optimizer: Adam\n",
      "Trial 23 | Epoch 10 | Train Loss: 28.9124 | Val Loss: 26.8792 | Optimizer: Adam\n",
      "Trial 23 | Epoch 11 | Train Loss: 29.2756 | Val Loss: 20.8372 | Optimizer: Adam\n",
      "Trial 23 | Epoch 12 | Train Loss: 26.3513 | Val Loss: 20.4635 | Optimizer: Adam\n",
      "Trial 23 | Epoch 13 | Train Loss: 28.3051 | Val Loss: 28.1092 | Optimizer: Adam\n",
      "Trial 23 | Epoch 14 | Train Loss: 30.7156 | Val Loss: 19.5860 | Optimizer: Adam\n",
      "Trial 23 | Epoch 15 | Train Loss: 25.4913 | Val Loss: 19.4397 | Optimizer: Adam\n",
      "Trial 23 | Epoch 16 | Train Loss: 28.9998 | Val Loss: 19.4899 | Optimizer: Adam\n",
      "Trial 23 | Epoch 17 | Train Loss: 24.9162 | Val Loss: 20.8580 | Optimizer: Adam\n",
      "Trial 23 | Epoch 18 | Train Loss: 22.4350 | Val Loss: 18.1609 | Optimizer: Adam\n",
      "Trial 23 | Epoch 19 | Train Loss: 25.2013 | Val Loss: 17.8297 | Optimizer: Adam\n",
      "Trial 23 | Epoch 20 | Train Loss: 24.0660 | Val Loss: 17.6404 | Optimizer: Adam\n",
      "Trial 23 | Epoch 21 | Train Loss: 25.4808 | Val Loss: 17.1849 | Optimizer: Adam\n",
      "Trial 23 | Epoch 22 | Train Loss: 27.3406 | Val Loss: 18.3741 | Optimizer: Adam\n",
      "Trial 23 | Epoch 23 | Train Loss: 22.5492 | Val Loss: 17.1432 | Optimizer: Adam\n",
      "Trial 23 | Epoch 24 | Train Loss: 26.1304 | Val Loss: 17.0783 | Optimizer: Adam\n",
      "Trial 23 | Epoch 25 | Train Loss: 26.7592 | Val Loss: 17.2732 | Optimizer: Adam\n",
      "Trial 23 | Epoch 26 | Train Loss: 21.7414 | Val Loss: 17.7688 | Optimizer: Adam\n",
      "Trial 23 | Epoch 27 | Train Loss: 22.6534 | Val Loss: 20.2406 | Optimizer: Adam\n",
      "Trial 23 | Epoch 28 | Train Loss: 24.4984 | Val Loss: 20.0588 | Optimizer: Adam\n",
      "Trial 23 | Epoch 29 | Train Loss: 21.7898 | Val Loss: 19.7010 | Optimizer: Adam\n",
      "Trial 23 | Epoch 30 | Train Loss: 21.1954 | Val Loss: 24.3748 | Optimizer: Adam\n",
      "Trial 23 | Epoch 31 | Train Loss: 23.1943 | Val Loss: 27.2152 | Optimizer: Adam\n",
      "Trial 23 | Epoch 32 | Train Loss: 24.2473 | Val Loss: 44.5919 | Optimizer: Adam\n",
      "Trial 23 | Epoch 33 | Train Loss: 24.7181 | Val Loss: 23.2689 | Optimizer: Adam\n",
      "Trial 23 | Epoch 34 | Train Loss: 23.2437 | Val Loss: 16.7563 | Optimizer: Adam\n",
      "Trial 23 | Epoch 35 | Train Loss: 20.3061 | Val Loss: 17.4988 | Optimizer: Adam\n",
      "Trial 23 | Epoch 36 | Train Loss: 20.5021 | Val Loss: 19.8823 | Optimizer: Adam\n",
      "Trial 23 | Epoch 37 | Train Loss: 21.7461 | Val Loss: 21.6171 | Optimizer: Adam\n",
      "Trial 23 | Epoch 38 | Train Loss: 18.9726 | Val Loss: 21.2729 | Optimizer: Adam\n",
      "Trial 23 | Epoch 39 | Train Loss: 21.3000 | Val Loss: 20.2356 | Optimizer: Adam\n",
      "Trial 23 | Epoch 40 | Train Loss: 18.8694 | Val Loss: 27.4949 | Optimizer: Adam\n",
      "Trial 23 | Epoch 41 | Train Loss: 21.9686 | Val Loss: 20.4662 | Optimizer: Adam\n",
      "Trial 23 | Epoch 42 | Train Loss: 21.7340 | Val Loss: 17.7860 | Optimizer: Adam\n",
      "Trial 23 | Epoch 43 | Train Loss: 18.8786 | Val Loss: 16.3333 | Optimizer: Adam\n",
      "Trial 23 | Epoch 44 | Train Loss: 18.5375 | Val Loss: 17.5851 | Optimizer: Adam\n",
      "Trial 23 | Epoch 45 | Train Loss: 18.2815 | Val Loss: 21.7482 | Optimizer: Adam\n",
      "Trial 23 | Epoch 46 | Train Loss: 22.3981 | Val Loss: 20.2263 | Optimizer: Adam\n",
      "Trial 23 | Epoch 47 | Train Loss: 23.2496 | Val Loss: 19.1463 | Optimizer: Adam\n",
      "Trial 23 | Epoch 48 | Train Loss: 25.5378 | Val Loss: 27.0017 | Optimizer: Adam\n",
      "Trial 23 | Epoch 49 | Train Loss: 22.6222 | Val Loss: 34.6362 | Optimizer: Adam\n",
      "Trial 23 | Epoch 50 | Train Loss: 23.4954 | Val Loss: 26.2173 | Optimizer: Adam\n",
      "Trial 23 | Epoch 51 | Train Loss: 22.5182 | Val Loss: 18.0511 | Optimizer: Adam\n",
      "Trial 23 | Epoch 52 | Train Loss: 17.6261 | Val Loss: 18.8151 | Optimizer: Adam\n",
      "Trial 23 | Epoch 53 | Train Loss: 20.0546 | Val Loss: 16.1914 | Optimizer: Adam\n",
      "Trial 23 | Epoch 54 | Train Loss: 18.2775 | Val Loss: 15.9224 | Optimizer: Adam\n",
      "Trial 23 | Epoch 55 | Train Loss: 18.6174 | Val Loss: 16.4785 | Optimizer: Adam\n",
      "Trial 23 | Epoch 56 | Train Loss: 17.3687 | Val Loss: 16.8761 | Optimizer: Adam\n",
      "Trial 23 | Epoch 57 | Train Loss: 18.5620 | Val Loss: 15.6717 | Optimizer: Adam\n",
      "Trial 23 | Epoch 58 | Train Loss: 21.2051 | Val Loss: 19.9947 | Optimizer: Adam\n",
      "Trial 23 | Epoch 59 | Train Loss: 17.4386 | Val Loss: 21.5803 | Optimizer: Adam\n",
      "Trial 23 | Epoch 60 | Train Loss: 18.8876 | Val Loss: 23.5765 | Optimizer: Adam\n",
      "Trial 23 | Epoch 61 | Train Loss: 19.5563 | Val Loss: 18.7264 | Optimizer: Adam\n",
      "Trial 23 | Epoch 62 | Train Loss: 17.0677 | Val Loss: 19.2235 | Optimizer: Adam\n",
      "Trial 23 | Epoch 63 | Train Loss: 16.9759 | Val Loss: 25.8455 | Optimizer: Adam\n",
      "Trial 23 | Epoch 64 | Train Loss: 17.7385 | Val Loss: 16.3573 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:19,881] Trial 23 finished with value: 15.67170184220725 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3996290969690424, 'lr': 0.00046646849425856896, 'activation': 'ReLU', 'optimizer': 'Adam', 'weight_decay': 6.989393781240468e-05}. Best is trial 20 with value: 15.11039302794914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 23 | Epoch 65 | Train Loss: 18.3151 | Val Loss: 22.8781 | Optimizer: Adam\n",
      "Trial 23 | Epoch 66 | Train Loss: 20.3416 | Val Loss: 21.9417 | Optimizer: Adam\n",
      "Trial 23 | Epoch 67 | Train Loss: 17.5090 | Val Loss: 18.4471 | Optimizer: Adam\n",
      "Trial 23 - Early stopping triggered at epoch 67\n",
      "Trial 24 | Epoch 01 | Train Loss: 187.7568 | Val Loss: 58.5602 | Optimizer: Adam\n",
      "Trial 24 | Epoch 02 | Train Loss: 58.3102 | Val Loss: 71.3605 | Optimizer: Adam\n",
      "Trial 24 | Epoch 03 | Train Loss: 52.4087 | Val Loss: 31.1949 | Optimizer: Adam\n",
      "Trial 24 | Epoch 04 | Train Loss: 41.2979 | Val Loss: 40.9306 | Optimizer: Adam\n",
      "Trial 24 | Epoch 05 | Train Loss: 40.6396 | Val Loss: 28.3293 | Optimizer: Adam\n",
      "Trial 24 | Epoch 06 | Train Loss: 34.7973 | Val Loss: 37.7174 | Optimizer: Adam\n",
      "Trial 24 | Epoch 07 | Train Loss: 32.4422 | Val Loss: 25.8110 | Optimizer: Adam\n",
      "Trial 24 | Epoch 08 | Train Loss: 33.0730 | Val Loss: 38.9830 | Optimizer: Adam\n",
      "Trial 24 | Epoch 09 | Train Loss: 37.6529 | Val Loss: 27.3764 | Optimizer: Adam\n",
      "Trial 24 | Epoch 10 | Train Loss: 35.2848 | Val Loss: 23.4435 | Optimizer: Adam\n",
      "Trial 24 | Epoch 11 | Train Loss: 32.5285 | Val Loss: 32.0430 | Optimizer: Adam\n",
      "Trial 24 | Epoch 12 | Train Loss: 29.8822 | Val Loss: 28.5910 | Optimizer: Adam\n",
      "Trial 24 | Epoch 13 | Train Loss: 27.9962 | Val Loss: 25.0312 | Optimizer: Adam\n",
      "Trial 24 | Epoch 14 | Train Loss: 29.1486 | Val Loss: 24.5363 | Optimizer: Adam\n",
      "Trial 24 | Epoch 15 | Train Loss: 28.9870 | Val Loss: 22.6167 | Optimizer: Adam\n",
      "Trial 24 | Epoch 16 | Train Loss: 29.7737 | Val Loss: 27.3780 | Optimizer: Adam\n",
      "Trial 24 | Epoch 17 | Train Loss: 26.4566 | Val Loss: 30.8462 | Optimizer: Adam\n",
      "Trial 24 | Epoch 18 | Train Loss: 26.7799 | Val Loss: 27.6123 | Optimizer: Adam\n",
      "Trial 24 | Epoch 19 | Train Loss: 28.9364 | Val Loss: 24.8505 | Optimizer: Adam\n",
      "Trial 24 | Epoch 20 | Train Loss: 26.6740 | Val Loss: 21.1149 | Optimizer: Adam\n",
      "Trial 24 | Epoch 21 | Train Loss: 27.4939 | Val Loss: 20.2074 | Optimizer: Adam\n",
      "Trial 24 | Epoch 22 | Train Loss: 24.6975 | Val Loss: 21.3622 | Optimizer: Adam\n",
      "Trial 24 | Epoch 23 | Train Loss: 23.8668 | Val Loss: 28.7143 | Optimizer: Adam\n",
      "Trial 24 | Epoch 24 | Train Loss: 24.6286 | Val Loss: 25.5281 | Optimizer: Adam\n",
      "Trial 24 | Epoch 25 | Train Loss: 25.3816 | Val Loss: 21.0982 | Optimizer: Adam\n",
      "Trial 24 | Epoch 26 | Train Loss: 25.1718 | Val Loss: 17.5026 | Optimizer: Adam\n",
      "Trial 24 | Epoch 27 | Train Loss: 25.7362 | Val Loss: 17.7162 | Optimizer: Adam\n",
      "Trial 24 | Epoch 28 | Train Loss: 26.2305 | Val Loss: 17.4006 | Optimizer: Adam\n",
      "Trial 24 | Epoch 29 | Train Loss: 24.9479 | Val Loss: 17.3756 | Optimizer: Adam\n",
      "Trial 24 | Epoch 30 | Train Loss: 25.0263 | Val Loss: 18.1004 | Optimizer: Adam\n",
      "Trial 24 | Epoch 31 | Train Loss: 30.0208 | Val Loss: 20.3341 | Optimizer: Adam\n",
      "Trial 24 | Epoch 32 | Train Loss: 23.2602 | Val Loss: 20.1589 | Optimizer: Adam\n",
      "Trial 24 | Epoch 33 | Train Loss: 22.1468 | Val Loss: 31.9917 | Optimizer: Adam\n",
      "Trial 24 | Epoch 34 | Train Loss: 25.6165 | Val Loss: 31.3417 | Optimizer: Adam\n",
      "Trial 24 | Epoch 35 | Train Loss: 24.3518 | Val Loss: 24.8336 | Optimizer: Adam\n",
      "Trial 24 | Epoch 36 | Train Loss: 22.6011 | Val Loss: 20.9292 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:22,993] Trial 24 finished with value: 17.375625951503352 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3986804230594082, 'lr': 0.0004559478574456893, 'activation': 'ReLU', 'optimizer': 'Adam', 'weight_decay': 9.896011996506398e-05}. Best is trial 20 with value: 15.11039302794914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24 | Epoch 37 | Train Loss: 21.7457 | Val Loss: 31.8053 | Optimizer: Adam\n",
      "Trial 24 | Epoch 38 | Train Loss: 22.0312 | Val Loss: 22.3701 | Optimizer: Adam\n",
      "Trial 24 | Epoch 39 | Train Loss: 21.0725 | Val Loss: 21.8283 | Optimizer: Adam\n",
      "Trial 24 - Early stopping triggered at epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:23,227] Trial 25 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25 | Epoch 01 | Train Loss: 110.0236 | Val Loss: 59.3931 | Optimizer: Adam\n",
      "Trial 25 | Epoch 02 | Train Loss: 66.7510 | Val Loss: 65.9796 | Optimizer: Adam\n",
      "Trial 25 | Epoch 03 | Train Loss: 66.8485 | Val Loss: 63.1976 | Optimizer: Adam\n",
      "Trial 26 | Epoch 01 | Train Loss: 179.8738 | Val Loss: 28.5037 | Optimizer: Adam\n",
      "Trial 26 | Epoch 02 | Train Loss: 63.7639 | Val Loss: 77.2739 | Optimizer: Adam\n",
      "Trial 26 | Epoch 03 | Train Loss: 56.8349 | Val Loss: 34.2865 | Optimizer: Adam\n",
      "Trial 26 | Epoch 04 | Train Loss: 42.8062 | Val Loss: 48.3217 | Optimizer: Adam\n",
      "Trial 26 | Epoch 05 | Train Loss: 42.5959 | Val Loss: 30.7419 | Optimizer: Adam\n",
      "Trial 26 | Epoch 06 | Train Loss: 36.6251 | Val Loss: 31.0038 | Optimizer: Adam\n",
      "Trial 26 | Epoch 07 | Train Loss: 33.7304 | Val Loss: 28.9952 | Optimizer: Adam\n",
      "Trial 26 | Epoch 08 | Train Loss: 31.4536 | Val Loss: 29.2991 | Optimizer: Adam\n",
      "Trial 26 | Epoch 09 | Train Loss: 31.1537 | Val Loss: 32.9029 | Optimizer: Adam\n",
      "Trial 26 | Epoch 10 | Train Loss: 24.9942 | Val Loss: 25.1374 | Optimizer: Adam\n",
      "Trial 26 | Epoch 11 | Train Loss: 29.0830 | Val Loss: 27.1038 | Optimizer: Adam\n",
      "Trial 26 | Epoch 12 | Train Loss: 27.0451 | Val Loss: 33.0374 | Optimizer: Adam\n",
      "Trial 26 | Epoch 13 | Train Loss: 27.5708 | Val Loss: 32.8841 | Optimizer: Adam\n",
      "Trial 26 | Epoch 14 | Train Loss: 25.3238 | Val Loss: 24.4204 | Optimizer: Adam\n",
      "Trial 26 | Epoch 15 | Train Loss: 25.8482 | Val Loss: 28.0877 | Optimizer: Adam\n",
      "Trial 26 | Epoch 16 | Train Loss: 22.9467 | Val Loss: 23.8708 | Optimizer: Adam\n",
      "Trial 26 | Epoch 17 | Train Loss: 25.4623 | Val Loss: 35.9357 | Optimizer: Adam\n",
      "Trial 26 | Epoch 18 | Train Loss: 26.3247 | Val Loss: 29.5463 | Optimizer: Adam\n",
      "Trial 26 | Epoch 19 | Train Loss: 25.3663 | Val Loss: 28.9967 | Optimizer: Adam\n",
      "Trial 26 | Epoch 20 | Train Loss: 23.7520 | Val Loss: 20.6469 | Optimizer: Adam\n",
      "Trial 26 | Epoch 21 | Train Loss: 22.4250 | Val Loss: 25.9459 | Optimizer: Adam\n",
      "Trial 26 | Epoch 22 | Train Loss: 23.0123 | Val Loss: 18.2935 | Optimizer: Adam\n",
      "Trial 26 | Epoch 23 | Train Loss: 21.5482 | Val Loss: 16.6350 | Optimizer: Adam\n",
      "Trial 26 | Epoch 24 | Train Loss: 21.8013 | Val Loss: 19.3829 | Optimizer: Adam\n",
      "Trial 26 | Epoch 25 | Train Loss: 22.4608 | Val Loss: 26.0949 | Optimizer: Adam\n",
      "Trial 26 | Epoch 26 | Train Loss: 21.8881 | Val Loss: 22.2621 | Optimizer: Adam\n",
      "Trial 26 | Epoch 27 | Train Loss: 21.6826 | Val Loss: 16.5266 | Optimizer: Adam\n",
      "Trial 26 | Epoch 28 | Train Loss: 26.0980 | Val Loss: 17.3111 | Optimizer: Adam\n",
      "Trial 26 | Epoch 29 | Train Loss: 21.7248 | Val Loss: 16.6031 | Optimizer: Adam\n",
      "Trial 26 | Epoch 30 | Train Loss: 19.5318 | Val Loss: 25.0484 | Optimizer: Adam\n",
      "Trial 26 | Epoch 31 | Train Loss: 21.4178 | Val Loss: 17.6282 | Optimizer: Adam\n",
      "Trial 26 | Epoch 32 | Train Loss: 21.4622 | Val Loss: 17.0962 | Optimizer: Adam\n",
      "Trial 26 | Epoch 33 | Train Loss: 23.7373 | Val Loss: 17.8249 | Optimizer: Adam\n",
      "Trial 26 | Epoch 34 | Train Loss: 23.6899 | Val Loss: 17.8517 | Optimizer: Adam\n",
      "Trial 26 | Epoch 35 | Train Loss: 23.5836 | Val Loss: 19.3239 | Optimizer: Adam\n",
      "Trial 26 | Epoch 36 | Train Loss: 20.6042 | Val Loss: 16.8829 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:26,188] Trial 26 finished with value: 16.52663712385224 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.38380299543850616, 'lr': 0.0006533321327397485, 'activation': 'ReLU', 'optimizer': 'Adam', 'weight_decay': 3.196270937358899e-05}. Best is trial 20 with value: 15.11039302794914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26 | Epoch 37 | Train Loss: 21.7466 | Val Loss: 17.9246 | Optimizer: Adam\n",
      "Trial 26 - Early stopping triggered at epoch 37\n",
      "Trial 27 | Epoch 01 | Train Loss: 162.6034 | Val Loss: 102.4386 | Optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:26,418] Trial 27 pruned. \n",
      "[I 2025-09-04 21:20:26,545] Trial 28 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27 | Epoch 02 | Train Loss: 74.7008 | Val Loss: 71.9983 | Optimizer: RMSprop\n",
      "Trial 28 | Epoch 01 | Train Loss: 225.6836 | Val Loss: 186.2428 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:26,687] Trial 29 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 29 | Epoch 01 | NaN loss detected so pruning trial\n",
      "Trial 30 | Epoch 01 | Train Loss: 129.1090 | Val Loss: 65.2265 | Optimizer: Adam\n",
      "Trial 30 | Epoch 02 | Train Loss: 73.0940 | Val Loss: 59.8544 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:26,920] Trial 30 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 | Epoch 03 | Train Loss: 61.5751 | Val Loss: 60.5188 | Optimizer: Adam\n",
      "Trial 31 | Epoch 01 | Train Loss: 196.6209 | Val Loss: 94.0556 | Optimizer: Adam\n",
      "Trial 31 | Epoch 02 | Train Loss: 58.1615 | Val Loss: 29.3706 | Optimizer: Adam\n",
      "Trial 31 | Epoch 03 | Train Loss: 43.5486 | Val Loss: 55.0450 | Optimizer: Adam\n",
      "Trial 31 | Epoch 04 | Train Loss: 40.6175 | Val Loss: 30.8064 | Optimizer: Adam\n",
      "Trial 31 | Epoch 05 | Train Loss: 41.9724 | Val Loss: 32.2782 | Optimizer: Adam\n",
      "Trial 31 | Epoch 06 | Train Loss: 32.1232 | Val Loss: 30.9399 | Optimizer: Adam\n",
      "Trial 31 | Epoch 07 | Train Loss: 32.3953 | Val Loss: 27.2317 | Optimizer: Adam\n",
      "Trial 31 | Epoch 08 | Train Loss: 28.7937 | Val Loss: 28.8404 | Optimizer: Adam\n",
      "Trial 31 | Epoch 09 | Train Loss: 28.2042 | Val Loss: 25.7299 | Optimizer: Adam\n",
      "Trial 31 | Epoch 10 | Train Loss: 26.7179 | Val Loss: 26.3586 | Optimizer: Adam\n",
      "Trial 31 | Epoch 11 | Train Loss: 25.5881 | Val Loss: 23.4915 | Optimizer: Adam\n",
      "Trial 31 | Epoch 12 | Train Loss: 25.0716 | Val Loss: 25.0706 | Optimizer: Adam\n",
      "Trial 31 | Epoch 13 | Train Loss: 25.4193 | Val Loss: 25.5860 | Optimizer: Adam\n",
      "Trial 31 | Epoch 14 | Train Loss: 24.7637 | Val Loss: 25.8649 | Optimizer: Adam\n",
      "Trial 31 | Epoch 15 | Train Loss: 23.4161 | Val Loss: 22.0236 | Optimizer: Adam\n",
      "Trial 31 | Epoch 16 | Train Loss: 24.6597 | Val Loss: 22.2496 | Optimizer: Adam\n",
      "Trial 31 | Epoch 17 | Train Loss: 22.6842 | Val Loss: 26.8423 | Optimizer: Adam\n",
      "Trial 31 | Epoch 18 | Train Loss: 23.6041 | Val Loss: 20.6603 | Optimizer: Adam\n",
      "Trial 31 | Epoch 19 | Train Loss: 23.6598 | Val Loss: 20.0562 | Optimizer: Adam\n",
      "Trial 31 | Epoch 20 | Train Loss: 23.4103 | Val Loss: 20.4182 | Optimizer: Adam\n",
      "Trial 31 | Epoch 21 | Train Loss: 22.4376 | Val Loss: 22.4143 | Optimizer: Adam\n",
      "Trial 31 | Epoch 22 | Train Loss: 22.9957 | Val Loss: 26.1623 | Optimizer: Adam\n",
      "Trial 31 | Epoch 23 | Train Loss: 22.6522 | Val Loss: 23.5251 | Optimizer: Adam\n",
      "Trial 31 | Epoch 24 | Train Loss: 20.9916 | Val Loss: 19.2805 | Optimizer: Adam\n",
      "Trial 31 | Epoch 25 | Train Loss: 20.4707 | Val Loss: 23.2833 | Optimizer: Adam\n",
      "Trial 31 | Epoch 26 | Train Loss: 21.0873 | Val Loss: 21.2314 | Optimizer: Adam\n",
      "Trial 31 | Epoch 27 | Train Loss: 19.0661 | Val Loss: 19.0723 | Optimizer: Adam\n",
      "Trial 31 | Epoch 28 | Train Loss: 19.5645 | Val Loss: 18.8808 | Optimizer: Adam\n",
      "Trial 31 | Epoch 29 | Train Loss: 18.0986 | Val Loss: 19.1844 | Optimizer: Adam\n",
      "Trial 31 | Epoch 30 | Train Loss: 18.7502 | Val Loss: 20.8277 | Optimizer: Adam\n",
      "Trial 31 | Epoch 31 | Train Loss: 19.1636 | Val Loss: 18.1153 | Optimizer: Adam\n",
      "Trial 31 | Epoch 32 | Train Loss: 19.2773 | Val Loss: 19.9888 | Optimizer: Adam\n",
      "Trial 31 | Epoch 33 | Train Loss: 20.1832 | Val Loss: 23.9505 | Optimizer: Adam\n",
      "Trial 31 | Epoch 34 | Train Loss: 17.5022 | Val Loss: 21.1590 | Optimizer: Adam\n",
      "Trial 31 | Epoch 35 | Train Loss: 17.9575 | Val Loss: 22.3743 | Optimizer: Adam\n",
      "Trial 31 | Epoch 36 | Train Loss: 19.8747 | Val Loss: 26.6175 | Optimizer: Adam\n",
      "Trial 31 | Epoch 37 | Train Loss: 20.8578 | Val Loss: 21.6614 | Optimizer: Adam\n",
      "Trial 31 | Epoch 38 | Train Loss: 17.0633 | Val Loss: 16.7507 | Optimizer: Adam\n",
      "Trial 31 | Epoch 39 | Train Loss: 19.4178 | Val Loss: 18.0212 | Optimizer: Adam\n",
      "Trial 31 | Epoch 40 | Train Loss: 18.9064 | Val Loss: 16.7289 | Optimizer: Adam\n",
      "Trial 31 | Epoch 41 | Train Loss: 17.5303 | Val Loss: 18.7121 | Optimizer: Adam\n",
      "Trial 31 | Epoch 42 | Train Loss: 22.1606 | Val Loss: 17.7044 | Optimizer: Adam\n",
      "Trial 31 | Epoch 43 | Train Loss: 18.7454 | Val Loss: 16.4071 | Optimizer: Adam\n",
      "Trial 31 | Epoch 44 | Train Loss: 18.8332 | Val Loss: 16.4792 | Optimizer: Adam\n",
      "Trial 31 | Epoch 45 | Train Loss: 16.2572 | Val Loss: 17.0004 | Optimizer: Adam\n",
      "Trial 31 | Epoch 46 | Train Loss: 19.4958 | Val Loss: 16.1267 | Optimizer: Adam\n",
      "Trial 31 | Epoch 47 | Train Loss: 18.4231 | Val Loss: 18.3224 | Optimizer: Adam\n",
      "Trial 31 | Epoch 48 | Train Loss: 19.9368 | Val Loss: 21.9413 | Optimizer: Adam\n",
      "Trial 31 | Epoch 49 | Train Loss: 18.2076 | Val Loss: 27.6591 | Optimizer: Adam\n",
      "Trial 31 | Epoch 50 | Train Loss: 19.8207 | Val Loss: 18.7117 | Optimizer: Adam\n",
      "Trial 31 | Epoch 51 | Train Loss: 20.3783 | Val Loss: 15.9620 | Optimizer: Adam\n",
      "Trial 31 | Epoch 52 | Train Loss: 16.7638 | Val Loss: 16.6278 | Optimizer: Adam\n",
      "Trial 31 | Epoch 53 | Train Loss: 16.4718 | Val Loss: 15.9852 | Optimizer: Adam\n",
      "Trial 31 | Epoch 54 | Train Loss: 15.7278 | Val Loss: 20.4890 | Optimizer: Adam\n",
      "Trial 31 | Epoch 55 | Train Loss: 15.9986 | Val Loss: 18.8001 | Optimizer: Adam\n",
      "Trial 31 | Epoch 56 | Train Loss: 16.0929 | Val Loss: 16.9961 | Optimizer: Adam\n",
      "Trial 31 | Epoch 57 | Train Loss: 17.0322 | Val Loss: 21.4077 | Optimizer: Adam\n",
      "Trial 31 | Epoch 58 | Train Loss: 17.1293 | Val Loss: 20.2682 | Optimizer: Adam\n",
      "Trial 31 | Epoch 59 | Train Loss: 15.9566 | Val Loss: 18.7278 | Optimizer: Adam\n",
      "Trial 31 | Epoch 60 | Train Loss: 16.4877 | Val Loss: 22.9023 | Optimizer: Adam\n",
      "Trial 31 | Epoch 61 | Train Loss: 18.7101 | Val Loss: 15.9009 | Optimizer: Adam\n",
      "Trial 31 | Epoch 62 | Train Loss: 22.4417 | Val Loss: 18.5403 | Optimizer: Adam\n",
      "Trial 31 | Epoch 63 | Train Loss: 18.7408 | Val Loss: 16.9448 | Optimizer: Adam\n",
      "Trial 31 | Epoch 64 | Train Loss: 18.6772 | Val Loss: 20.4096 | Optimizer: Adam\n",
      "Trial 31 | Epoch 65 | Train Loss: 17.5848 | Val Loss: 22.6801 | Optimizer: Adam\n",
      "Trial 31 | Epoch 66 | Train Loss: 15.8832 | Val Loss: 16.3681 | Optimizer: Adam\n",
      "Trial 31 | Epoch 67 | Train Loss: 15.7712 | Val Loss: 15.9014 | Optimizer: Adam\n",
      "Trial 31 | Epoch 68 | Train Loss: 15.2770 | Val Loss: 16.9347 | Optimizer: Adam\n",
      "Trial 31 | Epoch 69 | Train Loss: 18.0185 | Val Loss: 18.5974 | Optimizer: Adam\n",
      "Trial 31 | Epoch 70 | Train Loss: 16.7846 | Val Loss: 18.2414 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:32,520] Trial 31 finished with value: 15.900944097255303 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.2620629079167512, 'lr': 0.0002736701533782797, 'activation': 'ReLU', 'optimizer': 'Adam', 'weight_decay': 2.9607970760683065e-06}. Best is trial 20 with value: 15.11039302794914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 31 | Epoch 71 | Train Loss: 16.0953 | Val Loss: 18.6126 | Optimizer: Adam\n",
      "Trial 31 - Early stopping triggered at epoch 71\n",
      "Trial 32 | Epoch 01 | Train Loss: 239.0486 | Val Loss: 189.8429 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:32,680] Trial 32 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 33 | Epoch 01 | Train Loss: 157.1275 | Val Loss: 39.9228 | Optimizer: Adam\n",
      "Trial 33 | Epoch 02 | Train Loss: 52.0598 | Val Loss: 58.9854 | Optimizer: Adam\n",
      "Trial 33 | Epoch 03 | Train Loss: 46.5712 | Val Loss: 37.0002 | Optimizer: Adam\n",
      "Trial 33 | Epoch 04 | Train Loss: 45.8569 | Val Loss: 47.8933 | Optimizer: Adam\n",
      "Trial 33 | Epoch 05 | Train Loss: 41.8808 | Val Loss: 32.1600 | Optimizer: Adam\n",
      "Trial 33 | Epoch 06 | Train Loss: 38.5223 | Val Loss: 42.7244 | Optimizer: Adam\n",
      "Trial 33 | Epoch 07 | Train Loss: 38.1934 | Val Loss: 27.0490 | Optimizer: Adam\n",
      "Trial 33 | Epoch 08 | Train Loss: 31.1919 | Val Loss: 25.8548 | Optimizer: Adam\n",
      "Trial 33 | Epoch 09 | Train Loss: 30.6754 | Val Loss: 33.9047 | Optimizer: Adam\n",
      "Trial 33 | Epoch 10 | Train Loss: 30.9533 | Val Loss: 27.4536 | Optimizer: Adam\n",
      "Trial 33 | Epoch 11 | Train Loss: 30.1527 | Val Loss: 22.8286 | Optimizer: Adam\n",
      "Trial 33 | Epoch 12 | Train Loss: 27.6951 | Val Loss: 23.5579 | Optimizer: Adam\n",
      "Trial 33 | Epoch 13 | Train Loss: 32.7346 | Val Loss: 41.1346 | Optimizer: Adam\n",
      "Trial 33 | Epoch 14 | Train Loss: 30.5341 | Val Loss: 31.1433 | Optimizer: Adam\n",
      "Trial 33 | Epoch 15 | Train Loss: 29.9322 | Val Loss: 21.2413 | Optimizer: Adam\n",
      "Trial 33 | Epoch 16 | Train Loss: 28.3787 | Val Loss: 21.1743 | Optimizer: Adam\n",
      "Trial 33 | Epoch 17 | Train Loss: 25.4950 | Val Loss: 21.3554 | Optimizer: Adam\n",
      "Trial 33 | Epoch 18 | Train Loss: 27.2760 | Val Loss: 20.7841 | Optimizer: Adam\n",
      "Trial 33 | Epoch 19 | Train Loss: 27.5340 | Val Loss: 20.4469 | Optimizer: Adam\n",
      "Trial 33 | Epoch 20 | Train Loss: 28.4226 | Val Loss: 20.3837 | Optimizer: Adam\n",
      "Trial 33 | Epoch 21 | Train Loss: 24.2289 | Val Loss: 28.4273 | Optimizer: Adam\n",
      "Trial 33 | Epoch 22 | Train Loss: 23.9731 | Val Loss: 25.7978 | Optimizer: Adam\n",
      "Trial 33 | Epoch 23 | Train Loss: 22.1004 | Val Loss: 18.6562 | Optimizer: Adam\n",
      "Trial 33 | Epoch 24 | Train Loss: 25.3620 | Val Loss: 18.3261 | Optimizer: Adam\n",
      "Trial 33 | Epoch 25 | Train Loss: 22.9485 | Val Loss: 17.7171 | Optimizer: Adam\n",
      "Trial 33 | Epoch 26 | Train Loss: 26.4088 | Val Loss: 17.5733 | Optimizer: Adam\n",
      "Trial 33 | Epoch 27 | Train Loss: 22.2019 | Val Loss: 18.3750 | Optimizer: Adam\n",
      "Trial 33 | Epoch 28 | Train Loss: 22.5048 | Val Loss: 20.0200 | Optimizer: Adam\n",
      "Trial 33 | Epoch 29 | Train Loss: 22.7696 | Val Loss: 20.2673 | Optimizer: Adam\n",
      "Trial 33 | Epoch 30 | Train Loss: 21.8163 | Val Loss: 21.4847 | Optimizer: Adam\n",
      "Trial 33 | Epoch 31 | Train Loss: 19.8818 | Val Loss: 20.0973 | Optimizer: Adam\n",
      "Trial 33 | Epoch 32 | Train Loss: 19.5811 | Val Loss: 20.4318 | Optimizer: Adam\n",
      "Trial 33 | Epoch 33 | Train Loss: 21.0444 | Val Loss: 21.1089 | Optimizer: Adam\n",
      "Trial 33 | Epoch 34 | Train Loss: 21.2114 | Val Loss: 17.5644 | Optimizer: Adam\n",
      "Trial 33 | Epoch 35 | Train Loss: 20.2554 | Val Loss: 22.4401 | Optimizer: Adam\n",
      "Trial 33 | Epoch 36 | Train Loss: 21.4222 | Val Loss: 32.5221 | Optimizer: Adam\n",
      "Trial 33 | Epoch 37 | Train Loss: 21.3114 | Val Loss: 25.4046 | Optimizer: Adam\n",
      "Trial 33 | Epoch 38 | Train Loss: 20.6453 | Val Loss: 19.5985 | Optimizer: Adam\n",
      "Trial 33 | Epoch 39 | Train Loss: 20.1120 | Val Loss: 16.9875 | Optimizer: Adam\n",
      "Trial 33 | Epoch 40 | Train Loss: 20.5583 | Val Loss: 17.6573 | Optimizer: Adam\n",
      "Trial 33 | Epoch 41 | Train Loss: 20.5735 | Val Loss: 17.6716 | Optimizer: Adam\n",
      "Trial 33 | Epoch 42 | Train Loss: 19.7432 | Val Loss: 24.7061 | Optimizer: Adam\n",
      "Trial 33 | Epoch 43 | Train Loss: 21.1956 | Val Loss: 20.1581 | Optimizer: Adam\n",
      "Trial 33 | Epoch 44 | Train Loss: 18.1665 | Val Loss: 23.9934 | Optimizer: Adam\n",
      "Trial 33 | Epoch 45 | Train Loss: 18.8672 | Val Loss: 19.2293 | Optimizer: Adam\n",
      "Trial 33 | Epoch 46 | Train Loss: 20.7714 | Val Loss: 28.2752 | Optimizer: Adam\n",
      "Trial 33 | Epoch 47 | Train Loss: 21.8909 | Val Loss: 20.8712 | Optimizer: Adam\n",
      "Trial 33 | Epoch 48 | Train Loss: 18.8943 | Val Loss: 28.1864 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:36,579] Trial 33 finished with value: 16.987479923217276 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.39068129452156414, 'lr': 0.0005077854774388577, 'activation': 'ReLU', 'optimizer': 'Adam', 'weight_decay': 3.921292704647082e-06}. Best is trial 20 with value: 15.11039302794914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 33 | Epoch 49 | Train Loss: 20.5491 | Val Loss: 23.1204 | Optimizer: Adam\n",
      "Trial 33 - Early stopping triggered at epoch 49\n",
      "Trial 34 | Epoch 01 | Train Loss: 206.1800 | Val Loss: 135.8283 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:36,738] Trial 34 pruned. \n",
      "[I 2025-09-04 21:20:36,907] Trial 35 pruned. \n",
      "[I 2025-09-04 21:20:37,022] Trial 36 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 35 | Epoch 01 | Train Loss: 573.4631 | Val Loss: 579.9077 | Optimizer: SGD\n",
      "Trial 36 | Epoch 01 | Train Loss: 263.7311 | Val Loss: 195.5784 | Optimizer: Adam\n",
      "Trial 37 | Epoch 01 | Train Loss: 186.5043 | Val Loss: 28.0680 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 02 | Train Loss: 71.9118 | Val Loss: 97.5959 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 03 | Train Loss: 72.8571 | Val Loss: 51.3706 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 04 | Train Loss: 57.5185 | Val Loss: 45.0795 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 05 | Train Loss: 47.3603 | Val Loss: 44.2947 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 06 | Train Loss: 40.5010 | Val Loss: 33.4287 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 07 | Train Loss: 37.4962 | Val Loss: 31.4688 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 08 | Train Loss: 31.8010 | Val Loss: 26.4916 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 09 | Train Loss: 28.3590 | Val Loss: 25.9172 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 10 | Train Loss: 27.4699 | Val Loss: 25.1531 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 11 | Train Loss: 28.3038 | Val Loss: 28.5649 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 12 | Train Loss: 24.8769 | Val Loss: 24.5927 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 13 | Train Loss: 25.3544 | Val Loss: 30.4784 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 14 | Train Loss: 26.0796 | Val Loss: 28.4365 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 15 | Train Loss: 32.1427 | Val Loss: 20.3096 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 16 | Train Loss: 28.0262 | Val Loss: 19.6797 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 17 | Train Loss: 27.4468 | Val Loss: 20.8989 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 18 | Train Loss: 27.7337 | Val Loss: 19.8676 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 19 | Train Loss: 23.0389 | Val Loss: 24.9446 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 20 | Train Loss: 24.3614 | Val Loss: 25.1103 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 21 | Train Loss: 21.7504 | Val Loss: 26.1673 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 22 | Train Loss: 21.1083 | Val Loss: 20.0457 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 23 | Train Loss: 22.9972 | Val Loss: 22.2181 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 24 | Train Loss: 22.4700 | Val Loss: 21.6178 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 25 | Train Loss: 21.9902 | Val Loss: 18.2393 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 26 | Train Loss: 22.2900 | Val Loss: 17.4197 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 27 | Train Loss: 23.1223 | Val Loss: 19.3972 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 28 | Train Loss: 22.1768 | Val Loss: 20.4447 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 29 | Train Loss: 20.1519 | Val Loss: 17.7066 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 30 | Train Loss: 23.0551 | Val Loss: 28.2396 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 31 | Train Loss: 24.1836 | Val Loss: 24.7219 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 32 | Train Loss: 23.9844 | Val Loss: 22.7396 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 33 | Train Loss: 22.6464 | Val Loss: 18.6612 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:39,917] Trial 37 finished with value: 17.419660025495823 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3238815843153331, 'lr': 0.0006981973202746956, 'activation': 'ReLU', 'optimizer': 'AdamW', 'weight_decay': 1.2257465278792643e-05}. Best is trial 20 with value: 15.11039302794914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 37 | Epoch 34 | Train Loss: 18.7593 | Val Loss: 21.0187 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 35 | Train Loss: 21.9245 | Val Loss: 22.3650 | Optimizer: AdamW\n",
      "Trial 37 | Epoch 36 | Train Loss: 21.8787 | Val Loss: 21.8996 | Optimizer: AdamW\n",
      "Trial 37 - Early stopping triggered at epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:40,033] Trial 38 pruned. \n",
      "[I 2025-09-04 21:20:40,152] Trial 39 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 38 | Epoch 01 | Train Loss: 222.3536 | Val Loss: 239.5982 | Optimizer: Adam\n",
      "Trial 39 | Epoch 01 | Train Loss: 1000.9984 | Val Loss: 211.1967 | Optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:40,317] Trial 40 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40 | Epoch 01 | Train Loss: 646.3350 | Val Loss: 311.7246 | Optimizer: SGD\n",
      "Trial 41 | Epoch 01 | Train Loss: 257.3700 | Val Loss: 164.3759 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:40,491] Trial 41 pruned. \n",
      "[I 2025-09-04 21:20:40,647] Trial 42 pruned. \n",
      "[I 2025-09-04 21:20:40,807] Trial 43 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 42 | Epoch 01 | Train Loss: 234.8081 | Val Loss: 159.1279 | Optimizer: Adam\n",
      "Trial 43 | Epoch 01 | Train Loss: 227.3021 | Val Loss: 190.7773 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:40,922] Trial 44 pruned. \n",
      "[I 2025-09-04 21:20:41,081] Trial 45 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 44 | Epoch 01 | Train Loss: 212.6539 | Val Loss: 211.2085 | Optimizer: Adam\n",
      "Trial 45 | Epoch 01 | Train Loss: 227.3223 | Val Loss: 104.0438 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:41,305] Trial 46 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 46 | Epoch 01 | Train Loss: 167.3507 | Val Loss: 64.4288 | Optimizer: Adam\n",
      "Trial 46 | Epoch 02 | Train Loss: 61.7368 | Val Loss: 56.4745 | Optimizer: Adam\n",
      "Trial 46 | Epoch 03 | Train Loss: 66.2571 | Val Loss: 58.7906 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:41,465] Trial 47 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 47 | Epoch 01 | Train Loss: 261.6548 | Val Loss: 169.4645 | Optimizer: AdamW\n",
      "Trial 48 | Epoch 01 | Train Loss: 153.6046 | Val Loss: 70.2958 | Optimizer: Adam\n",
      "Trial 48 | Epoch 02 | Train Loss: 70.9349 | Val Loss: 59.8614 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:41,686] Trial 48 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 48 | Epoch 03 | Train Loss: 62.2872 | Val Loss: 64.4376 | Optimizer: Adam\n",
      "Trial 49 | Epoch 01 | Train Loss: 210.0227 | Val Loss: 62.6203 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 02 | Train Loss: 37.0658 | Val Loss: 27.1638 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 03 | Train Loss: 29.2888 | Val Loss: 26.1066 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 04 | Train Loss: 36.8031 | Val Loss: 29.8190 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 05 | Train Loss: 28.5236 | Val Loss: 23.9329 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 06 | Train Loss: 30.5199 | Val Loss: 23.8062 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 07 | Train Loss: 31.5308 | Val Loss: 24.1533 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 08 | Train Loss: 30.1750 | Val Loss: 25.6002 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 09 | Train Loss: 27.8116 | Val Loss: 23.6483 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 10 | Train Loss: 28.7207 | Val Loss: 24.3877 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 11 | Train Loss: 28.7909 | Val Loss: 23.2163 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 12 | Train Loss: 30.5590 | Val Loss: 23.3974 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 13 | Train Loss: 30.0173 | Val Loss: 27.5681 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 14 | Train Loss: 27.2838 | Val Loss: 22.8959 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 15 | Train Loss: 27.4170 | Val Loss: 23.9750 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 16 | Train Loss: 28.9153 | Val Loss: 23.1329 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 17 | Train Loss: 25.4966 | Val Loss: 24.0601 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 18 | Train Loss: 25.2569 | Val Loss: 22.7534 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 19 | Train Loss: 26.2895 | Val Loss: 22.5572 | Optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:43,454] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 49 | Epoch 20 | Train Loss: 28.1441 | Val Loss: 23.8073 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 21 | Train Loss: 28.0444 | Val Loss: 23.9061 | Optimizer: RMSprop\n",
      "Trial 49 | Epoch 22 | Train Loss: 26.2504 | Val Loss: 23.9629 | Optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:43,682] Trial 50 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 | Epoch 01 | Train Loss: 119.5365 | Val Loss: 74.3058 | Optimizer: Adam\n",
      "Trial 50 | Epoch 02 | Train Loss: 72.5576 | Val Loss: 60.7633 | Optimizer: Adam\n",
      "Trial 50 | Epoch 03 | Train Loss: 60.6594 | Val Loss: 62.3225 | Optimizer: Adam\n",
      "Trial 51 | Epoch 01 | Train Loss: 273.2917 | Val Loss: 229.1137 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:43,798] Trial 51 pruned. \n",
      "[I 2025-09-04 21:20:43,918] Trial 52 pruned. \n",
      "[I 2025-09-04 21:20:44,032] Trial 53 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 52 | Epoch 01 | Train Loss: 297.1594 | Val Loss: 265.6977 | Optimizer: Adam\n",
      "Trial 53 | Epoch 01 | Train Loss: 266.4954 | Val Loss: 253.8340 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:44,147] Trial 54 pruned. \n",
      "[I 2025-09-04 21:20:44,305] Trial 55 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 54 | Epoch 01 | Train Loss: 268.1742 | Val Loss: 245.1352 | Optimizer: Adam\n",
      "Trial 55 | Epoch 01 | Train Loss: 226.9038 | Val Loss: 168.1751 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\optuna\\pruners\\_percentile.py:21: RuntimeWarning:\n",
      "\n",
      "All-NaN slice encountered\n",
      "\n",
      "[I 2025-09-04 21:20:44,432] Trial 56 pruned. \n",
      "[I 2025-09-04 21:20:44,542] Trial 57 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 56 | Epoch 01 | Train Loss: inf | Val Loss: nan | Optimizer: SGD\n",
      "Trial 57 | Epoch 01 | Train Loss: 254.1601 | Val Loss: 179.9642 | Optimizer: Adam\n",
      "Trial 58 | Epoch 01 | Train Loss: 145.7810 | Val Loss: 52.0838 | Optimizer: Adam\n",
      "Trial 58 | Epoch 02 | Train Loss: 56.4512 | Val Loss: 40.0670 | Optimizer: Adam\n",
      "Trial 58 | Epoch 03 | Train Loss: 47.2528 | Val Loss: 49.0476 | Optimizer: Adam\n",
      "Trial 58 | Epoch 04 | Train Loss: 44.4701 | Val Loss: 32.1448 | Optimizer: Adam\n",
      "Trial 58 | Epoch 05 | Train Loss: 40.3243 | Val Loss: 29.6494 | Optimizer: Adam\n",
      "Trial 58 | Epoch 06 | Train Loss: 30.6567 | Val Loss: 24.1510 | Optimizer: Adam\n",
      "Trial 58 | Epoch 07 | Train Loss: 34.5664 | Val Loss: 22.4477 | Optimizer: Adam\n",
      "Trial 58 | Epoch 08 | Train Loss: 31.0370 | Val Loss: 26.5104 | Optimizer: Adam\n",
      "Trial 58 | Epoch 09 | Train Loss: 28.5141 | Val Loss: 20.9619 | Optimizer: Adam\n",
      "Trial 58 | Epoch 10 | Train Loss: 26.4954 | Val Loss: 21.5620 | Optimizer: Adam\n",
      "Trial 58 | Epoch 11 | Train Loss: 24.3994 | Val Loss: 24.2494 | Optimizer: Adam\n",
      "Trial 58 | Epoch 12 | Train Loss: 23.4529 | Val Loss: 21.7177 | Optimizer: Adam\n",
      "Trial 58 | Epoch 13 | Train Loss: 24.8118 | Val Loss: 31.3229 | Optimizer: Adam\n",
      "Trial 58 | Epoch 14 | Train Loss: 25.4478 | Val Loss: 34.9313 | Optimizer: Adam\n",
      "Trial 58 | Epoch 15 | Train Loss: 27.1545 | Val Loss: 18.3787 | Optimizer: Adam\n",
      "Trial 58 | Epoch 16 | Train Loss: 25.3122 | Val Loss: 19.5898 | Optimizer: Adam\n",
      "Trial 58 | Epoch 17 | Train Loss: 25.0925 | Val Loss: 21.5774 | Optimizer: Adam\n",
      "Trial 58 | Epoch 18 | Train Loss: 22.2032 | Val Loss: 22.9773 | Optimizer: Adam\n",
      "Trial 58 | Epoch 19 | Train Loss: 21.1164 | Val Loss: 24.5384 | Optimizer: Adam\n",
      "Trial 58 | Epoch 20 | Train Loss: 24.4722 | Val Loss: 22.3871 | Optimizer: Adam\n",
      "Trial 58 | Epoch 21 | Train Loss: 21.5903 | Val Loss: 20.9473 | Optimizer: Adam\n",
      "Trial 58 | Epoch 22 | Train Loss: 22.1561 | Val Loss: 21.2599 | Optimizer: Adam\n",
      "Trial 58 | Epoch 23 | Train Loss: 23.3228 | Val Loss: 20.2363 | Optimizer: Adam\n",
      "Trial 58 | Epoch 24 | Train Loss: 22.8571 | Val Loss: 20.4919 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:46,578] Trial 58 finished with value: 18.378675825227567 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.37470298453969536, 'lr': 0.0007871754763646908, 'activation': 'GELU', 'optimizer': 'Adam', 'weight_decay': 1.8084526585737036e-06}. Best is trial 20 with value: 15.11039302794914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 58 | Epoch 25 | Train Loss: 22.2458 | Val Loss: 23.8179 | Optimizer: Adam\n",
      "Trial 58 - Early stopping triggered at epoch 25\n",
      "Trial 59 | Epoch 01 | Train Loss: 184.2040 | Val Loss: 51.5042 | Optimizer: Adam\n",
      "Trial 59 | Epoch 02 | Train Loss: 48.8038 | Val Loss: 39.5984 | Optimizer: Adam\n",
      "Trial 59 | Epoch 03 | Train Loss: 42.7482 | Val Loss: 37.5012 | Optimizer: Adam\n",
      "Trial 59 | Epoch 04 | Train Loss: 35.8229 | Val Loss: 30.4593 | Optimizer: Adam\n",
      "Trial 59 | Epoch 05 | Train Loss: 33.7292 | Val Loss: 35.5486 | Optimizer: Adam\n",
      "Trial 59 | Epoch 06 | Train Loss: 32.1253 | Val Loss: 27.3873 | Optimizer: Adam\n",
      "Trial 59 | Epoch 07 | Train Loss: 30.4307 | Val Loss: 29.6833 | Optimizer: Adam\n",
      "Trial 59 | Epoch 08 | Train Loss: 28.9538 | Val Loss: 26.0670 | Optimizer: Adam\n",
      "Trial 59 | Epoch 09 | Train Loss: 27.2371 | Val Loss: 24.0288 | Optimizer: Adam\n",
      "Trial 59 | Epoch 10 | Train Loss: 26.6786 | Val Loss: 28.9673 | Optimizer: Adam\n",
      "Trial 59 | Epoch 11 | Train Loss: 26.5368 | Val Loss: 25.0489 | Optimizer: Adam\n",
      "Trial 59 | Epoch 12 | Train Loss: 26.3789 | Val Loss: 23.7307 | Optimizer: Adam\n",
      "Trial 59 | Epoch 13 | Train Loss: 23.8484 | Val Loss: 23.6461 | Optimizer: Adam\n",
      "Trial 59 | Epoch 14 | Train Loss: 24.5748 | Val Loss: 22.6307 | Optimizer: Adam\n",
      "Trial 59 | Epoch 15 | Train Loss: 23.4741 | Val Loss: 30.2081 | Optimizer: Adam\n",
      "Trial 59 | Epoch 16 | Train Loss: 22.1174 | Val Loss: 21.5190 | Optimizer: Adam\n",
      "Trial 59 | Epoch 17 | Train Loss: 23.4153 | Val Loss: 21.4778 | Optimizer: Adam\n",
      "Trial 59 | Epoch 18 | Train Loss: 23.8244 | Val Loss: 20.2743 | Optimizer: Adam\n",
      "Trial 59 | Epoch 19 | Train Loss: 24.6263 | Val Loss: 21.8309 | Optimizer: Adam\n",
      "Trial 59 | Epoch 20 | Train Loss: 20.7407 | Val Loss: 24.7350 | Optimizer: Adam\n",
      "Trial 59 | Epoch 21 | Train Loss: 22.5624 | Val Loss: 25.2377 | Optimizer: Adam\n",
      "Trial 59 | Epoch 22 | Train Loss: 22.8996 | Val Loss: 19.4601 | Optimizer: Adam\n",
      "Trial 59 | Epoch 23 | Train Loss: 23.6063 | Val Loss: 20.9522 | Optimizer: Adam\n",
      "Trial 59 | Epoch 24 | Train Loss: 23.6693 | Val Loss: 23.1531 | Optimizer: Adam\n",
      "Trial 59 | Epoch 25 | Train Loss: 20.3126 | Val Loss: 20.9432 | Optimizer: Adam\n",
      "Trial 59 | Epoch 26 | Train Loss: 20.9023 | Val Loss: 18.2805 | Optimizer: Adam\n",
      "Trial 59 | Epoch 27 | Train Loss: 22.4179 | Val Loss: 18.7146 | Optimizer: Adam\n",
      "Trial 59 | Epoch 28 | Train Loss: 21.5399 | Val Loss: 18.0886 | Optimizer: Adam\n",
      "Trial 59 | Epoch 29 | Train Loss: 19.8899 | Val Loss: 25.1035 | Optimizer: Adam\n",
      "Trial 59 | Epoch 30 | Train Loss: 21.2535 | Val Loss: 21.1846 | Optimizer: Adam\n",
      "Trial 59 | Epoch 31 | Train Loss: 19.1974 | Val Loss: 18.4995 | Optimizer: Adam\n",
      "Trial 59 | Epoch 32 | Train Loss: 19.2594 | Val Loss: 18.0242 | Optimizer: Adam\n",
      "Trial 59 | Epoch 33 | Train Loss: 20.9041 | Val Loss: 16.8770 | Optimizer: Adam\n",
      "Trial 59 | Epoch 34 | Train Loss: 21.0199 | Val Loss: 29.3436 | Optimizer: Adam\n",
      "Trial 59 | Epoch 35 | Train Loss: 23.7374 | Val Loss: 30.2580 | Optimizer: Adam\n",
      "Trial 59 | Epoch 36 | Train Loss: 21.2921 | Val Loss: 32.6294 | Optimizer: Adam\n",
      "Trial 59 | Epoch 37 | Train Loss: 22.7326 | Val Loss: 18.9431 | Optimizer: Adam\n",
      "Trial 59 | Epoch 38 | Train Loss: 19.8492 | Val Loss: 17.7544 | Optimizer: Adam\n",
      "Trial 59 | Epoch 39 | Train Loss: 17.9442 | Val Loss: 16.7846 | Optimizer: Adam\n",
      "Trial 59 | Epoch 40 | Train Loss: 18.3770 | Val Loss: 16.7623 | Optimizer: Adam\n",
      "Trial 59 | Epoch 41 | Train Loss: 18.6069 | Val Loss: 17.9773 | Optimizer: Adam\n",
      "Trial 59 | Epoch 42 | Train Loss: 18.6120 | Val Loss: 19.8487 | Optimizer: Adam\n",
      "Trial 59 | Epoch 43 | Train Loss: 20.0814 | Val Loss: 17.4357 | Optimizer: Adam\n",
      "Trial 59 | Epoch 44 | Train Loss: 20.9584 | Val Loss: 15.8581 | Optimizer: Adam\n",
      "Trial 59 | Epoch 45 | Train Loss: 18.4075 | Val Loss: 16.1802 | Optimizer: Adam\n",
      "Trial 59 | Epoch 46 | Train Loss: 17.2569 | Val Loss: 17.2521 | Optimizer: Adam\n",
      "Trial 59 | Epoch 47 | Train Loss: 17.0332 | Val Loss: 19.5248 | Optimizer: Adam\n",
      "Trial 59 | Epoch 48 | Train Loss: 18.0973 | Val Loss: 26.0796 | Optimizer: Adam\n",
      "Trial 59 | Epoch 49 | Train Loss: 18.2560 | Val Loss: 23.7248 | Optimizer: Adam\n",
      "Trial 59 | Epoch 50 | Train Loss: 18.1145 | Val Loss: 18.6636 | Optimizer: Adam\n",
      "Trial 59 | Epoch 51 | Train Loss: 16.1849 | Val Loss: 16.4025 | Optimizer: Adam\n",
      "Trial 59 | Epoch 52 | Train Loss: 16.9116 | Val Loss: 16.6094 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:50,859] Trial 59 finished with value: 15.858131369924157 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.2636706864969312, 'lr': 0.00029811082792742207, 'activation': 'ReLU', 'optimizer': 'Adam', 'weight_decay': 6.1920954181468565e-06}. Best is trial 20 with value: 15.11039302794914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 59 | Epoch 53 | Train Loss: 18.8586 | Val Loss: 19.3154 | Optimizer: Adam\n",
      "Trial 59 | Epoch 54 | Train Loss: 17.2573 | Val Loss: 22.9176 | Optimizer: Adam\n",
      "Trial 59 - Early stopping triggered at epoch 54\n",
      "Trial 60 | Epoch 01 | Train Loss: 156.8249 | Val Loss: 36.6321 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 02 | Train Loss: 50.1597 | Val Loss: 69.0367 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 03 | Train Loss: 50.6689 | Val Loss: 33.2000 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 04 | Train Loss: 41.2126 | Val Loss: 38.7631 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 05 | Train Loss: 40.5594 | Val Loss: 32.7528 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 06 | Train Loss: 33.8385 | Val Loss: 28.7595 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 07 | Train Loss: 30.7852 | Val Loss: 29.2918 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 08 | Train Loss: 30.1680 | Val Loss: 24.8968 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 09 | Train Loss: 27.7019 | Val Loss: 32.0249 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 10 | Train Loss: 28.8552 | Val Loss: 26.6732 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 11 | Train Loss: 28.3311 | Val Loss: 24.6627 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 12 | Train Loss: 27.8408 | Val Loss: 22.5518 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 13 | Train Loss: 25.1234 | Val Loss: 34.5110 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 14 | Train Loss: 25.2843 | Val Loss: 31.2860 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 15 | Train Loss: 27.6117 | Val Loss: 22.8790 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 16 | Train Loss: 29.9081 | Val Loss: 21.6627 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 17 | Train Loss: 26.7835 | Val Loss: 30.4974 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 18 | Train Loss: 25.3088 | Val Loss: 26.0589 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 19 | Train Loss: 23.3117 | Val Loss: 19.9245 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 20 | Train Loss: 24.9600 | Val Loss: 21.9009 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 21 | Train Loss: 22.4648 | Val Loss: 27.8335 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 22 | Train Loss: 21.6941 | Val Loss: 22.9901 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 23 | Train Loss: 22.9616 | Val Loss: 19.3896 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 24 | Train Loss: 20.6205 | Val Loss: 19.5147 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 25 | Train Loss: 22.1874 | Val Loss: 23.6731 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 26 | Train Loss: 21.9953 | Val Loss: 21.3375 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 27 | Train Loss: 21.2157 | Val Loss: 29.0226 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 28 | Train Loss: 22.6408 | Val Loss: 23.3942 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 29 | Train Loss: 22.3509 | Val Loss: 19.1309 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 30 | Train Loss: 18.9399 | Val Loss: 17.9626 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 31 | Train Loss: 22.5470 | Val Loss: 17.1998 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 32 | Train Loss: 21.1682 | Val Loss: 17.9439 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 33 | Train Loss: 21.2491 | Val Loss: 22.0018 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 34 | Train Loss: 19.2427 | Val Loss: 18.7519 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 35 | Train Loss: 19.6542 | Val Loss: 18.1349 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 36 | Train Loss: 19.8315 | Val Loss: 17.1667 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 37 | Train Loss: 17.9545 | Val Loss: 21.3487 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 38 | Train Loss: 17.9522 | Val Loss: 20.2859 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 39 | Train Loss: 18.6618 | Val Loss: 16.7275 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 40 | Train Loss: 19.7290 | Val Loss: 18.8255 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 41 | Train Loss: 18.2684 | Val Loss: 24.6691 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 42 | Train Loss: 17.6688 | Val Loss: 18.1316 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 43 | Train Loss: 16.5562 | Val Loss: 17.5255 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 44 | Train Loss: 18.4189 | Val Loss: 16.6392 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 45 | Train Loss: 22.4127 | Val Loss: 16.4171 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 46 | Train Loss: 21.7303 | Val Loss: 16.6757 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 47 | Train Loss: 20.6695 | Val Loss: 19.3121 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 48 | Train Loss: 16.3076 | Val Loss: 17.4663 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 49 | Train Loss: 18.6207 | Val Loss: 18.0859 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 50 | Train Loss: 18.4450 | Val Loss: 17.9494 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 51 | Train Loss: 18.5079 | Val Loss: 16.8434 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 52 | Train Loss: 17.5847 | Val Loss: 17.2091 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 53 | Train Loss: 16.8210 | Val Loss: 16.9518 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 54 | Train Loss: 19.9679 | Val Loss: 21.5966 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 55 | Train Loss: 21.0760 | Val Loss: 16.1711 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 56 | Train Loss: 17.7919 | Val Loss: 16.4593 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 57 | Train Loss: 17.7084 | Val Loss: 16.8898 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 58 | Train Loss: 18.2146 | Val Loss: 16.7306 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 59 | Train Loss: 17.1902 | Val Loss: 16.1766 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 60 | Train Loss: 16.5237 | Val Loss: 16.4963 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 61 | Train Loss: 16.1359 | Val Loss: 19.8673 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 62 | Train Loss: 19.1821 | Val Loss: 17.5197 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 63 | Train Loss: 18.6897 | Val Loss: 15.9378 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 64 | Train Loss: 16.4485 | Val Loss: 16.6237 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 65 | Train Loss: 15.8445 | Val Loss: 16.9623 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 66 | Train Loss: 17.9909 | Val Loss: 17.0567 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 67 | Train Loss: 17.2885 | Val Loss: 16.3123 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 68 | Train Loss: 16.6651 | Val Loss: 16.3118 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 69 | Train Loss: 16.5591 | Val Loss: 15.8801 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 70 | Train Loss: 17.6974 | Val Loss: 16.6435 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 71 | Train Loss: 16.5691 | Val Loss: 15.3205 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 72 | Train Loss: 15.0724 | Val Loss: 18.0112 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 73 | Train Loss: 17.9029 | Val Loss: 18.5017 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 74 | Train Loss: 15.7854 | Val Loss: 15.7257 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 75 | Train Loss: 13.4042 | Val Loss: 16.0287 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 76 | Train Loss: 15.0758 | Val Loss: 15.9608 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 77 | Train Loss: 15.2778 | Val Loss: 16.3791 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 78 | Train Loss: 14.8004 | Val Loss: 18.1798 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:20:57,273] Trial 60 finished with value: 15.320490627754026 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.275506892609232, 'lr': 0.0005061254008883885, 'activation': 'ReLU', 'optimizer': 'AdamW', 'weight_decay': 6.7314672712150585e-06}. Best is trial 20 with value: 15.11039302794914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 | Epoch 79 | Train Loss: 14.3495 | Val Loss: 16.3345 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 80 | Train Loss: 13.7951 | Val Loss: 16.5988 | Optimizer: AdamW\n",
      "Trial 60 | Epoch 81 | Train Loss: 14.7564 | Val Loss: 19.3325 | Optimizer: AdamW\n",
      "Trial 60 - Early stopping triggered at epoch 81\n",
      "Trial 61 | Epoch 01 | Train Loss: 131.8097 | Val Loss: 36.0186 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 02 | Train Loss: 42.0684 | Val Loss: 42.3106 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 03 | Train Loss: 39.0496 | Val Loss: 32.3943 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 04 | Train Loss: 34.6284 | Val Loss: 32.4819 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 05 | Train Loss: 29.8951 | Val Loss: 29.0514 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 06 | Train Loss: 28.9215 | Val Loss: 24.8745 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 07 | Train Loss: 25.5082 | Val Loss: 25.6738 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 08 | Train Loss: 28.4450 | Val Loss: 28.5995 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 09 | Train Loss: 24.5440 | Val Loss: 27.2085 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 10 | Train Loss: 24.7559 | Val Loss: 23.6780 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 11 | Train Loss: 24.7710 | Val Loss: 23.0937 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 12 | Train Loss: 23.3496 | Val Loss: 21.2474 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 13 | Train Loss: 24.4380 | Val Loss: 21.3938 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 14 | Train Loss: 22.1460 | Val Loss: 19.5294 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 15 | Train Loss: 22.7526 | Val Loss: 19.4642 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 16 | Train Loss: 21.5803 | Val Loss: 22.2001 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 17 | Train Loss: 24.5722 | Val Loss: 18.8062 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 18 | Train Loss: 23.4427 | Val Loss: 20.1642 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 19 | Train Loss: 22.6407 | Val Loss: 19.8718 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 20 | Train Loss: 19.5085 | Val Loss: 22.4747 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 21 | Train Loss: 21.0401 | Val Loss: 22.8892 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 22 | Train Loss: 23.5044 | Val Loss: 21.8869 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 23 | Train Loss: 26.5520 | Val Loss: 27.4573 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 24 | Train Loss: 25.8245 | Val Loss: 19.4533 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 25 | Train Loss: 24.1308 | Val Loss: 17.6004 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 26 | Train Loss: 20.3403 | Val Loss: 17.2611 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 27 | Train Loss: 19.1046 | Val Loss: 18.9870 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 28 | Train Loss: 18.5203 | Val Loss: 27.4815 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 29 | Train Loss: 19.9383 | Val Loss: 26.5926 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 30 | Train Loss: 20.8167 | Val Loss: 23.8967 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 31 | Train Loss: 19.9134 | Val Loss: 24.2605 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 32 | Train Loss: 18.5936 | Val Loss: 22.0691 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 33 | Train Loss: 19.1440 | Val Loss: 17.2577 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 34 | Train Loss: 17.7419 | Val Loss: 20.5458 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 35 | Train Loss: 18.5149 | Val Loss: 18.0568 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 36 | Train Loss: 17.3145 | Val Loss: 21.2602 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 37 | Train Loss: 15.3893 | Val Loss: 22.4913 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 38 | Train Loss: 17.9592 | Val Loss: 16.0545 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 39 | Train Loss: 20.2902 | Val Loss: 20.8512 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 40 | Train Loss: 20.4653 | Val Loss: 18.7302 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 41 | Train Loss: 19.9288 | Val Loss: 16.1286 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 42 | Train Loss: 17.8034 | Val Loss: 21.4923 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 43 | Train Loss: 17.4171 | Val Loss: 21.2555 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 44 | Train Loss: 16.9911 | Val Loss: 27.5177 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 45 | Train Loss: 18.6173 | Val Loss: 16.4279 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:01,091] Trial 61 finished with value: 16.054533741338467 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.2731898641278501, 'lr': 0.0004420920209902722, 'activation': 'ReLU', 'optimizer': 'AdamW', 'weight_decay': 6.455272411962794e-06}. Best is trial 20 with value: 15.11039302794914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 61 | Epoch 46 | Train Loss: 16.3559 | Val Loss: 16.1199 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 47 | Train Loss: 15.0684 | Val Loss: 17.1530 | Optimizer: AdamW\n",
      "Trial 61 | Epoch 48 | Train Loss: 15.5725 | Val Loss: 17.1063 | Optimizer: AdamW\n",
      "Trial 61 - Early stopping triggered at epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:01,254] Trial 62 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 62 | Epoch 01 | Train Loss: 170.9785 | Val Loss: 97.1498 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 01 | Train Loss: 201.1270 | Val Loss: 40.5877 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 02 | Train Loss: 43.4794 | Val Loss: 44.6797 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 03 | Train Loss: 44.8997 | Val Loss: 35.8685 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 04 | Train Loss: 38.1210 | Val Loss: 31.0800 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 05 | Train Loss: 35.7543 | Val Loss: 33.4099 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 06 | Train Loss: 34.3574 | Val Loss: 28.0363 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 07 | Train Loss: 29.4065 | Val Loss: 28.1929 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 08 | Train Loss: 29.6467 | Val Loss: 25.4981 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 09 | Train Loss: 28.8449 | Val Loss: 29.1458 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 10 | Train Loss: 25.5184 | Val Loss: 26.7319 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 11 | Train Loss: 26.3061 | Val Loss: 23.0530 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 12 | Train Loss: 25.6384 | Val Loss: 24.4538 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 13 | Train Loss: 26.8207 | Val Loss: 26.9395 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 14 | Train Loss: 26.8569 | Val Loss: 28.4777 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 15 | Train Loss: 25.0760 | Val Loss: 26.3878 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 16 | Train Loss: 24.7480 | Val Loss: 25.5786 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 17 | Train Loss: 23.7778 | Val Loss: 20.8203 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 18 | Train Loss: 26.3882 | Val Loss: 20.4529 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 19 | Train Loss: 24.2847 | Val Loss: 19.8063 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 20 | Train Loss: 26.3624 | Val Loss: 19.7424 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 21 | Train Loss: 27.0240 | Val Loss: 24.6338 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 22 | Train Loss: 25.0591 | Val Loss: 20.4919 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 23 | Train Loss: 25.8988 | Val Loss: 20.3402 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 24 | Train Loss: 27.6558 | Val Loss: 20.0409 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 25 | Train Loss: 26.4368 | Val Loss: 28.6432 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 26 | Train Loss: 26.2273 | Val Loss: 28.2551 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 27 | Train Loss: 26.0141 | Val Loss: 19.8425 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 28 | Train Loss: 23.7831 | Val Loss: 19.8853 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 29 | Train Loss: 22.7305 | Val Loss: 23.9981 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 30 | Train Loss: 21.8950 | Val Loss: 17.9402 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 31 | Train Loss: 23.0667 | Val Loss: 18.7904 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 32 | Train Loss: 20.4477 | Val Loss: 24.0275 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 33 | Train Loss: 22.0688 | Val Loss: 34.0008 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 34 | Train Loss: 26.3926 | Val Loss: 17.2411 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 35 | Train Loss: 21.0769 | Val Loss: 17.4729 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 36 | Train Loss: 21.9433 | Val Loss: 22.3835 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 37 | Train Loss: 18.2088 | Val Loss: 19.5336 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 38 | Train Loss: 17.7149 | Val Loss: 17.2713 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 39 | Train Loss: 19.0353 | Val Loss: 17.0279 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 40 | Train Loss: 18.2745 | Val Loss: 17.3970 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 41 | Train Loss: 18.1382 | Val Loss: 16.8046 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 42 | Train Loss: 19.5787 | Val Loss: 16.6533 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 43 | Train Loss: 20.1583 | Val Loss: 19.0149 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 44 | Train Loss: 18.9825 | Val Loss: 20.1814 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 45 | Train Loss: 20.8624 | Val Loss: 20.2501 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 46 | Train Loss: 19.8074 | Val Loss: 20.7818 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 47 | Train Loss: 17.9177 | Val Loss: 18.3660 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 48 | Train Loss: 18.6062 | Val Loss: 17.5752 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 49 | Train Loss: 17.9528 | Val Loss: 17.9509 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 50 | Train Loss: 16.1997 | Val Loss: 18.9454 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 51 | Train Loss: 18.6874 | Val Loss: 18.8623 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 52 | Train Loss: 20.1353 | Val Loss: 16.0923 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 53 | Train Loss: 17.9871 | Val Loss: 18.5836 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 54 | Train Loss: 19.3840 | Val Loss: 19.0138 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 55 | Train Loss: 17.8884 | Val Loss: 21.8658 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 56 | Train Loss: 19.9836 | Val Loss: 26.1951 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 57 | Train Loss: 21.2927 | Val Loss: 15.1474 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 58 | Train Loss: 16.8208 | Val Loss: 16.5941 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 59 | Train Loss: 19.3260 | Val Loss: 19.6621 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 60 | Train Loss: 17.4774 | Val Loss: 23.5424 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 61 | Train Loss: 18.8454 | Val Loss: 15.9059 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 62 | Train Loss: 17.5777 | Val Loss: 15.5592 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 63 | Train Loss: 17.5076 | Val Loss: 19.2212 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 64 | Train Loss: 15.3134 | Val Loss: 21.9044 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 65 | Train Loss: 18.1605 | Val Loss: 17.8687 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 66 | Train Loss: 17.9108 | Val Loss: 15.8134 | Optimizer: AdamW\n",
      "Trial 63 | Epoch 67 | Train Loss: 17.3194 | Val Loss: 15.3708 | Optimizer: AdamW\n",
      "Trial 63 - Early stopping triggered at epoch 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:06,564] Trial 63 finished with value: 15.147421030494256 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.27332450928665775, 'lr': 0.0003303636905644971, 'activation': 'ReLU', 'optimizer': 'AdamW', 'weight_decay': 6.332752365633072e-06}. Best is trial 20 with value: 15.11039302794914.\n",
      "[I 2025-09-04 21:21:06,722] Trial 64 pruned. \n",
      "[I 2025-09-04 21:21:06,879] Trial 65 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 64 | Epoch 01 | Train Loss: 248.2933 | Val Loss: 118.0272 | Optimizer: AdamW\n",
      "Trial 65 | Epoch 01 | Train Loss: 178.1236 | Val Loss: 105.8335 | Optimizer: AdamW\n",
      "Trial 66 | Epoch 01 | Train Loss: 168.4305 | Val Loss: 35.0269 | Optimizer: AdamW\n",
      "Trial 66 | Epoch 02 | Train Loss: 54.7502 | Val Loss: 80.5260 | Optimizer: AdamW\n",
      "Trial 66 | Epoch 03 | Train Loss: 57.4351 | Val Loss: 36.5254 | Optimizer: AdamW\n",
      "Trial 66 | Epoch 04 | Train Loss: 46.1229 | Val Loss: 36.0199 | Optimizer: AdamW\n",
      "Trial 66 | Epoch 05 | Train Loss: 37.6554 | Val Loss: 36.5312 | Optimizer: AdamW\n",
      "Trial 66 | Epoch 06 | Train Loss: 35.9207 | Val Loss: 29.8123 | Optimizer: AdamW\n",
      "Trial 66 | Epoch 07 | Train Loss: 33.0258 | Val Loss: 26.7608 | Optimizer: AdamW\n",
      "Trial 66 | Epoch 08 | Train Loss: 29.2045 | Val Loss: 31.2004 | Optimizer: AdamW\n",
      "Trial 66 | Epoch 09 | Train Loss: 29.0482 | Val Loss: 27.3606 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:07,808] Trial 66 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 66 | Epoch 10 | Train Loss: 27.7467 | Val Loss: 26.1015 | Optimizer: AdamW\n",
      "Trial 66 | Epoch 11 | Train Loss: 28.3099 | Val Loss: 31.3853 | Optimizer: AdamW\n",
      "Trial 67 | Epoch 01 | Train Loss: 185.0785 | Val Loss: 41.0928 | Optimizer: AdamW\n",
      "Trial 67 | Epoch 02 | Train Loss: 52.9312 | Val Loss: 49.4882 | Optimizer: AdamW\n",
      "Trial 67 | Epoch 03 | Train Loss: 51.3445 | Val Loss: 55.6538 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:08,283] Trial 67 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 67 | Epoch 04 | Train Loss: 48.6332 | Val Loss: 38.1788 | Optimizer: AdamW\n",
      "Trial 67 | Epoch 05 | Train Loss: 41.5884 | Val Loss: 36.9881 | Optimizer: AdamW\n",
      "Trial 68 | Epoch 01 | Train Loss: 161.3407 | Val Loss: 37.4035 | Optimizer: AdamW\n",
      "Trial 68 | Epoch 02 | Train Loss: 48.0859 | Val Loss: 45.4754 | Optimizer: AdamW\n",
      "Trial 68 | Epoch 03 | Train Loss: 43.0292 | Val Loss: 45.4923 | Optimizer: AdamW\n",
      "Trial 68 | Epoch 04 | Train Loss: 39.4671 | Val Loss: 31.0474 | Optimizer: AdamW\n",
      "Trial 68 | Epoch 05 | Train Loss: 39.3100 | Val Loss: 30.9637 | Optimizer: AdamW\n",
      "Trial 68 | Epoch 06 | Train Loss: 29.7381 | Val Loss: 32.3565 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:08,927] Trial 68 pruned. \n",
      "[I 2025-09-04 21:21:09,085] Trial 69 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 68 | Epoch 07 | Train Loss: 29.9879 | Val Loss: 35.7100 | Optimizer: AdamW\n",
      "Trial 69 | Epoch 01 | Train Loss: 252.8328 | Val Loss: 272.7005 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:09,246] Trial 70 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 70 | Epoch 01 | Train Loss: 9425.3722 | Val Loss: 237.9570 | Optimizer: RMSprop\n",
      "Trial 71 | Epoch 01 | Train Loss: 160.8722 | Val Loss: 104.1962 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:09,407] Trial 71 pruned. \n",
      "[I 2025-09-04 21:21:09,565] Trial 72 pruned. \n",
      "[I 2025-09-04 21:21:09,726] Trial 73 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 72 | Epoch 01 | Train Loss: 154.7349 | Val Loss: 69.4398 | Optimizer: AdamW\n",
      "Trial 73 | Epoch 01 | Train Loss: 192.9506 | Val Loss: 72.5910 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:09,885] Trial 74 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 74 | Epoch 01 | Train Loss: 178.3029 | Val Loss: 123.8578 | Optimizer: AdamW\n",
      "Trial 75 | Epoch 01 | Train Loss: 213.4518 | Val Loss: 47.8743 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:10,197] Trial 75 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 75 | Epoch 02 | Train Loss: 58.3538 | Val Loss: 55.2120 | Optimizer: AdamW\n",
      "Trial 75 | Epoch 03 | Train Loss: 64.1059 | Val Loss: 61.5999 | Optimizer: AdamW\n",
      "Trial 76 | Epoch 01 | NaN loss detected so pruning trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:10,310] Trial 76 pruned. \n",
      "[I 2025-09-04 21:21:10,473] Trial 77 pruned. \n",
      "[I 2025-09-04 21:21:10,637] Trial 78 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 77 | Epoch 01 | Train Loss: 255.6563 | Val Loss: 201.8780 | Optimizer: AdamW\n",
      "Trial 78 | Epoch 01 | Train Loss: 133.3046 | Val Loss: 65.4352 | Optimizer: Adam\n",
      "Trial 79 | Epoch 01 | Train Loss: 180.3015 | Val Loss: 30.2208 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 02 | Train Loss: 55.3104 | Val Loss: 56.4470 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 03 | Train Loss: 51.9653 | Val Loss: 44.2759 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 04 | Train Loss: 40.5164 | Val Loss: 34.5981 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 05 | Train Loss: 37.5088 | Val Loss: 32.1304 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 06 | Train Loss: 32.4577 | Val Loss: 30.4563 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 07 | Train Loss: 31.0455 | Val Loss: 26.3948 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 08 | Train Loss: 28.9018 | Val Loss: 27.1890 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 09 | Train Loss: 26.2943 | Val Loss: 23.5005 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 10 | Train Loss: 24.4385 | Val Loss: 25.1471 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 11 | Train Loss: 25.9198 | Val Loss: 22.2905 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 12 | Train Loss: 25.8426 | Val Loss: 22.4490 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 13 | Train Loss: 23.6593 | Val Loss: 25.3068 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 14 | Train Loss: 26.7485 | Val Loss: 21.7152 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 15 | Train Loss: 20.8682 | Val Loss: 24.1551 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 16 | Train Loss: 21.0323 | Val Loss: 22.8740 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 17 | Train Loss: 24.4356 | Val Loss: 20.2058 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 18 | Train Loss: 24.1147 | Val Loss: 21.4050 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 19 | Train Loss: 21.0639 | Val Loss: 19.3107 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 20 | Train Loss: 24.2643 | Val Loss: 23.0451 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 21 | Train Loss: 23.8153 | Val Loss: 23.3492 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 22 | Train Loss: 19.3841 | Val Loss: 20.1790 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 23 | Train Loss: 20.6754 | Val Loss: 19.2125 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 24 | Train Loss: 19.7656 | Val Loss: 19.2720 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 25 | Train Loss: 21.7968 | Val Loss: 19.5970 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 26 | Train Loss: 20.2604 | Val Loss: 17.2297 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 27 | Train Loss: 19.8353 | Val Loss: 18.6850 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 28 | Train Loss: 18.8423 | Val Loss: 15.9084 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 29 | Train Loss: 20.9795 | Val Loss: 16.1870 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 30 | Train Loss: 20.5011 | Val Loss: 17.5652 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 31 | Train Loss: 16.0185 | Val Loss: 24.2317 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 32 | Train Loss: 20.2920 | Val Loss: 18.3734 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 33 | Train Loss: 16.6053 | Val Loss: 24.7501 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 34 | Train Loss: 18.6385 | Val Loss: 27.7255 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 35 | Train Loss: 20.0419 | Val Loss: 20.9582 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 36 | Train Loss: 16.8913 | Val Loss: 23.2025 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:13,664] Trial 79 finished with value: 15.908379360912292 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.2615298760987676, 'lr': 0.00038716004266474186, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 4.141982647370079e-06}. Best is trial 20 with value: 15.11039302794914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 79 | Epoch 37 | Train Loss: 18.6119 | Val Loss: 16.8627 | Optimizer: AdamW\n",
      "Trial 79 | Epoch 38 | Train Loss: 19.0228 | Val Loss: 16.1468 | Optimizer: AdamW\n",
      "Trial 79 - Early stopping triggered at epoch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:13,825] Trial 80 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 80 | Epoch 01 | Train Loss: 3509.7725 | Val Loss: 1816.3967 | Optimizer: RMSprop\n",
      "Trial 81 | Epoch 01 | Train Loss: 227.6599 | Val Loss: 101.0581 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:13,983] Trial 81 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 82 | Epoch 01 | Train Loss: 148.2908 | Val Loss: 39.2145 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 02 | Train Loss: 44.5019 | Val Loss: 41.9379 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 03 | Train Loss: 39.6152 | Val Loss: 33.1098 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 04 | Train Loss: 36.2762 | Val Loss: 29.9684 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 05 | Train Loss: 34.5668 | Val Loss: 33.9590 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 06 | Train Loss: 29.8486 | Val Loss: 25.1143 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 07 | Train Loss: 28.5930 | Val Loss: 23.2319 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 08 | Train Loss: 26.6697 | Val Loss: 25.1886 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 09 | Train Loss: 24.6144 | Val Loss: 22.4929 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 10 | Train Loss: 24.1700 | Val Loss: 23.5216 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 11 | Train Loss: 25.8028 | Val Loss: 21.8905 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 12 | Train Loss: 22.7022 | Val Loss: 20.6286 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 13 | Train Loss: 22.5603 | Val Loss: 19.0967 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 14 | Train Loss: 23.5862 | Val Loss: 18.8706 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 15 | Train Loss: 20.8425 | Val Loss: 21.2344 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 16 | Train Loss: 20.1146 | Val Loss: 19.3047 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 17 | Train Loss: 22.1562 | Val Loss: 18.0942 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 18 | Train Loss: 21.1642 | Val Loss: 21.1650 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 19 | Train Loss: 22.0813 | Val Loss: 21.2263 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 20 | Train Loss: 20.1665 | Val Loss: 20.9260 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 21 | Train Loss: 19.5688 | Val Loss: 17.1640 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 22 | Train Loss: 19.8604 | Val Loss: 15.8276 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 23 | Train Loss: 19.0268 | Val Loss: 15.3835 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 24 | Train Loss: 21.5008 | Val Loss: 17.2155 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 25 | Train Loss: 19.7138 | Val Loss: 15.3950 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 26 | Train Loss: 18.2603 | Val Loss: 16.0422 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 27 | Train Loss: 20.6107 | Val Loss: 18.5519 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 28 | Train Loss: 20.1547 | Val Loss: 19.6110 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 29 | Train Loss: 17.9092 | Val Loss: 15.6631 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 30 | Train Loss: 18.6954 | Val Loss: 15.3591 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 31 | Train Loss: 21.1497 | Val Loss: 15.8146 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 32 | Train Loss: 22.9582 | Val Loss: 19.0629 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 33 | Train Loss: 20.7549 | Val Loss: 31.9609 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 34 | Train Loss: 20.1467 | Val Loss: 15.3957 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 35 | Train Loss: 15.6901 | Val Loss: 15.2734 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 36 | Train Loss: 16.2641 | Val Loss: 15.0090 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 37 | Train Loss: 16.4233 | Val Loss: 15.9580 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 38 | Train Loss: 18.5807 | Val Loss: 25.4738 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 39 | Train Loss: 18.9225 | Val Loss: 19.4990 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 40 | Train Loss: 17.9978 | Val Loss: 16.4144 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 41 | Train Loss: 19.1375 | Val Loss: 14.1877 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 42 | Train Loss: 18.6097 | Val Loss: 14.2345 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 43 | Train Loss: 17.8085 | Val Loss: 14.9294 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 44 | Train Loss: 15.2814 | Val Loss: 14.7121 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 45 | Train Loss: 14.6660 | Val Loss: 15.3837 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 46 | Train Loss: 15.0268 | Val Loss: 14.8107 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 47 | Train Loss: 14.5652 | Val Loss: 13.9080 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 48 | Train Loss: 14.4948 | Val Loss: 13.6041 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 49 | Train Loss: 14.6719 | Val Loss: 14.3658 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 50 | Train Loss: 16.4241 | Val Loss: 14.2296 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 51 | Train Loss: 16.0268 | Val Loss: 27.6702 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 52 | Train Loss: 17.7696 | Val Loss: 20.2862 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 53 | Train Loss: 17.3459 | Val Loss: 13.5135 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 54 | Train Loss: 15.3151 | Val Loss: 14.1731 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 55 | Train Loss: 17.5230 | Val Loss: 13.4176 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 56 | Train Loss: 14.5515 | Val Loss: 15.3387 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 57 | Train Loss: 14.4551 | Val Loss: 19.0379 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 58 | Train Loss: 14.9413 | Val Loss: 13.2870 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 59 | Train Loss: 15.0270 | Val Loss: 14.9682 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 60 | Train Loss: 12.9503 | Val Loss: 13.7157 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 61 | Train Loss: 13.5922 | Val Loss: 18.9576 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 62 | Train Loss: 15.5012 | Val Loss: 17.0443 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 63 | Train Loss: 14.6029 | Val Loss: 15.4661 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 64 | Train Loss: 15.2773 | Val Loss: 14.8755 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 65 | Train Loss: 13.6250 | Val Loss: 12.9417 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 66 | Train Loss: 13.4196 | Val Loss: 12.2472 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 67 | Train Loss: 12.7176 | Val Loss: 12.1451 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 68 | Train Loss: 13.8397 | Val Loss: 11.9840 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 69 | Train Loss: 13.2689 | Val Loss: 11.3371 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 70 | Train Loss: 12.2800 | Val Loss: 14.1683 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 71 | Train Loss: 12.7852 | Val Loss: 12.3803 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 72 | Train Loss: 11.7334 | Val Loss: 14.8815 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 73 | Train Loss: 12.5773 | Val Loss: 17.4252 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 74 | Train Loss: 13.9658 | Val Loss: 11.5870 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 75 | Train Loss: 11.8156 | Val Loss: 13.5998 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 76 | Train Loss: 11.5668 | Val Loss: 12.6136 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 77 | Train Loss: 12.3425 | Val Loss: 10.8097 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 78 | Train Loss: 13.3228 | Val Loss: 10.5588 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 79 | Train Loss: 12.6286 | Val Loss: 10.6273 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 80 | Train Loss: 11.5407 | Val Loss: 12.6160 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 81 | Train Loss: 13.2615 | Val Loss: 10.4861 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 82 | Train Loss: 11.1600 | Val Loss: 10.7281 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 83 | Train Loss: 10.8869 | Val Loss: 12.0366 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 84 | Train Loss: 10.9919 | Val Loss: 14.2658 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 85 | Train Loss: 11.4317 | Val Loss: 11.7756 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 86 | Train Loss: 10.1536 | Val Loss: 10.4363 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 87 | Train Loss: 11.1492 | Val Loss: 10.7718 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 88 | Train Loss: 11.0119 | Val Loss: 11.5133 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 89 | Train Loss: 9.8536 | Val Loss: 10.1607 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 90 | Train Loss: 12.4421 | Val Loss: 13.1082 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 91 | Train Loss: 10.8016 | Val Loss: 11.6769 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 92 | Train Loss: 10.4602 | Val Loss: 9.5311 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 93 | Train Loss: 10.6468 | Val Loss: 11.2610 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 94 | Train Loss: 10.3941 | Val Loss: 9.1349 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 95 | Train Loss: 10.0000 | Val Loss: 9.1997 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 96 | Train Loss: 9.7795 | Val Loss: 9.4897 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 97 | Train Loss: 9.3162 | Val Loss: 9.8540 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 98 | Train Loss: 9.7478 | Val Loss: 10.0707 | Optimizer: AdamW\n",
      "Trial 82 | Epoch 99 | Train Loss: 9.6269 | Val Loss: 9.5445 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:21,818] Trial 82 finished with value: 9.134948959195517 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.26234125716523543, 'lr': 0.0005228448618646409, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 6.454206158060796e-06}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 83 | Epoch 01 | Train Loss: 169.3696 | Val Loss: 50.3736 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 02 | Train Loss: 45.2865 | Val Loss: 48.9522 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 03 | Train Loss: 40.9628 | Val Loss: 32.4917 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 04 | Train Loss: 36.6290 | Val Loss: 30.0906 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 05 | Train Loss: 32.7051 | Val Loss: 26.0420 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 06 | Train Loss: 28.7895 | Val Loss: 26.3123 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 07 | Train Loss: 26.9474 | Val Loss: 30.4404 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 08 | Train Loss: 23.1353 | Val Loss: 22.1437 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 09 | Train Loss: 26.3729 | Val Loss: 22.0646 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 10 | Train Loss: 26.2548 | Val Loss: 20.7778 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 11 | Train Loss: 27.9229 | Val Loss: 28.7953 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 12 | Train Loss: 24.9805 | Val Loss: 26.2400 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 13 | Train Loss: 25.8283 | Val Loss: 20.9742 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 14 | Train Loss: 22.2728 | Val Loss: 20.0985 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 15 | Train Loss: 20.8050 | Val Loss: 20.6051 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 16 | Train Loss: 22.9997 | Val Loss: 21.9741 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 17 | Train Loss: 19.8565 | Val Loss: 18.6415 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 18 | Train Loss: 22.3383 | Val Loss: 18.0035 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 19 | Train Loss: 21.8628 | Val Loss: 18.0766 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 20 | Train Loss: 19.2796 | Val Loss: 18.1296 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 21 | Train Loss: 20.5232 | Val Loss: 20.2531 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 22 | Train Loss: 19.7308 | Val Loss: 17.9236 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 23 | Train Loss: 21.0255 | Val Loss: 16.9186 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 24 | Train Loss: 22.4345 | Val Loss: 16.5771 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 25 | Train Loss: 19.6575 | Val Loss: 15.9455 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 26 | Train Loss: 19.0987 | Val Loss: 16.5525 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 27 | Train Loss: 23.1602 | Val Loss: 16.6076 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 28 | Train Loss: 22.2145 | Val Loss: 18.3482 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 29 | Train Loss: 23.4643 | Val Loss: 18.3539 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 30 | Train Loss: 18.4389 | Val Loss: 21.9627 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 31 | Train Loss: 18.3486 | Val Loss: 17.9740 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 32 | Train Loss: 19.7733 | Val Loss: 15.0448 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 33 | Train Loss: 17.9678 | Val Loss: 15.4171 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 34 | Train Loss: 15.9534 | Val Loss: 15.9495 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 35 | Train Loss: 17.3604 | Val Loss: 15.4248 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 36 | Train Loss: 15.3913 | Val Loss: 14.8123 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 37 | Train Loss: 17.6310 | Val Loss: 14.6481 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 38 | Train Loss: 15.7855 | Val Loss: 16.4998 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 39 | Train Loss: 14.8946 | Val Loss: 20.1431 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 40 | Train Loss: 16.3529 | Val Loss: 18.1189 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 41 | Train Loss: 15.5756 | Val Loss: 19.1598 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 42 | Train Loss: 16.2705 | Val Loss: 18.2608 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 43 | Train Loss: 14.0043 | Val Loss: 15.4398 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 44 | Train Loss: 15.5977 | Val Loss: 20.6997 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 45 | Train Loss: 16.0970 | Val Loss: 19.3526 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 46 | Train Loss: 15.2837 | Val Loss: 15.2992 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 47 | Train Loss: 13.3676 | Val Loss: 14.4603 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 48 | Train Loss: 13.9459 | Val Loss: 14.6818 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 49 | Train Loss: 14.6516 | Val Loss: 13.8300 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 50 | Train Loss: 15.3189 | Val Loss: 13.7746 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 51 | Train Loss: 15.7912 | Val Loss: 13.6981 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 52 | Train Loss: 15.1560 | Val Loss: 13.2463 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 53 | Train Loss: 14.5224 | Val Loss: 14.4768 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 54 | Train Loss: 15.4175 | Val Loss: 14.5308 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 55 | Train Loss: 13.4449 | Val Loss: 21.5926 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 56 | Train Loss: 15.0494 | Val Loss: 21.9898 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 57 | Train Loss: 16.6691 | Val Loss: 24.2436 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 58 | Train Loss: 17.3351 | Val Loss: 22.8293 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 59 | Train Loss: 17.6062 | Val Loss: 13.8096 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 60 | Train Loss: 15.4118 | Val Loss: 12.8046 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 61 | Train Loss: 14.1627 | Val Loss: 13.6091 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 62 | Train Loss: 14.4539 | Val Loss: 12.2972 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 63 | Train Loss: 15.3165 | Val Loss: 13.2080 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 64 | Train Loss: 13.9791 | Val Loss: 13.0325 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 65 | Train Loss: 13.6742 | Val Loss: 14.8778 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 66 | Train Loss: 13.1925 | Val Loss: 13.2716 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 67 | Train Loss: 13.4059 | Val Loss: 13.9735 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 68 | Train Loss: 13.2041 | Val Loss: 11.9469 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 69 | Train Loss: 13.8801 | Val Loss: 13.4923 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 70 | Train Loss: 16.4835 | Val Loss: 13.0485 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 71 | Train Loss: 15.5134 | Val Loss: 11.9981 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 72 | Train Loss: 13.6530 | Val Loss: 11.9644 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 73 | Train Loss: 12.7760 | Val Loss: 14.3625 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 74 | Train Loss: 13.7358 | Val Loss: 19.9680 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 75 | Train Loss: 14.4034 | Val Loss: 12.7776 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 76 | Train Loss: 13.5267 | Val Loss: 11.5039 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 77 | Train Loss: 13.1074 | Val Loss: 15.3487 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 78 | Train Loss: 14.7185 | Val Loss: 18.0111 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 79 | Train Loss: 12.2468 | Val Loss: 15.8462 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 80 | Train Loss: 11.7620 | Val Loss: 12.6930 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 81 | Train Loss: 11.8723 | Val Loss: 13.6205 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 82 | Train Loss: 11.6528 | Val Loss: 14.2723 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 83 | Train Loss: 11.5541 | Val Loss: 14.7776 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 84 | Train Loss: 13.5545 | Val Loss: 14.2064 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:28,611] Trial 83 finished with value: 11.503930750901137 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.26100251167272137, 'lr': 0.0005478646358976391, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 3.7468630112209242e-06}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 83 | Epoch 85 | Train Loss: 11.8612 | Val Loss: 14.0305 | Optimizer: AdamW\n",
      "Trial 83 | Epoch 86 | Train Loss: 12.6439 | Val Loss: 16.7909 | Optimizer: AdamW\n",
      "Trial 83 - Early stopping triggered at epoch 86\n",
      "Trial 84 | Epoch 01 | Train Loss: 163.7402 | Val Loss: 31.4548 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 02 | Train Loss: 64.6759 | Val Loss: 80.6481 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 03 | Train Loss: 55.2822 | Val Loss: 36.0360 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 04 | Train Loss: 47.6673 | Val Loss: 42.7625 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 05 | Train Loss: 39.2079 | Val Loss: 31.0269 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 06 | Train Loss: 32.7077 | Val Loss: 30.2751 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 07 | Train Loss: 33.4928 | Val Loss: 25.8989 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 08 | Train Loss: 31.0209 | Val Loss: 24.2152 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 09 | Train Loss: 25.6006 | Val Loss: 22.1170 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 10 | Train Loss: 24.5478 | Val Loss: 25.9057 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 11 | Train Loss: 22.5805 | Val Loss: 21.3754 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 12 | Train Loss: 25.8339 | Val Loss: 20.3740 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 13 | Train Loss: 25.5570 | Val Loss: 20.1917 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 14 | Train Loss: 24.0916 | Val Loss: 24.5317 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 15 | Train Loss: 22.2278 | Val Loss: 21.3325 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 16 | Train Loss: 24.4041 | Val Loss: 21.9094 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 17 | Train Loss: 22.1184 | Val Loss: 22.0346 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 18 | Train Loss: 23.0171 | Val Loss: 23.5557 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 19 | Train Loss: 21.4456 | Val Loss: 18.1528 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 20 | Train Loss: 21.8093 | Val Loss: 17.6842 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 21 | Train Loss: 20.6166 | Val Loss: 19.6799 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 22 | Train Loss: 20.3848 | Val Loss: 20.6877 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 23 | Train Loss: 19.3690 | Val Loss: 17.7765 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 24 | Train Loss: 19.5279 | Val Loss: 15.5999 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 25 | Train Loss: 21.9815 | Val Loss: 22.5735 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 26 | Train Loss: 19.5136 | Val Loss: 16.2410 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 27 | Train Loss: 18.7858 | Val Loss: 14.9919 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 28 | Train Loss: 18.3509 | Val Loss: 16.6636 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 29 | Train Loss: 19.2499 | Val Loss: 14.2024 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 30 | Train Loss: 18.1961 | Val Loss: 15.4400 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 31 | Train Loss: 19.0378 | Val Loss: 15.4066 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 32 | Train Loss: 21.0697 | Val Loss: 15.4484 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 33 | Train Loss: 20.3523 | Val Loss: 14.9731 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 34 | Train Loss: 19.0287 | Val Loss: 20.2541 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 35 | Train Loss: 16.8893 | Val Loss: 15.4424 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 36 | Train Loss: 16.5201 | Val Loss: 13.8908 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 37 | Train Loss: 17.9547 | Val Loss: 14.4061 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 38 | Train Loss: 16.4865 | Val Loss: 13.7361 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 39 | Train Loss: 18.1739 | Val Loss: 15.4517 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 40 | Train Loss: 15.2796 | Val Loss: 15.6680 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 41 | Train Loss: 16.0762 | Val Loss: 15.6598 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 42 | Train Loss: 16.5507 | Val Loss: 14.4723 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 43 | Train Loss: 14.1712 | Val Loss: 13.9365 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 44 | Train Loss: 16.3543 | Val Loss: 12.6588 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 45 | Train Loss: 15.6105 | Val Loss: 12.4560 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 46 | Train Loss: 14.2716 | Val Loss: 13.8116 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 47 | Train Loss: 13.8819 | Val Loss: 13.2294 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 48 | Train Loss: 15.3625 | Val Loss: 19.9279 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 49 | Train Loss: 16.6372 | Val Loss: 21.9316 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 50 | Train Loss: 17.2941 | Val Loss: 17.0335 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 51 | Train Loss: 14.6660 | Val Loss: 14.7655 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 52 | Train Loss: 18.1477 | Val Loss: 13.4252 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 53 | Train Loss: 15.0699 | Val Loss: 12.4554 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 54 | Train Loss: 14.4367 | Val Loss: 12.3978 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 55 | Train Loss: 14.4661 | Val Loss: 12.8549 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 56 | Train Loss: 13.6590 | Val Loss: 12.3856 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 57 | Train Loss: 13.7412 | Val Loss: 12.6549 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 58 | Train Loss: 12.0184 | Val Loss: 11.3016 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 59 | Train Loss: 14.4782 | Val Loss: 15.2915 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 60 | Train Loss: 12.5717 | Val Loss: 14.5509 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 61 | Train Loss: 13.4563 | Val Loss: 12.5046 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 62 | Train Loss: 12.3669 | Val Loss: 10.9788 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 63 | Train Loss: 13.2461 | Val Loss: 10.3718 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 64 | Train Loss: 12.8221 | Val Loss: 11.8289 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 65 | Train Loss: 12.7088 | Val Loss: 13.0532 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 66 | Train Loss: 13.5712 | Val Loss: 14.7265 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 67 | Train Loss: 12.6462 | Val Loss: 11.0999 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 68 | Train Loss: 13.7518 | Val Loss: 14.6559 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 69 | Train Loss: 12.7963 | Val Loss: 11.1131 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 70 | Train Loss: 14.7537 | Val Loss: 10.7886 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 71 | Train Loss: 12.0974 | Val Loss: 10.6131 | Optimizer: AdamW\n",
      "Trial 84 | Epoch 72 | Train Loss: 11.7863 | Val Loss: 10.4711 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:34,341] Trial 84 finished with value: 10.371832537457225 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.2531696766329015, 'lr': 0.0006970885506031969, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 7.97796721138696e-06}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 84 | Epoch 73 | Train Loss: 11.6230 | Val Loss: 10.9385 | Optimizer: AdamW\n",
      "Trial 84 - Early stopping triggered at epoch 73\n",
      "Trial 85 | Epoch 01 | Train Loss: 129.6419 | Val Loss: 43.2600 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 02 | Train Loss: 54.7844 | Val Loss: 33.6137 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 03 | Train Loss: 40.9418 | Val Loss: 31.0005 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 04 | Train Loss: 39.1032 | Val Loss: 30.1974 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 05 | Train Loss: 34.9579 | Val Loss: 32.1652 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 06 | Train Loss: 29.4316 | Val Loss: 28.8439 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 07 | Train Loss: 31.8953 | Val Loss: 26.7901 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 08 | Train Loss: 32.0733 | Val Loss: 30.2535 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 09 | Train Loss: 27.1582 | Val Loss: 21.4511 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 10 | Train Loss: 26.3856 | Val Loss: 24.0496 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 11 | Train Loss: 24.6869 | Val Loss: 22.0884 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 12 | Train Loss: 25.2293 | Val Loss: 21.5051 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 13 | Train Loss: 25.0549 | Val Loss: 19.8126 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 14 | Train Loss: 24.4359 | Val Loss: 21.6626 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 15 | Train Loss: 23.7408 | Val Loss: 26.3763 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 16 | Train Loss: 25.5185 | Val Loss: 27.2752 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 17 | Train Loss: 23.2238 | Val Loss: 22.4452 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 18 | Train Loss: 22.3223 | Val Loss: 19.2195 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 19 | Train Loss: 21.1915 | Val Loss: 16.9024 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 20 | Train Loss: 21.3994 | Val Loss: 18.8169 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 21 | Train Loss: 21.8157 | Val Loss: 19.1812 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 22 | Train Loss: 24.3870 | Val Loss: 17.7645 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 23 | Train Loss: 23.7580 | Val Loss: 23.1689 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 24 | Train Loss: 24.9806 | Val Loss: 39.4161 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 25 | Train Loss: 24.0993 | Val Loss: 25.7050 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 26 | Train Loss: 20.9121 | Val Loss: 17.5715 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 27 | Train Loss: 17.7563 | Val Loss: 19.1582 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 28 | Train Loss: 19.3523 | Val Loss: 15.4288 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 29 | Train Loss: 21.3223 | Val Loss: 15.5480 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 30 | Train Loss: 20.5155 | Val Loss: 15.2483 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 31 | Train Loss: 19.3100 | Val Loss: 18.9105 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 32 | Train Loss: 19.6385 | Val Loss: 19.9316 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 33 | Train Loss: 21.1526 | Val Loss: 27.7735 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 34 | Train Loss: 19.2893 | Val Loss: 21.9195 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 35 | Train Loss: 20.7721 | Val Loss: 14.7856 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 36 | Train Loss: 17.5495 | Val Loss: 14.3322 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 37 | Train Loss: 17.1841 | Val Loss: 15.7913 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 38 | Train Loss: 18.5662 | Val Loss: 15.2204 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 39 | Train Loss: 16.0058 | Val Loss: 19.7614 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 40 | Train Loss: 17.8185 | Val Loss: 16.7278 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 41 | Train Loss: 15.7959 | Val Loss: 15.7044 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 42 | Train Loss: 16.5943 | Val Loss: 14.9838 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 43 | Train Loss: 16.2098 | Val Loss: 14.2737 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 44 | Train Loss: 17.9118 | Val Loss: 13.2839 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 45 | Train Loss: 16.9570 | Val Loss: 13.5782 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 46 | Train Loss: 14.6164 | Val Loss: 18.5316 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 47 | Train Loss: 16.5171 | Val Loss: 16.7970 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 48 | Train Loss: 17.2668 | Val Loss: 19.7507 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 49 | Train Loss: 19.9652 | Val Loss: 19.4102 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 50 | Train Loss: 17.4082 | Val Loss: 25.8815 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 51 | Train Loss: 16.3392 | Val Loss: 12.2581 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 52 | Train Loss: 18.7277 | Val Loss: 13.6744 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 53 | Train Loss: 17.1141 | Val Loss: 12.2559 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 54 | Train Loss: 13.8008 | Val Loss: 12.0567 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 55 | Train Loss: 15.9946 | Val Loss: 12.3333 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 56 | Train Loss: 15.2613 | Val Loss: 14.7619 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 57 | Train Loss: 14.0030 | Val Loss: 22.8686 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 58 | Train Loss: 15.5699 | Val Loss: 13.2281 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 59 | Train Loss: 14.8863 | Val Loss: 11.6098 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 60 | Train Loss: 14.3174 | Val Loss: 11.3328 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 61 | Train Loss: 14.5343 | Val Loss: 11.0006 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 62 | Train Loss: 12.3165 | Val Loss: 11.8929 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 63 | Train Loss: 12.1588 | Val Loss: 13.9301 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 64 | Train Loss: 14.5367 | Val Loss: 11.2697 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 65 | Train Loss: 14.0577 | Val Loss: 10.5443 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 66 | Train Loss: 15.0880 | Val Loss: 10.8245 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 67 | Train Loss: 13.8062 | Val Loss: 11.1282 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 68 | Train Loss: 13.4062 | Val Loss: 18.0690 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 69 | Train Loss: 14.4080 | Val Loss: 13.6022 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 70 | Train Loss: 13.3283 | Val Loss: 10.2644 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 71 | Train Loss: 12.0405 | Val Loss: 11.8832 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 72 | Train Loss: 12.3320 | Val Loss: 10.1243 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 73 | Train Loss: 12.0081 | Val Loss: 12.7789 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 74 | Train Loss: 11.7991 | Val Loss: 10.4333 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 75 | Train Loss: 12.2688 | Val Loss: 17.6956 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 76 | Train Loss: 13.5200 | Val Loss: 13.0614 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 77 | Train Loss: 13.1117 | Val Loss: 15.3554 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 78 | Train Loss: 12.9466 | Val Loss: 22.9007 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 79 | Train Loss: 16.2411 | Val Loss: 10.5061 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 80 | Train Loss: 13.3198 | Val Loss: 15.1868 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 81 | Train Loss: 15.9837 | Val Loss: 15.4977 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 82 | Train Loss: 13.2763 | Val Loss: 9.8399 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 83 | Train Loss: 13.7244 | Val Loss: 22.4144 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 84 | Train Loss: 16.3139 | Val Loss: 20.0504 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 85 | Train Loss: 14.8134 | Val Loss: 13.7843 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 86 | Train Loss: 12.3787 | Val Loss: 11.4994 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 87 | Train Loss: 11.2733 | Val Loss: 10.1709 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 88 | Train Loss: 10.8121 | Val Loss: 12.2846 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 89 | Train Loss: 12.3644 | Val Loss: 13.9819 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 90 | Train Loss: 14.4519 | Val Loss: 9.7982 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 91 | Train Loss: 11.0010 | Val Loss: 14.4557 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 92 | Train Loss: 12.8808 | Val Loss: 12.1052 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 93 | Train Loss: 12.3738 | Val Loss: 10.3342 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 94 | Train Loss: 11.0543 | Val Loss: 15.4289 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 95 | Train Loss: 12.0658 | Val Loss: 12.8308 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 96 | Train Loss: 10.0401 | Val Loss: 10.6820 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 97 | Train Loss: 10.8316 | Val Loss: 10.1562 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:42,101] Trial 85 finished with value: 9.798203173691665 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3562465266073639, 'lr': 0.0007307017675073904, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 1.1164779854151833e-05}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 85 | Epoch 98 | Train Loss: 11.7633 | Val Loss: 12.7352 | Optimizer: AdamW\n",
      "Trial 85 | Epoch 99 | Train Loss: 10.5598 | Val Loss: 12.6093 | Optimizer: AdamW\n",
      "Trial 86 | Epoch 01 | Train Loss: 157.1962 | Val Loss: 47.0438 | Optimizer: AdamW\n",
      "Trial 86 | Epoch 02 | Train Loss: 58.9489 | Val Loss: 64.9660 | Optimizer: AdamW\n",
      "Trial 86 | Epoch 03 | Train Loss: 56.4697 | Val Loss: 50.6067 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:42,423] Trial 86 pruned. \n",
      "[I 2025-09-04 21:21:42,584] Trial 87 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 87 | Epoch 01 | Train Loss: 119.3954 | Val Loss: 61.0581 | Optimizer: AdamW\n",
      "Trial 88 | Epoch 01 | Train Loss: 178.6160 | Val Loss: 43.6824 | Optimizer: AdamW\n",
      "Trial 88 | Epoch 02 | Train Loss: 56.5591 | Val Loss: 48.1315 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:42,818] Trial 88 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 88 | Epoch 03 | Train Loss: 53.0638 | Val Loss: 57.5709 | Optimizer: AdamW\n",
      "Trial 89 | Epoch 01 | Train Loss: 180.9608 | Val Loss: 34.5887 | Optimizer: AdamW\n",
      "Trial 89 | Epoch 02 | Train Loss: 74.9696 | Val Loss: 96.6578 | Optimizer: AdamW\n",
      "Trial 89 | Epoch 03 | Train Loss: 68.3565 | Val Loss: 44.0226 | Optimizer: AdamW\n",
      "Trial 89 | Epoch 04 | Train Loss: 51.0021 | Val Loss: 39.7113 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:43,284] Trial 89 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 89 | Epoch 05 | Train Loss: 42.2570 | Val Loss: 35.6632 | Optimizer: AdamW\n",
      "Trial 90 | Epoch 01 | Train Loss: 140.1836 | Val Loss: 46.6012 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:43,673] Trial 90 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 | Epoch 02 | Train Loss: 57.6166 | Val Loss: 55.2055 | Optimizer: AdamW\n",
      "Trial 90 | Epoch 03 | Train Loss: 51.2644 | Val Loss: 42.4638 | Optimizer: AdamW\n",
      "Trial 90 | Epoch 04 | Train Loss: 47.4102 | Val Loss: 40.5931 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:43,837] Trial 91 pruned. \n",
      "[I 2025-09-04 21:21:43,999] Trial 92 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 91 | Epoch 01 | Train Loss: 177.6171 | Val Loss: 132.4667 | Optimizer: AdamW\n",
      "Trial 92 | Epoch 01 | Train Loss: 163.5586 | Val Loss: 108.9519 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 01 | Train Loss: 98.0070 | Val Loss: 49.2421 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 02 | Train Loss: 51.9288 | Val Loss: 41.0156 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 03 | Train Loss: 42.5453 | Val Loss: 35.4199 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 04 | Train Loss: 34.5022 | Val Loss: 31.9699 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 05 | Train Loss: 33.3279 | Val Loss: 24.4658 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 06 | Train Loss: 29.4703 | Val Loss: 23.3460 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 07 | Train Loss: 27.9113 | Val Loss: 31.9366 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 08 | Train Loss: 25.5762 | Val Loss: 26.8704 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 09 | Train Loss: 27.1130 | Val Loss: 20.6157 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 10 | Train Loss: 26.9425 | Val Loss: 20.7437 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 11 | Train Loss: 31.1997 | Val Loss: 21.8755 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 12 | Train Loss: 26.1350 | Val Loss: 20.4540 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 13 | Train Loss: 27.1119 | Val Loss: 19.4106 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 14 | Train Loss: 26.8842 | Val Loss: 19.0008 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 15 | Train Loss: 27.8397 | Val Loss: 20.3840 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 16 | Train Loss: 25.5840 | Val Loss: 24.1870 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 17 | Train Loss: 31.0154 | Val Loss: 19.5328 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 18 | Train Loss: 25.5896 | Val Loss: 33.2818 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 19 | Train Loss: 26.2689 | Val Loss: 33.5318 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 20 | Train Loss: 25.7622 | Val Loss: 36.8086 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 21 | Train Loss: 25.5749 | Val Loss: 25.9203 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 22 | Train Loss: 22.0080 | Val Loss: 19.1120 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 23 | Train Loss: 23.4821 | Val Loss: 16.8468 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 24 | Train Loss: 24.7647 | Val Loss: 17.7845 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 25 | Train Loss: 26.1913 | Val Loss: 20.0940 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 26 | Train Loss: 21.9737 | Val Loss: 17.7867 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 27 | Train Loss: 22.3518 | Val Loss: 15.2670 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 28 | Train Loss: 17.9290 | Val Loss: 15.5998 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 29 | Train Loss: 21.0632 | Val Loss: 16.4270 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 30 | Train Loss: 21.2694 | Val Loss: 16.6429 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 31 | Train Loss: 18.0816 | Val Loss: 19.6500 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 32 | Train Loss: 20.8395 | Val Loss: 25.8155 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 33 | Train Loss: 27.1594 | Val Loss: 26.1529 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 34 | Train Loss: 24.7475 | Val Loss: 32.5409 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 35 | Train Loss: 24.8344 | Val Loss: 24.7805 | Optimizer: AdamW\n",
      "Trial 93 | Epoch 36 | Train Loss: 24.0226 | Val Loss: 24.9434 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:46,940] Trial 93 finished with value: 15.266971277996776 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3934287640738944, 'lr': 0.0008572762567986945, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 5.892695867520759e-06}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 93 | Epoch 37 | Train Loss: 19.6433 | Val Loss: 20.7486 | Optimizer: AdamW\n",
      "Trial 93 - Early stopping triggered at epoch 37\n",
      "Trial 94 | Epoch 01 | Train Loss: 183.6409 | Val Loss: 50.1178 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 02 | Train Loss: 57.6372 | Val Loss: 36.2465 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 03 | Train Loss: 48.1787 | Val Loss: 40.5001 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 04 | Train Loss: 43.4662 | Val Loss: 35.8961 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 05 | Train Loss: 39.2320 | Val Loss: 26.5465 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 06 | Train Loss: 33.7017 | Val Loss: 30.2749 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 07 | Train Loss: 28.6428 | Val Loss: 31.8598 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 08 | Train Loss: 27.5094 | Val Loss: 21.2923 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 09 | Train Loss: 32.1972 | Val Loss: 22.1916 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 10 | Train Loss: 32.0723 | Val Loss: 31.4093 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 11 | Train Loss: 27.8789 | Val Loss: 35.0996 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 12 | Train Loss: 30.7672 | Val Loss: 30.5784 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 13 | Train Loss: 33.3359 | Val Loss: 20.0026 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 14 | Train Loss: 28.3596 | Val Loss: 27.6562 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 15 | Train Loss: 27.8194 | Val Loss: 27.3361 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 16 | Train Loss: 24.5044 | Val Loss: 23.2321 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 17 | Train Loss: 23.9325 | Val Loss: 20.0600 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 18 | Train Loss: 22.3591 | Val Loss: 22.9104 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 19 | Train Loss: 23.7675 | Val Loss: 23.3184 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 20 | Train Loss: 23.2248 | Val Loss: 21.1378 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 21 | Train Loss: 20.1388 | Val Loss: 16.7362 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 22 | Train Loss: 23.5183 | Val Loss: 18.8658 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 23 | Train Loss: 19.9716 | Val Loss: 17.9368 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 24 | Train Loss: 22.3096 | Val Loss: 17.8473 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 25 | Train Loss: 21.1946 | Val Loss: 17.6347 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 26 | Train Loss: 21.7043 | Val Loss: 19.5570 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 27 | Train Loss: 24.1474 | Val Loss: 44.0776 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 28 | Train Loss: 31.5186 | Val Loss: 19.1427 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:49,403] Trial 94 finished with value: 16.736201464645262 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.39322284558999004, 'lr': 0.0008371586120191461, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 7.141185921307453e-06}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 94 | Epoch 29 | Train Loss: 29.6413 | Val Loss: 19.3201 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 30 | Train Loss: 26.7493 | Val Loss: 19.8417 | Optimizer: AdamW\n",
      "Trial 94 | Epoch 31 | Train Loss: 22.9847 | Val Loss: 24.4475 | Optimizer: AdamW\n",
      "Trial 94 - Early stopping triggered at epoch 31\n",
      "Trial 95 | Epoch 01 | Train Loss: 142.1150 | Val Loss: 45.1308 | Optimizer: AdamW\n",
      "Trial 95 | Epoch 02 | Train Loss: 49.3792 | Val Loss: 37.9205 | Optimizer: AdamW\n",
      "Trial 95 | Epoch 03 | Train Loss: 49.2660 | Val Loss: 37.9552 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:49,880] Trial 95 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 95 | Epoch 04 | Train Loss: 42.6454 | Val Loss: 33.9786 | Optimizer: AdamW\n",
      "Trial 95 | Epoch 05 | Train Loss: 34.0710 | Val Loss: 32.7431 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:50,042] Trial 96 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 96 | Epoch 01 | Train Loss: 135.4066 | Val Loss: 65.1487 | Optimizer: AdamW\n",
      "Trial 97 | Epoch 01 | Train Loss: 169.8715 | Val Loss: 39.6440 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:50,433] Trial 97 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 97 | Epoch 02 | Train Loss: 60.9307 | Val Loss: 75.5482 | Optimizer: AdamW\n",
      "Trial 97 | Epoch 03 | Train Loss: 61.9044 | Val Loss: 44.7905 | Optimizer: AdamW\n",
      "Trial 97 | Epoch 04 | Train Loss: 55.2048 | Val Loss: 42.5676 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 01 | Train Loss: 149.4807 | Val Loss: 34.2524 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 02 | Train Loss: 61.0937 | Val Loss: 56.0364 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 03 | Train Loss: 54.2686 | Val Loss: 40.3878 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 04 | Train Loss: 46.1252 | Val Loss: 39.7049 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 05 | Train Loss: 37.9062 | Val Loss: 31.8806 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 06 | Train Loss: 36.0963 | Val Loss: 26.4676 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 07 | Train Loss: 30.3622 | Val Loss: 23.2762 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 08 | Train Loss: 32.9243 | Val Loss: 21.8674 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 09 | Train Loss: 32.0619 | Val Loss: 21.0767 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 10 | Train Loss: 33.3467 | Val Loss: 26.2710 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 11 | Train Loss: 31.0244 | Val Loss: 27.4140 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 12 | Train Loss: 26.6027 | Val Loss: 21.0898 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 13 | Train Loss: 26.6838 | Val Loss: 20.7088 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 14 | Train Loss: 26.0803 | Val Loss: 24.7001 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 15 | Train Loss: 23.9140 | Val Loss: 29.2846 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 16 | Train Loss: 25.4495 | Val Loss: 21.7099 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 17 | Train Loss: 23.5705 | Val Loss: 21.9060 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 18 | Train Loss: 23.6341 | Val Loss: 21.3413 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 19 | Train Loss: 22.7320 | Val Loss: 18.1933 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 20 | Train Loss: 23.3419 | Val Loss: 17.1324 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 21 | Train Loss: 23.8517 | Val Loss: 19.2122 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 22 | Train Loss: 23.3214 | Val Loss: 18.0636 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 23 | Train Loss: 21.8802 | Val Loss: 20.6368 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 24 | Train Loss: 22.2081 | Val Loss: 18.7515 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 25 | Train Loss: 20.7963 | Val Loss: 16.0566 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 26 | Train Loss: 20.0822 | Val Loss: 16.2761 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 27 | Train Loss: 19.7338 | Val Loss: 15.4755 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 28 | Train Loss: 20.0518 | Val Loss: 19.7367 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 29 | Train Loss: 20.7113 | Val Loss: 21.6776 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 30 | Train Loss: 20.6433 | Val Loss: 26.1454 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 31 | Train Loss: 24.7391 | Val Loss: 24.9519 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 32 | Train Loss: 19.6611 | Val Loss: 15.0961 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 33 | Train Loss: 19.5432 | Val Loss: 14.5693 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 34 | Train Loss: 18.9440 | Val Loss: 14.5089 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 35 | Train Loss: 18.1084 | Val Loss: 17.2054 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 36 | Train Loss: 20.4314 | Val Loss: 18.6255 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 37 | Train Loss: 20.1752 | Val Loss: 19.7366 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 38 | Train Loss: 21.7690 | Val Loss: 15.4451 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 39 | Train Loss: 17.7786 | Val Loss: 14.3243 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 40 | Train Loss: 21.5042 | Val Loss: 14.4287 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 41 | Train Loss: 21.2491 | Val Loss: 14.9150 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 42 | Train Loss: 18.4768 | Val Loss: 12.8147 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 43 | Train Loss: 20.0671 | Val Loss: 17.9243 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 44 | Train Loss: 17.4083 | Val Loss: 17.3826 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 45 | Train Loss: 17.7600 | Val Loss: 16.2474 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 46 | Train Loss: 17.6139 | Val Loss: 19.2345 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 47 | Train Loss: 17.1257 | Val Loss: 19.1204 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 48 | Train Loss: 16.9626 | Val Loss: 13.3545 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 49 | Train Loss: 16.6838 | Val Loss: 13.4254 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 50 | Train Loss: 17.1497 | Val Loss: 16.2210 | Optimizer: AdamW\n",
      "Trial 98 | Epoch 51 | Train Loss: 17.5769 | Val Loss: 19.9026 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:54,486] Trial 98 finished with value: 12.814706848888862 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.38645954341635763, 'lr': 0.0006836145941956344, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 8.910044784234928e-05}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 98 | Epoch 52 | Train Loss: 17.9022 | Val Loss: 25.3849 | Optimizer: AdamW\n",
      "Trial 98 - Early stopping triggered at epoch 52\n",
      "Trial 99 | Epoch 01 | Train Loss: 115.8220 | Val Loss: 39.0184 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 02 | Train Loss: 44.6269 | Val Loss: 38.9975 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 03 | Train Loss: 40.4118 | Val Loss: 30.0289 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 04 | Train Loss: 36.6889 | Val Loss: 33.0315 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 05 | Train Loss: 34.0677 | Val Loss: 25.0535 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 06 | Train Loss: 31.2336 | Val Loss: 22.7645 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 07 | Train Loss: 31.0256 | Val Loss: 25.1620 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 08 | Train Loss: 32.7595 | Val Loss: 31.5075 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 09 | Train Loss: 29.5738 | Val Loss: 29.3829 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 10 | Train Loss: 26.6906 | Val Loss: 26.5542 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 11 | Train Loss: 26.3129 | Val Loss: 21.8443 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 12 | Train Loss: 26.9526 | Val Loss: 23.2957 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 13 | Train Loss: 26.7738 | Val Loss: 25.9636 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 14 | Train Loss: 22.7676 | Val Loss: 19.3919 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 15 | Train Loss: 23.7699 | Val Loss: 20.2981 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 16 | Train Loss: 22.1794 | Val Loss: 17.3562 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 17 | Train Loss: 21.0895 | Val Loss: 18.4529 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 18 | Train Loss: 21.9702 | Val Loss: 20.0315 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 19 | Train Loss: 23.9928 | Val Loss: 17.3757 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 20 | Train Loss: 25.3616 | Val Loss: 18.9733 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 21 | Train Loss: 20.0185 | Val Loss: 15.0955 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 22 | Train Loss: 24.2405 | Val Loss: 31.1542 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 23 | Train Loss: 26.3325 | Val Loss: 28.8678 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 24 | Train Loss: 24.5464 | Val Loss: 38.6906 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 25 | Train Loss: 24.0512 | Val Loss: 30.2792 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 26 | Train Loss: 24.1350 | Val Loss: 15.5842 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 27 | Train Loss: 26.6058 | Val Loss: 16.2340 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 28 | Train Loss: 24.8245 | Val Loss: 20.7388 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:56,934] Trial 99 finished with value: 15.095547412469134 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3868917770803063, 'lr': 0.0009842844443434418, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 7.426758100569708e-05}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 99 | Epoch 29 | Train Loss: 21.4763 | Val Loss: 17.0949 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 30 | Train Loss: 22.4568 | Val Loss: 17.6422 | Optimizer: AdamW\n",
      "Trial 99 | Epoch 31 | Train Loss: 23.3180 | Val Loss: 21.3688 | Optimizer: AdamW\n",
      "Trial 99 - Early stopping triggered at epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:21:57,090] Trial 100 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 | Epoch 01 | Train Loss: 180.3370 | Val Loss: 55.9579 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 01 | Train Loss: 198.8690 | Val Loss: 30.4688 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 02 | Train Loss: 57.1926 | Val Loss: 48.3953 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 03 | Train Loss: 40.5072 | Val Loss: 46.0790 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 04 | Train Loss: 45.6126 | Val Loss: 41.9727 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 05 | Train Loss: 35.7401 | Val Loss: 25.9797 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 06 | Train Loss: 34.1675 | Val Loss: 34.6878 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 07 | Train Loss: 35.1234 | Val Loss: 30.1160 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 08 | Train Loss: 33.9194 | Val Loss: 23.2422 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 09 | Train Loss: 31.5900 | Val Loss: 30.0194 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 10 | Train Loss: 27.0796 | Val Loss: 28.2202 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 11 | Train Loss: 32.7000 | Val Loss: 23.5542 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 12 | Train Loss: 28.0955 | Val Loss: 23.3459 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 13 | Train Loss: 26.2960 | Val Loss: 20.4761 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 14 | Train Loss: 26.7035 | Val Loss: 33.3990 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 15 | Train Loss: 25.9498 | Val Loss: 20.8099 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 16 | Train Loss: 23.1600 | Val Loss: 20.6071 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 17 | Train Loss: 22.8293 | Val Loss: 31.5177 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 18 | Train Loss: 25.9117 | Val Loss: 21.4115 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 19 | Train Loss: 25.8604 | Val Loss: 18.6633 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 20 | Train Loss: 23.6373 | Val Loss: 18.6898 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 21 | Train Loss: 25.3877 | Val Loss: 19.3689 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 22 | Train Loss: 18.7834 | Val Loss: 16.8069 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 23 | Train Loss: 23.2632 | Val Loss: 20.8490 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 24 | Train Loss: 23.1169 | Val Loss: 17.8857 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 25 | Train Loss: 20.9308 | Val Loss: 18.3441 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 26 | Train Loss: 22.1182 | Val Loss: 16.2296 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 27 | Train Loss: 24.1560 | Val Loss: 25.7675 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 28 | Train Loss: 22.7085 | Val Loss: 22.2159 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 29 | Train Loss: 25.1083 | Val Loss: 17.4185 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 30 | Train Loss: 22.2118 | Val Loss: 15.8227 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 31 | Train Loss: 23.4804 | Val Loss: 15.2760 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 32 | Train Loss: 24.8987 | Val Loss: 15.7140 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 33 | Train Loss: 24.4992 | Val Loss: 19.4102 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 34 | Train Loss: 20.2698 | Val Loss: 23.6432 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 35 | Train Loss: 21.2344 | Val Loss: 20.2114 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 36 | Train Loss: 19.5233 | Val Loss: 15.1243 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 37 | Train Loss: 18.7898 | Val Loss: 14.8618 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 38 | Train Loss: 21.4002 | Val Loss: 16.8969 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 39 | Train Loss: 18.0210 | Val Loss: 17.3698 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 40 | Train Loss: 19.9763 | Val Loss: 15.9093 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 41 | Train Loss: 22.8652 | Val Loss: 14.9625 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 42 | Train Loss: 20.7534 | Val Loss: 16.7864 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 43 | Train Loss: 18.8473 | Val Loss: 23.4214 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 44 | Train Loss: 18.4526 | Val Loss: 16.8466 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 45 | Train Loss: 20.4513 | Val Loss: 17.5456 | Optimizer: AdamW\n",
      "Trial 101 | Epoch 46 | Train Loss: 19.1415 | Val Loss: 18.1097 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:00,753] Trial 101 finished with value: 14.861772777588387 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.38493067355770816, 'lr': 0.0006715705678767695, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 7.731096998695225e-05}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 101 | Epoch 47 | Train Loss: 17.6288 | Val Loss: 19.8630 | Optimizer: AdamW\n",
      "Trial 101 - Early stopping triggered at epoch 47\n",
      "Trial 102 | Epoch 01 | Train Loss: 137.5554 | Val Loss: 38.7275 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 02 | Train Loss: 45.6631 | Val Loss: 36.8179 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 03 | Train Loss: 47.4178 | Val Loss: 37.0659 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 04 | Train Loss: 37.1132 | Val Loss: 37.0494 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 05 | Train Loss: 35.9153 | Val Loss: 25.9743 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 06 | Train Loss: 29.9621 | Val Loss: 23.5040 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 07 | Train Loss: 28.3440 | Val Loss: 29.4247 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 08 | Train Loss: 24.3649 | Val Loss: 23.0897 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 09 | Train Loss: 30.4625 | Val Loss: 21.3942 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 10 | Train Loss: 25.8297 | Val Loss: 27.7834 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 11 | Train Loss: 27.3806 | Val Loss: 23.0563 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 12 | Train Loss: 24.0199 | Val Loss: 20.0538 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 13 | Train Loss: 25.3056 | Val Loss: 23.0097 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 14 | Train Loss: 26.5645 | Val Loss: 27.4921 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 15 | Train Loss: 25.4949 | Val Loss: 19.3257 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 16 | Train Loss: 23.4518 | Val Loss: 21.5113 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 17 | Train Loss: 21.9396 | Val Loss: 26.1998 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 18 | Train Loss: 23.2131 | Val Loss: 22.9392 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 19 | Train Loss: 22.7205 | Val Loss: 17.8162 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 20 | Train Loss: 22.4953 | Val Loss: 17.5031 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 21 | Train Loss: 20.0319 | Val Loss: 15.2429 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 22 | Train Loss: 19.9120 | Val Loss: 16.4799 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 23 | Train Loss: 21.5937 | Val Loss: 18.0005 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 24 | Train Loss: 21.7936 | Val Loss: 17.8560 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 25 | Train Loss: 24.3265 | Val Loss: 14.9579 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 26 | Train Loss: 19.5599 | Val Loss: 18.6385 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 27 | Train Loss: 18.5195 | Val Loss: 24.7803 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 28 | Train Loss: 20.7417 | Val Loss: 22.7560 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 29 | Train Loss: 20.0076 | Val Loss: 18.5425 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 30 | Train Loss: 21.8205 | Val Loss: 14.1543 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 31 | Train Loss: 20.0511 | Val Loss: 21.3701 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 32 | Train Loss: 16.9189 | Val Loss: 22.8763 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 33 | Train Loss: 18.3712 | Val Loss: 18.2852 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 34 | Train Loss: 18.6643 | Val Loss: 20.0813 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 35 | Train Loss: 17.6495 | Val Loss: 15.4416 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 36 | Train Loss: 18.5725 | Val Loss: 14.8765 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 37 | Train Loss: 19.6840 | Val Loss: 16.6626 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 38 | Train Loss: 18.1277 | Val Loss: 23.7537 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 39 | Train Loss: 19.1924 | Val Loss: 20.9056 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 40 | Train Loss: 23.4728 | Val Loss: 13.3089 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 41 | Train Loss: 21.3982 | Val Loss: 17.9455 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 42 | Train Loss: 21.5002 | Val Loss: 13.9572 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 43 | Train Loss: 20.4221 | Val Loss: 17.6254 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 44 | Train Loss: 19.3617 | Val Loss: 27.2967 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 45 | Train Loss: 18.6535 | Val Loss: 23.5966 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 46 | Train Loss: 19.1559 | Val Loss: 14.2355 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 47 | Train Loss: 17.9955 | Val Loss: 14.8961 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 48 | Train Loss: 17.3087 | Val Loss: 13.8434 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 49 | Train Loss: 19.2638 | Val Loss: 13.0138 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 50 | Train Loss: 15.9307 | Val Loss: 13.2396 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 51 | Train Loss: 18.8241 | Val Loss: 16.9603 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 52 | Train Loss: 19.4016 | Val Loss: 21.8305 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 53 | Train Loss: 18.4750 | Val Loss: 21.5143 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 54 | Train Loss: 19.6611 | Val Loss: 14.4879 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 55 | Train Loss: 20.5867 | Val Loss: 13.0523 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 56 | Train Loss: 16.1159 | Val Loss: 13.7335 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 57 | Train Loss: 17.4655 | Val Loss: 14.3523 | Optimizer: AdamW\n",
      "Trial 102 | Epoch 58 | Train Loss: 17.2602 | Val Loss: 15.5090 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:05,361] Trial 102 finished with value: 13.01377438335884 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.37747008134327, 'lr': 0.0006815932289247624, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 8.028786810548833e-05}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 102 | Epoch 59 | Train Loss: 16.8953 | Val Loss: 16.7721 | Optimizer: AdamW\n",
      "Trial 102 - Early stopping triggered at epoch 59\n",
      "Trial 103 | Epoch 01 | Train Loss: 117.6584 | Val Loss: 60.9265 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:05,520] Trial 103 pruned. \n",
      "[I 2025-09-04 21:22:05,678] Trial 104 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 104 | Epoch 01 | Train Loss: 159.2478 | Val Loss: 51.4149 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 01 | Train Loss: 146.5925 | Val Loss: 31.1003 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 02 | Train Loss: 50.9435 | Val Loss: 48.2821 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 03 | Train Loss: 50.4290 | Val Loss: 44.0514 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 04 | Train Loss: 41.9756 | Val Loss: 32.3137 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 05 | Train Loss: 37.1507 | Val Loss: 26.6303 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 06 | Train Loss: 33.1510 | Val Loss: 32.0457 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 07 | Train Loss: 31.5087 | Val Loss: 35.9916 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 08 | Train Loss: 28.6375 | Val Loss: 21.8391 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 09 | Train Loss: 27.4640 | Val Loss: 21.7387 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 10 | Train Loss: 32.0137 | Val Loss: 35.4576 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 11 | Train Loss: 28.7622 | Val Loss: 25.4296 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 12 | Train Loss: 32.3329 | Val Loss: 20.7462 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 13 | Train Loss: 25.1812 | Val Loss: 24.2580 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 14 | Train Loss: 24.3722 | Val Loss: 22.4802 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 15 | Train Loss: 24.3613 | Val Loss: 19.8945 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 16 | Train Loss: 28.6983 | Val Loss: 25.5935 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 17 | Train Loss: 21.8584 | Val Loss: 22.4909 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 18 | Train Loss: 25.0156 | Val Loss: 19.5994 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 19 | Train Loss: 23.0095 | Val Loss: 20.7274 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 20 | Train Loss: 24.7996 | Val Loss: 25.4059 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 21 | Train Loss: 24.8986 | Val Loss: 17.0776 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 22 | Train Loss: 22.6578 | Val Loss: 16.4551 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 23 | Train Loss: 22.1861 | Val Loss: 19.2710 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 24 | Train Loss: 21.5242 | Val Loss: 18.7802 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 25 | Train Loss: 22.3000 | Val Loss: 17.8743 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 26 | Train Loss: 20.9590 | Val Loss: 19.8219 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 27 | Train Loss: 21.7635 | Val Loss: 23.9161 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 28 | Train Loss: 20.1600 | Val Loss: 22.0469 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 29 | Train Loss: 22.6899 | Val Loss: 23.1550 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 30 | Train Loss: 21.5260 | Val Loss: 26.7988 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 31 | Train Loss: 22.8945 | Val Loss: 21.6799 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 32 | Train Loss: 20.5160 | Val Loss: 16.1363 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 33 | Train Loss: 15.8536 | Val Loss: 15.7694 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 34 | Train Loss: 20.4201 | Val Loss: 15.3565 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 35 | Train Loss: 19.5192 | Val Loss: 15.6299 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 36 | Train Loss: 20.4002 | Val Loss: 17.1282 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 37 | Train Loss: 20.4130 | Val Loss: 19.4757 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 38 | Train Loss: 17.1022 | Val Loss: 15.0592 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 39 | Train Loss: 17.7935 | Val Loss: 17.1996 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 40 | Train Loss: 18.7545 | Val Loss: 16.3239 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 41 | Train Loss: 18.4054 | Val Loss: 14.9353 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 42 | Train Loss: 18.5921 | Val Loss: 15.0355 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 43 | Train Loss: 18.4044 | Val Loss: 14.2218 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 44 | Train Loss: 17.9202 | Val Loss: 14.3049 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 45 | Train Loss: 16.8700 | Val Loss: 15.6002 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 46 | Train Loss: 21.7680 | Val Loss: 13.9942 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 47 | Train Loss: 21.7544 | Val Loss: 17.8204 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 48 | Train Loss: 20.5259 | Val Loss: 17.8356 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 49 | Train Loss: 19.0036 | Val Loss: 21.2787 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 50 | Train Loss: 17.2694 | Val Loss: 17.1223 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 51 | Train Loss: 19.9150 | Val Loss: 16.9754 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 52 | Train Loss: 20.0091 | Val Loss: 16.3892 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 53 | Train Loss: 19.5772 | Val Loss: 15.8371 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 54 | Train Loss: 18.7546 | Val Loss: 13.9043 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 55 | Train Loss: 17.4719 | Val Loss: 14.1141 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 56 | Train Loss: 15.5920 | Val Loss: 15.8418 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 57 | Train Loss: 17.8548 | Val Loss: 18.9855 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 58 | Train Loss: 16.2731 | Val Loss: 17.3590 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 59 | Train Loss: 16.7802 | Val Loss: 14.9143 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 60 | Train Loss: 20.2709 | Val Loss: 12.1017 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 61 | Train Loss: 19.2058 | Val Loss: 24.4318 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 62 | Train Loss: 16.3186 | Val Loss: 21.1655 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 63 | Train Loss: 19.7945 | Val Loss: 21.2771 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 64 | Train Loss: 17.5213 | Val Loss: 14.9884 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 65 | Train Loss: 17.7336 | Val Loss: 18.2983 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 66 | Train Loss: 16.2066 | Val Loss: 22.1226 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 67 | Train Loss: 16.4324 | Val Loss: 15.7845 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 68 | Train Loss: 15.1820 | Val Loss: 13.2590 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 69 | Train Loss: 14.7604 | Val Loss: 11.4093 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 70 | Train Loss: 15.8133 | Val Loss: 11.8710 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 71 | Train Loss: 13.3546 | Val Loss: 12.9187 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 72 | Train Loss: 13.5690 | Val Loss: 12.9438 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 73 | Train Loss: 13.9318 | Val Loss: 11.3201 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 74 | Train Loss: 14.6783 | Val Loss: 11.2386 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 75 | Train Loss: 14.1407 | Val Loss: 11.4899 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 76 | Train Loss: 14.6590 | Val Loss: 11.9359 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 77 | Train Loss: 13.9011 | Val Loss: 10.5546 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 78 | Train Loss: 14.5673 | Val Loss: 10.4569 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 79 | Train Loss: 12.6649 | Val Loss: 12.4126 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 80 | Train Loss: 14.0021 | Val Loss: 13.0469 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 81 | Train Loss: 15.9107 | Val Loss: 10.7559 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 82 | Train Loss: 16.9932 | Val Loss: 11.2986 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 83 | Train Loss: 16.5156 | Val Loss: 10.5675 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 84 | Train Loss: 16.2925 | Val Loss: 10.7986 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 85 | Train Loss: 14.1603 | Val Loss: 10.4770 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 86 | Train Loss: 13.6130 | Val Loss: 10.9539 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 87 | Train Loss: 12.2492 | Val Loss: 11.8834 | Optimizer: AdamW\n",
      "Trial 105 | Epoch 88 | Train Loss: 13.4717 | Val Loss: 11.3211 | Optimizer: AdamW\n",
      "Trial 105 - Early stopping triggered at epoch 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:12,470] Trial 105 finished with value: 10.456899410340844 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.38400594715175823, 'lr': 0.0006942532388223274, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 7.746962586906582e-05}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 106 | Epoch 01 | Train Loss: 141.6016 | Val Loss: 43.4449 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 02 | Train Loss: 50.5416 | Val Loss: 53.9036 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 03 | Train Loss: 51.2343 | Val Loss: 39.6859 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 04 | Train Loss: 44.8838 | Val Loss: 29.3395 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 05 | Train Loss: 35.6374 | Val Loss: 26.7132 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 06 | Train Loss: 30.5417 | Val Loss: 25.0260 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 07 | Train Loss: 29.2589 | Val Loss: 28.7618 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 08 | Train Loss: 26.3988 | Val Loss: 28.3870 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 09 | Train Loss: 27.1232 | Val Loss: 24.5513 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 10 | Train Loss: 28.3613 | Val Loss: 32.5901 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 11 | Train Loss: 25.9164 | Val Loss: 26.1036 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 12 | Train Loss: 24.7114 | Val Loss: 23.0478 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 13 | Train Loss: 23.4941 | Val Loss: 20.3646 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 14 | Train Loss: 25.6031 | Val Loss: 26.5067 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 15 | Train Loss: 25.0878 | Val Loss: 24.9435 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 16 | Train Loss: 23.3699 | Val Loss: 21.2512 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 17 | Train Loss: 23.1309 | Val Loss: 18.3886 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 18 | Train Loss: 20.8166 | Val Loss: 21.0058 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 19 | Train Loss: 21.4055 | Val Loss: 19.8028 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 20 | Train Loss: 19.5707 | Val Loss: 16.5642 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 21 | Train Loss: 21.4231 | Val Loss: 17.2509 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 22 | Train Loss: 20.9324 | Val Loss: 16.8177 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 23 | Train Loss: 19.5077 | Val Loss: 17.8939 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 24 | Train Loss: 17.0287 | Val Loss: 15.3089 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 25 | Train Loss: 18.0432 | Val Loss: 19.3292 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 26 | Train Loss: 20.2722 | Val Loss: 21.5854 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 27 | Train Loss: 16.9741 | Val Loss: 20.0483 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 28 | Train Loss: 18.5271 | Val Loss: 20.2690 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 29 | Train Loss: 17.6907 | Val Loss: 17.1099 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 30 | Train Loss: 16.2063 | Val Loss: 19.1196 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 31 | Train Loss: 17.2181 | Val Loss: 19.9002 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 32 | Train Loss: 16.3453 | Val Loss: 20.2301 | Optimizer: AdamW\n",
      "Trial 106 | Epoch 33 | Train Loss: 17.8093 | Val Loss: 21.3802 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:15,145] Trial 106 finished with value: 15.308860716780996 and parameters: {'gnn_dim': 1024, 'hidden_dim': 384, 'dropout_rate': 0.38523942467790323, 'lr': 0.0006846229917331851, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 7.367672970630745e-05}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 106 | Epoch 34 | Train Loss: 17.0096 | Val Loss: 19.5949 | Optimizer: AdamW\n",
      "Trial 106 - Early stopping triggered at epoch 34\n",
      "Trial 107 | Epoch 01 | Train Loss: 129.3749 | Val Loss: 46.1266 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:15,461] Trial 107 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 107 | Epoch 02 | Train Loss: 56.1983 | Val Loss: 58.3165 | Optimizer: AdamW\n",
      "Trial 107 | Epoch 03 | Train Loss: 57.2151 | Val Loss: 46.5201 | Optimizer: AdamW\n",
      "Trial 108 | Epoch 01 | Train Loss: 145.4729 | Val Loss: 36.6413 | Optimizer: AdamW\n",
      "Trial 108 | Epoch 02 | Train Loss: 48.0120 | Val Loss: 37.0686 | Optimizer: AdamW\n",
      "Trial 108 | Epoch 03 | Train Loss: 41.8151 | Val Loss: 33.8109 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:15,937] Trial 108 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 108 | Epoch 04 | Train Loss: 38.9992 | Val Loss: 33.8435 | Optimizer: AdamW\n",
      "Trial 108 | Epoch 05 | Train Loss: 33.7501 | Val Loss: 32.5058 | Optimizer: AdamW\n",
      "Trial 109 | Epoch 01 | Train Loss: 161.8341 | Val Loss: 32.8450 | Optimizer: AdamW\n",
      "Trial 109 | Epoch 02 | Train Loss: 54.1206 | Val Loss: 70.4955 | Optimizer: AdamW\n",
      "Trial 109 | Epoch 03 | Train Loss: 57.4353 | Val Loss: 38.3456 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:16,399] Trial 109 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 109 | Epoch 04 | Train Loss: 51.3312 | Val Loss: 43.4699 | Optimizer: AdamW\n",
      "Trial 109 | Epoch 05 | Train Loss: 44.7459 | Val Loss: 34.7252 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:16,560] Trial 110 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 110 | Epoch 01 | Train Loss: 211.5550 | Val Loss: 72.4731 | Optimizer: AdamW\n",
      "Trial 111 | Epoch 01 | Train Loss: 157.5876 | Val Loss: 50.7772 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:16,720] Trial 111 pruned. \n",
      "[I 2025-09-04 21:22:16,885] Trial 112 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 112 | Epoch 01 | Train Loss: 128.9045 | Val Loss: 67.5492 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 01 | Train Loss: 149.0167 | Val Loss: 38.5317 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 02 | Train Loss: 45.9942 | Val Loss: 35.9444 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 03 | Train Loss: 47.8858 | Val Loss: 43.5136 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 04 | Train Loss: 39.3717 | Val Loss: 30.6204 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 05 | Train Loss: 36.1253 | Val Loss: 25.8914 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 06 | Train Loss: 29.9573 | Val Loss: 26.6715 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 07 | Train Loss: 27.2552 | Val Loss: 23.8706 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 08 | Train Loss: 27.1222 | Val Loss: 32.7810 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 09 | Train Loss: 27.4956 | Val Loss: 28.0357 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 10 | Train Loss: 29.5891 | Val Loss: 27.9451 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 11 | Train Loss: 25.6540 | Val Loss: 20.4078 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 12 | Train Loss: 23.4358 | Val Loss: 20.4600 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 13 | Train Loss: 25.9641 | Val Loss: 22.0979 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 14 | Train Loss: 25.0288 | Val Loss: 18.3079 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 15 | Train Loss: 25.5657 | Val Loss: 18.3230 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 16 | Train Loss: 22.8285 | Val Loss: 20.0821 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 17 | Train Loss: 22.3649 | Val Loss: 19.6779 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 18 | Train Loss: 21.4096 | Val Loss: 17.5057 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 19 | Train Loss: 21.2378 | Val Loss: 19.3531 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 20 | Train Loss: 21.4756 | Val Loss: 19.7386 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 21 | Train Loss: 21.0383 | Val Loss: 23.3158 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 22 | Train Loss: 19.9152 | Val Loss: 21.4791 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 23 | Train Loss: 20.3747 | Val Loss: 23.8497 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 24 | Train Loss: 21.3473 | Val Loss: 20.3618 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 25 | Train Loss: 18.9199 | Val Loss: 15.2404 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 26 | Train Loss: 21.8938 | Val Loss: 16.0253 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 27 | Train Loss: 22.8862 | Val Loss: 15.3848 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 28 | Train Loss: 19.1883 | Val Loss: 18.1274 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 29 | Train Loss: 18.0836 | Val Loss: 13.7327 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 30 | Train Loss: 19.0974 | Val Loss: 14.1081 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 31 | Train Loss: 18.8954 | Val Loss: 14.5316 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 32 | Train Loss: 18.5319 | Val Loss: 19.3519 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 33 | Train Loss: 17.6079 | Val Loss: 20.5707 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 34 | Train Loss: 19.2947 | Val Loss: 19.1437 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 35 | Train Loss: 18.9386 | Val Loss: 15.4644 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 36 | Train Loss: 17.5912 | Val Loss: 14.7946 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 37 | Train Loss: 17.7477 | Val Loss: 22.1186 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:19,948] Trial 113 finished with value: 13.73273016766804 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3768374139214, 'lr': 0.0006639319983501419, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 4.7829257875961916e-05}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 113 | Epoch 38 | Train Loss: 19.4448 | Val Loss: 30.7622 | Optimizer: AdamW\n",
      "Trial 113 | Epoch 39 | Train Loss: 20.3147 | Val Loss: 20.5591 | Optimizer: AdamW\n",
      "Trial 113 - Early stopping triggered at epoch 39\n",
      "Trial 114 | Epoch 01 | Train Loss: 192.5222 | Val Loss: 30.0417 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 02 | Train Loss: 59.1618 | Val Loss: 74.6858 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 03 | Train Loss: 50.5621 | Val Loss: 34.9191 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 04 | Train Loss: 44.3085 | Val Loss: 47.3238 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 05 | Train Loss: 43.0449 | Val Loss: 31.7389 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 06 | Train Loss: 32.4655 | Val Loss: 27.5610 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 07 | Train Loss: 30.7839 | Val Loss: 25.5439 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 08 | Train Loss: 32.3090 | Val Loss: 22.9594 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 09 | Train Loss: 29.1826 | Val Loss: 26.7366 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 10 | Train Loss: 27.9302 | Val Loss: 29.6348 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 11 | Train Loss: 29.1529 | Val Loss: 21.4038 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 12 | Train Loss: 27.6630 | Val Loss: 20.4201 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 13 | Train Loss: 28.7552 | Val Loss: 25.0623 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 14 | Train Loss: 26.6396 | Val Loss: 27.7525 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 15 | Train Loss: 25.4594 | Val Loss: 22.7835 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 16 | Train Loss: 25.1970 | Val Loss: 19.9772 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 17 | Train Loss: 26.9620 | Val Loss: 19.1674 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 18 | Train Loss: 26.9093 | Val Loss: 23.4267 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 19 | Train Loss: 23.9182 | Val Loss: 18.8137 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 20 | Train Loss: 22.9245 | Val Loss: 18.2983 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 21 | Train Loss: 26.4180 | Val Loss: 21.0099 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 22 | Train Loss: 23.1132 | Val Loss: 24.6868 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 23 | Train Loss: 23.9887 | Val Loss: 17.0124 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 24 | Train Loss: 22.7969 | Val Loss: 16.8487 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 25 | Train Loss: 23.5970 | Val Loss: 17.3262 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 26 | Train Loss: 20.0716 | Val Loss: 20.4309 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 27 | Train Loss: 22.3702 | Val Loss: 18.2552 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 28 | Train Loss: 20.1546 | Val Loss: 20.2916 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 29 | Train Loss: 22.6745 | Val Loss: 28.0589 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 30 | Train Loss: 22.7583 | Val Loss: 24.4206 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 31 | Train Loss: 22.1251 | Val Loss: 16.9441 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 32 | Train Loss: 24.3318 | Val Loss: 16.1130 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 33 | Train Loss: 22.8458 | Val Loss: 18.5292 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 34 | Train Loss: 23.3572 | Val Loss: 28.8025 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 35 | Train Loss: 20.6887 | Val Loss: 16.9260 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 36 | Train Loss: 18.7252 | Val Loss: 15.5901 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 37 | Train Loss: 20.9838 | Val Loss: 19.7865 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 38 | Train Loss: 18.5307 | Val Loss: 31.4270 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 39 | Train Loss: 23.4568 | Val Loss: 22.3657 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 40 | Train Loss: 21.0007 | Val Loss: 15.3447 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 41 | Train Loss: 22.3594 | Val Loss: 17.8876 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 42 | Train Loss: 19.3350 | Val Loss: 17.4792 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 43 | Train Loss: 18.0444 | Val Loss: 15.3698 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 44 | Train Loss: 18.8637 | Val Loss: 14.9003 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 45 | Train Loss: 16.7232 | Val Loss: 15.1644 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 46 | Train Loss: 16.5730 | Val Loss: 15.0225 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 47 | Train Loss: 16.1751 | Val Loss: 21.2658 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 48 | Train Loss: 17.6041 | Val Loss: 18.8052 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 49 | Train Loss: 18.4272 | Val Loss: 14.8464 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 50 | Train Loss: 16.8572 | Val Loss: 13.9280 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 51 | Train Loss: 16.9539 | Val Loss: 15.7579 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 52 | Train Loss: 17.1671 | Val Loss: 14.2854 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 53 | Train Loss: 19.3734 | Val Loss: 13.9929 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 54 | Train Loss: 18.1794 | Val Loss: 15.7573 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 55 | Train Loss: 18.9438 | Val Loss: 18.2024 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 56 | Train Loss: 18.1314 | Val Loss: 13.3603 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 57 | Train Loss: 18.0158 | Val Loss: 13.3721 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 58 | Train Loss: 17.3456 | Val Loss: 11.8435 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 59 | Train Loss: 17.8326 | Val Loss: 12.9652 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 60 | Train Loss: 16.0707 | Val Loss: 12.5203 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 61 | Train Loss: 15.9442 | Val Loss: 12.8157 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 62 | Train Loss: 15.3169 | Val Loss: 14.8759 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 63 | Train Loss: 15.4047 | Val Loss: 14.1404 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 64 | Train Loss: 16.1341 | Val Loss: 14.2486 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 65 | Train Loss: 16.1818 | Val Loss: 13.5331 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 66 | Train Loss: 14.2558 | Val Loss: 12.0635 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 67 | Train Loss: 17.2997 | Val Loss: 11.8297 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 68 | Train Loss: 14.8431 | Val Loss: 23.5692 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 69 | Train Loss: 17.1228 | Val Loss: 11.5972 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 70 | Train Loss: 15.9517 | Val Loss: 15.6729 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 71 | Train Loss: 15.5081 | Val Loss: 18.2232 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 72 | Train Loss: 15.7045 | Val Loss: 15.6561 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 73 | Train Loss: 15.3162 | Val Loss: 14.2066 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 74 | Train Loss: 14.4628 | Val Loss: 15.9222 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 75 | Train Loss: 17.1241 | Val Loss: 15.3260 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 76 | Train Loss: 15.9108 | Val Loss: 15.2836 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 77 | Train Loss: 14.3043 | Val Loss: 21.0819 | Optimizer: AdamW\n",
      "Trial 114 | Epoch 78 | Train Loss: 14.9466 | Val Loss: 13.0253 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:26,102] Trial 114 finished with value: 11.597237742044092 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.37807162727718013, 'lr': 0.0006501435270073714, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 3.101483668701196e-05}. Best is trial 82 with value: 9.134948959195517.\n",
      "[I 2025-09-04 21:22:26,263] Trial 115 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 114 | Epoch 79 | Train Loss: 14.5725 | Val Loss: 11.7151 | Optimizer: AdamW\n",
      "Trial 114 - Early stopping triggered at epoch 79\n",
      "Trial 115 | Epoch 01 | Train Loss: inf | Val Loss: nan | Optimizer: SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:26,386] Trial 116 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 116 | Epoch 01 | Train Loss: 139.6295 | Val Loss: 60.0520 | Optimizer: AdamW\n",
      "Trial 117 | Epoch 01 | Train Loss: 150.1937 | Val Loss: 45.3108 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:26,770] Trial 117 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 117 | Epoch 02 | Train Loss: 54.0629 | Val Loss: 57.2350 | Optimizer: AdamW\n",
      "Trial 117 | Epoch 03 | Train Loss: 48.0124 | Val Loss: 37.8633 | Optimizer: AdamW\n",
      "Trial 117 | Epoch 04 | Train Loss: 50.4901 | Val Loss: 37.6863 | Optimizer: AdamW\n",
      "Trial 118 | Epoch 01 | Train Loss: 129.9728 | Val Loss: 39.9823 | Optimizer: AdamW\n",
      "Trial 118 | Epoch 02 | Train Loss: 53.0528 | Val Loss: 47.9915 | Optimizer: AdamW\n",
      "Trial 118 | Epoch 03 | Train Loss: 49.8695 | Val Loss: 36.7873 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:27,153] Trial 118 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 118 | Epoch 04 | Train Loss: 40.2340 | Val Loss: 36.6009 | Optimizer: AdamW\n",
      "Trial 119 | Epoch 01 | Train Loss: 150.2064 | Val Loss: 43.5826 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:27,467] Trial 119 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 119 | Epoch 02 | Train Loss: 59.4971 | Val Loss: 71.5950 | Optimizer: AdamW\n",
      "Trial 119 | Epoch 03 | Train Loss: 58.6374 | Val Loss: 41.5109 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:27,628] Trial 120 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 120 | Epoch 01 | Train Loss: 170.3212 | Val Loss: 46.3568 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 01 | Train Loss: 157.6398 | Val Loss: 44.6185 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 02 | Train Loss: 44.2821 | Val Loss: 31.3753 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 03 | Train Loss: 41.4440 | Val Loss: 37.2708 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 04 | Train Loss: 32.0755 | Val Loss: 29.0061 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 05 | Train Loss: 30.0523 | Val Loss: 28.3540 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 06 | Train Loss: 30.0231 | Val Loss: 22.8912 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 07 | Train Loss: 29.4803 | Val Loss: 24.9357 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 08 | Train Loss: 27.0790 | Val Loss: 21.7930 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 09 | Train Loss: 26.3979 | Val Loss: 25.7439 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 10 | Train Loss: 28.6300 | Val Loss: 35.1080 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 11 | Train Loss: 29.1915 | Val Loss: 36.3085 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 12 | Train Loss: 27.5926 | Val Loss: 19.1732 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 13 | Train Loss: 27.5859 | Val Loss: 19.6438 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 14 | Train Loss: 29.2304 | Val Loss: 24.2618 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 15 | Train Loss: 25.7728 | Val Loss: 24.8861 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 16 | Train Loss: 27.1589 | Val Loss: 18.7733 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 17 | Train Loss: 27.8181 | Val Loss: 18.4384 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 18 | Train Loss: 23.3984 | Val Loss: 19.9545 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 19 | Train Loss: 21.8618 | Val Loss: 20.0087 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 20 | Train Loss: 22.7098 | Val Loss: 16.9781 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 21 | Train Loss: 21.0101 | Val Loss: 15.9229 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 22 | Train Loss: 20.9079 | Val Loss: 16.8781 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 23 | Train Loss: 24.4469 | Val Loss: 15.6024 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 24 | Train Loss: 21.9083 | Val Loss: 19.1912 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 25 | Train Loss: 21.1469 | Val Loss: 21.0614 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 26 | Train Loss: 20.5727 | Val Loss: 36.7489 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 27 | Train Loss: 22.5640 | Val Loss: 23.6221 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 28 | Train Loss: 20.9627 | Val Loss: 16.7085 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 29 | Train Loss: 21.0327 | Val Loss: 16.2400 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 30 | Train Loss: 19.3195 | Val Loss: 16.6005 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 31 | Train Loss: 18.5168 | Val Loss: 14.5460 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 32 | Train Loss: 20.9779 | Val Loss: 13.9077 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 33 | Train Loss: 18.8923 | Val Loss: 14.2181 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 34 | Train Loss: 20.7175 | Val Loss: 13.6582 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 35 | Train Loss: 19.6064 | Val Loss: 14.1948 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 36 | Train Loss: 19.5926 | Val Loss: 17.2721 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 37 | Train Loss: 19.6351 | Val Loss: 14.3174 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 38 | Train Loss: 17.8244 | Val Loss: 12.8473 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 39 | Train Loss: 17.3277 | Val Loss: 13.8786 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 40 | Train Loss: 18.5923 | Val Loss: 14.0695 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 41 | Train Loss: 22.3111 | Val Loss: 15.8958 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 42 | Train Loss: 16.7351 | Val Loss: 15.0679 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 43 | Train Loss: 17.8255 | Val Loss: 15.6722 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 44 | Train Loss: 15.2568 | Val Loss: 12.7881 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 45 | Train Loss: 15.4195 | Val Loss: 14.4889 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 46 | Train Loss: 17.1374 | Val Loss: 11.8424 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 47 | Train Loss: 17.0375 | Val Loss: 12.4825 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 48 | Train Loss: 16.6131 | Val Loss: 19.2849 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 49 | Train Loss: 16.1879 | Val Loss: 15.3353 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 50 | Train Loss: 15.9227 | Val Loss: 15.2042 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 51 | Train Loss: 15.7955 | Val Loss: 14.2655 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 52 | Train Loss: 16.9982 | Val Loss: 14.1062 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 53 | Train Loss: 17.2733 | Val Loss: 12.1327 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 54 | Train Loss: 16.3424 | Val Loss: 12.4271 | Optimizer: AdamW\n",
      "Trial 121 | Epoch 55 | Train Loss: 14.4034 | Val Loss: 14.2833 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:31,982] Trial 121 finished with value: 11.842361892141946 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.37704609872846584, 'lr': 0.0007638339834854481, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 8.927379680514828e-05}. Best is trial 82 with value: 9.134948959195517.\n",
      "[I 2025-09-04 21:22:32,138] Trial 122 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 121 | Epoch 56 | Train Loss: 17.3902 | Val Loss: 14.0269 | Optimizer: AdamW\n",
      "Trial 121 - Early stopping triggered at epoch 56\n",
      "Trial 122 | Epoch 01 | Train Loss: 143.1349 | Val Loss: 44.8344 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:32,303] Trial 123 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 123 | Epoch 01 | Train Loss: 155.2000 | Val Loss: 73.3710 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 01 | Train Loss: 141.4532 | Val Loss: 38.9118 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 02 | Train Loss: 47.8527 | Val Loss: 42.0947 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 03 | Train Loss: 46.0121 | Val Loss: 35.8198 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 04 | Train Loss: 39.3607 | Val Loss: 30.0795 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 05 | Train Loss: 36.5122 | Val Loss: 37.5309 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 06 | Train Loss: 32.3183 | Val Loss: 26.8705 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 07 | Train Loss: 31.4333 | Val Loss: 22.9999 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 08 | Train Loss: 29.8979 | Val Loss: 22.1079 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 09 | Train Loss: 26.6823 | Val Loss: 25.0136 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 10 | Train Loss: 26.9792 | Val Loss: 20.8943 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 11 | Train Loss: 25.1812 | Val Loss: 23.3019 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 12 | Train Loss: 24.7048 | Val Loss: 20.1256 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 13 | Train Loss: 25.5171 | Val Loss: 21.5164 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 14 | Train Loss: 24.0628 | Val Loss: 21.9393 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 15 | Train Loss: 23.3116 | Val Loss: 22.9453 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 16 | Train Loss: 23.6294 | Val Loss: 23.7794 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 17 | Train Loss: 23.0401 | Val Loss: 25.9498 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 18 | Train Loss: 23.0829 | Val Loss: 18.1126 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 19 | Train Loss: 24.6884 | Val Loss: 24.4258 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 20 | Train Loss: 23.0111 | Val Loss: 28.5352 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 21 | Train Loss: 24.2664 | Val Loss: 24.1613 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 22 | Train Loss: 22.0880 | Val Loss: 27.8765 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 23 | Train Loss: 22.1406 | Val Loss: 17.2878 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 24 | Train Loss: 21.8723 | Val Loss: 15.6734 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 25 | Train Loss: 21.2341 | Val Loss: 17.6605 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 26 | Train Loss: 22.4628 | Val Loss: 16.6529 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 27 | Train Loss: 22.4628 | Val Loss: 15.1533 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 28 | Train Loss: 19.6836 | Val Loss: 24.1644 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 29 | Train Loss: 20.6459 | Val Loss: 19.4331 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 30 | Train Loss: 19.3974 | Val Loss: 21.5343 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 31 | Train Loss: 20.9164 | Val Loss: 21.1679 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 32 | Train Loss: 19.2688 | Val Loss: 16.5742 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 33 | Train Loss: 19.9062 | Val Loss: 19.8705 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 34 | Train Loss: 19.2674 | Val Loss: 20.5872 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 35 | Train Loss: 17.9385 | Val Loss: 17.4057 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 36 | Train Loss: 18.6623 | Val Loss: 13.8771 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 37 | Train Loss: 19.1984 | Val Loss: 14.0246 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 38 | Train Loss: 18.7710 | Val Loss: 13.6458 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 39 | Train Loss: 16.7229 | Val Loss: 13.7204 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 40 | Train Loss: 17.6911 | Val Loss: 14.3093 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 41 | Train Loss: 19.2123 | Val Loss: 13.0200 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 42 | Train Loss: 15.5212 | Val Loss: 13.1785 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 43 | Train Loss: 17.0048 | Val Loss: 12.8317 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 44 | Train Loss: 15.2731 | Val Loss: 12.6808 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 45 | Train Loss: 15.3082 | Val Loss: 14.4753 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 46 | Train Loss: 20.0557 | Val Loss: 11.7699 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 47 | Train Loss: 18.7372 | Val Loss: 13.6294 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 48 | Train Loss: 17.6167 | Val Loss: 21.7710 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 49 | Train Loss: 17.4812 | Val Loss: 13.9906 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 50 | Train Loss: 18.8549 | Val Loss: 13.4420 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 51 | Train Loss: 16.8721 | Val Loss: 12.7414 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 52 | Train Loss: 16.9250 | Val Loss: 18.3950 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 53 | Train Loss: 17.5222 | Val Loss: 11.7288 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 54 | Train Loss: 15.5203 | Val Loss: 13.1445 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 55 | Train Loss: 16.3949 | Val Loss: 14.0943 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 56 | Train Loss: 16.8247 | Val Loss: 12.1369 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 57 | Train Loss: 15.4661 | Val Loss: 13.4765 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 58 | Train Loss: 13.9661 | Val Loss: 14.3180 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 59 | Train Loss: 14.3776 | Val Loss: 12.6055 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 60 | Train Loss: 16.2764 | Val Loss: 13.4898 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 61 | Train Loss: 13.8268 | Val Loss: 18.2328 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:37,234] Trial 124 finished with value: 11.728818932199866 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3773631447522626, 'lr': 0.0006777804388163587, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 8.577853595890161e-05}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 124 | Epoch 62 | Train Loss: 15.7442 | Val Loss: 12.0492 | Optimizer: AdamW\n",
      "Trial 124 | Epoch 63 | Train Loss: 14.2274 | Val Loss: 15.1739 | Optimizer: AdamW\n",
      "Trial 124 - Early stopping triggered at epoch 63\n",
      "Trial 125 | Epoch 01 | Train Loss: 186.3899 | Val Loss: 31.4244 | Optimizer: AdamW\n",
      "Trial 125 | Epoch 02 | Train Loss: 63.6229 | Val Loss: 76.0544 | Optimizer: AdamW\n",
      "Trial 125 | Epoch 03 | Train Loss: 56.1261 | Val Loss: 37.5776 | Optimizer: AdamW\n",
      "Trial 125 | Epoch 04 | Train Loss: 52.9357 | Val Loss: 41.5842 | Optimizer: AdamW\n",
      "Trial 125 | Epoch 05 | Train Loss: 42.2871 | Val Loss: 33.0119 | Optimizer: AdamW\n",
      "Trial 125 | Epoch 06 | Train Loss: 34.2106 | Val Loss: 29.2112 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:37,855] Trial 125 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 125 | Epoch 07 | Train Loss: 33.9823 | Val Loss: 31.2332 | Optimizer: AdamW\n",
      "Trial 126 | Epoch 01 | Train Loss: 164.9244 | Val Loss: 33.5932 | Optimizer: AdamW\n",
      "Trial 126 | Epoch 02 | Train Loss: 57.4600 | Val Loss: 67.3459 | Optimizer: AdamW\n",
      "Trial 126 | Epoch 03 | Train Loss: 54.0009 | Val Loss: 37.2592 | Optimizer: AdamW\n",
      "Trial 126 | Epoch 04 | Train Loss: 53.4245 | Val Loss: 55.1687 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:38,315] Trial 126 pruned. \n",
      "[I 2025-09-04 21:22:38,477] Trial 127 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 126 | Epoch 05 | Train Loss: 48.2232 | Val Loss: 32.4225 | Optimizer: AdamW\n",
      "Trial 127 | Epoch 01 | Train Loss: 190.8712 | Val Loss: 91.0519 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:38,637] Trial 128 pruned. \n",
      "[I 2025-09-04 21:22:38,795] Trial 129 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 128 | Epoch 01 | Train Loss: 146.1190 | Val Loss: 113.3967 | Optimizer: AdamW\n",
      "Trial 129 | Epoch 01 | Train Loss: 362256.0587 | Val Loss: 347.5827 | Optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:38,968] Trial 130 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 130 | Epoch 01 | Train Loss: 114.1920 | Val Loss: 58.6937 | Optimizer: AdamW\n",
      "Trial 131 | Epoch 01 | Train Loss: 190.4296 | Val Loss: 60.3320 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:39,126] Trial 131 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 132 | Epoch 01 | Train Loss: 132.8602 | Val Loss: 38.6221 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 02 | Train Loss: 48.4941 | Val Loss: 42.1089 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 03 | Train Loss: 46.1505 | Val Loss: 36.8070 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 04 | Train Loss: 41.8050 | Val Loss: 34.8552 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 05 | Train Loss: 39.0650 | Val Loss: 27.5358 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 06 | Train Loss: 28.7957 | Val Loss: 25.7221 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 07 | Train Loss: 29.5825 | Val Loss: 25.0888 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 08 | Train Loss: 27.7031 | Val Loss: 21.6778 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 09 | Train Loss: 26.0998 | Val Loss: 24.5127 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 10 | Train Loss: 24.3394 | Val Loss: 31.6597 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 11 | Train Loss: 27.0792 | Val Loss: 20.1518 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 12 | Train Loss: 25.8837 | Val Loss: 19.3975 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 13 | Train Loss: 22.9261 | Val Loss: 20.6043 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 14 | Train Loss: 25.4139 | Val Loss: 29.4616 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 15 | Train Loss: 27.8079 | Val Loss: 19.0704 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 16 | Train Loss: 27.1261 | Val Loss: 18.3461 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 17 | Train Loss: 23.0592 | Val Loss: 18.1327 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 18 | Train Loss: 26.8457 | Val Loss: 26.9689 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 19 | Train Loss: 26.6919 | Val Loss: 20.8709 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 20 | Train Loss: 22.5999 | Val Loss: 25.2217 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 21 | Train Loss: 23.1560 | Val Loss: 18.9413 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 22 | Train Loss: 23.9860 | Val Loss: 19.7079 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 23 | Train Loss: 24.3186 | Val Loss: 19.4794 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 24 | Train Loss: 23.0567 | Val Loss: 19.5739 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 25 | Train Loss: 24.5042 | Val Loss: 17.6898 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 26 | Train Loss: 21.5023 | Val Loss: 23.3181 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 27 | Train Loss: 25.1467 | Val Loss: 42.2424 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 28 | Train Loss: 26.7142 | Val Loss: 16.5275 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 29 | Train Loss: 21.1871 | Val Loss: 18.2268 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 30 | Train Loss: 23.4186 | Val Loss: 22.9040 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 31 | Train Loss: 21.7146 | Val Loss: 16.2076 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 32 | Train Loss: 21.0412 | Val Loss: 15.6065 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 33 | Train Loss: 19.0628 | Val Loss: 15.2550 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 34 | Train Loss: 17.8170 | Val Loss: 14.6479 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 35 | Train Loss: 19.8382 | Val Loss: 14.8130 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 36 | Train Loss: 16.7553 | Val Loss: 17.8530 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 37 | Train Loss: 19.3099 | Val Loss: 16.4987 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 38 | Train Loss: 18.2379 | Val Loss: 15.4568 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 39 | Train Loss: 17.2807 | Val Loss: 15.0798 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 40 | Train Loss: 16.7633 | Val Loss: 13.7681 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 41 | Train Loss: 18.6381 | Val Loss: 14.4806 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 42 | Train Loss: 19.4505 | Val Loss: 14.2565 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 43 | Train Loss: 18.9361 | Val Loss: 22.2165 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 44 | Train Loss: 20.7287 | Val Loss: 17.7291 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 45 | Train Loss: 18.3546 | Val Loss: 13.6528 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 46 | Train Loss: 19.5564 | Val Loss: 15.5163 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 47 | Train Loss: 18.6902 | Val Loss: 13.3729 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 48 | Train Loss: 16.7408 | Val Loss: 16.3472 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 49 | Train Loss: 16.9772 | Val Loss: 22.2971 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 50 | Train Loss: 19.5203 | Val Loss: 14.0825 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 51 | Train Loss: 19.0522 | Val Loss: 12.4692 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 52 | Train Loss: 16.2689 | Val Loss: 12.0594 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 53 | Train Loss: 16.7686 | Val Loss: 12.4128 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 54 | Train Loss: 16.6035 | Val Loss: 16.6166 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 55 | Train Loss: 22.0824 | Val Loss: 12.5819 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 56 | Train Loss: 19.6352 | Val Loss: 12.7763 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 57 | Train Loss: 17.8641 | Val Loss: 13.4377 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 58 | Train Loss: 14.9621 | Val Loss: 17.0377 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 59 | Train Loss: 15.0512 | Val Loss: 11.5169 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 60 | Train Loss: 14.7356 | Val Loss: 12.4294 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 61 | Train Loss: 16.1854 | Val Loss: 12.9995 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 62 | Train Loss: 16.7061 | Val Loss: 11.8714 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 63 | Train Loss: 13.9945 | Val Loss: 12.8639 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 64 | Train Loss: 15.0836 | Val Loss: 13.0307 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 65 | Train Loss: 15.7692 | Val Loss: 11.7849 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 66 | Train Loss: 13.1154 | Val Loss: 11.5803 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:44,492] Trial 132 finished with value: 11.516944877500457 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.37190771369349446, 'lr': 0.0006672913433432091, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 7.56833045550914e-05}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 132 | Epoch 67 | Train Loss: 15.4813 | Val Loss: 12.2549 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 68 | Train Loss: 14.2870 | Val Loss: 14.3861 | Optimizer: AdamW\n",
      "Trial 132 | Epoch 69 | Train Loss: 14.9852 | Val Loss: 14.7989 | Optimizer: AdamW\n",
      "Trial 132 - Early stopping triggered at epoch 69\n",
      "Trial 133 | Epoch 01 | Train Loss: 163.0050 | Val Loss: 34.4088 | Optimizer: AdamW\n",
      "Trial 133 | Epoch 02 | Train Loss: 61.6628 | Val Loss: 85.4033 | Optimizer: AdamW\n",
      "Trial 133 | Epoch 03 | Train Loss: 62.4046 | Val Loss: 41.6603 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:44,954] Trial 133 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 133 | Epoch 04 | Train Loss: 46.1337 | Val Loss: 36.1192 | Optimizer: AdamW\n",
      "Trial 133 | Epoch 05 | Train Loss: 44.3465 | Val Loss: 37.0566 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:45,118] Trial 134 pruned. \n",
      "[I 2025-09-04 21:22:45,277] Trial 135 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 134 | Epoch 01 | Train Loss: 183.6066 | Val Loss: 65.1227 | Optimizer: AdamW\n",
      "Trial 135 | Epoch 01 | Train Loss: 148.9403 | Val Loss: 47.3409 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:45,438] Trial 136 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 136 | Epoch 01 | Train Loss: 265.5020 | Val Loss: 280.1006 | Optimizer: AdamW\n",
      "Trial 137 | Epoch 01 | Train Loss: 243.4577 | Val Loss: 275.0498 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:45,596] Trial 137 pruned. \n",
      "[I 2025-09-04 21:22:45,719] Trial 138 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 138 | Epoch 01 | Train Loss: 193.7676 | Val Loss: 64.9136 | Optimizer: AdamW\n",
      "Trial 139 | Epoch 01 | Train Loss: 185.2691 | Val Loss: 34.7899 | Optimizer: AdamW\n",
      "Trial 139 | Epoch 02 | Train Loss: 59.2482 | Val Loss: 87.7535 | Optimizer: AdamW\n",
      "Trial 139 | Epoch 03 | Train Loss: 65.1403 | Val Loss: 48.6510 | Optimizer: AdamW\n",
      "Trial 139 | Epoch 04 | Train Loss: 49.2849 | Val Loss: 39.9912 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:46,185] Trial 139 pruned. \n",
      "[I 2025-09-04 21:22:46,328] Trial 140 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 139 | Epoch 05 | Train Loss: 49.1465 | Val Loss: 40.9636 | Optimizer: AdamW\n",
      "Trial 140 | Epoch 01 | NaN loss detected so pruning trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:46,487] Trial 141 pruned. \n",
      "[I 2025-09-04 21:22:46,646] Trial 142 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 141 | Epoch 01 | Train Loss: 163.4611 | Val Loss: 49.8631 | Optimizer: AdamW\n",
      "Trial 142 | Epoch 01 | Train Loss: 172.0230 | Val Loss: 62.4316 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 01 | Train Loss: 129.9113 | Val Loss: 38.9829 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 02 | Train Loss: 51.1961 | Val Loss: 37.3627 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 03 | Train Loss: 52.7623 | Val Loss: 41.4116 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 04 | Train Loss: 46.9358 | Val Loss: 32.5886 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 05 | Train Loss: 38.7395 | Val Loss: 29.7816 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 06 | Train Loss: 31.3532 | Val Loss: 25.0847 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 07 | Train Loss: 31.6994 | Val Loss: 22.2246 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 08 | Train Loss: 29.5430 | Val Loss: 22.1634 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 09 | Train Loss: 29.5527 | Val Loss: 21.1753 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 10 | Train Loss: 29.3342 | Val Loss: 27.2147 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 11 | Train Loss: 29.8081 | Val Loss: 30.1294 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 12 | Train Loss: 27.4193 | Val Loss: 31.4556 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 13 | Train Loss: 33.5936 | Val Loss: 21.6004 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 14 | Train Loss: 29.8306 | Val Loss: 26.9655 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 15 | Train Loss: 26.4861 | Val Loss: 28.4440 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 16 | Train Loss: 27.2935 | Val Loss: 22.3828 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 17 | Train Loss: 28.1917 | Val Loss: 19.3450 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 18 | Train Loss: 24.0616 | Val Loss: 19.4898 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 19 | Train Loss: 23.3318 | Val Loss: 23.5549 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 20 | Train Loss: 23.5025 | Val Loss: 23.0998 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 21 | Train Loss: 21.6739 | Val Loss: 22.4432 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 22 | Train Loss: 24.8965 | Val Loss: 18.9575 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 23 | Train Loss: 21.0885 | Val Loss: 17.1686 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 24 | Train Loss: 21.1432 | Val Loss: 16.7921 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 25 | Train Loss: 23.1695 | Val Loss: 17.6389 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 26 | Train Loss: 21.1889 | Val Loss: 22.1166 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 27 | Train Loss: 23.4292 | Val Loss: 22.9634 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 28 | Train Loss: 19.9189 | Val Loss: 15.0547 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 29 | Train Loss: 20.5531 | Val Loss: 19.4562 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 30 | Train Loss: 19.8186 | Val Loss: 16.1181 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 31 | Train Loss: 17.9791 | Val Loss: 16.9324 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 32 | Train Loss: 17.5514 | Val Loss: 17.0923 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 33 | Train Loss: 20.6585 | Val Loss: 16.6016 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 34 | Train Loss: 17.7117 | Val Loss: 16.4404 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 35 | Train Loss: 17.4147 | Val Loss: 14.8191 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 36 | Train Loss: 19.7261 | Val Loss: 18.9131 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 37 | Train Loss: 18.1112 | Val Loss: 18.1926 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 38 | Train Loss: 17.3236 | Val Loss: 14.8079 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 39 | Train Loss: 19.7970 | Val Loss: 14.9197 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 40 | Train Loss: 20.4006 | Val Loss: 14.1644 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 41 | Train Loss: 20.2600 | Val Loss: 16.5948 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 42 | Train Loss: 18.8515 | Val Loss: 18.0008 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 43 | Train Loss: 18.7910 | Val Loss: 14.1399 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 44 | Train Loss: 21.8231 | Val Loss: 15.6854 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 45 | Train Loss: 18.8754 | Val Loss: 13.9363 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 46 | Train Loss: 18.3091 | Val Loss: 13.5985 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 47 | Train Loss: 18.4533 | Val Loss: 13.4584 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 48 | Train Loss: 16.7893 | Val Loss: 13.7837 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 49 | Train Loss: 14.2187 | Val Loss: 16.9111 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 50 | Train Loss: 16.0545 | Val Loss: 16.1528 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 51 | Train Loss: 17.1403 | Val Loss: 19.8167 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 52 | Train Loss: 18.2695 | Val Loss: 29.0299 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 53 | Train Loss: 17.9607 | Val Loss: 20.1103 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 54 | Train Loss: 15.8645 | Val Loss: 17.8808 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:51,100] Trial 143 finished with value: 13.458390577052667 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3868556342903326, 'lr': 0.0007072870720599108, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 6.320898803235578e-05}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 143 | Epoch 55 | Train Loss: 17.7871 | Val Loss: 20.6494 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 56 | Train Loss: 18.1747 | Val Loss: 18.9870 | Optimizer: AdamW\n",
      "Trial 143 | Epoch 57 | Train Loss: 19.0509 | Val Loss: 23.1839 | Optimizer: AdamW\n",
      "Trial 143 - Early stopping triggered at epoch 57\n",
      "Trial 144 | Epoch 01 | Train Loss: 167.6479 | Val Loss: 34.0893 | Optimizer: AdamW\n",
      "Trial 144 | Epoch 02 | Train Loss: 58.2026 | Val Loss: 65.0495 | Optimizer: AdamW\n",
      "Trial 144 | Epoch 03 | Train Loss: 52.9802 | Val Loss: 39.1362 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:51,576] Trial 144 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 144 | Epoch 04 | Train Loss: 47.0180 | Val Loss: 44.6887 | Optimizer: AdamW\n",
      "Trial 144 | Epoch 05 | Train Loss: 42.4185 | Val Loss: 31.9355 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:51,742] Trial 145 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 145 | Epoch 01 | Train Loss: 182.4981 | Val Loss: 204.0006 | Optimizer: AdamW\n",
      "Trial 146 | Epoch 01 | Train Loss: 139.2698 | Val Loss: 32.3561 | Optimizer: AdamW\n",
      "Trial 146 | Epoch 02 | Train Loss: 59.9098 | Val Loss: 70.4877 | Optimizer: AdamW\n",
      "Trial 146 | Epoch 03 | Train Loss: 53.8228 | Val Loss: 41.7257 | Optimizer: AdamW\n",
      "Trial 146 | Epoch 04 | Train Loss: 51.1488 | Val Loss: 43.6706 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:52,215] Trial 146 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 146 | Epoch 05 | Train Loss: 43.0529 | Val Loss: 34.3800 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 01 | Train Loss: 148.7066 | Val Loss: 41.6960 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 02 | Train Loss: 43.1124 | Val Loss: 33.3422 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 03 | Train Loss: 45.8410 | Val Loss: 34.5258 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 04 | Train Loss: 40.3732 | Val Loss: 27.5023 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 05 | Train Loss: 33.1528 | Val Loss: 28.1440 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 06 | Train Loss: 32.2353 | Val Loss: 27.7461 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 07 | Train Loss: 28.9029 | Val Loss: 26.6897 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 08 | Train Loss: 28.0022 | Val Loss: 23.0170 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 09 | Train Loss: 29.6544 | Val Loss: 23.3699 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 10 | Train Loss: 28.0107 | Val Loss: 20.8443 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 11 | Train Loss: 31.2688 | Val Loss: 20.3357 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 12 | Train Loss: 27.2846 | Val Loss: 19.4640 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 13 | Train Loss: 26.4469 | Val Loss: 19.0226 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 14 | Train Loss: 25.7685 | Val Loss: 18.8128 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 15 | Train Loss: 27.2421 | Val Loss: 31.6818 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 16 | Train Loss: 24.4993 | Val Loss: 20.8008 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 17 | Train Loss: 24.9607 | Val Loss: 17.6286 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 18 | Train Loss: 22.9513 | Val Loss: 18.7474 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 19 | Train Loss: 22.5781 | Val Loss: 19.7285 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 20 | Train Loss: 21.8671 | Val Loss: 16.2932 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 21 | Train Loss: 22.0653 | Val Loss: 22.5688 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 22 | Train Loss: 21.7112 | Val Loss: 25.8898 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 23 | Train Loss: 23.3043 | Val Loss: 18.4977 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 24 | Train Loss: 19.8293 | Val Loss: 29.6787 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 25 | Train Loss: 26.8802 | Val Loss: 34.9981 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 26 | Train Loss: 25.1508 | Val Loss: 20.5108 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 27 | Train Loss: 20.9045 | Val Loss: 24.6463 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 28 | Train Loss: 22.1623 | Val Loss: 17.9990 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 29 | Train Loss: 19.8911 | Val Loss: 16.5848 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 30 | Train Loss: 18.4797 | Val Loss: 16.2493 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 31 | Train Loss: 18.3442 | Val Loss: 16.1092 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 32 | Train Loss: 18.4076 | Val Loss: 15.5301 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 33 | Train Loss: 20.2736 | Val Loss: 23.7399 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 34 | Train Loss: 18.9890 | Val Loss: 14.3682 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 35 | Train Loss: 21.1263 | Val Loss: 23.7919 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 36 | Train Loss: 21.4222 | Val Loss: 18.3601 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 37 | Train Loss: 19.3175 | Val Loss: 14.9498 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 38 | Train Loss: 21.3311 | Val Loss: 15.4160 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 39 | Train Loss: 20.1392 | Val Loss: 15.5406 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 40 | Train Loss: 22.0329 | Val Loss: 16.2965 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 41 | Train Loss: 20.7669 | Val Loss: 18.4431 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 42 | Train Loss: 18.8822 | Val Loss: 23.8432 | Optimizer: AdamW\n",
      "Trial 147 | Epoch 43 | Train Loss: 18.7895 | Val Loss: 23.0000 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:55,651] Trial 147 finished with value: 14.368151478651093 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3825658562457436, 'lr': 0.0007242441800119607, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 9.933016191772933e-05}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 147 | Epoch 44 | Train Loss: 17.9122 | Val Loss: 19.9230 | Optimizer: AdamW\n",
      "Trial 147 - Early stopping triggered at epoch 44\n",
      "Trial 148 | Epoch 01 | Train Loss: 213.8198 | Val Loss: 36.0794 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:56,037] Trial 148 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 148 | Epoch 02 | Train Loss: 83.4309 | Val Loss: 108.8058 | Optimizer: AdamW\n",
      "Trial 148 | Epoch 03 | Train Loss: 80.7192 | Val Loss: 50.3703 | Optimizer: AdamW\n",
      "Trial 148 | Epoch 04 | Train Loss: 53.6771 | Val Loss: 37.4159 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 01 | Train Loss: 147.8975 | Val Loss: 34.0088 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 02 | Train Loss: 45.6863 | Val Loss: 35.8574 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 03 | Train Loss: 39.7799 | Val Loss: 33.7506 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 04 | Train Loss: 39.6583 | Val Loss: 28.3660 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 05 | Train Loss: 36.8578 | Val Loss: 33.2674 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 06 | Train Loss: 32.6866 | Val Loss: 25.5935 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 07 | Train Loss: 28.3491 | Val Loss: 27.7618 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 08 | Train Loss: 35.8631 | Val Loss: 25.1620 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 09 | Train Loss: 28.0647 | Val Loss: 30.9450 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 10 | Train Loss: 29.7850 | Val Loss: 21.4298 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 11 | Train Loss: 29.3167 | Val Loss: 26.8346 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 12 | Train Loss: 26.9185 | Val Loss: 26.5364 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 13 | Train Loss: 24.7838 | Val Loss: 21.3703 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 14 | Train Loss: 26.1171 | Val Loss: 20.3686 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 15 | Train Loss: 25.7078 | Val Loss: 23.6857 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 16 | Train Loss: 26.4554 | Val Loss: 19.3747 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 17 | Train Loss: 22.2745 | Val Loss: 22.9873 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 18 | Train Loss: 25.3180 | Val Loss: 20.0514 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 19 | Train Loss: 24.0578 | Val Loss: 17.8693 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 20 | Train Loss: 27.4052 | Val Loss: 17.4600 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 21 | Train Loss: 24.7043 | Val Loss: 29.0482 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 22 | Train Loss: 23.7212 | Val Loss: 23.5411 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 23 | Train Loss: 24.4420 | Val Loss: 17.0463 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 24 | Train Loss: 23.7326 | Val Loss: 19.5268 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 25 | Train Loss: 22.3141 | Val Loss: 17.6291 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 26 | Train Loss: 19.9503 | Val Loss: 19.8786 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 27 | Train Loss: 20.0432 | Val Loss: 17.5216 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 28 | Train Loss: 21.0151 | Val Loss: 17.8073 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 29 | Train Loss: 18.5216 | Val Loss: 20.1560 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 30 | Train Loss: 19.9971 | Val Loss: 14.7756 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 31 | Train Loss: 22.0823 | Val Loss: 16.6988 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 32 | Train Loss: 20.6913 | Val Loss: 17.0187 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 33 | Train Loss: 20.8092 | Val Loss: 15.8173 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 34 | Train Loss: 18.6649 | Val Loss: 15.3249 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 35 | Train Loss: 19.9752 | Val Loss: 16.9972 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 36 | Train Loss: 19.3999 | Val Loss: 17.1217 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 37 | Train Loss: 19.0862 | Val Loss: 19.7791 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 38 | Train Loss: 18.8952 | Val Loss: 24.5378 | Optimizer: AdamW\n",
      "Trial 149 | Epoch 39 | Train Loss: 19.3723 | Val Loss: 23.0851 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:59,171] Trial 149 finished with value: 14.77558566302788 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.37098292733399957, 'lr': 0.0005267826646317277, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 1.6894197144251622e-05}. Best is trial 82 with value: 9.134948959195517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 149 | Epoch 40 | Train Loss: 18.3154 | Val Loss: 22.1286 | Optimizer: AdamW\n",
      "Trial 149 - Early stopping triggered at epoch 40\n",
      "Trial 150 | Epoch 01 | Train Loss: 19383.4598 | Val Loss: 3654.4555 | Optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:22:59,332] Trial 150 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 151 | Epoch 01 | Train Loss: 177.3579 | Val Loss: 41.6148 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 02 | Train Loss: 47.8845 | Val Loss: 42.6199 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 03 | Train Loss: 38.3445 | Val Loss: 30.5023 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 04 | Train Loss: 41.6407 | Val Loss: 30.1894 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 05 | Train Loss: 35.4872 | Val Loss: 28.7069 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 06 | Train Loss: 34.1330 | Val Loss: 27.3211 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 07 | Train Loss: 31.4365 | Val Loss: 23.5607 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 08 | Train Loss: 32.2345 | Val Loss: 28.0913 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 09 | Train Loss: 31.9217 | Val Loss: 29.3847 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 10 | Train Loss: 30.2842 | Val Loss: 21.8256 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 11 | Train Loss: 24.7341 | Val Loss: 20.8978 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 12 | Train Loss: 26.0340 | Val Loss: 21.7146 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 13 | Train Loss: 27.7323 | Val Loss: 22.4704 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 14 | Train Loss: 27.8965 | Val Loss: 22.5515 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 15 | Train Loss: 23.6829 | Val Loss: 25.8673 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 16 | Train Loss: 25.7722 | Val Loss: 21.5433 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 17 | Train Loss: 25.3616 | Val Loss: 26.5504 | Optimizer: AdamW\n",
      "Trial 151 | Epoch 18 | Train Loss: 26.2897 | Val Loss: 22.4238 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:00,860] Trial 151 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 151 | Epoch 19 | Train Loss: 24.2552 | Val Loss: 20.7383 | Optimizer: AdamW\n",
      "Trial 152 | Epoch 01 | Train Loss: 175.5634 | Val Loss: 34.7629 | Optimizer: AdamW\n",
      "Trial 152 | Epoch 02 | Train Loss: 56.3612 | Val Loss: 68.6914 | Optimizer: AdamW\n",
      "Trial 152 | Epoch 03 | Train Loss: 52.5293 | Val Loss: 34.5400 | Optimizer: AdamW\n",
      "Trial 152 | Epoch 04 | Train Loss: 51.8457 | Val Loss: 34.5757 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:01,403] Trial 152 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 152 | Epoch 05 | Train Loss: 38.8921 | Val Loss: 31.3065 | Optimizer: AdamW\n",
      "Trial 152 | Epoch 06 | Train Loss: 39.8507 | Val Loss: 31.1751 | Optimizer: AdamW\n",
      "Trial 153 | Epoch 01 | Train Loss: 165.7253 | Val Loss: 38.5453 | Optimizer: AdamW\n",
      "Trial 153 | Epoch 02 | Train Loss: 59.5984 | Val Loss: 72.9847 | Optimizer: AdamW\n",
      "Trial 153 | Epoch 03 | Train Loss: 57.9996 | Val Loss: 42.8914 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:01,798] Trial 153 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 153 | Epoch 04 | Train Loss: 50.8449 | Val Loss: 38.1562 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 01 | Train Loss: 170.0115 | Val Loss: 28.3626 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 02 | Train Loss: 55.9012 | Val Loss: 47.8959 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 03 | Train Loss: 46.1743 | Val Loss: 41.1033 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 04 | Train Loss: 46.2387 | Val Loss: 50.0076 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 05 | Train Loss: 43.3629 | Val Loss: 28.6475 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 06 | Train Loss: 34.7082 | Val Loss: 28.2988 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 07 | Train Loss: 33.1770 | Val Loss: 26.5375 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 08 | Train Loss: 31.2565 | Val Loss: 23.9155 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 09 | Train Loss: 29.7689 | Val Loss: 33.6273 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 10 | Train Loss: 33.1355 | Val Loss: 43.6996 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 11 | Train Loss: 32.4062 | Val Loss: 25.9387 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 12 | Train Loss: 29.0349 | Val Loss: 21.0688 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 13 | Train Loss: 27.8367 | Val Loss: 24.9678 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 14 | Train Loss: 29.0856 | Val Loss: 29.7149 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 15 | Train Loss: 26.7635 | Val Loss: 21.9493 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 16 | Train Loss: 28.5520 | Val Loss: 19.7992 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 17 | Train Loss: 26.7437 | Val Loss: 19.5375 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 18 | Train Loss: 26.7628 | Val Loss: 30.2322 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 19 | Train Loss: 25.5200 | Val Loss: 23.8530 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 20 | Train Loss: 22.4085 | Val Loss: 18.2178 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 21 | Train Loss: 23.8300 | Val Loss: 21.0617 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 22 | Train Loss: 23.0471 | Val Loss: 19.1570 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 23 | Train Loss: 22.2260 | Val Loss: 30.9515 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 24 | Train Loss: 22.8688 | Val Loss: 33.6153 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 25 | Train Loss: 25.0661 | Val Loss: 19.9574 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 26 | Train Loss: 25.6680 | Val Loss: 16.9062 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 27 | Train Loss: 21.3223 | Val Loss: 20.3390 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 28 | Train Loss: 20.2136 | Val Loss: 19.4835 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 29 | Train Loss: 17.8405 | Val Loss: 17.9886 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 30 | Train Loss: 17.7945 | Val Loss: 20.5453 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 31 | Train Loss: 20.5967 | Val Loss: 18.2196 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 32 | Train Loss: 20.3724 | Val Loss: 19.4196 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 33 | Train Loss: 19.4442 | Val Loss: 20.5372 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 34 | Train Loss: 18.5361 | Val Loss: 19.9384 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 35 | Train Loss: 20.6486 | Val Loss: 16.7276 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 36 | Train Loss: 21.1030 | Val Loss: 14.7786 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 37 | Train Loss: 21.3984 | Val Loss: 14.6306 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 38 | Train Loss: 18.4189 | Val Loss: 15.3062 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 39 | Train Loss: 19.0998 | Val Loss: 16.6143 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 40 | Train Loss: 16.1891 | Val Loss: 14.0845 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 41 | Train Loss: 21.5037 | Val Loss: 16.9988 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 42 | Train Loss: 20.2067 | Val Loss: 15.9302 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 43 | Train Loss: 19.2697 | Val Loss: 13.7787 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 44 | Train Loss: 19.9159 | Val Loss: 15.5008 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 45 | Train Loss: 19.4377 | Val Loss: 15.2191 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 46 | Train Loss: 21.1541 | Val Loss: 13.8759 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 47 | Train Loss: 18.6606 | Val Loss: 13.0073 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 48 | Train Loss: 18.8049 | Val Loss: 13.3697 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 49 | Train Loss: 17.6913 | Val Loss: 14.8009 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 50 | Train Loss: 18.8700 | Val Loss: 14.3729 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 51 | Train Loss: 16.3330 | Val Loss: 14.7917 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 52 | Train Loss: 18.1886 | Val Loss: 13.5424 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 53 | Train Loss: 17.8369 | Val Loss: 12.8719 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 54 | Train Loss: 18.2854 | Val Loss: 13.3361 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 55 | Train Loss: 18.2876 | Val Loss: 13.2663 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 56 | Train Loss: 18.8276 | Val Loss: 13.1594 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 57 | Train Loss: 17.6924 | Val Loss: 13.1275 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 58 | Train Loss: 16.8925 | Val Loss: 13.6147 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 59 | Train Loss: 18.1656 | Val Loss: 11.9500 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 60 | Train Loss: 17.6893 | Val Loss: 13.3510 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 61 | Train Loss: 16.3758 | Val Loss: 12.7622 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 62 | Train Loss: 16.2429 | Val Loss: 12.4270 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 63 | Train Loss: 15.7296 | Val Loss: 12.6919 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 64 | Train Loss: 17.0030 | Val Loss: 11.9505 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 65 | Train Loss: 14.8977 | Val Loss: 11.0297 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 66 | Train Loss: 16.7836 | Val Loss: 13.6131 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 67 | Train Loss: 16.2257 | Val Loss: 11.5930 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 68 | Train Loss: 16.5463 | Val Loss: 12.3370 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 69 | Train Loss: 17.3524 | Val Loss: 17.1609 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 70 | Train Loss: 17.7884 | Val Loss: 11.4723 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 71 | Train Loss: 14.7464 | Val Loss: 12.4404 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 72 | Train Loss: 15.2547 | Val Loss: 14.4557 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 73 | Train Loss: 15.3786 | Val Loss: 11.5355 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 74 | Train Loss: 16.0260 | Val Loss: 11.5457 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 75 | Train Loss: 14.1310 | Val Loss: 10.9040 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 76 | Train Loss: 13.7022 | Val Loss: 10.9669 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 77 | Train Loss: 16.5052 | Val Loss: 20.0408 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 78 | Train Loss: 15.4618 | Val Loss: 18.6278 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 79 | Train Loss: 16.3570 | Val Loss: 12.5063 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 80 | Train Loss: 16.2296 | Val Loss: 11.8224 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 81 | Train Loss: 15.4600 | Val Loss: 13.1231 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 82 | Train Loss: 16.1004 | Val Loss: 11.8480 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 83 | Train Loss: 15.7674 | Val Loss: 11.2600 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 84 | Train Loss: 14.1988 | Val Loss: 11.1937 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 85 | Train Loss: 15.1366 | Val Loss: 10.1650 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 86 | Train Loss: 16.6502 | Val Loss: 10.4882 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 87 | Train Loss: 16.5290 | Val Loss: 9.9085 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 88 | Train Loss: 13.5614 | Val Loss: 11.0251 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 89 | Train Loss: 13.3414 | Val Loss: 9.4712 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 90 | Train Loss: 12.4740 | Val Loss: 9.4593 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 91 | Train Loss: 13.3822 | Val Loss: 10.1921 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 92 | Train Loss: 13.1304 | Val Loss: 9.1886 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 93 | Train Loss: 12.8208 | Val Loss: 10.5763 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 94 | Train Loss: 13.3004 | Val Loss: 10.3673 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 95 | Train Loss: 11.4794 | Val Loss: 10.5844 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 96 | Train Loss: 11.4031 | Val Loss: 9.1244 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 97 | Train Loss: 13.0565 | Val Loss: 9.3142 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:09,456] Trial 154 finished with value: 9.124408535841035 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3807823079171701, 'lr': 0.0007112525279861455, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 2.164797185648682e-05}. Best is trial 154 with value: 9.124408535841035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 154 | Epoch 98 | Train Loss: 13.2710 | Val Loss: 9.5980 | Optimizer: AdamW\n",
      "Trial 154 | Epoch 99 | Train Loss: 11.2926 | Val Loss: 9.2183 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:09,615] Trial 155 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 155 | Epoch 01 | Train Loss: 158.3158 | Val Loss: 47.1390 | Optimizer: AdamW\n",
      "Trial 156 | Epoch 01 | Train Loss: 162.4973 | Val Loss: 34.3509 | Optimizer: AdamW\n",
      "Trial 156 | Epoch 02 | Train Loss: 51.2012 | Val Loss: 64.5845 | Optimizer: AdamW\n",
      "Trial 156 | Epoch 03 | Train Loss: 48.8169 | Val Loss: 32.3009 | Optimizer: AdamW\n",
      "Trial 156 | Epoch 04 | Train Loss: 38.3259 | Val Loss: 32.8286 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:10,075] Trial 156 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 156 | Epoch 05 | Train Loss: 44.5117 | Val Loss: 36.7280 | Optimizer: AdamW\n",
      "Trial 157 | Epoch 01 | Train Loss: 162.5621 | Val Loss: 46.2501 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:10,239] Trial 157 pruned. \n",
      "[I 2025-09-04 21:23:10,412] Trial 158 pruned. \n",
      "[I 2025-09-04 21:23:10,574] Trial 159 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 158 | Epoch 01 | Train Loss: 122.1156 | Val Loss: 54.4112 | Optimizer: AdamW\n",
      "Trial 159 | Epoch 01 | Train Loss: 288.4535 | Val Loss: 298.0681 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 01 | Train Loss: 208.2653 | Val Loss: 29.2159 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 02 | Train Loss: 55.6182 | Val Loss: 48.9857 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 03 | Train Loss: 43.2670 | Val Loss: 29.8196 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 04 | Train Loss: 35.1852 | Val Loss: 30.0708 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 05 | Train Loss: 36.4748 | Val Loss: 28.0452 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 06 | Train Loss: 31.8883 | Val Loss: 25.2992 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 07 | Train Loss: 28.7676 | Val Loss: 32.2726 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 08 | Train Loss: 32.0930 | Val Loss: 26.2771 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 09 | Train Loss: 31.2141 | Val Loss: 22.3929 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 10 | Train Loss: 26.4294 | Val Loss: 25.5034 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 11 | Train Loss: 27.7392 | Val Loss: 23.0424 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 12 | Train Loss: 26.2288 | Val Loss: 21.4984 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 13 | Train Loss: 26.4080 | Val Loss: 21.8969 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 14 | Train Loss: 26.1445 | Val Loss: 27.0618 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 15 | Train Loss: 27.3767 | Val Loss: 20.0309 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 16 | Train Loss: 27.6033 | Val Loss: 20.5768 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 17 | Train Loss: 22.5336 | Val Loss: 20.8209 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 18 | Train Loss: 25.9658 | Val Loss: 27.4814 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 19 | Train Loss: 26.7623 | Val Loss: 21.4782 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 20 | Train Loss: 26.1279 | Val Loss: 24.1213 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 21 | Train Loss: 23.1470 | Val Loss: 20.0477 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 22 | Train Loss: 22.4068 | Val Loss: 16.6603 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 23 | Train Loss: 23.2134 | Val Loss: 20.8931 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 24 | Train Loss: 22.3672 | Val Loss: 19.8804 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 25 | Train Loss: 20.9791 | Val Loss: 16.4395 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 26 | Train Loss: 20.4099 | Val Loss: 15.3518 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 27 | Train Loss: 20.2494 | Val Loss: 22.4185 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 28 | Train Loss: 21.5322 | Val Loss: 15.1136 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 29 | Train Loss: 25.1584 | Val Loss: 15.6863 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 30 | Train Loss: 23.3485 | Val Loss: 16.3762 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 31 | Train Loss: 20.5857 | Val Loss: 17.1115 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 32 | Train Loss: 20.5918 | Val Loss: 17.7041 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 33 | Train Loss: 22.2159 | Val Loss: 18.4079 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 34 | Train Loss: 20.9572 | Val Loss: 30.4957 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 35 | Train Loss: 22.4498 | Val Loss: 26.4544 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 36 | Train Loss: 22.0062 | Val Loss: 16.1524 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:13,549] Trial 160 finished with value: 15.113592093553 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.39119426891928016, 'lr': 0.0004458455035443059, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 1.1140857978727634e-05}. Best is trial 154 with value: 9.124408535841035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 160 | Epoch 37 | Train Loss: 19.9326 | Val Loss: 16.5970 | Optimizer: AdamW\n",
      "Trial 160 | Epoch 38 | Train Loss: 17.2573 | Val Loss: 19.2509 | Optimizer: AdamW\n",
      "Trial 160 - Early stopping triggered at epoch 38\n",
      "Trial 161 | Epoch 01 | Train Loss: 200.2981 | Val Loss: 35.3079 | Optimizer: AdamW\n",
      "Trial 161 | Epoch 02 | Train Loss: 74.4427 | Val Loss: 91.9724 | Optimizer: AdamW\n",
      "Trial 161 | Epoch 03 | Train Loss: 76.3193 | Val Loss: 49.9659 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:13,934] Trial 161 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 161 | Epoch 04 | Train Loss: 53.2580 | Val Loss: 40.2670 | Optimizer: AdamW\n",
      "Trial 162 | Epoch 01 | Train Loss: 188.7724 | Val Loss: 33.5137 | Optimizer: AdamW\n",
      "Trial 162 | Epoch 02 | Train Loss: 61.2863 | Val Loss: 88.8032 | Optimizer: AdamW\n",
      "Trial 162 | Epoch 03 | Train Loss: 63.4144 | Val Loss: 34.7818 | Optimizer: AdamW\n",
      "Trial 162 | Epoch 04 | Train Loss: 49.1287 | Val Loss: 49.6436 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:14,478] Trial 162 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 162 | Epoch 05 | Train Loss: 43.5477 | Val Loss: 30.8944 | Optimizer: AdamW\n",
      "Trial 162 | Epoch 06 | Train Loss: 37.3647 | Val Loss: 31.4584 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:14,642] Trial 163 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 163 | Epoch 01 | Train Loss: 197.4305 | Val Loss: 44.8953 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 01 | Train Loss: 146.6011 | Val Loss: 34.1254 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 02 | Train Loss: 42.2839 | Val Loss: 33.9830 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 03 | Train Loss: 44.1614 | Val Loss: 33.1965 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 04 | Train Loss: 37.5745 | Val Loss: 36.1837 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 05 | Train Loss: 32.4586 | Val Loss: 25.9951 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 06 | Train Loss: 28.9053 | Val Loss: 23.1304 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 07 | Train Loss: 28.2695 | Val Loss: 22.9392 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 08 | Train Loss: 27.1468 | Val Loss: 21.7443 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 09 | Train Loss: 28.1758 | Val Loss: 21.3778 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 10 | Train Loss: 25.1265 | Val Loss: 26.8101 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 11 | Train Loss: 25.6342 | Val Loss: 24.3850 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 12 | Train Loss: 23.6796 | Val Loss: 37.7199 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 13 | Train Loss: 27.5036 | Val Loss: 32.2745 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 14 | Train Loss: 29.3680 | Val Loss: 19.0749 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 15 | Train Loss: 24.0984 | Val Loss: 18.6697 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 16 | Train Loss: 25.6210 | Val Loss: 29.4015 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 17 | Train Loss: 22.7803 | Val Loss: 26.2509 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 18 | Train Loss: 20.3135 | Val Loss: 17.2736 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 19 | Train Loss: 21.8578 | Val Loss: 18.7139 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 20 | Train Loss: 18.2994 | Val Loss: 30.8433 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 21 | Train Loss: 23.3800 | Val Loss: 24.5438 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 22 | Train Loss: 22.5671 | Val Loss: 16.8869 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 23 | Train Loss: 19.9227 | Val Loss: 16.3603 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 24 | Train Loss: 19.8121 | Val Loss: 15.9190 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 25 | Train Loss: 21.1611 | Val Loss: 15.6771 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 26 | Train Loss: 18.5814 | Val Loss: 18.7281 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 27 | Train Loss: 19.3873 | Val Loss: 16.7701 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 28 | Train Loss: 18.5247 | Val Loss: 15.0950 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 29 | Train Loss: 22.3164 | Val Loss: 19.3823 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 30 | Train Loss: 19.5897 | Val Loss: 16.2699 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 31 | Train Loss: 19.5737 | Val Loss: 15.3629 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 32 | Train Loss: 18.5510 | Val Loss: 15.4276 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 33 | Train Loss: 17.9512 | Val Loss: 27.3264 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 34 | Train Loss: 19.9778 | Val Loss: 31.8361 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 35 | Train Loss: 22.8539 | Val Loss: 32.6025 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 36 | Train Loss: 21.6927 | Val Loss: 16.6566 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 37 | Train Loss: 24.8005 | Val Loss: 14.8475 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 38 | Train Loss: 17.9269 | Val Loss: 13.9224 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 39 | Train Loss: 18.9729 | Val Loss: 14.3875 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 40 | Train Loss: 16.4029 | Val Loss: 18.7459 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 41 | Train Loss: 17.2646 | Val Loss: 16.8074 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 42 | Train Loss: 20.2292 | Val Loss: 19.4333 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 43 | Train Loss: 20.4164 | Val Loss: 15.9680 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 44 | Train Loss: 19.4834 | Val Loss: 14.2753 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 45 | Train Loss: 17.8825 | Val Loss: 19.0388 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 46 | Train Loss: 17.4809 | Val Loss: 22.9986 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:18,387] Trial 164 finished with value: 13.922429937657302 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3031341789373579, 'lr': 0.0007123986253912668, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 8.114652457430634e-05}. Best is trial 154 with value: 9.124408535841035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 164 | Epoch 47 | Train Loss: 19.1754 | Val Loss: 16.6346 | Optimizer: AdamW\n",
      "Trial 164 | Epoch 48 | Train Loss: 16.1010 | Val Loss: 14.2070 | Optimizer: AdamW\n",
      "Trial 164 - Early stopping triggered at epoch 48\n",
      "Trial 165 | Epoch 01 | Train Loss: 131.3912 | Val Loss: 37.1273 | Optimizer: AdamW\n",
      "Trial 165 | Epoch 02 | Train Loss: 49.3209 | Val Loss: 37.7684 | Optimizer: AdamW\n",
      "Trial 165 | Epoch 03 | Train Loss: 44.1243 | Val Loss: 34.6816 | Optimizer: AdamW\n",
      "Trial 165 | Epoch 04 | Train Loss: 32.7277 | Val Loss: 28.3226 | Optimizer: AdamW\n",
      "Trial 165 | Epoch 05 | Train Loss: 27.9457 | Val Loss: 26.8519 | Optimizer: AdamW\n",
      "Trial 165 | Epoch 06 | Train Loss: 27.4071 | Val Loss: 37.7202 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:19,080] Trial 165 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 165 | Epoch 07 | Train Loss: 29.5778 | Val Loss: 29.1077 | Optimizer: AdamW\n",
      "Trial 165 | Epoch 08 | Train Loss: 26.7398 | Val Loss: 35.9178 | Optimizer: AdamW\n",
      "Trial 166 | Epoch 01 | Train Loss: 141.9085 | Val Loss: 56.5760 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:19,202] Trial 166 pruned. \n",
      "[I 2025-09-04 21:23:19,368] Trial 167 pruned. \n",
      "[I 2025-09-04 21:23:19,503] Trial 168 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 167 | Epoch 01 | Train Loss: 174.0129 | Val Loss: 42.3989 | Optimizer: AdamW\n",
      "Trial 168 | Epoch 01 | NaN loss detected so pruning trial\n",
      "Trial 169 | Epoch 01 | Train Loss: 149.6089 | Val Loss: 33.1882 | Optimizer: AdamW\n",
      "Trial 169 | Epoch 02 | Train Loss: 56.3131 | Val Loss: 64.3063 | Optimizer: AdamW\n",
      "Trial 169 | Epoch 03 | Train Loss: 51.1094 | Val Loss: 42.6490 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:19,970] Trial 169 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 169 | Epoch 04 | Train Loss: 48.1031 | Val Loss: 43.5943 | Optimizer: AdamW\n",
      "Trial 169 | Epoch 05 | Train Loss: 42.7102 | Val Loss: 35.2967 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:20,133] Trial 170 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 170 | Epoch 01 | Train Loss: 159.1537 | Val Loss: 52.0446 | Optimizer: AdamW\n",
      "Trial 171 | Epoch 01 | Train Loss: 164.2964 | Val Loss: 34.7115 | Optimizer: AdamW\n",
      "Trial 171 | Epoch 02 | Train Loss: 55.6363 | Val Loss: 49.2346 | Optimizer: AdamW\n",
      "Trial 171 | Epoch 03 | Train Loss: 51.2047 | Val Loss: 42.5487 | Optimizer: AdamW\n",
      "Trial 171 | Epoch 04 | Train Loss: 45.0148 | Val Loss: 31.8060 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:20,601] Trial 171 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 171 | Epoch 05 | Train Loss: 38.9629 | Val Loss: 33.5175 | Optimizer: AdamW\n",
      "Trial 172 | Epoch 01 | Train Loss: 133.3785 | Val Loss: 34.0958 | Optimizer: AdamW\n",
      "Trial 172 | Epoch 02 | Train Loss: 47.8053 | Val Loss: 34.3532 | Optimizer: AdamW\n",
      "Trial 172 | Epoch 03 | Train Loss: 38.4550 | Val Loss: 43.9017 | Optimizer: AdamW\n",
      "Trial 172 | Epoch 04 | Train Loss: 37.1870 | Val Loss: 29.6622 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:21,156] Trial 172 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 172 | Epoch 05 | Train Loss: 33.0186 | Val Loss: 31.8617 | Optimizer: AdamW\n",
      "Trial 172 | Epoch 06 | Train Loss: 32.9595 | Val Loss: 37.2311 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:21,321] Trial 173 pruned. \n",
      "[I 2025-09-04 21:23:21,482] Trial 174 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 173 | Epoch 01 | Train Loss: 249.3636 | Val Loss: 270.5058 | Optimizer: AdamW\n",
      "Trial 174 | Epoch 01 | Train Loss: 173.4389 | Val Loss: 119.3026 | Optimizer: AdamW\n",
      "Trial 175 | Epoch 01 | Train Loss: 168.8448 | Val Loss: 37.1739 | Optimizer: AdamW\n",
      "Trial 175 | Epoch 02 | Train Loss: 63.7029 | Val Loss: 87.5138 | Optimizer: AdamW\n",
      "Trial 175 | Epoch 03 | Train Loss: 69.7007 | Val Loss: 48.2896 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:21,869] Trial 175 pruned. \n",
      "[I 2025-09-04 21:23:22,030] Trial 176 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 175 | Epoch 04 | Train Loss: 44.2765 | Val Loss: 36.7433 | Optimizer: AdamW\n",
      "Trial 176 | Epoch 01 | Train Loss: 180.9517 | Val Loss: 63.3965 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:22,148] Trial 177 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 177 | Epoch 01 | Train Loss: 131.1174 | Val Loss: 61.4351 | Optimizer: AdamW\n",
      "Trial 178 | Epoch 01 | Train Loss: 166.5813 | Val Loss: 36.9254 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:22,533] Trial 178 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 178 | Epoch 02 | Train Loss: 55.2691 | Val Loss: 64.0184 | Optimizer: AdamW\n",
      "Trial 178 | Epoch 03 | Train Loss: 50.4126 | Val Loss: 37.9049 | Optimizer: AdamW\n",
      "Trial 178 | Epoch 04 | Train Loss: 41.8931 | Val Loss: 35.9652 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:22,693] Trial 179 pruned. \n",
      "[I 2025-09-04 21:23:22,854] Trial 180 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 179 | Epoch 01 | Train Loss: 167.2862 | Val Loss: 66.2001 | Optimizer: AdamW\n",
      "Trial 180 | Epoch 01 | Train Loss: 679781.9145 | Val Loss: 1651.0379 | Optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:23,017] Trial 181 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 181 | Epoch 01 | Train Loss: 213.2145 | Val Loss: 74.9668 | Optimizer: AdamW\n",
      "Trial 182 | Epoch 01 | Train Loss: 188.7516 | Val Loss: 56.9964 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:23,176] Trial 182 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 183 | Epoch 01 | Train Loss: 169.8465 | Val Loss: 33.1436 | Optimizer: AdamW\n",
      "Trial 183 | Epoch 02 | Train Loss: 55.8199 | Val Loss: 63.0836 | Optimizer: AdamW\n",
      "Trial 183 | Epoch 03 | Train Loss: 48.2963 | Val Loss: 40.4409 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:23,649] Trial 183 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 183 | Epoch 04 | Train Loss: 51.8069 | Val Loss: 45.4069 | Optimizer: AdamW\n",
      "Trial 183 | Epoch 05 | Train Loss: 42.4958 | Val Loss: 32.0014 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 01 | Train Loss: 155.1595 | Val Loss: 31.5323 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 02 | Train Loss: 57.0771 | Val Loss: 50.5690 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 03 | Train Loss: 49.4817 | Val Loss: 40.9090 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 04 | Train Loss: 38.8540 | Val Loss: 32.4203 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 05 | Train Loss: 36.6694 | Val Loss: 28.8610 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 06 | Train Loss: 34.4506 | Val Loss: 25.5675 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 07 | Train Loss: 30.3281 | Val Loss: 22.8638 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 08 | Train Loss: 30.1590 | Val Loss: 25.5956 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 09 | Train Loss: 30.4320 | Val Loss: 21.4016 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 10 | Train Loss: 28.0518 | Val Loss: 20.9498 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 11 | Train Loss: 28.9015 | Val Loss: 23.3031 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 12 | Train Loss: 25.1635 | Val Loss: 23.6699 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 13 | Train Loss: 24.2648 | Val Loss: 22.2996 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 14 | Train Loss: 26.1097 | Val Loss: 25.7750 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 15 | Train Loss: 25.1639 | Val Loss: 20.4636 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 16 | Train Loss: 25.6363 | Val Loss: 23.5454 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 17 | Train Loss: 25.0098 | Val Loss: 21.3633 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 18 | Train Loss: 21.7369 | Val Loss: 17.4575 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 19 | Train Loss: 21.4760 | Val Loss: 19.2751 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 20 | Train Loss: 22.3929 | Val Loss: 27.0621 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 21 | Train Loss: 25.6496 | Val Loss: 21.6980 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 22 | Train Loss: 22.8419 | Val Loss: 17.9595 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 23 | Train Loss: 24.0344 | Val Loss: 16.9840 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 24 | Train Loss: 22.8587 | Val Loss: 23.5549 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 25 | Train Loss: 22.1103 | Val Loss: 22.8271 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 26 | Train Loss: 20.7783 | Val Loss: 18.2399 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 27 | Train Loss: 22.8013 | Val Loss: 27.9703 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 28 | Train Loss: 21.9313 | Val Loss: 19.0364 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 29 | Train Loss: 20.7074 | Val Loss: 15.6234 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 30 | Train Loss: 19.1898 | Val Loss: 20.5251 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 31 | Train Loss: 20.3011 | Val Loss: 16.3687 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 32 | Train Loss: 17.6145 | Val Loss: 16.3833 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 33 | Train Loss: 17.7337 | Val Loss: 14.7398 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 34 | Train Loss: 18.8811 | Val Loss: 14.7522 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 35 | Train Loss: 19.0491 | Val Loss: 15.0975 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 36 | Train Loss: 19.9434 | Val Loss: 14.4765 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 37 | Train Loss: 16.5232 | Val Loss: 16.7321 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 38 | Train Loss: 19.1648 | Val Loss: 15.4566 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 39 | Train Loss: 19.0470 | Val Loss: 13.8073 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 40 | Train Loss: 21.1308 | Val Loss: 18.0811 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 41 | Train Loss: 15.7924 | Val Loss: 17.5213 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 42 | Train Loss: 18.2532 | Val Loss: 15.4719 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 43 | Train Loss: 19.9264 | Val Loss: 16.7830 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 44 | Train Loss: 18.8399 | Val Loss: 14.4259 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 45 | Train Loss: 21.6449 | Val Loss: 22.5638 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 46 | Train Loss: 22.4175 | Val Loss: 22.5015 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 47 | Train Loss: 18.9917 | Val Loss: 14.1458 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 48 | Train Loss: 18.5630 | Val Loss: 13.7234 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 49 | Train Loss: 18.8696 | Val Loss: 13.3808 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 50 | Train Loss: 17.4853 | Val Loss: 13.6196 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 51 | Train Loss: 16.2912 | Val Loss: 17.4641 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 52 | Train Loss: 17.5645 | Val Loss: 14.0831 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 53 | Train Loss: 18.0980 | Val Loss: 19.7946 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 54 | Train Loss: 16.0773 | Val Loss: 25.2498 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 55 | Train Loss: 19.6640 | Val Loss: 25.4835 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 56 | Train Loss: 18.8419 | Val Loss: 22.0248 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 57 | Train Loss: 19.4275 | Val Loss: 31.3893 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:28,239] Trial 184 finished with value: 13.38084742693397 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.380250945452507, 'lr': 0.0007535970200714215, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 4.62890573820893e-05}. Best is trial 154 with value: 9.124408535841035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 184 | Epoch 58 | Train Loss: 19.3413 | Val Loss: 16.8239 | Optimizer: AdamW\n",
      "Trial 184 | Epoch 59 | Train Loss: 18.4530 | Val Loss: 19.4534 | Optimizer: AdamW\n",
      "Trial 184 - Early stopping triggered at epoch 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:28,402] Trial 185 pruned. \n",
      "[I 2025-09-04 21:23:28,563] Trial 186 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 185 | Epoch 01 | Train Loss: 217.2842 | Val Loss: 48.6682 | Optimizer: AdamW\n",
      "Trial 186 | Epoch 01 | Train Loss: 160.4785 | Val Loss: 43.9449 | Optimizer: AdamW\n",
      "Trial 187 | Epoch 01 | Train Loss: 169.1454 | Val Loss: 32.4312 | Optimizer: AdamW\n",
      "Trial 187 | Epoch 02 | Train Loss: 59.7992 | Val Loss: 74.2080 | Optimizer: AdamW\n",
      "Trial 187 | Epoch 03 | Train Loss: 51.7689 | Val Loss: 38.9022 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:29,032] Trial 187 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 187 | Epoch 04 | Train Loss: 54.0858 | Val Loss: 53.9019 | Optimizer: AdamW\n",
      "Trial 187 | Epoch 05 | Train Loss: 45.8616 | Val Loss: 35.6107 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:29,197] Trial 188 pruned. \n",
      "[I 2025-09-04 21:23:29,359] Trial 189 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 188 | Epoch 01 | Train Loss: 117.3431 | Val Loss: 58.0589 | Optimizer: AdamW\n",
      "Trial 189 | Epoch 01 | Train Loss: 164.9106 | Val Loss: 46.1206 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:29,523] Trial 190 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 190 | Epoch 01 | Train Loss: 183.4093 | Val Loss: 42.8990 | Optimizer: AdamW\n",
      "Trial 191 | Epoch 01 | Train Loss: 205.7060 | Val Loss: 65.6852 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:29,689] Trial 191 pruned. \n",
      "[I 2025-09-04 21:23:29,853] Trial 192 pruned. \n",
      "[I 2025-09-04 21:23:30,013] Trial 193 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 192 | Epoch 01 | Train Loss: 136.5148 | Val Loss: 47.0181 | Optimizer: AdamW\n",
      "Trial 193 | Epoch 01 | Train Loss: 197.4408 | Val Loss: 60.9086 | Optimizer: AdamW\n",
      "Trial 194 | Epoch 01 | Train Loss: 188.5581 | Val Loss: 34.2824 | Optimizer: AdamW\n",
      "Trial 194 | Epoch 02 | Train Loss: 72.0382 | Val Loss: 96.2712 | Optimizer: AdamW\n",
      "Trial 194 | Epoch 03 | Train Loss: 76.4798 | Val Loss: 50.5129 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:30,481] Trial 194 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 194 | Epoch 04 | Train Loss: 48.9697 | Val Loss: 39.3401 | Optimizer: AdamW\n",
      "Trial 194 | Epoch 05 | Train Loss: 47.3710 | Val Loss: 43.4206 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:30,666] Trial 195 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 195 | Epoch 01 | Train Loss: 176.2592 | Val Loss: 55.4388 | Optimizer: AdamW\n",
      "Trial 196 | Epoch 01 | Train Loss: 150.1140 | Val Loss: 38.8818 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:30,982] Trial 196 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 196 | Epoch 02 | Train Loss: 69.0658 | Val Loss: 85.5967 | Optimizer: AdamW\n",
      "Trial 196 | Epoch 03 | Train Loss: 65.9763 | Val Loss: 50.7930 | Optimizer: AdamW\n",
      "Trial 197 | Epoch 01 | Train Loss: 153.0246 | Val Loss: 64.8323 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:31,107] Trial 197 pruned. \n",
      "[I 2025-09-04 21:23:31,273] Trial 198 pruned. \n",
      "[I 2025-09-04 21:23:31,435] Trial 199 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 198 | Epoch 01 | Train Loss: 255.3288 | Val Loss: 65.6071 | Optimizer: AdamW\n",
      "Trial 199 | Epoch 01 | Train Loss: 171.3155 | Val Loss: 49.7244 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:31,597] Trial 200 pruned. \n",
      "[I 2025-09-04 21:23:31,762] Trial 201 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 200 | Epoch 01 | Train Loss: 163.1842 | Val Loss: 41.2654 | Optimizer: AdamW\n",
      "Trial 201 | Epoch 01 | Train Loss: 124.4310 | Val Loss: 54.6378 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:31,911] Trial 202 pruned. \n",
      "[I 2025-09-04 21:23:32,075] Trial 203 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 202 | Epoch 01 | NaN loss detected so pruning trial\n",
      "Trial 203 | Epoch 01 | Train Loss: 156.6722 | Val Loss: 43.8810 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:32,239] Trial 204 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 204 | Epoch 01 | Train Loss: 180.2661 | Val Loss: 61.7543 | Optimizer: AdamW\n",
      "Trial 205 | Epoch 01 | Train Loss: 137.3283 | Val Loss: 39.1806 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:32,555] Trial 205 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 205 | Epoch 02 | Train Loss: 58.7769 | Val Loss: 69.9825 | Optimizer: AdamW\n",
      "Trial 205 | Epoch 03 | Train Loss: 58.5162 | Val Loss: 45.6684 | Optimizer: AdamW\n",
      "Trial 206 | Epoch 01 | Train Loss: 197.2223 | Val Loss: 32.7274 | Optimizer: AdamW\n",
      "Trial 206 | Epoch 02 | Train Loss: 59.7583 | Val Loss: 83.4140 | Optimizer: AdamW\n",
      "Trial 206 | Epoch 03 | Train Loss: 58.5381 | Val Loss: 36.8484 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:33,025] Trial 206 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 206 | Epoch 04 | Train Loss: 42.5602 | Val Loss: 42.4902 | Optimizer: AdamW\n",
      "Trial 206 | Epoch 05 | Train Loss: 42.4294 | Val Loss: 33.4864 | Optimizer: AdamW\n",
      "Trial 207 | Epoch 01 | Train Loss: 8885.2739 | Val Loss: 39.1100 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 02 | Train Loss: 82.4942 | Val Loss: 36.9143 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 03 | Train Loss: 42.9862 | Val Loss: 26.8612 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 04 | Train Loss: 39.2689 | Val Loss: 24.7145 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 05 | Train Loss: 37.3940 | Val Loss: 27.4518 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 06 | Train Loss: 34.5640 | Val Loss: 28.2331 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 07 | Train Loss: 41.1649 | Val Loss: 35.6471 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 08 | Train Loss: 32.5666 | Val Loss: 24.9849 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 09 | Train Loss: 29.8535 | Val Loss: 26.8068 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 10 | Train Loss: 32.4469 | Val Loss: 22.7257 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 11 | Train Loss: 32.4390 | Val Loss: 41.0300 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 12 | Train Loss: 32.8309 | Val Loss: 26.9123 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 13 | Train Loss: 31.8488 | Val Loss: 22.2071 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 14 | Train Loss: 29.8174 | Val Loss: 23.2710 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 15 | Train Loss: 29.6452 | Val Loss: 25.4398 | Optimizer: RMSprop\n",
      "Trial 207 | Epoch 16 | Train Loss: 34.2467 | Val Loss: 28.4482 | Optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:33,953] Trial 207 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 207 | Epoch 17 | Train Loss: 34.0040 | Val Loss: 22.0969 | Optimizer: RMSprop\n",
      "Trial 208 | Epoch 01 | Train Loss: 200.2035 | Val Loss: 73.5068 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:34,116] Trial 208 pruned. \n",
      "[I 2025-09-04 21:23:34,284] Trial 209 pruned. \n",
      "[I 2025-09-04 21:23:34,451] Trial 210 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 209 | Epoch 01 | Train Loss: 129.7320 | Val Loss: 52.1010 | Optimizer: AdamW\n",
      "Trial 210 | Epoch 01 | Train Loss: 177.7072 | Val Loss: 82.7813 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 01 | Train Loss: 159.7491 | Val Loss: 31.2053 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 02 | Train Loss: 56.3340 | Val Loss: 49.7051 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 03 | Train Loss: 45.9895 | Val Loss: 40.1677 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 04 | Train Loss: 43.0372 | Val Loss: 44.9140 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 05 | Train Loss: 41.3983 | Val Loss: 27.8511 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 06 | Train Loss: 39.5613 | Val Loss: 35.9078 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 07 | Train Loss: 35.2642 | Val Loss: 27.0694 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 08 | Train Loss: 31.4024 | Val Loss: 24.4381 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 09 | Train Loss: 29.3709 | Val Loss: 23.9392 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 10 | Train Loss: 32.3140 | Val Loss: 36.0794 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 11 | Train Loss: 28.6738 | Val Loss: 24.2588 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 12 | Train Loss: 34.7700 | Val Loss: 21.2830 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 13 | Train Loss: 32.5473 | Val Loss: 29.5849 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 14 | Train Loss: 28.2518 | Val Loss: 28.4344 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 15 | Train Loss: 25.2867 | Val Loss: 21.3867 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 16 | Train Loss: 26.0273 | Val Loss: 22.8938 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 17 | Train Loss: 25.7559 | Val Loss: 24.5869 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 18 | Train Loss: 25.6977 | Val Loss: 23.0308 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 19 | Train Loss: 23.5149 | Val Loss: 18.3265 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 20 | Train Loss: 29.5240 | Val Loss: 18.2042 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 21 | Train Loss: 25.7140 | Val Loss: 31.5464 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 22 | Train Loss: 26.7209 | Val Loss: 21.7993 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 23 | Train Loss: 23.8253 | Val Loss: 17.8004 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 24 | Train Loss: 23.5931 | Val Loss: 18.5510 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 25 | Train Loss: 20.9168 | Val Loss: 17.2365 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 26 | Train Loss: 20.9846 | Val Loss: 25.5058 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 27 | Train Loss: 24.7124 | Val Loss: 31.9377 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 28 | Train Loss: 23.0110 | Val Loss: 23.8299 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 29 | Train Loss: 21.6841 | Val Loss: 21.5456 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 30 | Train Loss: 21.1062 | Val Loss: 17.2157 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 31 | Train Loss: 20.4193 | Val Loss: 16.2638 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 32 | Train Loss: 20.2946 | Val Loss: 14.3381 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 33 | Train Loss: 21.2771 | Val Loss: 24.4866 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 34 | Train Loss: 25.6091 | Val Loss: 20.2474 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 35 | Train Loss: 23.9683 | Val Loss: 15.8458 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 36 | Train Loss: 21.4600 | Val Loss: 15.4394 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 37 | Train Loss: 21.1343 | Val Loss: 22.7473 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 38 | Train Loss: 22.1112 | Val Loss: 24.2308 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 39 | Train Loss: 21.8314 | Val Loss: 18.3074 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:37,747] Trial 211 finished with value: 14.338142867979965 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3934249503905161, 'lr': 0.000591119540759259, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 1.1551553281367087e-05}. Best is trial 154 with value: 9.124408535841035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 211 | Epoch 40 | Train Loss: 20.6001 | Val Loss: 15.7071 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 41 | Train Loss: 19.6720 | Val Loss: 16.4414 | Optimizer: AdamW\n",
      "Trial 211 | Epoch 42 | Train Loss: 19.0657 | Val Loss: 32.7463 | Optimizer: AdamW\n",
      "Trial 211 - Early stopping triggered at epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:37,912] Trial 212 pruned. \n",
      "[I 2025-09-04 21:23:38,078] Trial 213 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 212 | Epoch 01 | Train Loss: 166.3231 | Val Loss: 45.8211 | Optimizer: AdamW\n",
      "Trial 213 | Epoch 01 | Train Loss: 181.3899 | Val Loss: 40.9482 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:38,245] Trial 214 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 214 | Epoch 01 | Train Loss: 183.2679 | Val Loss: 99.2308 | Optimizer: AdamW\n",
      "Trial 215 | Epoch 01 | Train Loss: 134.8578 | Val Loss: 44.5481 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:38,409] Trial 215 pruned. \n",
      "[I 2025-09-04 21:23:38,577] Trial 216 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 216 | Epoch 01 | Train Loss: 189.3626 | Val Loss: 56.0456 | Optimizer: AdamW\n",
      "Trial 217 | Epoch 01 | Train Loss: 177.4319 | Val Loss: 35.4636 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:38,967] Trial 217 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 217 | Epoch 02 | Train Loss: 55.1478 | Val Loss: 70.5530 | Optimizer: AdamW\n",
      "Trial 217 | Epoch 03 | Train Loss: 51.3803 | Val Loss: 38.1750 | Optimizer: AdamW\n",
      "Trial 217 | Epoch 04 | Train Loss: 46.2656 | Val Loss: 47.8535 | Optimizer: AdamW\n",
      "Trial 218 | Epoch 01 | Train Loss: 159.0166 | Val Loss: 39.2603 | Optimizer: AdamW\n",
      "Trial 218 | Epoch 02 | Train Loss: 57.3516 | Val Loss: 76.5812 | Optimizer: AdamW\n",
      "Trial 218 | Epoch 03 | Train Loss: 58.4557 | Val Loss: 42.2120 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:39,363] Trial 218 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 218 | Epoch 04 | Train Loss: 46.1320 | Val Loss: 39.0281 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 01 | Train Loss: 152.1858 | Val Loss: 40.5839 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 02 | Train Loss: 44.1396 | Val Loss: 42.9405 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 03 | Train Loss: 40.0456 | Val Loss: 33.3896 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 04 | Train Loss: 33.5484 | Val Loss: 29.8323 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 05 | Train Loss: 31.5052 | Val Loss: 26.3908 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 06 | Train Loss: 25.4362 | Val Loss: 24.0644 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 07 | Train Loss: 26.8031 | Val Loss: 23.1021 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 08 | Train Loss: 26.3271 | Val Loss: 21.7117 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 09 | Train Loss: 26.5283 | Val Loss: 22.2501 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 10 | Train Loss: 23.3944 | Val Loss: 25.4494 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 11 | Train Loss: 23.2828 | Val Loss: 26.5341 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 12 | Train Loss: 24.1856 | Val Loss: 19.5591 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 13 | Train Loss: 25.0462 | Val Loss: 19.1056 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 14 | Train Loss: 25.4089 | Val Loss: 32.2445 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 15 | Train Loss: 26.8209 | Val Loss: 27.8181 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 16 | Train Loss: 23.8781 | Val Loss: 19.4155 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 17 | Train Loss: 21.9724 | Val Loss: 18.5407 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 18 | Train Loss: 23.8490 | Val Loss: 23.7109 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 19 | Train Loss: 21.7364 | Val Loss: 27.5024 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 20 | Train Loss: 24.2847 | Val Loss: 17.7925 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 21 | Train Loss: 22.2975 | Val Loss: 17.3400 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 22 | Train Loss: 23.7945 | Val Loss: 30.5111 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 23 | Train Loss: 21.4925 | Val Loss: 19.3678 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 24 | Train Loss: 20.8901 | Val Loss: 17.9849 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 25 | Train Loss: 22.0972 | Val Loss: 18.8239 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 26 | Train Loss: 19.5293 | Val Loss: 19.5910 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 27 | Train Loss: 19.3312 | Val Loss: 22.3573 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 28 | Train Loss: 20.2425 | Val Loss: 19.9716 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 29 | Train Loss: 18.2215 | Val Loss: 14.9958 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 30 | Train Loss: 18.2746 | Val Loss: 16.9776 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 31 | Train Loss: 20.2409 | Val Loss: 15.2183 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 32 | Train Loss: 18.7056 | Val Loss: 15.5542 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 33 | Train Loss: 19.0471 | Val Loss: 15.2066 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 34 | Train Loss: 16.4865 | Val Loss: 20.0046 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 35 | Train Loss: 17.1519 | Val Loss: 20.0715 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 36 | Train Loss: 15.5540 | Val Loss: 15.7314 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 37 | Train Loss: 15.6519 | Val Loss: 14.5585 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 38 | Train Loss: 17.0664 | Val Loss: 17.3870 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 39 | Train Loss: 16.8378 | Val Loss: 14.4804 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 40 | Train Loss: 15.8347 | Val Loss: 20.0941 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 41 | Train Loss: 17.8478 | Val Loss: 18.9131 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 42 | Train Loss: 15.6792 | Val Loss: 15.3660 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 43 | Train Loss: 15.0614 | Val Loss: 13.5999 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 44 | Train Loss: 16.0869 | Val Loss: 18.7063 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 45 | Train Loss: 15.7031 | Val Loss: 13.9280 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 46 | Train Loss: 13.8742 | Val Loss: 14.8706 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 47 | Train Loss: 14.2438 | Val Loss: 13.7298 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 48 | Train Loss: 15.9634 | Val Loss: 13.4689 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 49 | Train Loss: 14.4848 | Val Loss: 15.7298 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 50 | Train Loss: 15.6336 | Val Loss: 12.9448 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 51 | Train Loss: 13.7793 | Val Loss: 12.8565 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 52 | Train Loss: 15.0138 | Val Loss: 12.8419 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 53 | Train Loss: 16.6710 | Val Loss: 12.8020 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 54 | Train Loss: 15.7223 | Val Loss: 12.5295 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 55 | Train Loss: 14.6896 | Val Loss: 13.3532 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 56 | Train Loss: 13.0245 | Val Loss: 13.0245 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 57 | Train Loss: 15.0271 | Val Loss: 12.3283 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 58 | Train Loss: 16.4149 | Val Loss: 13.5982 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 59 | Train Loss: 17.6959 | Val Loss: 12.7379 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 60 | Train Loss: 16.4506 | Val Loss: 12.6548 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 61 | Train Loss: 14.3113 | Val Loss: 13.3213 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 62 | Train Loss: 14.1734 | Val Loss: 12.3483 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 63 | Train Loss: 14.4618 | Val Loss: 11.9021 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 64 | Train Loss: 14.3720 | Val Loss: 11.5144 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 65 | Train Loss: 13.0824 | Val Loss: 11.5768 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 66 | Train Loss: 13.2701 | Val Loss: 11.9701 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 67 | Train Loss: 14.5642 | Val Loss: 16.9297 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 68 | Train Loss: 12.8129 | Val Loss: 12.3967 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 69 | Train Loss: 12.1134 | Val Loss: 13.8213 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 70 | Train Loss: 11.7079 | Val Loss: 10.4541 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 71 | Train Loss: 12.1923 | Val Loss: 11.1428 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 72 | Train Loss: 11.1843 | Val Loss: 10.9723 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 73 | Train Loss: 11.7524 | Val Loss: 10.0730 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 74 | Train Loss: 14.1801 | Val Loss: 10.7474 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 75 | Train Loss: 11.6814 | Val Loss: 11.2006 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 76 | Train Loss: 13.6546 | Val Loss: 14.1861 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 77 | Train Loss: 11.9265 | Val Loss: 13.1244 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 78 | Train Loss: 12.5155 | Val Loss: 11.0833 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 79 | Train Loss: 10.8484 | Val Loss: 13.0226 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 80 | Train Loss: 11.1317 | Val Loss: 10.2350 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 81 | Train Loss: 10.4369 | Val Loss: 10.5461 | Optimizer: AdamW\n",
      "Trial 219 | Epoch 82 | Train Loss: 11.9186 | Val Loss: 10.4073 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:45,802] Trial 219 finished with value: 10.07298573052011 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.30503452797449326, 'lr': 0.0005340323929366291, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 1.1684772228773459e-05}. Best is trial 154 with value: 9.124408535841035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 219 | Epoch 83 | Train Loss: 14.2605 | Val Loss: 13.4628 | Optimizer: AdamW\n",
      "Trial 219 - Early stopping triggered at epoch 83\n",
      "Trial 220 | Epoch 01 | Train Loss: 166.2772 | Val Loss: 44.0999 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:45,968] Trial 220 pruned. \n",
      "[I 2025-09-04 21:23:46,133] Trial 221 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 221 | Epoch 01 | Train Loss: 158.0256 | Val Loss: 44.6779 | Optimizer: AdamW\n",
      "Trial 222 | Epoch 01 | Train Loss: 232.4893 | Val Loss: 42.4172 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:46,302] Trial 222 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 223 | Epoch 01 | Train Loss: 160.5728 | Val Loss: 32.5861 | Optimizer: AdamW\n",
      "Trial 223 | Epoch 02 | Train Loss: 49.9258 | Val Loss: 56.9674 | Optimizer: AdamW\n",
      "Trial 223 | Epoch 03 | Train Loss: 49.4445 | Val Loss: 41.6739 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:46,771] Trial 223 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 223 | Epoch 04 | Train Loss: 44.9741 | Val Loss: 39.7461 | Optimizer: AdamW\n",
      "Trial 223 | Epoch 05 | Train Loss: 37.2397 | Val Loss: 32.3237 | Optimizer: AdamW\n",
      "Trial 224 | Epoch 01 | Train Loss: 148.8035 | Val Loss: 38.4153 | Optimizer: AdamW\n",
      "Trial 224 | Epoch 02 | Train Loss: 54.5863 | Val Loss: 47.9359 | Optimizer: AdamW\n",
      "Trial 224 | Epoch 03 | Train Loss: 40.0687 | Val Loss: 33.0613 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:47,238] Trial 224 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 224 | Epoch 04 | Train Loss: 38.1069 | Val Loss: 31.8558 | Optimizer: AdamW\n",
      "Trial 224 | Epoch 05 | Train Loss: 33.2375 | Val Loss: 33.9036 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:47,403] Trial 225 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 225 | Epoch 01 | Train Loss: 187.1793 | Val Loss: 41.8316 | Optimizer: AdamW\n",
      "Trial 226 | Epoch 01 | Train Loss: 156.7778 | Val Loss: 45.7076 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:47,588] Trial 226 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 227 | Epoch 01 | Train Loss: 119.0493 | Val Loss: 37.9072 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 02 | Train Loss: 51.9538 | Val Loss: 38.7906 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 03 | Train Loss: 50.1352 | Val Loss: 46.4788 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 04 | Train Loss: 44.0182 | Val Loss: 27.8021 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 05 | Train Loss: 31.2207 | Val Loss: 24.4746 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 06 | Train Loss: 32.3266 | Val Loss: 22.7756 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 07 | Train Loss: 30.3745 | Val Loss: 25.8250 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 08 | Train Loss: 30.0249 | Val Loss: 32.7981 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 09 | Train Loss: 26.7712 | Val Loss: 23.6181 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 10 | Train Loss: 26.0245 | Val Loss: 20.7887 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 11 | Train Loss: 26.7643 | Val Loss: 22.4044 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 12 | Train Loss: 28.3526 | Val Loss: 23.6594 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 13 | Train Loss: 25.9614 | Val Loss: 24.8225 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 14 | Train Loss: 22.5117 | Val Loss: 27.3972 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 15 | Train Loss: 25.0317 | Val Loss: 19.4928 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 16 | Train Loss: 26.5044 | Val Loss: 18.4770 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 17 | Train Loss: 22.2430 | Val Loss: 20.5819 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 18 | Train Loss: 26.0194 | Val Loss: 23.3536 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 19 | Train Loss: 23.6993 | Val Loss: 21.9909 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 20 | Train Loss: 23.0846 | Val Loss: 26.0327 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 21 | Train Loss: 22.2345 | Val Loss: 22.1492 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 22 | Train Loss: 21.2309 | Val Loss: 17.2130 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 23 | Train Loss: 18.7796 | Val Loss: 20.4976 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 24 | Train Loss: 19.7876 | Val Loss: 33.3999 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 25 | Train Loss: 22.8437 | Val Loss: 32.5633 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 26 | Train Loss: 22.9900 | Val Loss: 22.3868 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 27 | Train Loss: 21.2509 | Val Loss: 18.2468 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 28 | Train Loss: 23.5445 | Val Loss: 21.9708 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 29 | Train Loss: 23.4069 | Val Loss: 19.2786 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 30 | Train Loss: 21.5715 | Val Loss: 33.1611 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 31 | Train Loss: 23.2746 | Val Loss: 29.0015 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 32 | Train Loss: 19.4531 | Val Loss: 14.6976 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 33 | Train Loss: 20.0159 | Val Loss: 15.7020 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 34 | Train Loss: 18.9977 | Val Loss: 19.8716 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 35 | Train Loss: 18.3640 | Val Loss: 14.9739 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 36 | Train Loss: 18.1390 | Val Loss: 15.5826 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 37 | Train Loss: 17.0173 | Val Loss: 18.3608 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 38 | Train Loss: 16.7094 | Val Loss: 13.2562 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 39 | Train Loss: 20.5077 | Val Loss: 13.0961 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 40 | Train Loss: 17.2360 | Val Loss: 15.5367 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 41 | Train Loss: 17.0523 | Val Loss: 20.0123 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 42 | Train Loss: 18.9886 | Val Loss: 20.8813 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 43 | Train Loss: 20.7545 | Val Loss: 24.4364 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 44 | Train Loss: 17.8272 | Val Loss: 12.7210 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 45 | Train Loss: 18.0610 | Val Loss: 21.2095 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 46 | Train Loss: 18.1998 | Val Loss: 18.8410 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 47 | Train Loss: 17.4371 | Val Loss: 15.4150 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 48 | Train Loss: 15.4094 | Val Loss: 12.7019 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 49 | Train Loss: 17.3424 | Val Loss: 13.1016 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 50 | Train Loss: 16.8210 | Val Loss: 12.6635 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 51 | Train Loss: 14.7907 | Val Loss: 14.1636 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 52 | Train Loss: 16.3277 | Val Loss: 15.2554 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 53 | Train Loss: 15.7905 | Val Loss: 16.0910 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 54 | Train Loss: 15.3248 | Val Loss: 14.4317 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 55 | Train Loss: 16.2377 | Val Loss: 13.4755 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 56 | Train Loss: 15.4245 | Val Loss: 11.3746 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 57 | Train Loss: 15.7315 | Val Loss: 11.9523 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 58 | Train Loss: 15.5788 | Val Loss: 11.1050 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 59 | Train Loss: 14.2188 | Val Loss: 14.4892 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 60 | Train Loss: 16.1464 | Val Loss: 15.0535 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 61 | Train Loss: 13.8169 | Val Loss: 12.0794 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 62 | Train Loss: 15.8834 | Val Loss: 10.7250 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 63 | Train Loss: 15.3954 | Val Loss: 11.1643 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 64 | Train Loss: 14.2058 | Val Loss: 10.5329 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 65 | Train Loss: 11.5461 | Val Loss: 9.6437 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 66 | Train Loss: 14.3465 | Val Loss: 11.0402 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 67 | Train Loss: 14.4428 | Val Loss: 9.7730 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 68 | Train Loss: 15.3464 | Val Loss: 15.1302 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 69 | Train Loss: 13.7379 | Val Loss: 15.6218 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 70 | Train Loss: 14.1817 | Val Loss: 15.6978 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 71 | Train Loss: 16.4772 | Val Loss: 12.6907 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 72 | Train Loss: 14.5682 | Val Loss: 12.2859 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 73 | Train Loss: 14.8064 | Val Loss: 14.0802 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 74 | Train Loss: 12.8504 | Val Loss: 13.4529 | Optimizer: AdamW\n",
      "Trial 227 | Epoch 75 | Train Loss: 11.6056 | Val Loss: 10.7645 | Optimizer: AdamW\n",
      "Trial 227 - Early stopping triggered at epoch 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:53,411] Trial 227 finished with value: 9.64368356534136 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.39967898982478123, 'lr': 0.000617809494718854, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 7.738304830222328e-05}. Best is trial 154 with value: 9.124408535841035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 228 | Epoch 01 | Train Loss: 156.0962 | Val Loss: 38.2640 | Optimizer: AdamW\n",
      "Trial 228 | Epoch 02 | Train Loss: 59.4239 | Val Loss: 69.4451 | Optimizer: AdamW\n",
      "Trial 228 | Epoch 03 | Train Loss: 57.4831 | Val Loss: 41.4921 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:53,807] Trial 228 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 228 | Epoch 04 | Train Loss: 50.1293 | Val Loss: 39.8601 | Optimizer: AdamW\n",
      "Trial 229 | Epoch 01 | Train Loss: 174.7957 | Val Loss: 34.3748 | Optimizer: AdamW\n",
      "Trial 229 | Epoch 02 | Train Loss: 58.4728 | Val Loss: 77.0494 | Optimizer: AdamW\n",
      "Trial 229 | Epoch 03 | Train Loss: 59.9627 | Val Loss: 43.4351 | Optimizer: AdamW\n",
      "Trial 229 | Epoch 04 | Train Loss: 45.0578 | Val Loss: 39.5154 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:54,283] Trial 229 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 229 | Epoch 05 | Train Loss: 49.0325 | Val Loss: 42.0300 | Optimizer: AdamW\n",
      "Trial 230 | Epoch 01 | Train Loss: 142.8125 | Val Loss: 33.8249 | Optimizer: AdamW\n",
      "Trial 230 | Epoch 02 | Train Loss: 48.5356 | Val Loss: 40.5984 | Optimizer: AdamW\n",
      "Trial 230 | Epoch 03 | Train Loss: 43.2486 | Val Loss: 35.4227 | Optimizer: AdamW\n",
      "Trial 230 | Epoch 04 | Train Loss: 41.3966 | Val Loss: 31.4818 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:54,844] Trial 230 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 230 | Epoch 05 | Train Loss: 34.3244 | Val Loss: 28.4957 | Optimizer: AdamW\n",
      "Trial 230 | Epoch 06 | Train Loss: 36.6073 | Val Loss: 28.5989 | Optimizer: AdamW\n",
      "Trial 231 | Epoch 01 | Train Loss: 188.2045 | Val Loss: 31.9420 | Optimizer: AdamW\n",
      "Trial 231 | Epoch 02 | Train Loss: 72.3845 | Val Loss: 64.9918 | Optimizer: AdamW\n",
      "Trial 231 | Epoch 03 | Train Loss: 46.9599 | Val Loss: 43.9428 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:55,315] Trial 231 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 231 | Epoch 04 | Train Loss: 45.8829 | Val Loss: 33.8336 | Optimizer: AdamW\n",
      "Trial 231 | Epoch 05 | Train Loss: 38.1537 | Val Loss: 30.8537 | Optimizer: AdamW\n",
      "Trial 232 | Epoch 01 | Train Loss: 162.7782 | Val Loss: 34.9247 | Optimizer: AdamW\n",
      "Trial 232 | Epoch 02 | Train Loss: 54.7548 | Val Loss: 35.2343 | Optimizer: AdamW\n",
      "Trial 232 | Epoch 03 | Train Loss: 52.6767 | Val Loss: 51.8090 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:55,873] Trial 232 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 232 | Epoch 04 | Train Loss: 42.4125 | Val Loss: 30.2121 | Optimizer: AdamW\n",
      "Trial 232 | Epoch 05 | Train Loss: 42.5060 | Val Loss: 28.8039 | Optimizer: AdamW\n",
      "Trial 232 | Epoch 06 | Train Loss: 32.8555 | Val Loss: 32.1213 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:56,040] Trial 233 pruned. \n",
      "[I 2025-09-04 21:23:56,204] Trial 234 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 233 | Epoch 01 | Train Loss: 177.3766 | Val Loss: 49.6798 | Optimizer: Adam\n",
      "Trial 234 | Epoch 01 | Train Loss: 177.2973 | Val Loss: 139.1419 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:56,356] Trial 235 pruned. \n",
      "[I 2025-09-04 21:23:56,483] Trial 236 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 235 | Epoch 01 | NaN loss detected so pruning trial\n",
      "Trial 236 | Epoch 01 | Train Loss: 253.0643 | Val Loss: 247.7677 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:56,649] Trial 237 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 237 | Epoch 01 | Train Loss: 154.9651 | Val Loss: 45.6290 | Optimizer: AdamW\n",
      "Trial 238 | Epoch 01 | Train Loss: 155.1369 | Val Loss: 31.2742 | Optimizer: AdamW\n",
      "Trial 238 | Epoch 02 | Train Loss: 48.5843 | Val Loss: 38.1669 | Optimizer: AdamW\n",
      "Trial 238 | Epoch 03 | Train Loss: 38.2046 | Val Loss: 34.1685 | Optimizer: AdamW\n",
      "Trial 238 | Epoch 04 | Train Loss: 40.5501 | Val Loss: 34.0753 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:57,118] Trial 238 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 238 | Epoch 05 | Train Loss: 33.0608 | Val Loss: 33.1032 | Optimizer: AdamW\n",
      "Trial 239 | Epoch 01 | Train Loss: 157.1782 | Val Loss: 41.1686 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:57,283] Trial 239 pruned. \n",
      "[I 2025-09-04 21:23:57,451] Trial 240 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 240 | Epoch 01 | Train Loss: 311.6356 | Val Loss: 339.9693 | Optimizer: AdamW\n",
      "Trial 241 | Epoch 01 | Train Loss: 184.2028 | Val Loss: 68.1386 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:57,614] Trial 241 pruned. \n",
      "[I 2025-09-04 21:23:57,781] Trial 242 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 242 | Epoch 01 | Train Loss: 133.7500 | Val Loss: 43.4092 | Optimizer: AdamW\n",
      "Trial 243 | Epoch 01 | Train Loss: 177.4432 | Val Loss: 50.3530 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:57,947] Trial 243 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 244 | Epoch 01 | Train Loss: 176.4252 | Val Loss: 37.3369 | Optimizer: AdamW\n",
      "Trial 244 | Epoch 02 | Train Loss: 63.7974 | Val Loss: 74.5563 | Optimizer: AdamW\n",
      "Trial 244 | Epoch 03 | Train Loss: 57.5201 | Val Loss: 38.8116 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:58,365] Trial 244 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 244 | Epoch 04 | Train Loss: 47.5515 | Val Loss: 46.6309 | Optimizer: AdamW\n",
      "Trial 245 | Epoch 01 | Train Loss: 184.6436 | Val Loss: 37.0706 | Optimizer: AdamW\n",
      "Trial 245 | Epoch 02 | Train Loss: 54.3623 | Val Loss: 63.6569 | Optimizer: AdamW\n",
      "Trial 245 | Epoch 03 | Train Loss: 51.2668 | Val Loss: 31.5167 | Optimizer: AdamW\n",
      "Trial 245 | Epoch 04 | Train Loss: 43.1028 | Val Loss: 32.7884 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:58,833] Trial 245 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 245 | Epoch 05 | Train Loss: 39.8085 | Val Loss: 34.2716 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:23:59,250] Trial 246 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 246 | Epoch 01 | Train Loss: 143.7516 | Val Loss: 44.8532 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 01 | Train Loss: 159.6257 | Val Loss: 31.2068 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 02 | Train Loss: 52.5850 | Val Loss: 39.9032 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 03 | Train Loss: 45.6417 | Val Loss: 53.8307 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 04 | Train Loss: 44.6548 | Val Loss: 33.8733 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 05 | Train Loss: 39.5527 | Val Loss: 27.6978 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 06 | Train Loss: 32.5857 | Val Loss: 35.3947 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 07 | Train Loss: 32.3812 | Val Loss: 25.7815 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 08 | Train Loss: 30.3940 | Val Loss: 22.3126 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 09 | Train Loss: 29.0885 | Val Loss: 22.5761 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 10 | Train Loss: 23.9800 | Val Loss: 24.5489 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 11 | Train Loss: 26.8266 | Val Loss: 25.6378 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 12 | Train Loss: 28.0984 | Val Loss: 28.1183 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 13 | Train Loss: 26.9771 | Val Loss: 21.8770 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 14 | Train Loss: 25.5797 | Val Loss: 23.0330 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 15 | Train Loss: 25.1659 | Val Loss: 24.8508 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 16 | Train Loss: 24.1966 | Val Loss: 19.1795 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 17 | Train Loss: 24.8959 | Val Loss: 22.9863 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 18 | Train Loss: 22.6985 | Val Loss: 18.5111 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 19 | Train Loss: 21.4990 | Val Loss: 16.6567 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 20 | Train Loss: 23.4741 | Val Loss: 20.2942 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 21 | Train Loss: 22.8329 | Val Loss: 22.0831 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 22 | Train Loss: 21.0710 | Val Loss: 27.8085 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 23 | Train Loss: 24.3160 | Val Loss: 40.0242 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 24 | Train Loss: 25.9761 | Val Loss: 43.4263 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 25 | Train Loss: 25.3657 | Val Loss: 18.3304 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 26 | Train Loss: 20.8749 | Val Loss: 17.8533 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 27 | Train Loss: 21.4328 | Val Loss: 18.7423 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 28 | Train Loss: 19.6973 | Val Loss: 15.5061 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 29 | Train Loss: 20.3181 | Val Loss: 14.4448 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 30 | Train Loss: 22.9248 | Val Loss: 15.2407 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 31 | Train Loss: 23.2748 | Val Loss: 17.9912 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 32 | Train Loss: 26.2848 | Val Loss: 16.4116 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 33 | Train Loss: 22.5960 | Val Loss: 29.5035 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 34 | Train Loss: 20.4907 | Val Loss: 17.1608 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 35 | Train Loss: 18.1493 | Val Loss: 18.6202 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 36 | Train Loss: 17.3816 | Val Loss: 19.2346 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 37 | Train Loss: 18.7946 | Val Loss: 20.1998 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:02,315] Trial 247 finished with value: 14.444800361385191 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3873689472754691, 'lr': 0.0006890112003441306, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 1.6256341623589214e-05}. Best is trial 154 with value: 9.124408535841035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 247 | Epoch 38 | Train Loss: 17.4093 | Val Loss: 17.2467 | Optimizer: AdamW\n",
      "Trial 247 | Epoch 39 | Train Loss: 19.2861 | Val Loss: 16.1706 | Optimizer: AdamW\n",
      "Trial 247 - Early stopping triggered at epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:02,481] Trial 248 pruned. \n",
      "[I 2025-09-04 21:24:02,610] Trial 249 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 248 | Epoch 01 | Train Loss: 142.7454 | Val Loss: 48.1855 | Optimizer: AdamW\n",
      "Trial 249 | Epoch 01 | Train Loss: 172.6467 | Val Loss: 54.1839 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:02,774] Trial 250 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 250 | Epoch 01 | Train Loss: 156.1496 | Val Loss: 50.6912 | Optimizer: AdamW\n",
      "Trial 251 | Epoch 01 | Train Loss: 186.7178 | Val Loss: 32.7395 | Optimizer: Adam\n",
      "Trial 251 | Epoch 02 | Train Loss: 51.0354 | Val Loss: 32.9721 | Optimizer: Adam\n",
      "Trial 251 | Epoch 03 | Train Loss: 39.9633 | Val Loss: 35.3405 | Optimizer: Adam\n",
      "Trial 251 | Epoch 04 | Train Loss: 37.0688 | Val Loss: 35.8292 | Optimizer: Adam\n",
      "Trial 251 | Epoch 05 | Train Loss: 32.9050 | Val Loss: 27.6056 | Optimizer: Adam\n",
      "Trial 251 | Epoch 06 | Train Loss: 34.7969 | Val Loss: 35.5097 | Optimizer: Adam\n",
      "Trial 251 | Epoch 07 | Train Loss: 31.5015 | Val Loss: 25.3839 | Optimizer: Adam\n",
      "Trial 251 | Epoch 08 | Train Loss: 34.5961 | Val Loss: 22.1128 | Optimizer: Adam\n",
      "Trial 251 | Epoch 09 | Train Loss: 28.4262 | Val Loss: 28.1877 | Optimizer: Adam\n",
      "Trial 251 | Epoch 10 | Train Loss: 29.8810 | Val Loss: 23.5870 | Optimizer: Adam\n",
      "Trial 251 | Epoch 11 | Train Loss: 24.8619 | Val Loss: 27.0886 | Optimizer: Adam\n",
      "Trial 251 | Epoch 12 | Train Loss: 29.3083 | Val Loss: 22.2241 | Optimizer: Adam\n",
      "Trial 251 | Epoch 13 | Train Loss: 26.5640 | Val Loss: 20.4364 | Optimizer: Adam\n",
      "Trial 251 | Epoch 14 | Train Loss: 27.6981 | Val Loss: 19.6643 | Optimizer: Adam\n",
      "Trial 251 | Epoch 15 | Train Loss: 27.0063 | Val Loss: 22.8544 | Optimizer: Adam\n",
      "Trial 251 | Epoch 16 | Train Loss: 25.2559 | Val Loss: 21.8107 | Optimizer: Adam\n",
      "Trial 251 | Epoch 17 | Train Loss: 27.4477 | Val Loss: 24.3381 | Optimizer: Adam\n",
      "Trial 251 | Epoch 18 | Train Loss: 25.3343 | Val Loss: 24.0687 | Optimizer: Adam\n",
      "Trial 251 | Epoch 19 | Train Loss: 23.0912 | Val Loss: 17.9607 | Optimizer: Adam\n",
      "Trial 251 | Epoch 20 | Train Loss: 22.3963 | Val Loss: 17.4158 | Optimizer: Adam\n",
      "Trial 251 | Epoch 21 | Train Loss: 25.1396 | Val Loss: 21.7923 | Optimizer: Adam\n",
      "Trial 251 | Epoch 22 | Train Loss: 21.4082 | Val Loss: 29.8753 | Optimizer: Adam\n",
      "Trial 251 | Epoch 23 | Train Loss: 24.6926 | Val Loss: 22.6323 | Optimizer: Adam\n",
      "Trial 251 | Epoch 24 | Train Loss: 24.2720 | Val Loss: 17.9874 | Optimizer: Adam\n",
      "Trial 251 | Epoch 25 | Train Loss: 20.2142 | Val Loss: 17.5425 | Optimizer: Adam\n",
      "Trial 251 | Epoch 26 | Train Loss: 20.3572 | Val Loss: 19.2926 | Optimizer: Adam\n",
      "Trial 251 | Epoch 27 | Train Loss: 20.2600 | Val Loss: 16.0529 | Optimizer: Adam\n",
      "Trial 251 | Epoch 28 | Train Loss: 18.2736 | Val Loss: 15.2274 | Optimizer: Adam\n",
      "Trial 251 | Epoch 29 | Train Loss: 25.4542 | Val Loss: 17.7738 | Optimizer: Adam\n",
      "Trial 251 | Epoch 30 | Train Loss: 22.2413 | Val Loss: 24.2194 | Optimizer: Adam\n",
      "Trial 251 | Epoch 31 | Train Loss: 25.1290 | Val Loss: 19.2181 | Optimizer: Adam\n",
      "Trial 251 | Epoch 32 | Train Loss: 23.7214 | Val Loss: 15.1389 | Optimizer: Adam\n",
      "Trial 251 | Epoch 33 | Train Loss: 17.5636 | Val Loss: 15.8995 | Optimizer: Adam\n",
      "Trial 251 | Epoch 34 | Train Loss: 20.1636 | Val Loss: 16.2158 | Optimizer: Adam\n",
      "Trial 251 | Epoch 35 | Train Loss: 17.9039 | Val Loss: 19.5991 | Optimizer: Adam\n",
      "Trial 251 | Epoch 36 | Train Loss: 20.5758 | Val Loss: 18.7197 | Optimizer: Adam\n",
      "Trial 251 | Epoch 37 | Train Loss: 20.0163 | Val Loss: 20.0595 | Optimizer: Adam\n",
      "Trial 251 | Epoch 38 | Train Loss: 20.1481 | Val Loss: 16.5450 | Optimizer: Adam\n",
      "Trial 251 | Epoch 39 | Train Loss: 19.8120 | Val Loss: 22.0187 | Optimizer: Adam\n",
      "Trial 251 | Epoch 40 | Train Loss: 23.1739 | Val Loss: 29.2500 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:06,083] Trial 251 finished with value: 15.138944393251 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3843876814470079, 'lr': 0.0006175535159740012, 'activation': 'GELU', 'optimizer': 'Adam', 'weight_decay': 3.3512939319704674e-06}. Best is trial 154 with value: 9.124408535841035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 251 | Epoch 41 | Train Loss: 20.3536 | Val Loss: 17.4091 | Optimizer: Adam\n",
      "Trial 251 | Epoch 42 | Train Loss: 19.6702 | Val Loss: 18.4770 | Optimizer: Adam\n",
      "Trial 251 - Early stopping triggered at epoch 42\n",
      "Trial 252 | Epoch 01 | Train Loss: 175.4161 | Val Loss: 32.7537 | Optimizer: AdamW\n",
      "Trial 252 | Epoch 02 | Train Loss: 58.4261 | Val Loss: 70.6369 | Optimizer: AdamW\n",
      "Trial 252 | Epoch 03 | Train Loss: 54.1714 | Val Loss: 41.8351 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:06,555] Trial 252 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 252 | Epoch 04 | Train Loss: 53.9338 | Val Loss: 45.1981 | Optimizer: AdamW\n",
      "Trial 252 | Epoch 05 | Train Loss: 45.7810 | Val Loss: 35.1523 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:06,725] Trial 253 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 253 | Epoch 01 | Train Loss: 5943287.0952 | Val Loss: 390.8010 | Optimizer: RMSprop\n",
      "Trial 254 | Epoch 01 | Train Loss: 139.8045 | Val Loss: 48.4731 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:06,893] Trial 254 pruned. \n",
      "[I 2025-09-04 21:24:07,057] Trial 255 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 255 | Epoch 01 | Train Loss: 155.6073 | Val Loss: 43.4732 | Optimizer: AdamW\n",
      "Trial 256 | Epoch 01 | Train Loss: 147.6277 | Val Loss: 49.6464 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:07,225] Trial 256 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 257 | Epoch 01 | Train Loss: 121.1052 | Val Loss: 39.0993 | Optimizer: AdamW\n",
      "Trial 257 | Epoch 02 | Train Loss: 51.6919 | Val Loss: 48.5939 | Optimizer: AdamW\n",
      "Trial 257 | Epoch 03 | Train Loss: 48.1700 | Val Loss: 36.6657 | Optimizer: AdamW\n",
      "Trial 257 | Epoch 04 | Train Loss: 41.8537 | Val Loss: 31.8011 | Optimizer: AdamW\n",
      "Trial 257 | Epoch 05 | Train Loss: 33.0199 | Val Loss: 26.3102 | Optimizer: AdamW\n",
      "Trial 257 | Epoch 06 | Train Loss: 29.9843 | Val Loss: 24.8790 | Optimizer: AdamW\n",
      "Trial 257 | Epoch 07 | Train Loss: 29.1275 | Val Loss: 26.2368 | Optimizer: AdamW\n",
      "Trial 257 | Epoch 08 | Train Loss: 28.1703 | Val Loss: 27.9469 | Optimizer: AdamW\n",
      "Trial 257 | Epoch 09 | Train Loss: 26.8902 | Val Loss: 27.9630 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:08,143] Trial 257 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 257 | Epoch 10 | Train Loss: 26.6936 | Val Loss: 29.1064 | Optimizer: AdamW\n",
      "Trial 257 | Epoch 11 | Train Loss: 27.5459 | Val Loss: 28.6292 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:08,314] Trial 258 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 258 | Epoch 01 | Train Loss: 125.2489 | Val Loss: 59.0588 | Optimizer: AdamW\n",
      "Trial 259 | Epoch 01 | Train Loss: 162.3029 | Val Loss: 37.5763 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:08,702] Trial 259 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 259 | Epoch 02 | Train Loss: 52.6161 | Val Loss: 60.7288 | Optimizer: AdamW\n",
      "Trial 259 | Epoch 03 | Train Loss: 51.9031 | Val Loss: 36.8417 | Optimizer: AdamW\n",
      "Trial 259 | Epoch 04 | Train Loss: 49.2485 | Val Loss: 52.1705 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:08,879] Trial 260 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 260 | Epoch 01 | Train Loss: 285.9138 | Val Loss: 303.8017 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 01 | Train Loss: 162.4354 | Val Loss: 38.9007 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 02 | Train Loss: 46.5587 | Val Loss: 34.2646 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 03 | Train Loss: 41.6036 | Val Loss: 35.5373 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 04 | Train Loss: 34.2316 | Val Loss: 31.1616 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 05 | Train Loss: 32.8143 | Val Loss: 25.7233 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 06 | Train Loss: 35.7695 | Val Loss: 22.0796 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 07 | Train Loss: 34.5528 | Val Loss: 36.3697 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 08 | Train Loss: 28.7469 | Val Loss: 24.7942 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 09 | Train Loss: 28.4277 | Val Loss: 21.2681 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 10 | Train Loss: 28.7707 | Val Loss: 22.6778 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 11 | Train Loss: 26.5198 | Val Loss: 20.6605 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 12 | Train Loss: 27.2662 | Val Loss: 22.5573 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 13 | Train Loss: 23.1769 | Val Loss: 18.8639 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 14 | Train Loss: 25.1514 | Val Loss: 19.3847 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 15 | Train Loss: 25.5781 | Val Loss: 21.1307 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 16 | Train Loss: 22.8856 | Val Loss: 26.5645 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 17 | Train Loss: 22.8369 | Val Loss: 23.2227 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 18 | Train Loss: 21.8299 | Val Loss: 24.5441 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 19 | Train Loss: 19.4464 | Val Loss: 15.7947 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 20 | Train Loss: 19.1840 | Val Loss: 19.1275 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 21 | Train Loss: 21.1324 | Val Loss: 29.5964 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 22 | Train Loss: 23.2227 | Val Loss: 29.1527 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 23 | Train Loss: 23.9480 | Val Loss: 20.4335 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 24 | Train Loss: 21.9110 | Val Loss: 16.4275 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 25 | Train Loss: 19.8083 | Val Loss: 23.4577 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 26 | Train Loss: 20.3702 | Val Loss: 27.2856 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 27 | Train Loss: 20.4102 | Val Loss: 25.1222 | Optimizer: AdamW\n",
      "Trial 261 | Epoch 28 | Train Loss: 19.5575 | Val Loss: 19.2034 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:11,168] Trial 261 finished with value: 15.794723882907775 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3668839580006545, 'lr': 0.0007572733685600326, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 2.941374843469981e-06}. Best is trial 154 with value: 9.124408535841035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 261 | Epoch 29 | Train Loss: 18.3188 | Val Loss: 17.7590 | Optimizer: AdamW\n",
      "Trial 261 - Early stopping triggered at epoch 29\n",
      "Trial 262 | Epoch 01 | Train Loss: 147.5715 | Val Loss: 35.2651 | Optimizer: AdamW\n",
      "Trial 262 | Epoch 02 | Train Loss: 53.5672 | Val Loss: 43.3757 | Optimizer: AdamW\n",
      "Trial 262 | Epoch 03 | Train Loss: 48.7310 | Val Loss: 49.0760 | Optimizer: AdamW\n",
      "Trial 262 | Epoch 04 | Train Loss: 44.7551 | Val Loss: 33.8796 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:11,709] Trial 262 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 262 | Epoch 05 | Train Loss: 39.3168 | Val Loss: 28.5219 | Optimizer: AdamW\n",
      "Trial 262 | Epoch 06 | Train Loss: 33.1546 | Val Loss: 30.2035 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:11,841] Trial 263 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 263 | Epoch 01 | Train Loss: 157.6585 | Val Loss: 49.3797 | Optimizer: AdamW\n",
      "Trial 264 | Epoch 01 | Train Loss: 221.8043 | Val Loss: 78.2229 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:12,007] Trial 264 pruned. \n",
      "[I 2025-09-04 21:24:12,180] Trial 265 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 265 | Epoch 01 | NaN loss detected so pruning trial\n",
      "Trial 266 | Epoch 01 | Train Loss: 180.3403 | Val Loss: 30.2523 | Optimizer: Adam\n",
      "Trial 266 | Epoch 02 | Train Loss: 61.6614 | Val Loss: 64.3645 | Optimizer: Adam\n",
      "Trial 266 | Epoch 03 | Train Loss: 51.2994 | Val Loss: 39.3753 | Optimizer: Adam\n",
      "Trial 266 | Epoch 04 | Train Loss: 44.7967 | Val Loss: 36.4995 | Optimizer: Adam\n",
      "Trial 266 | Epoch 05 | Train Loss: 38.7746 | Val Loss: 29.7581 | Optimizer: Adam\n",
      "Trial 266 | Epoch 06 | Train Loss: 31.9742 | Val Loss: 26.7079 | Optimizer: Adam\n",
      "Trial 266 | Epoch 07 | Train Loss: 31.9806 | Val Loss: 24.2701 | Optimizer: Adam\n",
      "Trial 266 | Epoch 08 | Train Loss: 29.2651 | Val Loss: 22.0930 | Optimizer: Adam\n",
      "Trial 266 | Epoch 09 | Train Loss: 28.9621 | Val Loss: 25.1070 | Optimizer: Adam\n",
      "Trial 266 | Epoch 10 | Train Loss: 26.2435 | Val Loss: 30.9012 | Optimizer: Adam\n",
      "Trial 266 | Epoch 11 | Train Loss: 28.4286 | Val Loss: 23.0618 | Optimizer: Adam\n",
      "Trial 266 | Epoch 12 | Train Loss: 24.5006 | Val Loss: 22.4675 | Optimizer: Adam\n",
      "Trial 266 | Epoch 13 | Train Loss: 25.5328 | Val Loss: 24.6853 | Optimizer: Adam\n",
      "Trial 266 | Epoch 14 | Train Loss: 25.9068 | Val Loss: 30.1870 | Optimizer: Adam\n",
      "Trial 266 | Epoch 15 | Train Loss: 27.8163 | Val Loss: 19.5576 | Optimizer: Adam\n",
      "Trial 266 | Epoch 16 | Train Loss: 26.6526 | Val Loss: 21.7858 | Optimizer: Adam\n",
      "Trial 266 | Epoch 17 | Train Loss: 27.7239 | Val Loss: 28.7007 | Optimizer: Adam\n",
      "Trial 266 | Epoch 18 | Train Loss: 27.6101 | Val Loss: 21.2511 | Optimizer: Adam\n",
      "Trial 266 | Epoch 19 | Train Loss: 26.7347 | Val Loss: 21.8520 | Optimizer: Adam\n",
      "Trial 266 | Epoch 20 | Train Loss: 22.7393 | Val Loss: 18.0973 | Optimizer: Adam\n",
      "Trial 266 | Epoch 21 | Train Loss: 22.3849 | Val Loss: 21.4743 | Optimizer: Adam\n",
      "Trial 266 | Epoch 22 | Train Loss: 23.0365 | Val Loss: 27.5049 | Optimizer: Adam\n",
      "Trial 266 | Epoch 23 | Train Loss: 21.6168 | Val Loss: 17.8960 | Optimizer: Adam\n",
      "Trial 266 | Epoch 24 | Train Loss: 23.9467 | Val Loss: 26.1988 | Optimizer: Adam\n",
      "Trial 266 | Epoch 25 | Train Loss: 24.1401 | Val Loss: 19.6591 | Optimizer: Adam\n",
      "Trial 266 | Epoch 26 | Train Loss: 23.3771 | Val Loss: 17.6064 | Optimizer: Adam\n",
      "Trial 266 | Epoch 27 | Train Loss: 24.9297 | Val Loss: 16.6260 | Optimizer: Adam\n",
      "Trial 266 | Epoch 28 | Train Loss: 22.4077 | Val Loss: 15.8666 | Optimizer: Adam\n",
      "Trial 266 | Epoch 29 | Train Loss: 24.0444 | Val Loss: 17.5428 | Optimizer: Adam\n",
      "Trial 266 | Epoch 30 | Train Loss: 22.0977 | Val Loss: 21.7773 | Optimizer: Adam\n",
      "Trial 266 | Epoch 31 | Train Loss: 23.3160 | Val Loss: 18.7791 | Optimizer: Adam\n",
      "Trial 266 | Epoch 32 | Train Loss: 19.9564 | Val Loss: 15.1885 | Optimizer: Adam\n",
      "Trial 266 | Epoch 33 | Train Loss: 21.1539 | Val Loss: 17.6648 | Optimizer: Adam\n",
      "Trial 266 | Epoch 34 | Train Loss: 19.5262 | Val Loss: 26.2440 | Optimizer: Adam\n",
      "Trial 266 | Epoch 35 | Train Loss: 20.3278 | Val Loss: 19.5618 | Optimizer: Adam\n",
      "Trial 266 | Epoch 36 | Train Loss: 19.2824 | Val Loss: 15.1198 | Optimizer: Adam\n",
      "Trial 266 | Epoch 37 | Train Loss: 21.7877 | Val Loss: 18.9945 | Optimizer: Adam\n",
      "Trial 266 | Epoch 38 | Train Loss: 20.5380 | Val Loss: 15.3285 | Optimizer: Adam\n",
      "Trial 266 | Epoch 39 | Train Loss: 18.6742 | Val Loss: 18.3133 | Optimizer: Adam\n",
      "Trial 266 | Epoch 40 | Train Loss: 19.8742 | Val Loss: 17.0058 | Optimizer: Adam\n",
      "Trial 266 | Epoch 41 | Train Loss: 17.5732 | Val Loss: 17.0928 | Optimizer: Adam\n",
      "Trial 266 | Epoch 42 | Train Loss: 17.9494 | Val Loss: 21.5878 | Optimizer: Adam\n",
      "Trial 266 | Epoch 43 | Train Loss: 16.7471 | Val Loss: 16.2661 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:15,783] Trial 266 finished with value: 15.11982818541488 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3789455284721415, 'lr': 0.000713418694813003, 'activation': 'GELU', 'optimizer': 'Adam', 'weight_decay': 8.871961149756802e-05}. Best is trial 154 with value: 9.124408535841035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 266 | Epoch 44 | Train Loss: 18.9877 | Val Loss: 19.2901 | Optimizer: Adam\n",
      "Trial 266 | Epoch 45 | Train Loss: 18.4975 | Val Loss: 17.5458 | Optimizer: Adam\n",
      "Trial 266 | Epoch 46 | Train Loss: 15.6048 | Val Loss: 20.8365 | Optimizer: Adam\n",
      "Trial 266 - Early stopping triggered at epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:15,953] Trial 267 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 267 | Epoch 01 | Train Loss: 174.3552 | Val Loss: 39.6115 | Optimizer: AdamW\n",
      "Trial 268 | Epoch 01 | Train Loss: 165.4642 | Val Loss: 53.3826 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:16,137] Trial 268 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 269 | Epoch 01 | Train Loss: 136.7898 | Val Loss: 37.9497 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 02 | Train Loss: 41.7756 | Val Loss: 32.0168 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 03 | Train Loss: 37.1532 | Val Loss: 30.6373 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 04 | Train Loss: 32.0252 | Val Loss: 27.3247 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 05 | Train Loss: 26.9050 | Val Loss: 26.1355 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 06 | Train Loss: 27.1243 | Val Loss: 22.6622 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 07 | Train Loss: 25.7982 | Val Loss: 21.6889 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 08 | Train Loss: 26.2738 | Val Loss: 21.8263 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 09 | Train Loss: 25.3178 | Val Loss: 28.1450 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 10 | Train Loss: 25.7787 | Val Loss: 20.9886 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 11 | Train Loss: 23.9006 | Val Loss: 20.8297 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 12 | Train Loss: 23.0602 | Val Loss: 22.4533 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 13 | Train Loss: 22.4863 | Val Loss: 20.5085 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 14 | Train Loss: 21.6792 | Val Loss: 19.9025 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 15 | Train Loss: 21.7311 | Val Loss: 19.5451 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 16 | Train Loss: 20.6215 | Val Loss: 21.9203 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 17 | Train Loss: 19.1258 | Val Loss: 19.1596 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 18 | Train Loss: 20.8841 | Val Loss: 20.9016 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 19 | Train Loss: 20.4061 | Val Loss: 17.4822 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 20 | Train Loss: 19.3854 | Val Loss: 20.7750 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 21 | Train Loss: 18.2952 | Val Loss: 18.8786 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 22 | Train Loss: 18.8899 | Val Loss: 20.4088 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 23 | Train Loss: 20.4202 | Val Loss: 22.8393 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 24 | Train Loss: 19.3984 | Val Loss: 16.6910 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 25 | Train Loss: 19.7023 | Val Loss: 19.6268 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 26 | Train Loss: 20.5039 | Val Loss: 26.0795 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 27 | Train Loss: 20.8196 | Val Loss: 24.0247 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 28 | Train Loss: 19.6991 | Val Loss: 16.1988 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 29 | Train Loss: 18.8026 | Val Loss: 16.4497 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 30 | Train Loss: 18.1142 | Val Loss: 16.1059 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 31 | Train Loss: 17.0422 | Val Loss: 14.5265 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 32 | Train Loss: 17.4073 | Val Loss: 17.1480 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 33 | Train Loss: 20.0303 | Val Loss: 20.2857 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 34 | Train Loss: 16.3673 | Val Loss: 19.1345 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 35 | Train Loss: 15.8276 | Val Loss: 16.9064 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 36 | Train Loss: 17.5718 | Val Loss: 13.9782 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 37 | Train Loss: 16.7516 | Val Loss: 14.0499 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 38 | Train Loss: 16.6719 | Val Loss: 14.1982 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 39 | Train Loss: 13.8616 | Val Loss: 15.2846 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 40 | Train Loss: 16.5054 | Val Loss: 14.2012 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 41 | Train Loss: 16.7022 | Val Loss: 23.0812 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 42 | Train Loss: 17.4926 | Val Loss: 21.0002 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 43 | Train Loss: 17.2513 | Val Loss: 18.0106 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 44 | Train Loss: 16.5182 | Val Loss: 13.8945 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 45 | Train Loss: 15.0921 | Val Loss: 12.9545 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 46 | Train Loss: 16.3397 | Val Loss: 14.7864 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 47 | Train Loss: 14.7627 | Val Loss: 13.4039 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 48 | Train Loss: 15.3905 | Val Loss: 12.5748 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 49 | Train Loss: 15.0962 | Val Loss: 12.3924 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 50 | Train Loss: 16.8068 | Val Loss: 14.6006 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 51 | Train Loss: 20.1065 | Val Loss: 17.5377 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 52 | Train Loss: 16.8296 | Val Loss: 20.5606 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 53 | Train Loss: 16.7595 | Val Loss: 13.0280 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 54 | Train Loss: 14.8805 | Val Loss: 13.9092 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 55 | Train Loss: 14.9618 | Val Loss: 16.1308 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 56 | Train Loss: 14.4044 | Val Loss: 13.7768 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 57 | Train Loss: 13.5361 | Val Loss: 12.9194 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:20,752] Trial 269 finished with value: 12.392426103111205 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.2614910288587241, 'lr': 0.0006687300139298326, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 1.2833561902519631e-05}. Best is trial 154 with value: 9.124408535841035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 269 | Epoch 58 | Train Loss: 14.7243 | Val Loss: 14.6267 | Optimizer: AdamW\n",
      "Trial 269 | Epoch 59 | Train Loss: 13.0526 | Val Loss: 13.2763 | Optimizer: AdamW\n",
      "Trial 269 - Early stopping triggered at epoch 59\n",
      "Trial 270 | Epoch 01 | Train Loss: 172.8707 | Val Loss: 32.2128 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 02 | Train Loss: 55.6558 | Val Loss: 56.8988 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 03 | Train Loss: 40.8408 | Val Loss: 39.0382 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 04 | Train Loss: 42.4674 | Val Loss: 43.4493 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 05 | Train Loss: 35.1965 | Val Loss: 29.6124 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 06 | Train Loss: 32.6180 | Val Loss: 27.6029 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 07 | Train Loss: 30.0210 | Val Loss: 24.5344 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 08 | Train Loss: 27.7743 | Val Loss: 24.5670 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 09 | Train Loss: 27.6406 | Val Loss: 31.5348 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 10 | Train Loss: 26.6894 | Val Loss: 22.0471 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 11 | Train Loss: 22.6218 | Val Loss: 20.9584 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 12 | Train Loss: 24.0370 | Val Loss: 25.8151 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 13 | Train Loss: 23.7987 | Val Loss: 20.9883 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 14 | Train Loss: 22.9037 | Val Loss: 21.3152 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 15 | Train Loss: 22.5006 | Val Loss: 22.6328 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 16 | Train Loss: 22.0815 | Val Loss: 20.1716 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 17 | Train Loss: 21.8438 | Val Loss: 21.4678 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 18 | Train Loss: 23.5090 | Val Loss: 18.9831 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 19 | Train Loss: 26.5270 | Val Loss: 20.8035 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 20 | Train Loss: 25.4891 | Val Loss: 21.8339 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 21 | Train Loss: 22.4946 | Val Loss: 21.7486 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 22 | Train Loss: 22.2827 | Val Loss: 19.9178 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 23 | Train Loss: 23.1798 | Val Loss: 20.3177 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 24 | Train Loss: 18.7166 | Val Loss: 20.1143 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 25 | Train Loss: 18.0299 | Val Loss: 21.9968 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 26 | Train Loss: 21.4996 | Val Loss: 20.6675 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 27 | Train Loss: 19.8656 | Val Loss: 18.0377 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 28 | Train Loss: 18.4517 | Val Loss: 23.5165 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 29 | Train Loss: 20.1935 | Val Loss: 16.3256 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 30 | Train Loss: 20.6107 | Val Loss: 19.1042 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 31 | Train Loss: 19.8026 | Val Loss: 22.5932 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 32 | Train Loss: 19.8493 | Val Loss: 22.7979 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 33 | Train Loss: 20.4511 | Val Loss: 21.7709 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 34 | Train Loss: 19.1467 | Val Loss: 16.2833 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 35 | Train Loss: 19.4611 | Val Loss: 15.5589 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 36 | Train Loss: 17.3072 | Val Loss: 17.1747 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 37 | Train Loss: 19.2919 | Val Loss: 16.2189 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 38 | Train Loss: 17.3685 | Val Loss: 17.0194 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 39 | Train Loss: 18.0361 | Val Loss: 19.1696 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 40 | Train Loss: 18.9638 | Val Loss: 18.4176 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 41 | Train Loss: 17.4061 | Val Loss: 15.2879 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 42 | Train Loss: 17.5515 | Val Loss: 16.9255 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 43 | Train Loss: 17.1569 | Val Loss: 22.6249 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 44 | Train Loss: 16.9661 | Val Loss: 21.3156 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 45 | Train Loss: 16.5030 | Val Loss: 15.7896 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 46 | Train Loss: 15.3144 | Val Loss: 13.5645 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 47 | Train Loss: 15.2550 | Val Loss: 17.1336 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 48 | Train Loss: 15.2960 | Val Loss: 14.3221 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 49 | Train Loss: 14.5667 | Val Loss: 18.5078 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 50 | Train Loss: 18.1175 | Val Loss: 23.0591 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 51 | Train Loss: 17.2019 | Val Loss: 24.5926 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 52 | Train Loss: 18.1598 | Val Loss: 20.2773 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 53 | Train Loss: 16.2254 | Val Loss: 13.9739 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 54 | Train Loss: 17.0291 | Val Loss: 19.2881 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:25,146] Trial 270 finished with value: 13.564487077356354 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.2629713632294253, 'lr': 0.0006605755371672007, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 1.2363843556305364e-05}. Best is trial 154 with value: 9.124408535841035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 270 | Epoch 55 | Train Loss: 16.9331 | Val Loss: 14.4943 | Optimizer: AdamW\n",
      "Trial 270 | Epoch 56 | Train Loss: 15.1153 | Val Loss: 14.0545 | Optimizer: AdamW\n",
      "Trial 270 - Early stopping triggered at epoch 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:25,272] Trial 271 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 271 | Epoch 01 | Train Loss: 176.4575 | Val Loss: 48.6754 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 01 | Train Loss: 136.3734 | Val Loss: 34.5759 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 02 | Train Loss: 45.8803 | Val Loss: 33.9564 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 03 | Train Loss: 38.6024 | Val Loss: 36.5050 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 04 | Train Loss: 36.4364 | Val Loss: 27.4566 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 05 | Train Loss: 32.1211 | Val Loss: 31.8630 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 06 | Train Loss: 26.3174 | Val Loss: 22.6376 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 07 | Train Loss: 27.0984 | Val Loss: 21.9073 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 08 | Train Loss: 23.8654 | Val Loss: 25.0463 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 09 | Train Loss: 23.9426 | Val Loss: 21.8686 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 10 | Train Loss: 23.5450 | Val Loss: 21.3458 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 11 | Train Loss: 25.0869 | Val Loss: 20.6613 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 12 | Train Loss: 21.6942 | Val Loss: 21.5239 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 13 | Train Loss: 20.8779 | Val Loss: 21.0437 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 14 | Train Loss: 22.7018 | Val Loss: 21.6320 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 15 | Train Loss: 22.0527 | Val Loss: 23.2990 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 16 | Train Loss: 22.6839 | Val Loss: 27.0185 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 17 | Train Loss: 20.4261 | Val Loss: 19.1939 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 18 | Train Loss: 21.9786 | Val Loss: 30.7567 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 19 | Train Loss: 22.1088 | Val Loss: 24.5686 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 20 | Train Loss: 21.3752 | Val Loss: 25.7446 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 21 | Train Loss: 23.0583 | Val Loss: 23.5039 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 22 | Train Loss: 21.2927 | Val Loss: 18.5489 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 23 | Train Loss: 19.3000 | Val Loss: 21.5120 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 24 | Train Loss: 20.4355 | Val Loss: 19.5279 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 25 | Train Loss: 19.8787 | Val Loss: 22.2953 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 26 | Train Loss: 18.9021 | Val Loss: 20.9149 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 27 | Train Loss: 19.7874 | Val Loss: 21.4594 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 28 | Train Loss: 19.1160 | Val Loss: 22.2730 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 29 | Train Loss: 18.8879 | Val Loss: 25.0046 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 30 | Train Loss: 19.3813 | Val Loss: 23.3000 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 31 | Train Loss: 18.4108 | Val Loss: 24.5120 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 32 | Train Loss: 16.5370 | Val Loss: 16.5318 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 33 | Train Loss: 16.0337 | Val Loss: 16.3273 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 34 | Train Loss: 15.3838 | Val Loss: 14.4845 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 35 | Train Loss: 17.7376 | Val Loss: 14.1592 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 36 | Train Loss: 14.7455 | Val Loss: 15.0708 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 37 | Train Loss: 15.4639 | Val Loss: 14.8063 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 38 | Train Loss: 14.1859 | Val Loss: 14.3719 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 39 | Train Loss: 15.5740 | Val Loss: 15.9347 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 40 | Train Loss: 15.9518 | Val Loss: 16.5806 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 41 | Train Loss: 15.7068 | Val Loss: 14.2482 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 42 | Train Loss: 15.6378 | Val Loss: 14.5148 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 43 | Train Loss: 14.9319 | Val Loss: 15.4937 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 44 | Train Loss: 14.2254 | Val Loss: 15.7209 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 45 | Train Loss: 14.1734 | Val Loss: 13.4839 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 46 | Train Loss: 14.6726 | Val Loss: 20.8533 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 47 | Train Loss: 13.6712 | Val Loss: 18.0047 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 48 | Train Loss: 15.0671 | Val Loss: 14.4351 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 49 | Train Loss: 15.9914 | Val Loss: 13.0250 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 50 | Train Loss: 15.5469 | Val Loss: 13.3973 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 51 | Train Loss: 14.6071 | Val Loss: 14.0117 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 52 | Train Loss: 14.2404 | Val Loss: 13.2153 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 53 | Train Loss: 13.8682 | Val Loss: 12.7830 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 54 | Train Loss: 13.5917 | Val Loss: 14.4194 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 55 | Train Loss: 13.9813 | Val Loss: 13.2087 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 56 | Train Loss: 15.4393 | Val Loss: 11.6485 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 57 | Train Loss: 14.9792 | Val Loss: 12.2440 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 58 | Train Loss: 14.8955 | Val Loss: 14.6019 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 59 | Train Loss: 13.7328 | Val Loss: 15.3988 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 60 | Train Loss: 14.3920 | Val Loss: 21.1303 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 61 | Train Loss: 13.9632 | Val Loss: 13.2514 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 62 | Train Loss: 13.7131 | Val Loss: 13.3014 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 63 | Train Loss: 13.2965 | Val Loss: 11.7160 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 64 | Train Loss: 12.0657 | Val Loss: 12.4954 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 65 | Train Loss: 12.4182 | Val Loss: 12.8352 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 66 | Train Loss: 12.3691 | Val Loss: 10.5772 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 67 | Train Loss: 13.1741 | Val Loss: 12.4458 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 68 | Train Loss: 12.7964 | Val Loss: 10.8064 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 69 | Train Loss: 11.8398 | Val Loss: 16.0495 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 70 | Train Loss: 12.1391 | Val Loss: 9.8024 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 71 | Train Loss: 13.1703 | Val Loss: 12.6361 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 72 | Train Loss: 14.0282 | Val Loss: 10.1857 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 73 | Train Loss: 13.0960 | Val Loss: 10.2507 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 74 | Train Loss: 13.3667 | Val Loss: 10.4056 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 75 | Train Loss: 12.7571 | Val Loss: 11.3003 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 76 | Train Loss: 11.8964 | Val Loss: 9.8488 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 77 | Train Loss: 11.0972 | Val Loss: 9.4993 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 78 | Train Loss: 10.4764 | Val Loss: 12.3239 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 79 | Train Loss: 12.8461 | Val Loss: 20.6919 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 80 | Train Loss: 13.3356 | Val Loss: 12.3840 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 81 | Train Loss: 11.7954 | Val Loss: 12.0389 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 82 | Train Loss: 12.3736 | Val Loss: 9.7066 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 83 | Train Loss: 11.2762 | Val Loss: 9.3774 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 84 | Train Loss: 10.2267 | Val Loss: 8.3365 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 85 | Train Loss: 9.4735 | Val Loss: 9.6990 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 86 | Train Loss: 10.2519 | Val Loss: 9.5878 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 87 | Train Loss: 9.6302 | Val Loss: 10.9109 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 88 | Train Loss: 9.5643 | Val Loss: 9.0452 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 89 | Train Loss: 11.4924 | Val Loss: 8.9424 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 90 | Train Loss: 12.7426 | Val Loss: 9.3529 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 91 | Train Loss: 11.5758 | Val Loss: 9.7490 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 92 | Train Loss: 11.2113 | Val Loss: 10.2829 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 93 | Train Loss: 11.8000 | Val Loss: 9.5668 | Optimizer: AdamW\n",
      "Trial 272 | Epoch 94 | Train Loss: 10.6447 | Val Loss: 8.8814 | Optimizer: AdamW\n",
      "Trial 272 - Early stopping triggered at epoch 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:32,582] Trial 272 finished with value: 8.336457074173097 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.26835948201850623, 'lr': 0.0006858724864946909, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 1.2164081006161759e-05}. Best is trial 272 with value: 8.336457074173097.\n",
      "[I 2025-09-04 21:24:32,765] Trial 273 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 273 | Epoch 01 | Train Loss: 205.0190 | Val Loss: 42.8821 | Optimizer: AdamW\n",
      "Trial 274 | Epoch 01 | Train Loss: 186.4382 | Val Loss: 162.8726 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:32,931] Trial 274 pruned. \n",
      "[I 2025-09-04 21:24:33,099] Trial 275 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 275 | Epoch 01 | Train Loss: 168.8925 | Val Loss: 63.9077 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 01 | Train Loss: 143.1379 | Val Loss: 34.3053 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 02 | Train Loss: 43.3002 | Val Loss: 35.8730 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 03 | Train Loss: 42.7771 | Val Loss: 37.6038 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 04 | Train Loss: 37.9719 | Val Loss: 28.6277 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 05 | Train Loss: 33.9911 | Val Loss: 27.3140 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 06 | Train Loss: 29.8284 | Val Loss: 29.3350 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 07 | Train Loss: 26.9474 | Val Loss: 25.4773 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 08 | Train Loss: 25.1797 | Val Loss: 21.5830 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 09 | Train Loss: 27.2145 | Val Loss: 21.3138 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 10 | Train Loss: 24.7170 | Val Loss: 21.0520 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 11 | Train Loss: 25.2030 | Val Loss: 23.4136 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 12 | Train Loss: 24.6827 | Val Loss: 23.8711 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 13 | Train Loss: 24.2844 | Val Loss: 21.2351 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 14 | Train Loss: 24.4426 | Val Loss: 20.9802 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 15 | Train Loss: 23.1647 | Val Loss: 20.3654 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 16 | Train Loss: 25.7268 | Val Loss: 21.8408 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 17 | Train Loss: 22.9443 | Val Loss: 24.2876 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 18 | Train Loss: 22.6919 | Val Loss: 19.4802 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 19 | Train Loss: 22.7746 | Val Loss: 19.2268 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 20 | Train Loss: 20.2030 | Val Loss: 19.7946 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 21 | Train Loss: 21.2740 | Val Loss: 24.5456 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 22 | Train Loss: 21.0686 | Val Loss: 23.4349 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 23 | Train Loss: 23.6582 | Val Loss: 18.5343 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 24 | Train Loss: 22.1364 | Val Loss: 17.8309 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 25 | Train Loss: 22.2909 | Val Loss: 17.4163 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 26 | Train Loss: 19.7694 | Val Loss: 20.0528 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 27 | Train Loss: 18.3230 | Val Loss: 20.9607 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 28 | Train Loss: 20.1179 | Val Loss: 23.9607 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 29 | Train Loss: 19.3082 | Val Loss: 18.1603 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 30 | Train Loss: 19.4689 | Val Loss: 20.9929 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 31 | Train Loss: 20.1393 | Val Loss: 15.9135 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 32 | Train Loss: 17.9849 | Val Loss: 20.1785 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 33 | Train Loss: 18.5769 | Val Loss: 16.3606 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 34 | Train Loss: 16.3514 | Val Loss: 17.7203 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 35 | Train Loss: 16.2850 | Val Loss: 14.9783 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 36 | Train Loss: 18.5862 | Val Loss: 15.2508 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 37 | Train Loss: 17.3465 | Val Loss: 14.0193 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 38 | Train Loss: 17.7241 | Val Loss: 14.1404 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 39 | Train Loss: 17.4890 | Val Loss: 14.7163 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 40 | Train Loss: 14.6129 | Val Loss: 14.0575 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 41 | Train Loss: 14.3597 | Val Loss: 15.1836 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 42 | Train Loss: 18.2011 | Val Loss: 14.2688 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 43 | Train Loss: 15.3545 | Val Loss: 20.1265 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 44 | Train Loss: 14.5275 | Val Loss: 21.1286 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 45 | Train Loss: 16.9056 | Val Loss: 21.5658 | Optimizer: AdamW\n",
      "Trial 276 | Epoch 46 | Train Loss: 16.7611 | Val Loss: 19.3815 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:36,787] Trial 276 finished with value: 14.019267376845445 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.2590970855716058, 'lr': 0.0007112098494546964, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 1.2722438934304967e-05}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 276 | Epoch 47 | Train Loss: 15.6886 | Val Loss: 14.6365 | Optimizer: AdamW\n",
      "Trial 276 - Early stopping triggered at epoch 47\n",
      "Trial 277 | Epoch 01 | Train Loss: 166.7805 | Val Loss: 41.0105 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:36,958] Trial 277 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 278 | Epoch 01 | Train Loss: 138.4336 | Val Loss: 31.4055 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 02 | Train Loss: 50.5188 | Val Loss: 52.4289 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 03 | Train Loss: 43.6979 | Val Loss: 35.1333 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 04 | Train Loss: 40.1613 | Val Loss: 36.0746 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 05 | Train Loss: 34.8406 | Val Loss: 27.4339 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 06 | Train Loss: 29.4015 | Val Loss: 24.7928 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 07 | Train Loss: 26.6665 | Val Loss: 24.1510 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 08 | Train Loss: 25.4771 | Val Loss: 22.8214 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 09 | Train Loss: 25.7230 | Val Loss: 26.9060 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 10 | Train Loss: 22.4947 | Val Loss: 23.6890 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 11 | Train Loss: 24.7647 | Val Loss: 21.4203 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 12 | Train Loss: 23.2185 | Val Loss: 23.4747 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 13 | Train Loss: 22.9406 | Val Loss: 20.7021 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 14 | Train Loss: 23.3047 | Val Loss: 20.5347 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 15 | Train Loss: 23.5412 | Val Loss: 21.1145 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 16 | Train Loss: 22.9320 | Val Loss: 21.3613 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 17 | Train Loss: 21.8936 | Val Loss: 19.3302 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 18 | Train Loss: 21.7436 | Val Loss: 18.8751 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 19 | Train Loss: 20.0305 | Val Loss: 21.7308 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 20 | Train Loss: 19.4236 | Val Loss: 20.9472 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 21 | Train Loss: 19.2037 | Val Loss: 19.5154 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 22 | Train Loss: 18.0422 | Val Loss: 25.9652 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 23 | Train Loss: 20.5947 | Val Loss: 22.8949 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 24 | Train Loss: 20.8756 | Val Loss: 16.5881 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 25 | Train Loss: 21.8455 | Val Loss: 18.1761 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 26 | Train Loss: 20.0902 | Val Loss: 16.9391 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 27 | Train Loss: 19.2547 | Val Loss: 20.5488 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 28 | Train Loss: 18.5353 | Val Loss: 17.8622 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 29 | Train Loss: 18.6314 | Val Loss: 16.9192 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 30 | Train Loss: 16.2798 | Val Loss: 15.8788 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 31 | Train Loss: 16.6913 | Val Loss: 22.3915 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 32 | Train Loss: 17.1049 | Val Loss: 19.0188 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 33 | Train Loss: 15.7723 | Val Loss: 21.7954 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 34 | Train Loss: 19.1279 | Val Loss: 16.6831 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 35 | Train Loss: 18.4215 | Val Loss: 17.7311 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 36 | Train Loss: 16.1254 | Val Loss: 14.5911 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 37 | Train Loss: 19.4822 | Val Loss: 15.6748 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 38 | Train Loss: 16.6600 | Val Loss: 15.9067 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 39 | Train Loss: 17.0333 | Val Loss: 20.5692 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 40 | Train Loss: 17.3364 | Val Loss: 17.7736 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 41 | Train Loss: 15.1929 | Val Loss: 14.1847 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 42 | Train Loss: 13.5544 | Val Loss: 18.9940 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 43 | Train Loss: 15.4987 | Val Loss: 15.2996 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 44 | Train Loss: 15.6563 | Val Loss: 14.7661 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 45 | Train Loss: 13.4395 | Val Loss: 16.2325 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 46 | Train Loss: 15.9917 | Val Loss: 19.5130 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 47 | Train Loss: 15.2420 | Val Loss: 16.7737 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 48 | Train Loss: 14.3850 | Val Loss: 13.9957 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 49 | Train Loss: 13.5938 | Val Loss: 14.2229 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 50 | Train Loss: 14.1587 | Val Loss: 14.1408 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 51 | Train Loss: 13.6066 | Val Loss: 13.1592 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 52 | Train Loss: 14.2041 | Val Loss: 16.1958 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 53 | Train Loss: 13.6100 | Val Loss: 13.4701 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 54 | Train Loss: 15.2057 | Val Loss: 15.6376 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 55 | Train Loss: 13.2016 | Val Loss: 17.9020 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 56 | Train Loss: 14.4967 | Val Loss: 12.2220 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 57 | Train Loss: 14.3261 | Val Loss: 12.8695 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 58 | Train Loss: 13.6000 | Val Loss: 12.6433 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 59 | Train Loss: 15.6981 | Val Loss: 16.4739 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 60 | Train Loss: 12.6722 | Val Loss: 22.6913 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 61 | Train Loss: 16.3116 | Val Loss: 13.6211 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 62 | Train Loss: 14.3510 | Val Loss: 13.3814 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 63 | Train Loss: 13.8957 | Val Loss: 13.9953 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 64 | Train Loss: 14.6684 | Val Loss: 12.5047 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 65 | Train Loss: 14.7443 | Val Loss: 15.1428 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 66 | Train Loss: 13.8778 | Val Loss: 11.9493 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 67 | Train Loss: 13.3647 | Val Loss: 14.5508 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 68 | Train Loss: 13.8396 | Val Loss: 14.1586 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 69 | Train Loss: 12.6299 | Val Loss: 12.1680 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 70 | Train Loss: 13.3806 | Val Loss: 14.8631 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 71 | Train Loss: 11.9604 | Val Loss: 13.3413 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 72 | Train Loss: 12.3171 | Val Loss: 23.5580 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 73 | Train Loss: 13.8626 | Val Loss: 15.9258 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 74 | Train Loss: 12.9083 | Val Loss: 12.9105 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 75 | Train Loss: 13.9589 | Val Loss: 11.2613 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 76 | Train Loss: 12.7400 | Val Loss: 11.5003 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 77 | Train Loss: 11.9311 | Val Loss: 10.9732 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 78 | Train Loss: 12.8384 | Val Loss: 11.7834 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 79 | Train Loss: 10.9832 | Val Loss: 12.5416 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 80 | Train Loss: 12.5758 | Val Loss: 12.8051 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 81 | Train Loss: 13.9498 | Val Loss: 12.0071 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 82 | Train Loss: 12.1358 | Val Loss: 10.6394 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 83 | Train Loss: 10.6388 | Val Loss: 15.2782 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 84 | Train Loss: 10.8846 | Val Loss: 14.5877 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 85 | Train Loss: 11.1770 | Val Loss: 13.5716 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 86 | Train Loss: 12.7478 | Val Loss: 11.5347 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 87 | Train Loss: 13.1507 | Val Loss: 12.1310 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 88 | Train Loss: 12.0766 | Val Loss: 13.4412 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 89 | Train Loss: 11.0733 | Val Loss: 11.3343 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 90 | Train Loss: 11.4833 | Val Loss: 11.5642 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:44,110] Trial 278 finished with value: 10.63941675666871 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.25655551254389586, 'lr': 0.0006682737417009588, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 9.866546858624851e-06}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 278 | Epoch 91 | Train Loss: 11.4713 | Val Loss: 12.9471 | Optimizer: AdamW\n",
      "Trial 278 | Epoch 92 | Train Loss: 10.0462 | Val Loss: 10.8584 | Optimizer: AdamW\n",
      "Trial 278 - Early stopping triggered at epoch 92\n",
      "Trial 279 | Epoch 01 | Train Loss: 183.0325 | Val Loss: 33.2451 | Optimizer: AdamW\n",
      "Trial 279 | Epoch 02 | Train Loss: 56.3744 | Val Loss: 67.9440 | Optimizer: AdamW\n",
      "Trial 279 | Epoch 03 | Train Loss: 48.0909 | Val Loss: 32.7941 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:44,577] Trial 279 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 279 | Epoch 04 | Train Loss: 41.4067 | Val Loss: 33.1979 | Optimizer: AdamW\n",
      "Trial 279 | Epoch 05 | Train Loss: 35.3874 | Val Loss: 31.6449 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:44,748] Trial 280 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 280 | Epoch 01 | Train Loss: 159.2951 | Val Loss: 47.4913 | Optimizer: AdamW\n",
      "Trial 281 | Epoch 01 | Train Loss: 87283.5279 | Val Loss: 37.2428 | Optimizer: RMSprop\n",
      "Trial 281 | Epoch 02 | Train Loss: 79.9824 | Val Loss: 89.2322 | Optimizer: RMSprop\n",
      "Trial 281 | Epoch 03 | Train Loss: 33.3916 | Val Loss: 49.3505 | Optimizer: RMSprop\n",
      "Trial 281 | Epoch 04 | Train Loss: 28.1849 | Val Loss: 29.6340 | Optimizer: RMSprop\n",
      "Trial 281 | Epoch 05 | Train Loss: 32.5551 | Val Loss: 26.8287 | Optimizer: RMSprop\n",
      "Trial 281 | Epoch 06 | Train Loss: 32.1819 | Val Loss: 25.5329 | Optimizer: RMSprop\n",
      "Trial 281 | Epoch 07 | Train Loss: 32.4168 | Val Loss: 34.7543 | Optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:45,460] Trial 281 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 281 | Epoch 08 | Train Loss: 25.9685 | Val Loss: 27.5276 | Optimizer: RMSprop\n",
      "Trial 282 | Epoch 01 | Train Loss: 113.4037 | Val Loss: 61.5942 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:45,630] Trial 282 pruned. \n",
      "[I 2025-09-04 21:24:45,801] Trial 283 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 283 | Epoch 01 | Train Loss: 128.8907 | Val Loss: 42.9448 | Optimizer: AdamW\n",
      "Trial 284 | Epoch 01 | Train Loss: 176.3456 | Val Loss: 50.5693 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:45,970] Trial 284 pruned. \n",
      "[I 2025-09-04 21:24:46,138] Trial 285 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 285 | Epoch 01 | Train Loss: 319.6692 | Val Loss: 281.3475 | Optimizer: AdamW\n",
      "Trial 286 | Epoch 01 | Train Loss: 177.8010 | Val Loss: 45.7830 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:46,306] Trial 286 pruned. \n",
      "[I 2025-09-04 21:24:46,438] Trial 287 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 287 | Epoch 01 | Train Loss: 145.7988 | Val Loss: 64.8364 | Optimizer: AdamW\n",
      "Trial 288 | Epoch 01 | Train Loss: 160.3285 | Val Loss: 40.3077 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:46,605] Trial 288 pruned. \n",
      "[I 2025-09-04 21:24:46,778] Trial 289 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 289 | Epoch 01 | Train Loss: 150.9630 | Val Loss: 44.2221 | Optimizer: AdamW\n",
      "Trial 290 | Epoch 01 | Train Loss: 145.1953 | Val Loss: 31.3289 | Optimizer: AdamW\n",
      "Trial 290 | Epoch 02 | Train Loss: 49.2508 | Val Loss: 55.1562 | Optimizer: AdamW\n",
      "Trial 290 | Epoch 03 | Train Loss: 44.1479 | Val Loss: 38.2129 | Optimizer: AdamW\n",
      "Trial 290 | Epoch 04 | Train Loss: 43.3789 | Val Loss: 35.1814 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:47,325] Trial 290 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 290 | Epoch 05 | Train Loss: 34.9799 | Val Loss: 28.7086 | Optimizer: AdamW\n",
      "Trial 290 | Epoch 06 | Train Loss: 30.3715 | Val Loss: 28.9419 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:47,508] Trial 291 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 291 | Epoch 01 | Train Loss: 144.4795 | Val Loss: 44.9860 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 01 | Train Loss: 168.4730 | Val Loss: 34.6802 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 02 | Train Loss: 44.0492 | Val Loss: 35.3714 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 03 | Train Loss: 34.5781 | Val Loss: 28.5977 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 04 | Train Loss: 31.3910 | Val Loss: 27.6873 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 05 | Train Loss: 27.6882 | Val Loss: 24.5889 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 06 | Train Loss: 29.1728 | Val Loss: 23.4545 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 07 | Train Loss: 26.4222 | Val Loss: 28.2685 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 08 | Train Loss: 24.7602 | Val Loss: 25.4250 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 09 | Train Loss: 22.9765 | Val Loss: 21.1130 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 10 | Train Loss: 23.4405 | Val Loss: 22.2890 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 11 | Train Loss: 24.3909 | Val Loss: 26.6957 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 12 | Train Loss: 22.4176 | Val Loss: 20.3502 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 13 | Train Loss: 23.2815 | Val Loss: 21.5668 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 14 | Train Loss: 20.3076 | Val Loss: 20.0647 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 15 | Train Loss: 22.9054 | Val Loss: 20.5064 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 16 | Train Loss: 21.2504 | Val Loss: 21.3638 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 17 | Train Loss: 22.2646 | Val Loss: 20.2558 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 18 | Train Loss: 20.3107 | Val Loss: 19.6828 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 19 | Train Loss: 23.2311 | Val Loss: 19.0144 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 20 | Train Loss: 21.1777 | Val Loss: 21.0205 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 21 | Train Loss: 20.8650 | Val Loss: 22.1428 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 22 | Train Loss: 20.4959 | Val Loss: 20.6623 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 23 | Train Loss: 19.2800 | Val Loss: 17.5115 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 24 | Train Loss: 20.4775 | Val Loss: 17.5333 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 25 | Train Loss: 21.0916 | Val Loss: 17.1954 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 26 | Train Loss: 20.3284 | Val Loss: 16.8784 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 27 | Train Loss: 20.2133 | Val Loss: 18.8223 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 28 | Train Loss: 19.6197 | Val Loss: 21.5243 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 29 | Train Loss: 16.4696 | Val Loss: 20.6964 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 30 | Train Loss: 17.2179 | Val Loss: 15.5381 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 31 | Train Loss: 19.0433 | Val Loss: 15.7857 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 32 | Train Loss: 18.5324 | Val Loss: 15.3984 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 33 | Train Loss: 19.3242 | Val Loss: 15.1390 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 34 | Train Loss: 17.9253 | Val Loss: 15.0928 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 35 | Train Loss: 17.4715 | Val Loss: 23.9094 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 36 | Train Loss: 20.3695 | Val Loss: 26.3774 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 37 | Train Loss: 17.4900 | Val Loss: 20.5826 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 38 | Train Loss: 17.3133 | Val Loss: 17.7651 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 39 | Train Loss: 16.1950 | Val Loss: 15.1331 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 40 | Train Loss: 17.1998 | Val Loss: 14.8218 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 41 | Train Loss: 16.3164 | Val Loss: 15.2631 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 42 | Train Loss: 13.5748 | Val Loss: 16.2939 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 43 | Train Loss: 14.3003 | Val Loss: 15.4263 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 44 | Train Loss: 15.1551 | Val Loss: 14.9204 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 45 | Train Loss: 17.1397 | Val Loss: 14.4537 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 46 | Train Loss: 16.4449 | Val Loss: 14.4125 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 47 | Train Loss: 14.7818 | Val Loss: 15.3460 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 48 | Train Loss: 15.9828 | Val Loss: 14.3154 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 49 | Train Loss: 16.1368 | Val Loss: 14.0666 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 50 | Train Loss: 15.1219 | Val Loss: 15.5356 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 51 | Train Loss: 16.2348 | Val Loss: 20.2711 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 52 | Train Loss: 15.0942 | Val Loss: 15.0699 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 53 | Train Loss: 14.9950 | Val Loss: 14.2970 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 54 | Train Loss: 15.5381 | Val Loss: 13.8791 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 55 | Train Loss: 14.8351 | Val Loss: 17.3442 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 56 | Train Loss: 14.1160 | Val Loss: 16.0271 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 57 | Train Loss: 14.3542 | Val Loss: 14.0978 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 58 | Train Loss: 14.1089 | Val Loss: 13.7060 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 59 | Train Loss: 13.5964 | Val Loss: 15.5430 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 60 | Train Loss: 14.9092 | Val Loss: 12.4432 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 61 | Train Loss: 13.3259 | Val Loss: 13.2543 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 62 | Train Loss: 12.5024 | Val Loss: 14.2309 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 63 | Train Loss: 10.9688 | Val Loss: 15.5641 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 64 | Train Loss: 12.9015 | Val Loss: 15.4169 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 65 | Train Loss: 13.0521 | Val Loss: 13.1836 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 66 | Train Loss: 13.0797 | Val Loss: 12.8788 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 67 | Train Loss: 13.8108 | Val Loss: 12.8002 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 68 | Train Loss: 13.7416 | Val Loss: 16.0547 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 69 | Train Loss: 15.7909 | Val Loss: 13.1173 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 70 | Train Loss: 14.6476 | Val Loss: 12.3136 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 71 | Train Loss: 13.8323 | Val Loss: 13.3768 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 72 | Train Loss: 13.3166 | Val Loss: 14.2285 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 73 | Train Loss: 13.3792 | Val Loss: 13.6506 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 74 | Train Loss: 11.4982 | Val Loss: 15.4054 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 75 | Train Loss: 11.6948 | Val Loss: 13.4677 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 76 | Train Loss: 12.5655 | Val Loss: 13.0660 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 77 | Train Loss: 11.3674 | Val Loss: 14.5455 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 78 | Train Loss: 11.6131 | Val Loss: 13.3623 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 79 | Train Loss: 11.4901 | Val Loss: 14.9988 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:53,890] Trial 292 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 292 | Epoch 80 | Train Loss: 12.0632 | Val Loss: 12.1800 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 81 | Train Loss: 11.0467 | Val Loss: 12.0285 | Optimizer: AdamW\n",
      "Trial 292 | Epoch 82 | Train Loss: 11.3974 | Val Loss: 12.9570 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:24:54,058] Trial 293 pruned. \n",
      "[I 2025-09-04 21:24:54,185] Trial 294 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 293 | Epoch 01 | Train Loss: 149.7829 | Val Loss: 39.0266 | Optimizer: AdamW\n",
      "Trial 294 | Epoch 01 | Train Loss: 205.9742 | Val Loss: 62.5776 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 01 | Train Loss: 140.1472 | Val Loss: 32.0943 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 02 | Train Loss: 45.0707 | Val Loss: 38.5623 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 03 | Train Loss: 38.6825 | Val Loss: 33.0452 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 04 | Train Loss: 35.8863 | Val Loss: 27.5299 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 05 | Train Loss: 31.3658 | Val Loss: 27.8751 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 06 | Train Loss: 25.5648 | Val Loss: 23.2418 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 07 | Train Loss: 24.6380 | Val Loss: 21.7865 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 08 | Train Loss: 23.7057 | Val Loss: 21.7001 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 09 | Train Loss: 25.6915 | Val Loss: 20.7047 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 10 | Train Loss: 24.0766 | Val Loss: 25.5622 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 11 | Train Loss: 25.7634 | Val Loss: 20.5916 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 12 | Train Loss: 23.7921 | Val Loss: 20.7149 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 13 | Train Loss: 23.6264 | Val Loss: 19.7603 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 14 | Train Loss: 20.8671 | Val Loss: 22.2709 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 15 | Train Loss: 22.8871 | Val Loss: 23.1521 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 16 | Train Loss: 21.9436 | Val Loss: 20.6411 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 17 | Train Loss: 21.1570 | Val Loss: 18.8442 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 18 | Train Loss: 23.7285 | Val Loss: 20.3936 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 19 | Train Loss: 20.4712 | Val Loss: 17.7855 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 20 | Train Loss: 20.4898 | Val Loss: 17.6008 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 21 | Train Loss: 19.1719 | Val Loss: 21.9051 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 22 | Train Loss: 19.6068 | Val Loss: 18.8226 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 23 | Train Loss: 18.1996 | Val Loss: 23.9350 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 24 | Train Loss: 20.5336 | Val Loss: 15.2013 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 25 | Train Loss: 17.3298 | Val Loss: 18.9125 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 26 | Train Loss: 17.1772 | Val Loss: 20.6605 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 27 | Train Loss: 17.1838 | Val Loss: 15.5478 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 28 | Train Loss: 17.2547 | Val Loss: 14.2686 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 29 | Train Loss: 17.0772 | Val Loss: 22.5540 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 30 | Train Loss: 21.9388 | Val Loss: 16.6466 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 31 | Train Loss: 18.9917 | Val Loss: 15.6594 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 32 | Train Loss: 15.6299 | Val Loss: 14.9510 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 33 | Train Loss: 18.1323 | Val Loss: 22.3313 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 34 | Train Loss: 18.3467 | Val Loss: 15.1350 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 35 | Train Loss: 16.5035 | Val Loss: 13.6010 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 36 | Train Loss: 17.0810 | Val Loss: 13.4536 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 37 | Train Loss: 15.2118 | Val Loss: 13.9662 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 38 | Train Loss: 18.7687 | Val Loss: 17.8031 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 39 | Train Loss: 16.9703 | Val Loss: 18.8322 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 40 | Train Loss: 19.2574 | Val Loss: 14.7643 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 41 | Train Loss: 16.4341 | Val Loss: 18.6400 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 42 | Train Loss: 16.7141 | Val Loss: 23.3790 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 43 | Train Loss: 17.5172 | Val Loss: 17.3079 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 44 | Train Loss: 17.1261 | Val Loss: 13.9865 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 45 | Train Loss: 17.6603 | Val Loss: 13.5198 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 46 | Train Loss: 14.1052 | Val Loss: 13.4167 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 47 | Train Loss: 15.6159 | Val Loss: 14.8612 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 48 | Train Loss: 14.6915 | Val Loss: 13.1465 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 49 | Train Loss: 14.0983 | Val Loss: 14.3201 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 50 | Train Loss: 14.6717 | Val Loss: 14.8630 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 51 | Train Loss: 14.9373 | Val Loss: 13.7844 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 52 | Train Loss: 14.3458 | Val Loss: 13.0403 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 53 | Train Loss: 13.5839 | Val Loss: 12.4577 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 54 | Train Loss: 14.5272 | Val Loss: 11.3294 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 55 | Train Loss: 14.4436 | Val Loss: 13.5434 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 56 | Train Loss: 14.7647 | Val Loss: 12.4212 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 57 | Train Loss: 15.4373 | Val Loss: 12.2033 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 58 | Train Loss: 14.1534 | Val Loss: 11.6535 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 59 | Train Loss: 16.6841 | Val Loss: 11.6296 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 60 | Train Loss: 15.2211 | Val Loss: 13.9812 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 61 | Train Loss: 11.7561 | Val Loss: 12.4005 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 62 | Train Loss: 14.1348 | Val Loss: 10.8129 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 63 | Train Loss: 14.3732 | Val Loss: 12.3333 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 64 | Train Loss: 12.2963 | Val Loss: 11.6377 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 65 | Train Loss: 11.6636 | Val Loss: 11.9234 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 66 | Train Loss: 12.5238 | Val Loss: 12.8741 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 67 | Train Loss: 13.2181 | Val Loss: 13.0128 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 68 | Train Loss: 13.5670 | Val Loss: 12.3730 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 69 | Train Loss: 12.2961 | Val Loss: 10.7506 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 70 | Train Loss: 10.9515 | Val Loss: 10.8886 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 71 | Train Loss: 10.8698 | Val Loss: 11.3411 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 72 | Train Loss: 11.1784 | Val Loss: 10.4239 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 73 | Train Loss: 12.2918 | Val Loss: 10.6481 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 74 | Train Loss: 12.8705 | Val Loss: 10.8113 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 75 | Train Loss: 11.8793 | Val Loss: 10.1334 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 76 | Train Loss: 12.3897 | Val Loss: 11.8381 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 77 | Train Loss: 9.9279 | Val Loss: 13.0541 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 78 | Train Loss: 11.1070 | Val Loss: 11.6041 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 79 | Train Loss: 11.8458 | Val Loss: 11.5673 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 80 | Train Loss: 11.9638 | Val Loss: 12.3768 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 81 | Train Loss: 10.7223 | Val Loss: 12.7723 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 82 | Train Loss: 10.6914 | Val Loss: 11.5636 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 83 | Train Loss: 11.1684 | Val Loss: 10.5975 | Optimizer: AdamW\n",
      "Trial 295 | Epoch 84 | Train Loss: 8.9083 | Val Loss: 11.2886 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:00,853] Trial 295 finished with value: 10.133446631392813 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.26376792880323235, 'lr': 0.000651568224844297, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 1.2896784380230221e-05}. Best is trial 272 with value: 8.336457074173097.\n",
      "[I 2025-09-04 21:25:01,000] Trial 296 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 295 | Epoch 85 | Train Loss: 10.2500 | Val Loss: 12.8204 | Optimizer: AdamW\n",
      "Trial 295 - Early stopping triggered at epoch 85\n",
      "Trial 296 | Epoch 01 | NaN loss detected so pruning trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:01,169] Trial 297 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 297 | Epoch 01 | Train Loss: 149.1128 | Val Loss: 40.0359 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 01 | Train Loss: 185.5313 | Val Loss: 33.5315 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 02 | Train Loss: 37.6169 | Val Loss: 31.3578 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 03 | Train Loss: 32.4661 | Val Loss: 31.9172 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 04 | Train Loss: 32.8784 | Val Loss: 26.4357 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 05 | Train Loss: 28.3186 | Val Loss: 35.2636 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 06 | Train Loss: 27.9150 | Val Loss: 24.3140 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 07 | Train Loss: 28.1434 | Val Loss: 23.4144 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 08 | Train Loss: 27.1272 | Val Loss: 23.7558 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 09 | Train Loss: 25.0318 | Val Loss: 22.8190 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 10 | Train Loss: 24.5758 | Val Loss: 27.3558 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 11 | Train Loss: 26.3839 | Val Loss: 21.3531 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 12 | Train Loss: 24.6350 | Val Loss: 21.3399 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 13 | Train Loss: 23.5653 | Val Loss: 21.6043 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 14 | Train Loss: 22.8314 | Val Loss: 21.3508 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 15 | Train Loss: 22.7581 | Val Loss: 19.8076 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 16 | Train Loss: 23.1460 | Val Loss: 19.5150 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 17 | Train Loss: 23.2151 | Val Loss: 20.4959 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 18 | Train Loss: 22.0137 | Val Loss: 21.0718 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 19 | Train Loss: 21.7363 | Val Loss: 18.1561 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 20 | Train Loss: 21.8185 | Val Loss: 18.4656 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 21 | Train Loss: 21.4256 | Val Loss: 27.7413 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 22 | Train Loss: 20.7020 | Val Loss: 20.4693 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 23 | Train Loss: 19.7458 | Val Loss: 18.8065 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 24 | Train Loss: 18.7961 | Val Loss: 20.0108 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 25 | Train Loss: 19.5277 | Val Loss: 28.9762 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 26 | Train Loss: 20.7790 | Val Loss: 20.9750 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 27 | Train Loss: 20.1530 | Val Loss: 16.2298 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 28 | Train Loss: 18.5102 | Val Loss: 16.4698 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 29 | Train Loss: 18.6480 | Val Loss: 15.4899 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 30 | Train Loss: 17.9729 | Val Loss: 16.0981 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 31 | Train Loss: 20.6416 | Val Loss: 16.3210 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 32 | Train Loss: 17.1325 | Val Loss: 16.0150 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 33 | Train Loss: 17.4769 | Val Loss: 15.8602 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 34 | Train Loss: 16.7335 | Val Loss: 16.1927 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 35 | Train Loss: 15.2033 | Val Loss: 17.4238 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 36 | Train Loss: 15.1039 | Val Loss: 14.4889 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 37 | Train Loss: 17.0787 | Val Loss: 16.1469 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 38 | Train Loss: 16.6697 | Val Loss: 15.0508 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 39 | Train Loss: 16.7028 | Val Loss: 14.5677 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 40 | Train Loss: 15.4791 | Val Loss: 16.7904 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 41 | Train Loss: 15.0558 | Val Loss: 14.0444 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 42 | Train Loss: 14.5224 | Val Loss: 14.9898 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 43 | Train Loss: 15.9609 | Val Loss: 14.3049 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 44 | Train Loss: 15.9363 | Val Loss: 14.2820 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 45 | Train Loss: 15.6675 | Val Loss: 19.1481 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 46 | Train Loss: 16.5115 | Val Loss: 18.1557 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 47 | Train Loss: 16.6557 | Val Loss: 18.9171 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 48 | Train Loss: 16.5376 | Val Loss: 20.8698 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 49 | Train Loss: 16.7985 | Val Loss: 14.2888 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:05,146] Trial 298 finished with value: 14.044357284297789 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.26672149928890154, 'lr': 0.0006566251260670098, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 1.2333161038689704e-05}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 298 | Epoch 50 | Train Loss: 16.4402 | Val Loss: 15.1934 | Optimizer: AdamW\n",
      "Trial 298 | Epoch 51 | Train Loss: 14.8629 | Val Loss: 15.1651 | Optimizer: AdamW\n",
      "Trial 298 - Early stopping triggered at epoch 51\n",
      "Trial 299 | Epoch 01 | Train Loss: 146.6136 | Val Loss: 37.6802 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 02 | Train Loss: 45.1556 | Val Loss: 35.3240 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 03 | Train Loss: 39.3691 | Val Loss: 35.9729 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 04 | Train Loss: 36.2986 | Val Loss: 27.6026 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 05 | Train Loss: 29.5203 | Val Loss: 28.9550 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 06 | Train Loss: 25.9094 | Val Loss: 24.1076 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 07 | Train Loss: 25.1097 | Val Loss: 22.4236 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 08 | Train Loss: 22.5082 | Val Loss: 24.3409 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 09 | Train Loss: 24.8812 | Val Loss: 21.5508 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 10 | Train Loss: 23.0976 | Val Loss: 20.9830 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 11 | Train Loss: 22.8624 | Val Loss: 20.9609 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 12 | Train Loss: 25.5272 | Val Loss: 23.4627 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 13 | Train Loss: 23.1070 | Val Loss: 20.5032 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 14 | Train Loss: 22.3487 | Val Loss: 20.5551 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 15 | Train Loss: 21.0246 | Val Loss: 18.5211 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 16 | Train Loss: 22.6124 | Val Loss: 18.8191 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 17 | Train Loss: 19.5975 | Val Loss: 19.3651 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 18 | Train Loss: 21.1644 | Val Loss: 15.9839 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 19 | Train Loss: 19.9824 | Val Loss: 16.2718 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 20 | Train Loss: 23.6437 | Val Loss: 21.1227 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 21 | Train Loss: 27.0444 | Val Loss: 50.6093 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 22 | Train Loss: 34.0599 | Val Loss: 36.2399 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 23 | Train Loss: 26.2553 | Val Loss: 28.6135 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 24 | Train Loss: 23.3141 | Val Loss: 20.2242 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 25 | Train Loss: 22.1087 | Val Loss: 21.5129 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 26 | Train Loss: 22.4675 | Val Loss: 22.1700 | Optimizer: AdamW\n",
      "Trial 299 | Epoch 27 | Train Loss: 19.6423 | Val Loss: 20.0343 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:07,399] Trial 299 finished with value: 15.983942737424277 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.2618979494451215, 'lr': 0.000682469968481782, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 1.322437152747346e-05}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 299 | Epoch 28 | Train Loss: 19.0514 | Val Loss: 22.1404 | Optimizer: AdamW\n",
      "Trial 299 - Early stopping triggered at epoch 28\n",
      "Trial 300 | Epoch 01 | Train Loss: 191.7794 | Val Loss: 55.0853 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:07,567] Trial 300 pruned. \n",
      "[I 2025-09-04 21:25:07,739] Trial 301 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 301 | Epoch 01 | Train Loss: 140.4925 | Val Loss: 40.2773 | Optimizer: AdamW\n",
      "Trial 302 | Epoch 01 | Train Loss: 180.3061 | Val Loss: 33.1668 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:08,130] Trial 302 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 302 | Epoch 02 | Train Loss: 54.9441 | Val Loss: 68.4512 | Optimizer: AdamW\n",
      "Trial 302 | Epoch 03 | Train Loss: 46.5272 | Val Loss: 34.1978 | Optimizer: AdamW\n",
      "Trial 302 | Epoch 04 | Train Loss: 42.8590 | Val Loss: 34.1080 | Optimizer: AdamW\n",
      "Trial 303 | Epoch 01 | Train Loss: 189.2302 | Val Loss: 36.1797 | Optimizer: AdamW\n",
      "Trial 303 | Epoch 02 | Train Loss: 69.3667 | Val Loss: 91.7429 | Optimizer: AdamW\n",
      "Trial 303 | Epoch 03 | Train Loss: 64.1621 | Val Loss: 44.4008 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:08,530] Trial 303 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 303 | Epoch 04 | Train Loss: 43.5967 | Val Loss: 37.3995 | Optimizer: AdamW\n",
      "Trial 304 | Epoch 01 | Train Loss: 126.5068 | Val Loss: 58.3934 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:08,696] Trial 304 pruned. \n",
      "[I 2025-09-04 21:25:08,889] Trial 305 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 305 | Epoch 01 | Train Loss: 139.9899 | Val Loss: 44.6260 | Optimizer: AdamW\n",
      "Trial 306 | Epoch 01 | Train Loss: 152.6131 | Val Loss: 35.9514 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:09,280] Trial 306 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 306 | Epoch 02 | Train Loss: 52.1314 | Val Loss: 65.7783 | Optimizer: AdamW\n",
      "Trial 306 | Epoch 03 | Train Loss: 50.3088 | Val Loss: 38.5241 | Optimizer: AdamW\n",
      "Trial 306 | Epoch 04 | Train Loss: 45.9265 | Val Loss: 35.2049 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:09,443] Trial 307 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 307 | Epoch 01 | Train Loss: 6707013.3550 | Val Loss: 219658.1280 | Optimizer: RMSprop\n",
      "Trial 308 | Epoch 01 | Train Loss: 156.8318 | Val Loss: 32.3485 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 02 | Train Loss: 39.2278 | Val Loss: 32.4038 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 03 | Train Loss: 37.7383 | Val Loss: 41.4020 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 04 | Train Loss: 40.1426 | Val Loss: 27.4043 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 05 | Train Loss: 30.7736 | Val Loss: 30.5064 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 06 | Train Loss: 29.5971 | Val Loss: 23.5538 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 07 | Train Loss: 25.5091 | Val Loss: 29.2392 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 08 | Train Loss: 24.5521 | Val Loss: 21.6902 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 09 | Train Loss: 22.9823 | Val Loss: 21.8759 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 10 | Train Loss: 24.0961 | Val Loss: 24.4961 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 11 | Train Loss: 23.2288 | Val Loss: 23.4899 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 12 | Train Loss: 22.3084 | Val Loss: 20.8249 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 13 | Train Loss: 21.4427 | Val Loss: 24.1297 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 14 | Train Loss: 23.2183 | Val Loss: 20.7849 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 15 | Train Loss: 22.9229 | Val Loss: 19.6089 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 16 | Train Loss: 21.1310 | Val Loss: 19.2139 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 17 | Train Loss: 20.0796 | Val Loss: 20.4637 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 18 | Train Loss: 19.7346 | Val Loss: 21.2475 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 19 | Train Loss: 20.2780 | Val Loss: 20.7519 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 20 | Train Loss: 20.1499 | Val Loss: 22.0394 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 21 | Train Loss: 20.6062 | Val Loss: 22.5227 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 22 | Train Loss: 21.6475 | Val Loss: 20.3637 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 23 | Train Loss: 17.0253 | Val Loss: 22.1696 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 24 | Train Loss: 19.9219 | Val Loss: 21.1366 | Optimizer: AdamW\n",
      "Trial 308 | Epoch 25 | Train Loss: 20.3984 | Val Loss: 22.3367 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:11,515] Trial 308 finished with value: 19.213915065052063 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.2551677674660391, 'lr': 0.0007116512787633273, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 7.690613374340105e-06}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 308 | Epoch 26 | Train Loss: 20.0306 | Val Loss: 21.6478 | Optimizer: AdamW\n",
      "Trial 308 - Early stopping triggered at epoch 26\n",
      "Trial 309 | Epoch 01 | Train Loss: 187.4294 | Val Loss: 31.8234 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 02 | Train Loss: 56.0749 | Val Loss: 60.9086 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 03 | Train Loss: 42.6082 | Val Loss: 33.5956 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 04 | Train Loss: 41.6817 | Val Loss: 35.4931 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 05 | Train Loss: 34.7889 | Val Loss: 27.2668 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 06 | Train Loss: 31.6111 | Val Loss: 26.1783 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 07 | Train Loss: 27.9725 | Val Loss: 23.5452 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 08 | Train Loss: 26.5930 | Val Loss: 30.2686 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 09 | Train Loss: 25.7553 | Val Loss: 22.1942 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 10 | Train Loss: 25.9715 | Val Loss: 26.6058 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 11 | Train Loss: 25.6545 | Val Loss: 21.2102 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 12 | Train Loss: 24.8766 | Val Loss: 23.3258 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 13 | Train Loss: 23.6474 | Val Loss: 20.9570 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 14 | Train Loss: 25.0894 | Val Loss: 22.7672 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 15 | Train Loss: 23.0632 | Val Loss: 20.6515 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 16 | Train Loss: 23.3945 | Val Loss: 22.0627 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 17 | Train Loss: 23.8166 | Val Loss: 23.7040 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 18 | Train Loss: 22.1878 | Val Loss: 20.1595 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 19 | Train Loss: 22.2865 | Val Loss: 21.5460 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 20 | Train Loss: 24.0155 | Val Loss: 19.5491 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 21 | Train Loss: 23.6036 | Val Loss: 26.2577 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 22 | Train Loss: 22.6748 | Val Loss: 18.9756 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 23 | Train Loss: 21.1583 | Val Loss: 19.1414 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 24 | Train Loss: 21.2571 | Val Loss: 17.1919 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 25 | Train Loss: 18.9745 | Val Loss: 18.7848 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 26 | Train Loss: 21.4684 | Val Loss: 18.8312 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 27 | Train Loss: 21.2453 | Val Loss: 27.9022 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 28 | Train Loss: 23.1980 | Val Loss: 25.3433 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 29 | Train Loss: 22.6806 | Val Loss: 18.4098 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 30 | Train Loss: 21.0293 | Val Loss: 19.8241 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 31 | Train Loss: 20.7028 | Val Loss: 17.1955 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 32 | Train Loss: 19.7505 | Val Loss: 19.6519 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 33 | Train Loss: 18.9054 | Val Loss: 18.0140 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 34 | Train Loss: 19.4443 | Val Loss: 15.6461 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 35 | Train Loss: 17.1803 | Val Loss: 15.8282 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 36 | Train Loss: 16.5496 | Val Loss: 17.1998 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 37 | Train Loss: 18.5683 | Val Loss: 14.6750 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 38 | Train Loss: 17.2349 | Val Loss: 15.0042 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 39 | Train Loss: 17.0036 | Val Loss: 14.7604 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 40 | Train Loss: 17.1472 | Val Loss: 13.9807 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 41 | Train Loss: 16.1247 | Val Loss: 14.3164 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 42 | Train Loss: 17.1445 | Val Loss: 15.8385 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 43 | Train Loss: 15.2012 | Val Loss: 14.3234 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 44 | Train Loss: 18.2100 | Val Loss: 16.8195 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 45 | Train Loss: 19.0828 | Val Loss: 16.6947 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 46 | Train Loss: 17.1512 | Val Loss: 16.3535 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 47 | Train Loss: 15.8068 | Val Loss: 22.9692 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 48 | Train Loss: 19.3062 | Val Loss: 17.0908 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 49 | Train Loss: 15.6307 | Val Loss: 13.9604 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 50 | Train Loss: 16.5053 | Val Loss: 14.2078 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 51 | Train Loss: 14.7361 | Val Loss: 14.6247 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 52 | Train Loss: 16.4972 | Val Loss: 13.8008 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 53 | Train Loss: 13.9344 | Val Loss: 13.7642 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 54 | Train Loss: 15.3371 | Val Loss: 14.0742 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 55 | Train Loss: 14.7194 | Val Loss: 15.6935 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 56 | Train Loss: 15.6957 | Val Loss: 18.8531 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 57 | Train Loss: 16.1895 | Val Loss: 15.2506 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 58 | Train Loss: 13.2544 | Val Loss: 13.1889 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 59 | Train Loss: 14.5307 | Val Loss: 12.7079 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 60 | Train Loss: 15.4128 | Val Loss: 13.6993 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 61 | Train Loss: 15.3316 | Val Loss: 12.9664 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 62 | Train Loss: 15.1061 | Val Loss: 13.8305 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 63 | Train Loss: 18.0281 | Val Loss: 13.2994 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 64 | Train Loss: 17.4757 | Val Loss: 12.5183 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 65 | Train Loss: 19.3916 | Val Loss: 13.6532 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 66 | Train Loss: 20.0670 | Val Loss: 14.2284 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 67 | Train Loss: 17.0408 | Val Loss: 16.5128 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 68 | Train Loss: 16.6626 | Val Loss: 16.5220 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 69 | Train Loss: 16.8675 | Val Loss: 18.6927 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 70 | Train Loss: 17.0359 | Val Loss: 15.0863 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 71 | Train Loss: 13.5508 | Val Loss: 13.1708 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 72 | Train Loss: 13.8179 | Val Loss: 13.9453 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 73 | Train Loss: 12.7288 | Val Loss: 13.3447 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 74 | Train Loss: 13.9703 | Val Loss: 11.9131 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 75 | Train Loss: 10.6507 | Val Loss: 11.4327 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 76 | Train Loss: 12.9804 | Val Loss: 11.6937 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 77 | Train Loss: 12.3025 | Val Loss: 13.1190 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 78 | Train Loss: 11.5199 | Val Loss: 11.9463 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 79 | Train Loss: 12.9741 | Val Loss: 11.5334 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 80 | Train Loss: 12.1554 | Val Loss: 11.2973 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 81 | Train Loss: 12.7008 | Val Loss: 10.9684 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 82 | Train Loss: 11.9265 | Val Loss: 10.3167 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 83 | Train Loss: 12.9292 | Val Loss: 10.0536 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 84 | Train Loss: 12.9520 | Val Loss: 11.0082 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 85 | Train Loss: 13.5941 | Val Loss: 10.9994 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 86 | Train Loss: 13.1127 | Val Loss: 12.7476 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 87 | Train Loss: 13.0618 | Val Loss: 19.3983 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 88 | Train Loss: 15.1996 | Val Loss: 11.7278 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 89 | Train Loss: 13.5382 | Val Loss: 10.4198 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 90 | Train Loss: 12.5453 | Val Loss: 13.5171 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 91 | Train Loss: 11.6780 | Val Loss: 11.1334 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:18,720] Trial 309 finished with value: 10.053647929090795 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.2751109485606933, 'lr': 0.0006221742482883665, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 1.5395768087303467e-05}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 309 | Epoch 92 | Train Loss: 11.5500 | Val Loss: 10.4068 | Optimizer: AdamW\n",
      "Trial 309 | Epoch 93 | Train Loss: 10.9129 | Val Loss: 10.5870 | Optimizer: AdamW\n",
      "Trial 309 - Early stopping triggered at epoch 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:18,889] Trial 310 pruned. \n",
      "[I 2025-09-04 21:25:19,018] Trial 311 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 310 | Epoch 01 | Train Loss: 185.6839 | Val Loss: 114.9081 | Optimizer: AdamW\n",
      "Trial 311 | Epoch 01 | Train Loss: 146.8097 | Val Loss: 40.6895 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:19,186] Trial 312 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 312 | Epoch 01 | Train Loss: 175.6736 | Val Loss: 198.1621 | Optimizer: AdamW\n",
      "Trial 313 | Epoch 01 | Train Loss: 149.4294 | Val Loss: 49.0006 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:19,354] Trial 313 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 314 | Epoch 01 | Train Loss: 173.5893 | Val Loss: 36.4650 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 02 | Train Loss: 51.4517 | Val Loss: 52.1702 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 03 | Train Loss: 44.3770 | Val Loss: 29.0342 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 04 | Train Loss: 36.8408 | Val Loss: 34.7631 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 05 | Train Loss: 32.0504 | Val Loss: 25.7318 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 06 | Train Loss: 32.0804 | Val Loss: 31.1883 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 07 | Train Loss: 28.2380 | Val Loss: 23.5616 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 08 | Train Loss: 27.4445 | Val Loss: 25.0104 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 09 | Train Loss: 27.6814 | Val Loss: 22.2525 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 10 | Train Loss: 29.7822 | Val Loss: 22.2316 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 11 | Train Loss: 26.7558 | Val Loss: 30.5038 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 12 | Train Loss: 28.8712 | Val Loss: 21.1091 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 13 | Train Loss: 26.8670 | Val Loss: 22.0471 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 14 | Train Loss: 25.7952 | Val Loss: 22.7961 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 15 | Train Loss: 23.5614 | Val Loss: 21.3913 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 16 | Train Loss: 23.8598 | Val Loss: 21.0381 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 17 | Train Loss: 23.4499 | Val Loss: 22.4244 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 18 | Train Loss: 22.6268 | Val Loss: 25.2393 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 19 | Train Loss: 24.1528 | Val Loss: 20.1018 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 20 | Train Loss: 22.0062 | Val Loss: 19.7326 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 21 | Train Loss: 22.6469 | Val Loss: 21.5690 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 22 | Train Loss: 22.9859 | Val Loss: 21.4042 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 23 | Train Loss: 21.7041 | Val Loss: 18.4811 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 24 | Train Loss: 21.2632 | Val Loss: 18.9183 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 25 | Train Loss: 22.5495 | Val Loss: 18.7439 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 26 | Train Loss: 23.5558 | Val Loss: 21.9971 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 27 | Train Loss: 21.1762 | Val Loss: 26.0227 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 28 | Train Loss: 21.0864 | Val Loss: 20.2862 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 29 | Train Loss: 23.5268 | Val Loss: 17.1652 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 30 | Train Loss: 20.6958 | Val Loss: 16.4993 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 31 | Train Loss: 21.6022 | Val Loss: 16.5975 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 32 | Train Loss: 19.2395 | Val Loss: 17.5530 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 33 | Train Loss: 17.8716 | Val Loss: 17.2557 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 34 | Train Loss: 21.0186 | Val Loss: 19.4069 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 35 | Train Loss: 17.9210 | Val Loss: 26.5292 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 36 | Train Loss: 19.5183 | Val Loss: 21.5715 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 37 | Train Loss: 18.0097 | Val Loss: 16.5027 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 38 | Train Loss: 17.8466 | Val Loss: 14.8417 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 39 | Train Loss: 17.4216 | Val Loss: 14.2570 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 40 | Train Loss: 17.6260 | Val Loss: 15.6040 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 41 | Train Loss: 18.2077 | Val Loss: 15.9725 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 42 | Train Loss: 16.8217 | Val Loss: 17.1802 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 43 | Train Loss: 16.8474 | Val Loss: 18.0689 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 44 | Train Loss: 18.1142 | Val Loss: 17.4289 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 45 | Train Loss: 15.3218 | Val Loss: 13.5864 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 46 | Train Loss: 16.6776 | Val Loss: 14.1697 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 47 | Train Loss: 15.5919 | Val Loss: 14.9140 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 48 | Train Loss: 16.8157 | Val Loss: 14.6448 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 49 | Train Loss: 15.3859 | Val Loss: 14.6130 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 50 | Train Loss: 16.4775 | Val Loss: 14.0361 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 51 | Train Loss: 15.2097 | Val Loss: 13.4117 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 52 | Train Loss: 13.1774 | Val Loss: 15.6947 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 53 | Train Loss: 16.4798 | Val Loss: 17.9592 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 54 | Train Loss: 16.7879 | Val Loss: 13.6937 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 55 | Train Loss: 14.6887 | Val Loss: 13.6620 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 56 | Train Loss: 14.6219 | Val Loss: 14.6175 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 57 | Train Loss: 14.8688 | Val Loss: 14.4421 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 58 | Train Loss: 15.6002 | Val Loss: 13.1533 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 59 | Train Loss: 15.8750 | Val Loss: 13.6149 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 60 | Train Loss: 14.7562 | Val Loss: 12.9418 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 61 | Train Loss: 14.0624 | Val Loss: 13.9626 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 62 | Train Loss: 16.1831 | Val Loss: 13.1107 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 63 | Train Loss: 16.0021 | Val Loss: 13.7230 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 64 | Train Loss: 14.7777 | Val Loss: 13.5684 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 65 | Train Loss: 13.8511 | Val Loss: 12.1159 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 66 | Train Loss: 14.4340 | Val Loss: 13.3924 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 67 | Train Loss: 15.3318 | Val Loss: 14.1447 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 68 | Train Loss: 12.8411 | Val Loss: 12.6551 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 69 | Train Loss: 14.1109 | Val Loss: 12.1843 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 70 | Train Loss: 15.7924 | Val Loss: 11.8126 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 71 | Train Loss: 14.7226 | Val Loss: 12.7662 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 72 | Train Loss: 14.6663 | Val Loss: 14.1487 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 73 | Train Loss: 14.3128 | Val Loss: 11.5371 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 74 | Train Loss: 13.1866 | Val Loss: 12.9467 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 75 | Train Loss: 14.2309 | Val Loss: 11.2823 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 76 | Train Loss: 12.8320 | Val Loss: 12.7691 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 77 | Train Loss: 11.6428 | Val Loss: 11.4639 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 78 | Train Loss: 11.3550 | Val Loss: 9.9955 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 79 | Train Loss: 11.3195 | Val Loss: 9.9558 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 80 | Train Loss: 11.7991 | Val Loss: 9.8296 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 81 | Train Loss: 11.4047 | Val Loss: 11.2811 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 82 | Train Loss: 12.0810 | Val Loss: 10.0526 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 83 | Train Loss: 13.2960 | Val Loss: 9.9824 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 84 | Train Loss: 12.0666 | Val Loss: 9.4795 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 85 | Train Loss: 9.5714 | Val Loss: 9.6186 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 86 | Train Loss: 11.6265 | Val Loss: 9.9980 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 87 | Train Loss: 13.0820 | Val Loss: 9.8908 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 88 | Train Loss: 11.9177 | Val Loss: 9.7852 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 89 | Train Loss: 11.4804 | Val Loss: 11.0001 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 90 | Train Loss: 13.3391 | Val Loss: 10.9785 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 91 | Train Loss: 13.3312 | Val Loss: 11.0479 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 92 | Train Loss: 12.5100 | Val Loss: 10.6880 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 93 | Train Loss: 11.1334 | Val Loss: 11.9161 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 94 | Train Loss: 10.9028 | Val Loss: 8.9797 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 95 | Train Loss: 11.1480 | Val Loss: 9.6385 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 96 | Train Loss: 11.1658 | Val Loss: 9.9330 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 97 | Train Loss: 11.1986 | Val Loss: 10.7264 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 98 | Train Loss: 10.6517 | Val Loss: 11.7083 | Optimizer: AdamW\n",
      "Trial 314 | Epoch 99 | Train Loss: 12.8762 | Val Loss: 10.4556 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:27,037] Trial 314 finished with value: 8.979677227454458 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3037109072828516, 'lr': 0.0006250957284443225, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 1.3411681277405122e-05}. Best is trial 272 with value: 8.336457074173097.\n",
      "[I 2025-09-04 21:25:27,211] Trial 315 pruned. \n",
      "[I 2025-09-04 21:25:27,377] Trial 316 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 315 | Epoch 01 | Train Loss: 270.1016 | Val Loss: 248.9079 | Optimizer: AdamW\n",
      "Trial 316 | Epoch 01 | Train Loss: 191.7147 | Val Loss: 56.6891 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:27,502] Trial 317 pruned. \n",
      "[I 2025-09-04 21:25:27,674] Trial 318 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 317 | Epoch 01 | Train Loss: 149.1407 | Val Loss: 52.0247 | Optimizer: AdamW\n",
      "Trial 318 | Epoch 01 | Train Loss: 186.8084 | Val Loss: 124.0017 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:27,832] Trial 319 pruned. \n",
      "[I 2025-09-04 21:25:28,000] Trial 320 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 319 | Epoch 01 | NaN loss detected so pruning trial\n",
      "Trial 320 | Epoch 01 | Train Loss: 188.1151 | Val Loss: 67.1916 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:28,171] Trial 321 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 321 | Epoch 01 | Train Loss: 159.5656 | Val Loss: 53.3258 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 01 | Train Loss: 132.1877 | Val Loss: 34.6388 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 02 | Train Loss: 42.0804 | Val Loss: 34.5336 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 03 | Train Loss: 43.3129 | Val Loss: 41.1044 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 04 | Train Loss: 36.6511 | Val Loss: 27.4503 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 05 | Train Loss: 34.2837 | Val Loss: 27.3680 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 06 | Train Loss: 29.3001 | Val Loss: 22.8934 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 07 | Train Loss: 24.5608 | Val Loss: 27.9449 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 08 | Train Loss: 25.7168 | Val Loss: 22.9187 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 09 | Train Loss: 23.0948 | Val Loss: 21.8867 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 10 | Train Loss: 22.6360 | Val Loss: 25.0770 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 11 | Train Loss: 26.3560 | Val Loss: 22.5992 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 12 | Train Loss: 23.8739 | Val Loss: 22.1718 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 13 | Train Loss: 23.7281 | Val Loss: 20.4868 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 14 | Train Loss: 22.6805 | Val Loss: 19.9839 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 15 | Train Loss: 26.0963 | Val Loss: 21.6630 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 16 | Train Loss: 24.6462 | Val Loss: 22.0224 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 17 | Train Loss: 22.9327 | Val Loss: 21.5592 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 18 | Train Loss: 23.6311 | Val Loss: 20.2886 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 19 | Train Loss: 22.8687 | Val Loss: 26.0557 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 20 | Train Loss: 23.9658 | Val Loss: 26.3584 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 21 | Train Loss: 29.6830 | Val Loss: 29.3694 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 22 | Train Loss: 24.8714 | Val Loss: 23.9406 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 23 | Train Loss: 24.5169 | Val Loss: 24.3592 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 24 | Train Loss: 21.0335 | Val Loss: 19.2341 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 25 | Train Loss: 23.7559 | Val Loss: 18.2313 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 26 | Train Loss: 20.4380 | Val Loss: 18.6151 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 27 | Train Loss: 20.3806 | Val Loss: 24.3833 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 28 | Train Loss: 19.4068 | Val Loss: 20.7880 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 29 | Train Loss: 21.2292 | Val Loss: 22.5024 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 30 | Train Loss: 18.7011 | Val Loss: 22.8457 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 31 | Train Loss: 19.3645 | Val Loss: 22.1790 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 32 | Train Loss: 16.4945 | Val Loss: 16.6762 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 33 | Train Loss: 19.2847 | Val Loss: 16.2364 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 34 | Train Loss: 18.1196 | Val Loss: 16.9774 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 35 | Train Loss: 19.3390 | Val Loss: 15.6653 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 36 | Train Loss: 19.5808 | Val Loss: 16.0632 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 37 | Train Loss: 18.2963 | Val Loss: 18.3617 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 38 | Train Loss: 17.7759 | Val Loss: 18.0222 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 39 | Train Loss: 19.0069 | Val Loss: 18.6329 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 40 | Train Loss: 18.9223 | Val Loss: 15.1552 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 41 | Train Loss: 16.9821 | Val Loss: 19.7769 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 42 | Train Loss: 16.9658 | Val Loss: 15.3360 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 43 | Train Loss: 16.6578 | Val Loss: 19.1236 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 44 | Train Loss: 16.4877 | Val Loss: 18.0281 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 45 | Train Loss: 16.3670 | Val Loss: 15.6425 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 46 | Train Loss: 16.0615 | Val Loss: 15.3435 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 47 | Train Loss: 15.4646 | Val Loss: 20.4926 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 48 | Train Loss: 16.0520 | Val Loss: 13.7130 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 49 | Train Loss: 16.9598 | Val Loss: 21.9111 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 50 | Train Loss: 16.1639 | Val Loss: 17.6604 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 51 | Train Loss: 15.2081 | Val Loss: 19.7222 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 52 | Train Loss: 15.7739 | Val Loss: 13.9339 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 53 | Train Loss: 17.1226 | Val Loss: 18.7335 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 54 | Train Loss: 15.3257 | Val Loss: 15.4818 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 55 | Train Loss: 13.0503 | Val Loss: 13.6050 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 56 | Train Loss: 15.2662 | Val Loss: 13.1299 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 57 | Train Loss: 15.3844 | Val Loss: 13.9516 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 58 | Train Loss: 14.3991 | Val Loss: 13.9312 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 59 | Train Loss: 18.6512 | Val Loss: 15.0845 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 60 | Train Loss: 15.9908 | Val Loss: 13.8783 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 61 | Train Loss: 16.4753 | Val Loss: 14.5249 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 62 | Train Loss: 16.3560 | Val Loss: 14.2030 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 63 | Train Loss: 14.3725 | Val Loss: 14.0395 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 64 | Train Loss: 13.0551 | Val Loss: 14.7229 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 65 | Train Loss: 14.2987 | Val Loss: 11.5840 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 66 | Train Loss: 15.4644 | Val Loss: 12.3884 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 67 | Train Loss: 14.7410 | Val Loss: 14.1528 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 68 | Train Loss: 15.1175 | Val Loss: 13.6843 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 69 | Train Loss: 14.4159 | Val Loss: 12.1430 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 70 | Train Loss: 15.4686 | Val Loss: 13.7217 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 71 | Train Loss: 15.4190 | Val Loss: 11.2940 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 72 | Train Loss: 14.2964 | Val Loss: 12.1555 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 73 | Train Loss: 15.6839 | Val Loss: 13.2572 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 74 | Train Loss: 14.3010 | Val Loss: 13.8871 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 75 | Train Loss: 13.6858 | Val Loss: 14.6094 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 76 | Train Loss: 13.1651 | Val Loss: 23.2566 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 77 | Train Loss: 14.4705 | Val Loss: 14.3164 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 78 | Train Loss: 14.7333 | Val Loss: 11.5833 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 79 | Train Loss: 14.0404 | Val Loss: 11.4485 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 80 | Train Loss: 14.9877 | Val Loss: 12.3636 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 81 | Train Loss: 13.5155 | Val Loss: 10.4523 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 82 | Train Loss: 11.3568 | Val Loss: 10.8019 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 83 | Train Loss: 11.6358 | Val Loss: 10.8170 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 84 | Train Loss: 10.6569 | Val Loss: 16.3534 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 85 | Train Loss: 13.6676 | Val Loss: 16.5169 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 86 | Train Loss: 13.9166 | Val Loss: 12.2452 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 87 | Train Loss: 12.1731 | Val Loss: 13.1930 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 88 | Train Loss: 12.7256 | Val Loss: 15.3897 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:35,216] Trial 322 finished with value: 10.452346972333707 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.31278802667276084, 'lr': 0.0006854282937101316, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 8.347373789789642e-05}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 322 | Epoch 89 | Train Loss: 11.6194 | Val Loss: 11.8738 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 90 | Train Loss: 9.1658 | Val Loss: 13.2190 | Optimizer: AdamW\n",
      "Trial 322 | Epoch 91 | Train Loss: 13.5740 | Val Loss: 12.1013 | Optimizer: AdamW\n",
      "Trial 322 - Early stopping triggered at epoch 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:35,393] Trial 323 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 323 | Epoch 01 | Train Loss: 157.3863 | Val Loss: 49.3763 | Optimizer: AdamW\n",
      "Trial 324 | Epoch 01 | Train Loss: 145.4210 | Val Loss: 35.1575 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:35,787] Trial 324 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 324 | Epoch 02 | Train Loss: 51.7898 | Val Loss: 54.9243 | Optimizer: AdamW\n",
      "Trial 324 | Epoch 03 | Train Loss: 44.7707 | Val Loss: 38.5408 | Optimizer: AdamW\n",
      "Trial 324 | Epoch 04 | Train Loss: 46.4519 | Val Loss: 41.7015 | Optimizer: AdamW\n",
      "Trial 325 | Epoch 01 | Train Loss: 183.3379 | Val Loss: 35.2509 | Optimizer: AdamW\n",
      "Trial 325 | Epoch 02 | Train Loss: 58.3087 | Val Loss: 67.3628 | Optimizer: AdamW\n",
      "Trial 325 | Epoch 03 | Train Loss: 53.5750 | Val Loss: 34.8437 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:36,182] Trial 325 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 325 | Epoch 04 | Train Loss: 44.7442 | Val Loss: 39.1608 | Optimizer: AdamW\n",
      "Trial 326 | Epoch 01 | Train Loss: 205.7001 | Val Loss: 43.0836 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:36,349] Trial 326 pruned. \n",
      "[I 2025-09-04 21:25:36,520] Trial 327 pruned. \n",
      "[I 2025-09-04 21:25:36,688] Trial 328 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 327 | Epoch 01 | Train Loss: 133.9857 | Val Loss: 56.8115 | Optimizer: AdamW\n",
      "Trial 328 | Epoch 01 | Train Loss: 170.8485 | Val Loss: 46.0470 | Optimizer: AdamW\n",
      "Trial 329 | Epoch 01 | Train Loss: 203.5428 | Val Loss: 27.6761 | Optimizer: AdamW\n",
      "Trial 329 | Epoch 02 | Train Loss: 53.1821 | Val Loss: 60.2849 | Optimizer: AdamW\n",
      "Trial 329 | Epoch 03 | Train Loss: 55.8934 | Val Loss: 42.6745 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:37,235] Trial 329 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 329 | Epoch 04 | Train Loss: 37.6311 | Val Loss: 34.5479 | Optimizer: AdamW\n",
      "Trial 329 | Epoch 05 | Train Loss: 37.7976 | Val Loss: 34.8976 | Optimizer: AdamW\n",
      "Trial 329 | Epoch 06 | Train Loss: 35.0196 | Val Loss: 28.0947 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:37,410] Trial 330 pruned. \n",
      "[I 2025-09-04 21:25:37,576] Trial 331 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 330 | Epoch 01 | Train Loss: 180733.2474 | Val Loss: 11487.0538 | Optimizer: RMSprop\n",
      "Trial 331 | Epoch 01 | Train Loss: 120.0513 | Val Loss: 39.5483 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:37,917] Trial 332 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 332 | Epoch 01 | Train Loss: 180.9913 | Val Loss: 38.0892 | Optimizer: AdamW\n",
      "Trial 332 | Epoch 02 | Train Loss: 63.1705 | Val Loss: 87.8992 | Optimizer: AdamW\n",
      "Trial 332 | Epoch 03 | Train Loss: 69.6446 | Val Loss: 46.8425 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:38,087] Trial 333 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 333 | Epoch 01 | Train Loss: 182.2643 | Val Loss: 124.7342 | Optimizer: AdamW\n",
      "Trial 334 | Epoch 01 | Train Loss: 157.5190 | Val Loss: 38.6632 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:38,255] Trial 334 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 335 | Epoch 01 | Train Loss: 184.3377 | Val Loss: 36.1040 | Optimizer: AdamW\n",
      "Trial 335 | Epoch 02 | Train Loss: 61.8019 | Val Loss: 50.8590 | Optimizer: AdamW\n",
      "Trial 335 | Epoch 03 | Train Loss: 67.6562 | Val Loss: 60.6915 | Optimizer: AdamW\n",
      "Trial 335 | Epoch 04 | Train Loss: 56.5252 | Val Loss: 46.6171 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:38,548] Trial 335 pruned. \n",
      "[I 2025-09-04 21:25:38,721] Trial 336 pruned. \n",
      "[I 2025-09-04 21:25:38,889] Trial 337 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 336 | Epoch 01 | Train Loss: 175.0793 | Val Loss: 46.4112 | Optimizer: AdamW\n",
      "Trial 337 | Epoch 01 | Train Loss: 154.8635 | Val Loss: 42.3627 | Optimizer: AdamW\n",
      "Trial 338 | Epoch 01 | Train Loss: 183.1754 | Val Loss: 34.8204 | Optimizer: AdamW\n",
      "Trial 338 | Epoch 02 | Train Loss: 61.1835 | Val Loss: 72.6463 | Optimizer: AdamW\n",
      "Trial 338 | Epoch 03 | Train Loss: 49.6269 | Val Loss: 34.6318 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:39,289] Trial 338 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 338 | Epoch 04 | Train Loss: 45.3857 | Val Loss: 42.6458 | Optimizer: AdamW\n",
      "Trial 339 | Epoch 01 | Train Loss: 162.7535 | Val Loss: 38.2830 | Optimizer: AdamW\n",
      "Trial 339 | Epoch 02 | Train Loss: 50.0916 | Val Loss: 61.4401 | Optimizer: AdamW\n",
      "Trial 339 | Epoch 03 | Train Loss: 47.1985 | Val Loss: 34.0077 | Optimizer: AdamW\n",
      "Trial 339 | Epoch 04 | Train Loss: 38.0967 | Val Loss: 32.3772 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:39,766] Trial 339 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 339 | Epoch 05 | Train Loss: 36.5057 | Val Loss: 30.1977 | Optimizer: AdamW\n",
      "Trial 340 | Epoch 01 | Train Loss: 131.8184 | Val Loss: 43.8764 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:39,933] Trial 340 pruned. \n",
      "[I 2025-09-04 21:25:40,062] Trial 341 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 341 | Epoch 01 | Train Loss: 172.0950 | Val Loss: 66.1036 | Optimizer: AdamW\n",
      "Trial 342 | Epoch 01 | Train Loss: 124.8090 | Val Loss: 50.9714 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:40,230] Trial 342 pruned. \n",
      "[I 2025-09-04 21:25:40,552] Trial 343 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 343 | Epoch 01 | Train Loss: 163.2730 | Val Loss: 38.1375 | Optimizer: AdamW\n",
      "Trial 343 | Epoch 02 | Train Loss: 60.2187 | Val Loss: 78.5830 | Optimizer: AdamW\n",
      "Trial 343 | Epoch 03 | Train Loss: 58.7834 | Val Loss: 44.3377 | Optimizer: AdamW\n",
      "Trial 344 | Epoch 01 | Train Loss: 225.2627 | Val Loss: 33.4888 | Optimizer: AdamW\n",
      "Trial 344 | Epoch 02 | Train Loss: 65.0882 | Val Loss: 77.1475 | Optimizer: AdamW\n",
      "Trial 344 | Epoch 03 | Train Loss: 54.6067 | Val Loss: 32.9146 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:41,046] Trial 344 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 344 | Epoch 04 | Train Loss: 40.6505 | Val Loss: 41.5681 | Optimizer: AdamW\n",
      "Trial 344 | Epoch 05 | Train Loss: 38.6739 | Val Loss: 30.2565 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:41,196] Trial 345 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 345 | Epoch 01 | NaN loss detected so pruning trial\n",
      "Trial 346 | Epoch 01 | Train Loss: 281.9566 | Val Loss: 118.8550 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:41,363] Trial 346 pruned. \n",
      "[I 2025-09-04 21:25:41,536] Trial 347 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 347 | Epoch 01 | Train Loss: 176.5688 | Val Loss: 104.8022 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 01 | Train Loss: 120.8092 | Val Loss: 34.3872 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 02 | Train Loss: 44.3713 | Val Loss: 36.9263 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 03 | Train Loss: 39.6845 | Val Loss: 33.6522 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 04 | Train Loss: 31.9075 | Val Loss: 27.5075 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 05 | Train Loss: 29.8029 | Val Loss: 25.0548 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 06 | Train Loss: 26.5360 | Val Loss: 25.7763 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 07 | Train Loss: 24.2571 | Val Loss: 21.8819 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 08 | Train Loss: 24.0500 | Val Loss: 29.4171 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 09 | Train Loss: 25.8550 | Val Loss: 27.8164 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 10 | Train Loss: 23.9605 | Val Loss: 21.8507 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 11 | Train Loss: 24.2240 | Val Loss: 22.9281 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 12 | Train Loss: 21.4413 | Val Loss: 21.8025 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 13 | Train Loss: 21.9736 | Val Loss: 22.8260 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 14 | Train Loss: 22.2368 | Val Loss: 21.7683 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 15 | Train Loss: 21.3473 | Val Loss: 19.8231 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 16 | Train Loss: 23.7302 | Val Loss: 19.9957 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 17 | Train Loss: 23.9392 | Val Loss: 22.5397 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 18 | Train Loss: 23.0706 | Val Loss: 28.1860 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 19 | Train Loss: 23.0039 | Val Loss: 19.7292 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 20 | Train Loss: 20.1300 | Val Loss: 19.4690 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 21 | Train Loss: 19.8195 | Val Loss: 25.2181 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 22 | Train Loss: 22.1838 | Val Loss: 24.7546 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 23 | Train Loss: 20.5997 | Val Loss: 19.0820 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 24 | Train Loss: 18.9251 | Val Loss: 16.3392 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 25 | Train Loss: 19.5530 | Val Loss: 18.5609 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 26 | Train Loss: 19.8432 | Val Loss: 19.9864 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 27 | Train Loss: 20.1881 | Val Loss: 19.7583 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 28 | Train Loss: 17.4849 | Val Loss: 15.4229 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 29 | Train Loss: 18.4191 | Val Loss: 17.0820 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 30 | Train Loss: 19.0638 | Val Loss: 17.6286 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 31 | Train Loss: 22.4291 | Val Loss: 17.3435 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 32 | Train Loss: 17.7486 | Val Loss: 16.4504 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 33 | Train Loss: 15.1035 | Val Loss: 15.1188 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 34 | Train Loss: 16.6350 | Val Loss: 16.9084 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 35 | Train Loss: 20.1820 | Val Loss: 15.8871 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 36 | Train Loss: 17.5535 | Val Loss: 16.2876 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 37 | Train Loss: 16.5468 | Val Loss: 15.7202 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 38 | Train Loss: 15.1734 | Val Loss: 15.4538 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 39 | Train Loss: 15.1008 | Val Loss: 15.9852 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 40 | Train Loss: 16.1011 | Val Loss: 15.6811 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:44,936] Trial 348 finished with value: 15.11876135725316 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.261161479720435, 'lr': 0.0006203305576102897, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 6.78308636067942e-05}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 348 | Epoch 41 | Train Loss: 15.3002 | Val Loss: 15.6553 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 42 | Train Loss: 14.7659 | Val Loss: 15.7744 | Optimizer: AdamW\n",
      "Trial 348 | Epoch 43 | Train Loss: 19.3364 | Val Loss: 16.9608 | Optimizer: AdamW\n",
      "Trial 348 - Early stopping triggered at epoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:45,111] Trial 349 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 349 | Epoch 01 | Train Loss: 279.0314 | Val Loss: 283.4968 | Optimizer: AdamW\n",
      "Trial 350 | Epoch 01 | Train Loss: 99.5834 | Val Loss: 49.2838 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:45,284] Trial 350 pruned. \n",
      "[I 2025-09-04 21:25:45,464] Trial 351 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 351 | Epoch 01 | Train Loss: 151.6596 | Val Loss: 48.0454 | Optimizer: AdamW\n",
      "Trial 352 | Epoch 01 | Train Loss: 152.2198 | Val Loss: 50.6790 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:45,644] Trial 352 pruned. \n",
      "[I 2025-09-04 21:25:45,815] Trial 353 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 353 | Epoch 01 | Train Loss: 260.3186 | Val Loss: 216.3233 | Optimizer: RMSprop\n",
      "Trial 354 | Epoch 01 | Train Loss: 201.3982 | Val Loss: 33.2434 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:46,216] Trial 354 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 354 | Epoch 02 | Train Loss: 50.7879 | Val Loss: 61.2723 | Optimizer: AdamW\n",
      "Trial 354 | Epoch 03 | Train Loss: 45.1953 | Val Loss: 34.1855 | Optimizer: AdamW\n",
      "Trial 354 | Epoch 04 | Train Loss: 40.3574 | Val Loss: 36.2604 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:46,454] Trial 355 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 355 | Epoch 01 | Train Loss: 181.2199 | Val Loss: 37.3384 | Optimizer: AdamW\n",
      "Trial 355 | Epoch 02 | Train Loss: 59.9060 | Val Loss: 55.1382 | Optimizer: AdamW\n",
      "Trial 355 | Epoch 03 | Train Loss: 62.9572 | Val Loss: 66.4212 | Optimizer: AdamW\n",
      "Trial 356 | Epoch 01 | Train Loss: 214.6133 | Val Loss: 32.9996 | Optimizer: AdamW\n",
      "Trial 356 | Epoch 02 | Train Loss: 70.1824 | Val Loss: 94.3527 | Optimizer: AdamW\n",
      "Trial 356 | Epoch 03 | Train Loss: 63.1388 | Val Loss: 33.7268 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:46,853] Trial 356 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 356 | Epoch 04 | Train Loss: 44.8421 | Val Loss: 48.1951 | Optimizer: AdamW\n",
      "Trial 357 | Epoch 01 | Train Loss: 196.1780 | Val Loss: 85.8955 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:47,058] Trial 357 pruned. \n",
      "[I 2025-09-04 21:25:47,236] Trial 358 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 358 | Epoch 01 | Train Loss: 128.6829 | Val Loss: 62.9787 | Optimizer: AdamW\n",
      "Trial 359 | Epoch 01 | Train Loss: 157.8682 | Val Loss: 36.4617 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:47,690] Trial 359 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 359 | Epoch 02 | Train Loss: 42.0484 | Val Loss: 34.1258 | Optimizer: AdamW\n",
      "Trial 359 | Epoch 03 | Train Loss: 43.0897 | Val Loss: 39.3504 | Optimizer: AdamW\n",
      "Trial 359 | Epoch 04 | Train Loss: 37.5660 | Val Loss: 35.5505 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:47,892] Trial 360 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 360 | Epoch 01 | Train Loss: 168.4552 | Val Loss: 50.4889 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:48,088] Trial 361 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 361 | Epoch 01 | Train Loss: 147.5960 | Val Loss: 41.9262 | Optimizer: AdamW\n",
      "Trial 362 | Epoch 01 | Train Loss: 190.0663 | Val Loss: 35.0928 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:48,490] Trial 362 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 362 | Epoch 02 | Train Loss: 68.5291 | Val Loss: 82.8533 | Optimizer: AdamW\n",
      "Trial 362 | Epoch 03 | Train Loss: 57.4178 | Val Loss: 36.7380 | Optimizer: AdamW\n",
      "Trial 362 | Epoch 04 | Train Loss: 44.5582 | Val Loss: 42.3560 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:48,662] Trial 363 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 363 | Epoch 01 | Train Loss: 237.6564 | Val Loss: 156.4958 | Optimizer: AdamW\n",
      "Trial 364 | Epoch 01 | Train Loss: 169.0877 | Val Loss: 59.8372 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:48,836] Trial 364 pruned. \n",
      "[I 2025-09-04 21:25:48,961] Trial 365 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 365 | Epoch 01 | Train Loss: 169.1289 | Val Loss: 61.0040 | Optimizer: AdamW\n",
      "Trial 366 | Epoch 01 | Train Loss: 312.7717 | Val Loss: 317.8101 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:49,134] Trial 366 pruned. \n",
      "[I 2025-09-04 21:25:49,342] Trial 367 pruned. \n",
      "[I 2025-09-04 21:25:49,499] Trial 368 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 367 | Epoch 01 | Train Loss: 257.2925 | Val Loss: 246.0163 | Optimizer: AdamW\n",
      "Trial 368 | Epoch 01 | NaN loss detected so pruning trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:49,671] Trial 369 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 369 | Epoch 01 | Train Loss: 210.9423 | Val Loss: 54.7843 | Optimizer: AdamW\n",
      "Trial 370 | Epoch 01 | Train Loss: 257.6126 | Val Loss: 259.3395 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:49,847] Trial 370 pruned. \n",
      "[I 2025-09-04 21:25:50,020] Trial 371 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 371 | Epoch 01 | Train Loss: 174.7419 | Val Loss: 67.8402 | Optimizer: AdamW\n",
      "Trial 372 | Epoch 01 | Train Loss: 136.3920 | Val Loss: 44.4124 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:50,191] Trial 372 pruned. \n",
      "[I 2025-09-04 21:25:50,521] Trial 373 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 373 | Epoch 01 | Train Loss: 172.0228 | Val Loss: 37.6400 | Optimizer: AdamW\n",
      "Trial 373 | Epoch 02 | Train Loss: 65.3523 | Val Loss: 80.7714 | Optimizer: AdamW\n",
      "Trial 373 | Epoch 03 | Train Loss: 61.9174 | Val Loss: 42.7993 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:50,698] Trial 374 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 374 | Epoch 01 | Train Loss: 199.3078 | Val Loss: 47.4549 | Optimizer: AdamW\n",
      "Trial 375 | Epoch 01 | Train Loss: 214.7466 | Val Loss: 63.8006 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:50,876] Trial 375 pruned. \n",
      "[I 2025-09-04 21:25:51,045] Trial 376 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 376 | Epoch 01 | Train Loss: 6573.1834 | Val Loss: 242.0078 | Optimizer: RMSprop\n",
      "Trial 377 | Epoch 01 | Train Loss: 179.6772 | Val Loss: 77.5229 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:51,216] Trial 377 pruned. \n",
      "[I 2025-09-04 21:25:51,394] Trial 378 pruned. \n",
      "[I 2025-09-04 21:25:51,527] Trial 379 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 378 | Epoch 01 | Train Loss: 170.0921 | Val Loss: 44.9528 | Optimizer: Adam\n",
      "Trial 379 | Epoch 01 | Train Loss: 173.6524 | Val Loss: 74.1387 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:51,699] Trial 380 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 380 | Epoch 01 | Train Loss: 148.3477 | Val Loss: 41.8370 | Optimizer: AdamW\n",
      "Trial 381 | Epoch 01 | Train Loss: 203.2382 | Val Loss: 48.6614 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:51,893] Trial 381 pruned. \n",
      "[I 2025-09-04 21:25:52,063] Trial 382 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 382 | Epoch 01 | Train Loss: 154.0040 | Val Loss: 43.7314 | Optimizer: AdamW\n",
      "Trial 383 | Epoch 01 | Train Loss: 158.0519 | Val Loss: 41.5723 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:52,239] Trial 383 pruned. \n",
      "[I 2025-09-04 21:25:52,426] Trial 384 pruned. \n",
      "[I 2025-09-04 21:25:52,579] Trial 385 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 384 | Epoch 01 | Train Loss: 200.4851 | Val Loss: 44.9020 | Optimizer: AdamW\n",
      "Trial 385 | Epoch 01 | Train Loss: 111.0283 | Val Loss: 56.3821 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:52,769] Trial 386 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 386 | Epoch 01 | Train Loss: 275.7717 | Val Loss: 212.3257 | Optimizer: AdamW\n",
      "Trial 387 | Epoch 01 | Train Loss: 177.5245 | Val Loss: 34.4708 | Optimizer: AdamW\n",
      "Trial 387 | Epoch 02 | Train Loss: 51.8857 | Val Loss: 39.0696 | Optimizer: AdamW\n",
      "Trial 387 | Epoch 03 | Train Loss: 43.0373 | Val Loss: 35.8016 | Optimizer: AdamW\n",
      "Trial 387 | Epoch 04 | Train Loss: 37.0679 | Val Loss: 27.8966 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:53,378] Trial 387 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 387 | Epoch 05 | Train Loss: 34.6676 | Val Loss: 30.8102 | Optimizer: AdamW\n",
      "Trial 387 | Epoch 06 | Train Loss: 32.3915 | Val Loss: 37.7058 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:53,549] Trial 388 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 388 | Epoch 01 | Train Loss: 179.6217 | Val Loss: 48.3400 | Optimizer: AdamW\n",
      "Trial 389 | Epoch 01 | Train Loss: 141.4271 | Val Loss: 40.3328 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:53,718] Trial 389 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 390 | Epoch 01 | Train Loss: 173.2416 | Val Loss: 30.9103 | Optimizer: AdamW\n",
      "Trial 390 | Epoch 02 | Train Loss: 59.4277 | Val Loss: 67.9322 | Optimizer: AdamW\n",
      "Trial 390 | Epoch 03 | Train Loss: 45.9212 | Val Loss: 40.1981 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:54,195] Trial 390 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 390 | Epoch 04 | Train Loss: 42.0727 | Val Loss: 36.8014 | Optimizer: AdamW\n",
      "Trial 390 | Epoch 05 | Train Loss: 36.3024 | Val Loss: 30.4500 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:54,356] Trial 391 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 391 | Epoch 01 | NaN loss detected so pruning trial\n",
      "Trial 392 | Epoch 01 | Train Loss: 185.4015 | Val Loss: 107.2838 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:54,543] Trial 392 pruned. \n",
      "[I 2025-09-04 21:25:54,715] Trial 393 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 393 | Epoch 01 | Train Loss: 147.1738 | Val Loss: 43.8302 | Optimizer: AdamW\n",
      "Trial 394 | Epoch 01 | Train Loss: 183.9222 | Val Loss: 122.1236 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:54,885] Trial 394 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 395 | Epoch 01 | Train Loss: 145.7946 | Val Loss: 31.6497 | Optimizer: AdamW\n",
      "Trial 395 | Epoch 02 | Train Loss: 34.1846 | Val Loss: 36.0107 | Optimizer: AdamW\n",
      "Trial 395 | Epoch 03 | Train Loss: 35.4355 | Val Loss: 30.0268 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:55,360] Trial 395 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 395 | Epoch 04 | Train Loss: 30.3192 | Val Loss: 34.2501 | Optimizer: AdamW\n",
      "Trial 395 | Epoch 05 | Train Loss: 32.2055 | Val Loss: 37.8229 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:55,537] Trial 396 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 396 | Epoch 01 | Train Loss: 155.1381 | Val Loss: 51.0431 | Optimizer: AdamW\n",
      "Trial 397 | Epoch 01 | Train Loss: 139.7080 | Val Loss: 49.5359 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:55,709] Trial 397 pruned. \n",
      "[I 2025-09-04 21:25:55,882] Trial 398 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 398 | Epoch 01 | Train Loss: 173.6823 | Val Loss: 64.8496 | Optimizer: AdamW\n",
      "Trial 399 | Epoch 01 | Train Loss: 184.0164 | Val Loss: 59.3541 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:56,056] Trial 399 pruned. \n",
      "[I 2025-09-04 21:25:56,190] Trial 400 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 400 | Epoch 01 | Train Loss: 145.8689 | Val Loss: 50.0414 | Optimizer: AdamW\n",
      "Trial 401 | Epoch 01 | Train Loss: 158.1392 | Val Loss: 60.1360 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:56,361] Trial 401 pruned. \n",
      "[I 2025-09-04 21:25:56,533] Trial 402 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 402 | Epoch 01 | Train Loss: 66248.2340 | Val Loss: 397580.1240 | Optimizer: RMSprop\n",
      "Trial 403 | Epoch 01 | Train Loss: 196.1616 | Val Loss: 146.3653 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:56,705] Trial 403 pruned. \n",
      "[I 2025-09-04 21:25:56,878] Trial 404 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 404 | Epoch 01 | Train Loss: 156.4241 | Val Loss: 53.7943 | Optimizer: AdamW\n",
      "Trial 405 | Epoch 01 | Train Loss: 137.4788 | Val Loss: 43.2129 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:57,053] Trial 405 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 406 | Epoch 01 | Train Loss: 163.3019 | Val Loss: 31.9713 | Optimizer: AdamW\n",
      "Trial 406 | Epoch 02 | Train Loss: 56.6239 | Val Loss: 62.8495 | Optimizer: AdamW\n",
      "Trial 406 | Epoch 03 | Train Loss: 58.5193 | Val Loss: 40.6220 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:57,647] Trial 406 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 406 | Epoch 04 | Train Loss: 45.9802 | Val Loss: 35.2356 | Optimizer: AdamW\n",
      "Trial 406 | Epoch 05 | Train Loss: 43.5864 | Val Loss: 28.9572 | Optimizer: AdamW\n",
      "Trial 406 | Epoch 06 | Train Loss: 35.8630 | Val Loss: 30.0500 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:57,785] Trial 407 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 407 | Epoch 01 | Train Loss: 137.0417 | Val Loss: 54.5304 | Optimizer: AdamW\n",
      "Trial 408 | Epoch 01 | Train Loss: 167.7493 | Val Loss: 35.3775 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:58,201] Trial 408 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 408 | Epoch 02 | Train Loss: 55.5648 | Val Loss: 73.4655 | Optimizer: AdamW\n",
      "Trial 408 | Epoch 03 | Train Loss: 59.6049 | Val Loss: 41.5565 | Optimizer: AdamW\n",
      "Trial 408 | Epoch 04 | Train Loss: 44.6149 | Val Loss: 38.4590 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:58,380] Trial 409 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 409 | Epoch 01 | Train Loss: 169.1021 | Val Loss: 52.9828 | Optimizer: AdamW\n",
      "Trial 410 | Epoch 01 | Train Loss: 172.6936 | Val Loss: 40.9443 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:58,561] Trial 410 pruned. \n",
      "[I 2025-09-04 21:25:58,737] Trial 411 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 411 | Epoch 01 | Train Loss: 154.5117 | Val Loss: 50.1681 | Optimizer: AdamW\n",
      "Trial 412 | Epoch 01 | Train Loss: 149.2698 | Val Loss: 34.0188 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:59,149] Trial 412 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 412 | Epoch 02 | Train Loss: 60.2954 | Val Loss: 70.1807 | Optimizer: AdamW\n",
      "Trial 412 | Epoch 03 | Train Loss: 55.2715 | Val Loss: 40.6299 | Optimizer: AdamW\n",
      "Trial 412 | Epoch 04 | Train Loss: 50.7657 | Val Loss: 41.1438 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:59,330] Trial 413 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 413 | Epoch 01 | Train Loss: 177.2039 | Val Loss: 185.5234 | Optimizer: AdamW\n",
      "Trial 414 | Epoch 01 | Train Loss: 213.7674 | Val Loss: 31.9288 | Optimizer: AdamW\n",
      "Trial 414 | Epoch 02 | Train Loss: 69.9595 | Val Loss: 81.1226 | Optimizer: AdamW\n",
      "Trial 414 | Epoch 03 | Train Loss: 56.7912 | Val Loss: 35.2891 | Optimizer: AdamW\n",
      "Trial 414 | Epoch 04 | Train Loss: 46.7595 | Val Loss: 41.9711 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:25:59,813] Trial 414 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 414 | Epoch 05 | Train Loss: 40.1631 | Val Loss: 32.8793 | Optimizer: AdamW\n",
      "Trial 415 | Epoch 01 | Train Loss: 125.4749 | Val Loss: 61.0561 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:00,007] Trial 415 pruned. \n",
      "[I 2025-09-04 21:26:00,188] Trial 416 pruned. \n",
      "[I 2025-09-04 21:26:00,340] Trial 417 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 416 | Epoch 01 | Train Loss: 164.3524 | Val Loss: 41.8244 | Optimizer: AdamW\n",
      "Trial 417 | Epoch 01 | NaN loss detected so pruning trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:00,516] Trial 418 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 418 | Epoch 01 | Train Loss: 157.8306 | Val Loss: 48.1541 | Optimizer: AdamW\n",
      "Trial 419 | Epoch 01 | Train Loss: 187.0816 | Val Loss: 54.1676 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:00,690] Trial 419 pruned. \n",
      "[I 2025-09-04 21:26:00,868] Trial 420 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 420 | Epoch 01 | Train Loss: 133.9367 | Val Loss: 48.9333 | Optimizer: AdamW\n",
      "Trial 421 | Epoch 01 | Train Loss: 262.2728 | Val Loss: 167.1325 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:01,042] Trial 421 pruned. \n",
      "[I 2025-09-04 21:26:01,182] Trial 422 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 422 | Epoch 01 | Train Loss: 144.0736 | Val Loss: 66.2292 | Optimizer: AdamW\n",
      "Trial 423 | Epoch 01 | Train Loss: 149.7577 | Val Loss: 59.6291 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:01,361] Trial 423 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 424 | Epoch 01 | Train Loss: 176.9535 | Val Loss: 31.5543 | Optimizer: AdamW\n",
      "Trial 424 | Epoch 02 | Train Loss: 55.9654 | Val Loss: 72.8965 | Optimizer: AdamW\n",
      "Trial 424 | Epoch 03 | Train Loss: 49.6481 | Val Loss: 32.0528 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:01,849] Trial 424 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 424 | Epoch 04 | Train Loss: 42.2207 | Val Loss: 31.2147 | Optimizer: AdamW\n",
      "Trial 424 | Epoch 05 | Train Loss: 38.8100 | Val Loss: 34.2182 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:02,023] Trial 425 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 425 | Epoch 01 | Train Loss: 138.2782 | Val Loss: 45.0558 | Optimizer: Adam\n",
      "Trial 426 | Epoch 01 | Train Loss: 229.0966 | Val Loss: 59.3786 | Optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:02,200] Trial 426 pruned. \n",
      "[I 2025-09-04 21:26:02,372] Trial 427 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 427 | Epoch 01 | Train Loss: 303.6553 | Val Loss: 312.2870 | Optimizer: AdamW\n",
      "Trial 428 | Epoch 01 | Train Loss: 141.6305 | Val Loss: 36.0482 | Optimizer: AdamW\n",
      "Trial 428 | Epoch 02 | Train Loss: 41.6691 | Val Loss: 43.9814 | Optimizer: AdamW\n",
      "Trial 428 | Epoch 03 | Train Loss: 39.4439 | Val Loss: 30.8551 | Optimizer: AdamW\n",
      "Trial 428 | Epoch 04 | Train Loss: 35.9525 | Val Loss: 35.1716 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:02,864] Trial 428 pruned. \n",
      "[I 2025-09-04 21:26:02,991] Trial 429 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 428 | Epoch 05 | Train Loss: 31.7850 | Val Loss: 31.6813 | Optimizer: AdamW\n",
      "Trial 429 | Epoch 01 | Train Loss: 116.7339 | Val Loss: 61.3855 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:03,167] Trial 430 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 430 | Epoch 01 | Train Loss: 156.2038 | Val Loss: 49.7937 | Optimizer: AdamW\n",
      "Trial 431 | Epoch 01 | Train Loss: 164.7229 | Val Loss: 28.9702 | Optimizer: AdamW\n",
      "Trial 431 | Epoch 02 | Train Loss: 56.9407 | Val Loss: 65.7119 | Optimizer: AdamW\n",
      "Trial 431 | Epoch 03 | Train Loss: 49.0979 | Val Loss: 36.8134 | Optimizer: AdamW\n",
      "Trial 431 | Epoch 04 | Train Loss: 43.2688 | Val Loss: 34.0627 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:03,717] Trial 431 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 431 | Epoch 05 | Train Loss: 36.7560 | Val Loss: 39.3552 | Optimizer: AdamW\n",
      "Trial 431 | Epoch 06 | Train Loss: 38.2759 | Val Loss: 29.0856 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:03,891] Trial 432 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 432 | Epoch 01 | Train Loss: 121.6326 | Val Loss: 55.2871 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 01 | Train Loss: 169.7091 | Val Loss: 34.3780 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 02 | Train Loss: 44.0077 | Val Loss: 35.8207 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 03 | Train Loss: 40.3135 | Val Loss: 44.2698 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 04 | Train Loss: 38.5895 | Val Loss: 28.6933 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 05 | Train Loss: 34.0920 | Val Loss: 26.2227 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 06 | Train Loss: 32.7883 | Val Loss: 25.7938 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 07 | Train Loss: 29.3964 | Val Loss: 27.8303 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 08 | Train Loss: 28.5192 | Val Loss: 21.8743 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 09 | Train Loss: 27.6013 | Val Loss: 22.0230 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 10 | Train Loss: 30.5578 | Val Loss: 20.9613 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 11 | Train Loss: 31.9032 | Val Loss: 30.3699 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 12 | Train Loss: 29.0102 | Val Loss: 35.2982 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 13 | Train Loss: 25.7185 | Val Loss: 20.8392 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 14 | Train Loss: 25.1155 | Val Loss: 19.9664 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 15 | Train Loss: 27.8488 | Val Loss: 25.4420 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 16 | Train Loss: 23.8576 | Val Loss: 25.3731 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 17 | Train Loss: 23.4074 | Val Loss: 19.5329 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 18 | Train Loss: 25.3857 | Val Loss: 18.1728 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 19 | Train Loss: 24.1357 | Val Loss: 19.6592 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 20 | Train Loss: 22.6778 | Val Loss: 21.5814 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 21 | Train Loss: 21.0751 | Val Loss: 17.1562 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 22 | Train Loss: 23.9695 | Val Loss: 19.0595 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 23 | Train Loss: 21.6526 | Val Loss: 22.2653 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 24 | Train Loss: 21.4320 | Val Loss: 20.3722 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 25 | Train Loss: 22.8727 | Val Loss: 23.3608 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 26 | Train Loss: 20.7368 | Val Loss: 16.5382 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 27 | Train Loss: 20.5167 | Val Loss: 19.2559 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 28 | Train Loss: 20.3043 | Val Loss: 20.0966 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 29 | Train Loss: 19.6863 | Val Loss: 17.5058 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 30 | Train Loss: 18.6398 | Val Loss: 24.7040 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 31 | Train Loss: 21.7460 | Val Loss: 26.7867 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 32 | Train Loss: 21.0556 | Val Loss: 21.7288 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 33 | Train Loss: 20.5701 | Val Loss: 15.1664 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 34 | Train Loss: 19.4086 | Val Loss: 16.7515 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 35 | Train Loss: 19.7810 | Val Loss: 15.2415 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 36 | Train Loss: 20.3040 | Val Loss: 15.6933 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 37 | Train Loss: 18.0281 | Val Loss: 14.6873 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 38 | Train Loss: 19.2833 | Val Loss: 14.4748 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 39 | Train Loss: 17.6987 | Val Loss: 16.7820 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 40 | Train Loss: 17.8141 | Val Loss: 19.6461 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 41 | Train Loss: 18.6741 | Val Loss: 20.5990 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 42 | Train Loss: 17.6853 | Val Loss: 20.9459 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 43 | Train Loss: 18.7554 | Val Loss: 21.3672 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 44 | Train Loss: 16.9773 | Val Loss: 17.9956 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 45 | Train Loss: 18.2346 | Val Loss: 15.4656 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 46 | Train Loss: 17.8778 | Val Loss: 15.1334 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 47 | Train Loss: 15.5915 | Val Loss: 14.1770 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 48 | Train Loss: 16.4508 | Val Loss: 19.8479 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 49 | Train Loss: 16.8130 | Val Loss: 20.5208 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 50 | Train Loss: 18.6122 | Val Loss: 27.3939 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 51 | Train Loss: 18.9450 | Val Loss: 15.2627 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 52 | Train Loss: 16.3874 | Val Loss: 15.1601 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 53 | Train Loss: 14.9631 | Val Loss: 13.4671 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 54 | Train Loss: 16.3084 | Val Loss: 15.1494 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 55 | Train Loss: 15.2330 | Val Loss: 13.1630 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 56 | Train Loss: 15.4992 | Val Loss: 19.0865 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 57 | Train Loss: 16.1147 | Val Loss: 15.6541 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 58 | Train Loss: 17.7573 | Val Loss: 13.5872 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 59 | Train Loss: 16.9277 | Val Loss: 15.1455 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 60 | Train Loss: 16.7100 | Val Loss: 13.2262 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 61 | Train Loss: 17.1228 | Val Loss: 14.0097 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 62 | Train Loss: 15.7594 | Val Loss: 19.1214 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 63 | Train Loss: 20.0201 | Val Loss: 24.6025 | Optimizer: AdamW\n",
      "Trial 433 | Epoch 64 | Train Loss: 18.0297 | Val Loss: 21.4220 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:08,944] Trial 433 finished with value: 13.162951577969682 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3774428959934768, 'lr': 0.0005704155988689003, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 4.4917467255026655e-06}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 433 | Epoch 65 | Train Loss: 18.6795 | Val Loss: 20.0676 | Optimizer: AdamW\n",
      "Trial 433 - Early stopping triggered at epoch 65\n",
      "Trial 434 | Epoch 01 | Train Loss: 209.1645 | Val Loss: 73.7450 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:09,118] Trial 434 pruned. \n",
      "[I 2025-09-04 21:26:09,291] Trial 435 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 435 | Epoch 01 | Train Loss: 156.3642 | Val Loss: 111.7631 | Optimizer: AdamW\n",
      "Trial 436 | Epoch 01 | Train Loss: 162.8530 | Val Loss: 38.7169 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:09,468] Trial 436 pruned. \n",
      "[I 2025-09-04 21:26:09,640] Trial 437 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 437 | Epoch 01 | Train Loss: 170.8748 | Val Loss: 111.0392 | Optimizer: AdamW\n",
      "Trial 438 | Epoch 01 | Train Loss: 175.5979 | Val Loss: 51.1082 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:09,813] Trial 438 pruned. \n",
      "[I 2025-09-04 21:26:10,007] Trial 439 pruned. \n",
      "[I 2025-09-04 21:26:10,167] Trial 440 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 439 | Epoch 01 | Train Loss: 239.3487 | Val Loss: 252.5371 | Optimizer: AdamW\n",
      "Trial 440 | Epoch 01 | NaN loss detected so pruning trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:10,342] Trial 441 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 441 | Epoch 01 | Train Loss: 160.9064 | Val Loss: 41.8799 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 01 | Train Loss: 146.3179 | Val Loss: 37.6406 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 02 | Train Loss: 49.1355 | Val Loss: 33.9857 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 03 | Train Loss: 37.9502 | Val Loss: 29.6875 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 04 | Train Loss: 33.0433 | Val Loss: 30.5909 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 05 | Train Loss: 32.9313 | Val Loss: 27.0285 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 06 | Train Loss: 29.3413 | Val Loss: 26.0513 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 07 | Train Loss: 24.7073 | Val Loss: 21.4736 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 08 | Train Loss: 28.5478 | Val Loss: 21.1475 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 09 | Train Loss: 28.7957 | Val Loss: 23.5642 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 10 | Train Loss: 30.0915 | Val Loss: 31.7326 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 11 | Train Loss: 28.8602 | Val Loss: 35.6654 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 12 | Train Loss: 27.0101 | Val Loss: 20.6554 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 13 | Train Loss: 24.9643 | Val Loss: 20.0398 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 14 | Train Loss: 25.2797 | Val Loss: 35.6403 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 15 | Train Loss: 29.7025 | Val Loss: 21.2982 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 16 | Train Loss: 27.7853 | Val Loss: 19.3785 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 17 | Train Loss: 24.2626 | Val Loss: 28.1019 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 18 | Train Loss: 21.6471 | Val Loss: 20.1648 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 19 | Train Loss: 23.2703 | Val Loss: 18.3886 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 20 | Train Loss: 24.5559 | Val Loss: 19.7861 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 21 | Train Loss: 22.9787 | Val Loss: 20.6828 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 22 | Train Loss: 24.0337 | Val Loss: 24.7553 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 23 | Train Loss: 20.6926 | Val Loss: 18.6776 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 24 | Train Loss: 19.5031 | Val Loss: 17.8966 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 25 | Train Loss: 21.5303 | Val Loss: 16.5006 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 26 | Train Loss: 20.0121 | Val Loss: 20.4750 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 27 | Train Loss: 20.2116 | Val Loss: 16.5952 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 28 | Train Loss: 20.4163 | Val Loss: 15.8220 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 29 | Train Loss: 19.9584 | Val Loss: 17.9274 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 30 | Train Loss: 19.1396 | Val Loss: 14.5941 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 31 | Train Loss: 17.7504 | Val Loss: 20.5336 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 32 | Train Loss: 16.9707 | Val Loss: 16.7223 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 33 | Train Loss: 17.1655 | Val Loss: 14.5222 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 34 | Train Loss: 19.6894 | Val Loss: 16.0078 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 35 | Train Loss: 18.4184 | Val Loss: 14.6934 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 36 | Train Loss: 17.6494 | Val Loss: 16.4841 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 37 | Train Loss: 18.0127 | Val Loss: 16.7848 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 38 | Train Loss: 15.6871 | Val Loss: 19.9240 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 39 | Train Loss: 17.3368 | Val Loss: 18.2404 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 40 | Train Loss: 19.1030 | Val Loss: 15.0825 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 41 | Train Loss: 17.1225 | Val Loss: 14.2365 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 42 | Train Loss: 16.6093 | Val Loss: 16.1612 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 43 | Train Loss: 17.2013 | Val Loss: 15.4174 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 44 | Train Loss: 16.7627 | Val Loss: 13.4746 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 45 | Train Loss: 16.7640 | Val Loss: 16.6146 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 46 | Train Loss: 16.5062 | Val Loss: 13.9935 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 47 | Train Loss: 16.7889 | Val Loss: 12.6957 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 48 | Train Loss: 16.2317 | Val Loss: 18.1887 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 49 | Train Loss: 16.0890 | Val Loss: 13.9724 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 50 | Train Loss: 16.2582 | Val Loss: 12.5089 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 51 | Train Loss: 14.3792 | Val Loss: 11.3403 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 52 | Train Loss: 15.6028 | Val Loss: 12.0562 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 53 | Train Loss: 18.2753 | Val Loss: 20.4580 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 54 | Train Loss: 15.8598 | Val Loss: 17.2602 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 55 | Train Loss: 14.5497 | Val Loss: 20.3894 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 56 | Train Loss: 16.6620 | Val Loss: 19.9417 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 57 | Train Loss: 17.5700 | Val Loss: 23.0859 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 58 | Train Loss: 17.0034 | Val Loss: 21.0921 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 59 | Train Loss: 19.2868 | Val Loss: 21.3953 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 60 | Train Loss: 17.7083 | Val Loss: 20.1209 | Optimizer: AdamW\n",
      "Trial 442 | Epoch 61 | Train Loss: 17.5166 | Val Loss: 12.0946 | Optimizer: AdamW\n",
      "Trial 442 - Early stopping triggered at epoch 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:15,101] Trial 442 finished with value: 11.340287317105425 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3536126077063912, 'lr': 0.0006620236950166221, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 5.803814532956909e-06}. Best is trial 272 with value: 8.336457074173097.\n",
      "[I 2025-09-04 21:26:15,274] Trial 443 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 443 | Epoch 01 | Train Loss: 177.9850 | Val Loss: 59.2775 | Optimizer: AdamW\n",
      "Trial 444 | Epoch 01 | Train Loss: 229.7895 | Val Loss: 204.2193 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:15,447] Trial 444 pruned. \n",
      "[I 2025-09-04 21:26:15,588] Trial 445 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 445 | Epoch 01 | Train Loss: 214.0908 | Val Loss: 93.4195 | Optimizer: AdamW\n",
      "Trial 446 | Epoch 01 | Train Loss: 169.7497 | Val Loss: 34.2097 | Optimizer: AdamW\n",
      "Trial 446 | Epoch 02 | Train Loss: 47.5862 | Val Loss: 31.6905 | Optimizer: AdamW\n",
      "Trial 446 | Epoch 03 | Train Loss: 40.0715 | Val Loss: 29.5612 | Optimizer: AdamW\n",
      "Trial 446 | Epoch 04 | Train Loss: 36.6658 | Val Loss: 27.3850 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:16,138] Trial 446 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 446 | Epoch 05 | Train Loss: 31.3610 | Val Loss: 35.4398 | Optimizer: AdamW\n",
      "Trial 446 | Epoch 06 | Train Loss: 35.8973 | Val Loss: 31.9249 | Optimizer: AdamW\n",
      "Trial 447 | Epoch 01 | Train Loss: 196.1206 | Val Loss: 36.4771 | Optimizer: AdamW\n",
      "Trial 447 | Epoch 02 | Train Loss: 70.0145 | Val Loss: 93.5754 | Optimizer: AdamW\n",
      "Trial 447 | Epoch 03 | Train Loss: 69.3719 | Val Loss: 46.9503 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:16,538] Trial 447 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 447 | Epoch 04 | Train Loss: 49.1776 | Val Loss: 39.0255 | Optimizer: AdamW\n",
      "Trial 448 | Epoch 01 | Train Loss: 179.9977 | Val Loss: 40.1140 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:16,711] Trial 448 pruned. \n",
      "[I 2025-09-04 21:26:16,884] Trial 449 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 449 | Epoch 01 | Train Loss: 145.5850 | Val Loss: 80.5096 | Optimizer: Adam\n",
      "Trial 450 | Epoch 01 | Train Loss: 122.1273 | Val Loss: 64.1358 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:17,058] Trial 450 pruned. \n",
      "[I 2025-09-04 21:26:17,231] Trial 451 pruned. \n",
      "[I 2025-09-04 21:26:17,361] Trial 452 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 451 | Epoch 01 | Train Loss: 9052719.2295 | Val Loss: 38984.0048 | Optimizer: RMSprop\n",
      "Trial 452 | Epoch 01 | Train Loss: 148.3228 | Val Loss: 65.4348 | Optimizer: AdamW\n",
      "Trial 453 | Epoch 01 | Train Loss: 171.1466 | Val Loss: 32.7450 | Optimizer: AdamW\n",
      "Trial 453 | Epoch 02 | Train Loss: 56.1822 | Val Loss: 55.5774 | Optimizer: AdamW\n",
      "Trial 453 | Epoch 03 | Train Loss: 50.6332 | Val Loss: 37.7896 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:17,864] Trial 453 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 453 | Epoch 04 | Train Loss: 47.3213 | Val Loss: 30.9002 | Optimizer: AdamW\n",
      "Trial 453 | Epoch 05 | Train Loss: 34.7459 | Val Loss: 33.5028 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:18,042] Trial 454 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 454 | Epoch 01 | Train Loss: 128.0673 | Val Loss: 39.7820 | Optimizer: AdamW\n",
      "Trial 455 | Epoch 01 | Train Loss: 141.7991 | Val Loss: 32.6270 | Optimizer: AdamW\n",
      "Trial 455 | Epoch 02 | Train Loss: 49.1565 | Val Loss: 44.0070 | Optimizer: AdamW\n",
      "Trial 455 | Epoch 03 | Train Loss: 40.8816 | Val Loss: 36.9293 | Optimizer: AdamW\n",
      "Trial 455 | Epoch 04 | Train Loss: 44.5572 | Val Loss: 31.0909 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:18,516] Trial 455 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 455 | Epoch 05 | Train Loss: 34.9859 | Val Loss: 29.8972 | Optimizer: AdamW\n",
      "Trial 456 | Epoch 01 | Train Loss: 197.6056 | Val Loss: 52.3816 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:18,690] Trial 456 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 457 | Epoch 01 | Train Loss: 189.2321 | Val Loss: 32.0751 | Optimizer: AdamW\n",
      "Trial 457 | Epoch 02 | Train Loss: 71.7793 | Val Loss: 76.5072 | Optimizer: AdamW\n",
      "Trial 457 | Epoch 03 | Train Loss: 51.6015 | Val Loss: 35.0593 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:19,170] Trial 457 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 457 | Epoch 04 | Train Loss: 43.5068 | Val Loss: 37.8315 | Optimizer: AdamW\n",
      "Trial 457 | Epoch 05 | Train Loss: 40.0915 | Val Loss: 33.1243 | Optimizer: AdamW\n",
      "Trial 458 | Epoch 01 | Train Loss: 225.5076 | Val Loss: 34.9551 | Optimizer: AdamW\n",
      "Trial 458 | Epoch 02 | Train Loss: 45.8219 | Val Loss: 59.0468 | Optimizer: AdamW\n",
      "Trial 458 | Epoch 03 | Train Loss: 40.0931 | Val Loss: 34.6731 | Optimizer: AdamW\n",
      "Trial 458 | Epoch 04 | Train Loss: 43.5215 | Val Loss: 29.9757 | Optimizer: AdamW\n",
      "Trial 458 | Epoch 05 | Train Loss: 32.9944 | Val Loss: 28.6631 | Optimizer: AdamW\n",
      "Trial 458 | Epoch 06 | Train Loss: 29.9427 | Val Loss: 25.3459 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:19,879] Trial 458 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 458 | Epoch 07 | Train Loss: 27.7642 | Val Loss: 25.1105 | Optimizer: AdamW\n",
      "Trial 458 | Epoch 08 | Train Loss: 26.2142 | Val Loss: 35.2887 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:20,054] Trial 459 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 459 | Epoch 01 | Train Loss: 154.8841 | Val Loss: 56.3880 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 01 | Train Loss: 210.0288 | Val Loss: 27.2664 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 02 | Train Loss: 43.4225 | Val Loss: 33.6217 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 03 | Train Loss: 36.9954 | Val Loss: 32.3129 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 04 | Train Loss: 38.1988 | Val Loss: 25.7790 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 05 | Train Loss: 34.4461 | Val Loss: 24.9633 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 06 | Train Loss: 30.4604 | Val Loss: 23.6366 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 07 | Train Loss: 27.9070 | Val Loss: 27.5509 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 08 | Train Loss: 31.8672 | Val Loss: 22.4192 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 09 | Train Loss: 28.1925 | Val Loss: 21.3789 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 10 | Train Loss: 30.1732 | Val Loss: 20.6682 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 11 | Train Loss: 27.7249 | Val Loss: 24.8955 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 12 | Train Loss: 24.5290 | Val Loss: 20.5978 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 13 | Train Loss: 28.1642 | Val Loss: 23.3789 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 14 | Train Loss: 29.5568 | Val Loss: 33.3884 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 15 | Train Loss: 28.5951 | Val Loss: 31.0549 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 16 | Train Loss: 24.3493 | Val Loss: 22.3394 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 17 | Train Loss: 22.3846 | Val Loss: 20.0573 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 18 | Train Loss: 25.4891 | Val Loss: 18.3794 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 19 | Train Loss: 23.3702 | Val Loss: 18.8018 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 20 | Train Loss: 24.6385 | Val Loss: 22.0143 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 21 | Train Loss: 22.3003 | Val Loss: 20.6576 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 22 | Train Loss: 22.5185 | Val Loss: 16.9132 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 23 | Train Loss: 26.0880 | Val Loss: 17.7763 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 24 | Train Loss: 22.2454 | Val Loss: 16.3545 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 25 | Train Loss: 21.1726 | Val Loss: 16.1735 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 26 | Train Loss: 22.1276 | Val Loss: 18.1756 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 27 | Train Loss: 22.0497 | Val Loss: 28.6115 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 28 | Train Loss: 20.1972 | Val Loss: 22.1942 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 29 | Train Loss: 20.1242 | Val Loss: 16.9905 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 30 | Train Loss: 18.5883 | Val Loss: 18.4122 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 31 | Train Loss: 18.5780 | Val Loss: 19.9257 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 32 | Train Loss: 20.7557 | Val Loss: 20.0126 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 33 | Train Loss: 24.7337 | Val Loss: 26.7437 | Optimizer: AdamW\n",
      "Trial 460 | Epoch 34 | Train Loss: 21.3868 | Val Loss: 22.1161 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:22,899] Trial 460 finished with value: 16.17349267199757 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.38752265473680564, 'lr': 0.0004657913848936423, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 2.1692543046116824e-05}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 460 | Epoch 35 | Train Loss: 18.4095 | Val Loss: 16.4719 | Optimizer: AdamW\n",
      "Trial 460 - Early stopping triggered at epoch 35\n",
      "Trial 461 | Epoch 01 | Train Loss: 136.0007 | Val Loss: 51.4120 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:23,074] Trial 461 pruned. \n",
      "[I 2025-09-04 21:26:23,407] Trial 462 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 462 | Epoch 01 | Train Loss: 170.0893 | Val Loss: 37.6919 | Optimizer: AdamW\n",
      "Trial 462 | Epoch 02 | Train Loss: 60.5924 | Val Loss: 65.2118 | Optimizer: AdamW\n",
      "Trial 462 | Epoch 03 | Train Loss: 52.3683 | Val Loss: 37.8380 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:23,590] Trial 463 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 463 | Epoch 01 | Train Loss: 127.7436 | Val Loss: 52.1618 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 01 | Train Loss: 169.6796 | Val Loss: 33.6213 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 02 | Train Loss: 50.9499 | Val Loss: 38.1094 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 03 | Train Loss: 45.7173 | Val Loss: 41.7714 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 04 | Train Loss: 45.0938 | Val Loss: 29.4502 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 05 | Train Loss: 32.6734 | Val Loss: 25.2878 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 06 | Train Loss: 34.0597 | Val Loss: 25.8429 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 07 | Train Loss: 30.6420 | Val Loss: 31.1104 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 08 | Train Loss: 25.6087 | Val Loss: 21.5289 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 09 | Train Loss: 27.5509 | Val Loss: 21.0776 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 10 | Train Loss: 28.0489 | Val Loss: 21.1624 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 11 | Train Loss: 27.8073 | Val Loss: 23.0381 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 12 | Train Loss: 25.8148 | Val Loss: 23.2304 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 13 | Train Loss: 22.5411 | Val Loss: 30.5281 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 14 | Train Loss: 26.3268 | Val Loss: 21.2932 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 15 | Train Loss: 24.0128 | Val Loss: 19.6976 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 16 | Train Loss: 23.2747 | Val Loss: 18.2003 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 17 | Train Loss: 26.9313 | Val Loss: 18.3445 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 18 | Train Loss: 22.9937 | Val Loss: 17.8710 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 19 | Train Loss: 22.8195 | Val Loss: 16.9599 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 20 | Train Loss: 24.4963 | Val Loss: 22.1888 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 21 | Train Loss: 23.7333 | Val Loss: 22.9258 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 22 | Train Loss: 22.0859 | Val Loss: 33.0475 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 23 | Train Loss: 24.6770 | Val Loss: 22.2723 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 24 | Train Loss: 21.0163 | Val Loss: 21.6525 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 25 | Train Loss: 19.8478 | Val Loss: 16.8453 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 26 | Train Loss: 20.7650 | Val Loss: 27.3254 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 27 | Train Loss: 20.0407 | Val Loss: 28.3565 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 28 | Train Loss: 23.8243 | Val Loss: 19.0277 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 29 | Train Loss: 17.7311 | Val Loss: 15.6957 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 30 | Train Loss: 22.3574 | Val Loss: 19.3101 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 31 | Train Loss: 20.8586 | Val Loss: 22.9257 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 32 | Train Loss: 21.8829 | Val Loss: 19.8573 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 33 | Train Loss: 21.9062 | Val Loss: 17.5993 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 34 | Train Loss: 24.9770 | Val Loss: 17.5040 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 35 | Train Loss: 20.3704 | Val Loss: 24.1291 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 36 | Train Loss: 23.0439 | Val Loss: 23.8633 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 37 | Train Loss: 17.9358 | Val Loss: 18.1889 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:26,758] Trial 464 finished with value: 15.695714229490699 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.3847369190905062, 'lr': 0.0006927562043496528, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 9.481155802701844e-05}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 464 | Epoch 38 | Train Loss: 17.8387 | Val Loss: 29.2283 | Optimizer: AdamW\n",
      "Trial 464 | Epoch 39 | Train Loss: 19.6537 | Val Loss: 21.4622 | Optimizer: AdamW\n",
      "Trial 464 - Early stopping triggered at epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:26,937] Trial 465 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 465 | Epoch 01 | Train Loss: 171.3046 | Val Loss: 38.5478 | Optimizer: AdamW\n",
      "Trial 466 | Epoch 01 | NaN loss detected so pruning trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:27,098] Trial 466 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 467 | Epoch 01 | Train Loss: 120.2856 | Val Loss: 36.0390 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 02 | Train Loss: 45.9539 | Val Loss: 38.8752 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 03 | Train Loss: 40.9198 | Val Loss: 36.7336 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 04 | Train Loss: 34.3555 | Val Loss: 28.9292 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 05 | Train Loss: 28.8771 | Val Loss: 23.7612 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 06 | Train Loss: 26.9888 | Val Loss: 21.8202 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 07 | Train Loss: 28.9629 | Val Loss: 21.1545 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 08 | Train Loss: 24.6230 | Val Loss: 20.9536 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 09 | Train Loss: 25.0998 | Val Loss: 28.5483 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 10 | Train Loss: 23.4647 | Val Loss: 23.3664 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 11 | Train Loss: 25.9982 | Val Loss: 20.2203 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 12 | Train Loss: 24.3641 | Val Loss: 19.4242 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 13 | Train Loss: 24.0006 | Val Loss: 19.4638 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 14 | Train Loss: 23.0202 | Val Loss: 22.3370 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 15 | Train Loss: 22.0759 | Val Loss: 17.9076 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 16 | Train Loss: 21.9265 | Val Loss: 17.4837 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 17 | Train Loss: 21.0395 | Val Loss: 16.4906 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 18 | Train Loss: 22.6331 | Val Loss: 17.2370 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 19 | Train Loss: 21.3079 | Val Loss: 21.2355 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 20 | Train Loss: 19.0861 | Val Loss: 17.8041 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 21 | Train Loss: 18.8450 | Val Loss: 21.3984 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 22 | Train Loss: 23.5052 | Val Loss: 24.3816 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 23 | Train Loss: 22.9211 | Val Loss: 29.3329 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 24 | Train Loss: 22.7915 | Val Loss: 36.9944 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 25 | Train Loss: 23.5345 | Val Loss: 16.5642 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 26 | Train Loss: 21.1291 | Val Loss: 16.3893 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 27 | Train Loss: 19.8062 | Val Loss: 21.1086 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 28 | Train Loss: 20.1135 | Val Loss: 23.6590 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 29 | Train Loss: 17.1387 | Val Loss: 17.4135 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 30 | Train Loss: 19.1466 | Val Loss: 14.8828 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 31 | Train Loss: 18.9364 | Val Loss: 15.6191 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 32 | Train Loss: 18.9014 | Val Loss: 20.1929 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 33 | Train Loss: 20.4822 | Val Loss: 22.3865 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 34 | Train Loss: 17.5733 | Val Loss: 19.2715 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 35 | Train Loss: 18.2964 | Val Loss: 17.4048 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 36 | Train Loss: 18.5360 | Val Loss: 19.3495 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 37 | Train Loss: 17.4346 | Val Loss: 17.3455 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 38 | Train Loss: 18.1275 | Val Loss: 17.6135 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 39 | Train Loss: 16.4866 | Val Loss: 20.9947 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 40 | Train Loss: 16.5661 | Val Loss: 14.3305 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 41 | Train Loss: 15.4212 | Val Loss: 13.4659 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 42 | Train Loss: 15.7114 | Val Loss: 14.0669 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 43 | Train Loss: 16.0742 | Val Loss: 14.5849 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 44 | Train Loss: 18.6600 | Val Loss: 18.1642 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 45 | Train Loss: 17.2705 | Val Loss: 18.4083 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 46 | Train Loss: 16.4333 | Val Loss: 13.9470 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 47 | Train Loss: 21.2295 | Val Loss: 17.1382 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 48 | Train Loss: 20.5698 | Val Loss: 21.8248 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:31,189] Trial 467 finished with value: 13.46592564311454 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.32753050280126955, 'lr': 0.0005965819640688847, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 4.3582446833661025e-06}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 467 | Epoch 49 | Train Loss: 17.1056 | Val Loss: 18.3994 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 50 | Train Loss: 17.0079 | Val Loss: 25.8722 | Optimizer: AdamW\n",
      "Trial 467 | Epoch 51 | Train Loss: 17.1140 | Val Loss: 22.0838 | Optimizer: AdamW\n",
      "Trial 467 - Early stopping triggered at epoch 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:31,365] Trial 468 pruned. \n",
      "[I 2025-09-04 21:26:31,507] Trial 469 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 468 | Epoch 01 | Train Loss: 166.3414 | Val Loss: 62.0779 | Optimizer: AdamW\n",
      "Trial 469 | Epoch 01 | Train Loss: 153.3804 | Val Loss: 51.2160 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:31,697] Trial 470 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 470 | Epoch 01 | Train Loss: 172.7904 | Val Loss: 58.0539 | Optimizer: AdamW\n",
      "Trial 471 | Epoch 01 | Train Loss: 184.6626 | Val Loss: 109.6046 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:31,875] Trial 471 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 472 | Epoch 01 | Train Loss: 138.6121 | Val Loss: 37.0358 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 02 | Train Loss: 42.3496 | Val Loss: 36.0013 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 03 | Train Loss: 38.4121 | Val Loss: 32.3733 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 04 | Train Loss: 36.1486 | Val Loss: 27.3935 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 05 | Train Loss: 29.2735 | Val Loss: 24.2208 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 06 | Train Loss: 24.5899 | Val Loss: 22.6114 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 07 | Train Loss: 22.3412 | Val Loss: 23.1243 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 08 | Train Loss: 23.8027 | Val Loss: 24.0229 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 09 | Train Loss: 25.1895 | Val Loss: 21.1914 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 10 | Train Loss: 25.3200 | Val Loss: 24.0951 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 11 | Train Loss: 24.8365 | Val Loss: 22.7717 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 12 | Train Loss: 23.9037 | Val Loss: 21.1638 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 13 | Train Loss: 21.7933 | Val Loss: 21.0373 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 14 | Train Loss: 21.8509 | Val Loss: 19.5056 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 15 | Train Loss: 23.7708 | Val Loss: 18.3034 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 16 | Train Loss: 22.5577 | Val Loss: 25.8719 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 17 | Train Loss: 22.3390 | Val Loss: 26.4079 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 18 | Train Loss: 21.9732 | Val Loss: 18.3801 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 19 | Train Loss: 22.5456 | Val Loss: 18.7410 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 20 | Train Loss: 19.9089 | Val Loss: 16.9196 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 21 | Train Loss: 19.7035 | Val Loss: 15.4786 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 22 | Train Loss: 21.5771 | Val Loss: 15.3885 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 23 | Train Loss: 20.4149 | Val Loss: 15.4016 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 24 | Train Loss: 16.7445 | Val Loss: 17.3791 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 25 | Train Loss: 19.0526 | Val Loss: 25.3031 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 26 | Train Loss: 19.4363 | Val Loss: 14.7433 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 27 | Train Loss: 16.2675 | Val Loss: 18.1000 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 28 | Train Loss: 17.2588 | Val Loss: 15.0610 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 29 | Train Loss: 18.1129 | Val Loss: 16.9592 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 30 | Train Loss: 17.6971 | Val Loss: 20.4516 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 31 | Train Loss: 16.6847 | Val Loss: 25.2950 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 32 | Train Loss: 16.0704 | Val Loss: 16.9963 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 33 | Train Loss: 16.3640 | Val Loss: 14.6886 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 34 | Train Loss: 17.6284 | Val Loss: 18.2698 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 35 | Train Loss: 16.3008 | Val Loss: 13.4291 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 36 | Train Loss: 19.7405 | Val Loss: 13.9115 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 37 | Train Loss: 18.3378 | Val Loss: 15.5379 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 38 | Train Loss: 16.3027 | Val Loss: 15.6703 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 39 | Train Loss: 19.4501 | Val Loss: 13.9142 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 40 | Train Loss: 19.1360 | Val Loss: 15.3929 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 41 | Train Loss: 18.5679 | Val Loss: 14.5307 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 42 | Train Loss: 16.4757 | Val Loss: 13.6574 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:35,500] Trial 472 finished with value: 13.429110038571242 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.25593973143533477, 'lr': 0.0006734500184595535, 'activation': 'GELU', 'optimizer': 'AdamW', 'weight_decay': 6.759399315542203e-05}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 472 | Epoch 43 | Train Loss: 15.9204 | Val Loss: 14.6373 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 44 | Train Loss: 16.3440 | Val Loss: 17.8557 | Optimizer: AdamW\n",
      "Trial 472 | Epoch 45 | Train Loss: 17.4337 | Val Loss: 18.6553 | Optimizer: AdamW\n",
      "Trial 472 - Early stopping triggered at epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:35,679] Trial 473 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 473 | Epoch 01 | Train Loss: 185.3054 | Val Loss: 56.3358 | Optimizer: AdamW\n",
      "Trial 474 | Epoch 01 | Train Loss: 159.5832 | Val Loss: 55.3276 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:35,881] Trial 474 pruned. \n",
      "[I 2025-09-04 21:26:36,017] Trial 475 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 475 | Epoch 01 | Train Loss: 102.6893 | Val Loss: 62.3280 | Optimizer: AdamW\n",
      "Trial 476 | Epoch 01 | Train Loss: 77024.1166 | Val Loss: 300.2364 | Optimizer: RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:36,195] Trial 476 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 477 | Epoch 01 | Train Loss: 132.0564 | Val Loss: 33.8129 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 02 | Train Loss: 47.2787 | Val Loss: 40.5465 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 03 | Train Loss: 43.2997 | Val Loss: 35.1448 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 04 | Train Loss: 35.0209 | Val Loss: 26.9886 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 05 | Train Loss: 29.5864 | Val Loss: 25.6009 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 06 | Train Loss: 25.9883 | Val Loss: 22.9729 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 07 | Train Loss: 24.5587 | Val Loss: 22.3781 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 08 | Train Loss: 24.3768 | Val Loss: 28.7956 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 09 | Train Loss: 24.2824 | Val Loss: 22.8858 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 10 | Train Loss: 22.3579 | Val Loss: 20.3113 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 11 | Train Loss: 20.8157 | Val Loss: 23.0202 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 12 | Train Loss: 20.5034 | Val Loss: 20.1539 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 13 | Train Loss: 21.5176 | Val Loss: 21.6844 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 14 | Train Loss: 22.5094 | Val Loss: 19.7229 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 15 | Train Loss: 22.3376 | Val Loss: 20.4606 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 16 | Train Loss: 21.7384 | Val Loss: 24.0170 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 17 | Train Loss: 20.0078 | Val Loss: 19.6835 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 18 | Train Loss: 20.8830 | Val Loss: 18.8225 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 19 | Train Loss: 22.8143 | Val Loss: 18.5309 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 20 | Train Loss: 20.8198 | Val Loss: 18.2420 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 21 | Train Loss: 19.1465 | Val Loss: 21.3603 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 22 | Train Loss: 21.1585 | Val Loss: 20.6417 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 23 | Train Loss: 20.3862 | Val Loss: 17.4188 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 24 | Train Loss: 18.6510 | Val Loss: 16.7277 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 25 | Train Loss: 21.9890 | Val Loss: 17.5845 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 26 | Train Loss: 17.3077 | Val Loss: 16.0425 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 27 | Train Loss: 19.9040 | Val Loss: 18.7094 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 28 | Train Loss: 20.3276 | Val Loss: 18.9029 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 29 | Train Loss: 19.3286 | Val Loss: 27.1380 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 30 | Train Loss: 21.0433 | Val Loss: 18.7141 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 31 | Train Loss: 18.2091 | Val Loss: 15.6635 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 32 | Train Loss: 18.6176 | Val Loss: 19.6353 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 33 | Train Loss: 20.2574 | Val Loss: 24.0113 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 34 | Train Loss: 18.9607 | Val Loss: 23.2245 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 35 | Train Loss: 19.0040 | Val Loss: 16.5379 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 36 | Train Loss: 17.6608 | Val Loss: 15.2473 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 37 | Train Loss: 17.7468 | Val Loss: 15.3415 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 38 | Train Loss: 17.2658 | Val Loss: 16.0368 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 39 | Train Loss: 17.7870 | Val Loss: 14.9916 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 40 | Train Loss: 16.2904 | Val Loss: 18.8280 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 41 | Train Loss: 16.0835 | Val Loss: 17.9585 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 42 | Train Loss: 15.6391 | Val Loss: 15.7148 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 43 | Train Loss: 15.2359 | Val Loss: 19.0763 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 44 | Train Loss: 15.3486 | Val Loss: 17.1321 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 45 | Train Loss: 14.8043 | Val Loss: 14.4367 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 46 | Train Loss: 15.6490 | Val Loss: 14.5349 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 47 | Train Loss: 13.9787 | Val Loss: 13.3719 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 48 | Train Loss: 16.1981 | Val Loss: 13.8340 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 49 | Train Loss: 14.1793 | Val Loss: 13.2143 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 50 | Train Loss: 15.0111 | Val Loss: 13.9311 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 51 | Train Loss: 15.2840 | Val Loss: 13.3459 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 52 | Train Loss: 16.0045 | Val Loss: 15.2333 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 53 | Train Loss: 13.4866 | Val Loss: 17.2686 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 54 | Train Loss: 13.8048 | Val Loss: 15.4428 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 55 | Train Loss: 14.8677 | Val Loss: 13.9778 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 56 | Train Loss: 14.4967 | Val Loss: 14.2183 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 57 | Train Loss: 12.8439 | Val Loss: 12.1298 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 58 | Train Loss: 14.6228 | Val Loss: 12.3261 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 59 | Train Loss: 13.3229 | Val Loss: 13.1789 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 60 | Train Loss: 13.8918 | Val Loss: 19.7585 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 61 | Train Loss: 13.4802 | Val Loss: 17.8503 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 62 | Train Loss: 13.7672 | Val Loss: 16.8382 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 63 | Train Loss: 12.5459 | Val Loss: 14.4518 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 64 | Train Loss: 12.0852 | Val Loss: 12.7276 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 65 | Train Loss: 11.9690 | Val Loss: 12.0310 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 66 | Train Loss: 11.2845 | Val Loss: 11.7704 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 67 | Train Loss: 11.6823 | Val Loss: 11.5968 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 68 | Train Loss: 10.6299 | Val Loss: 11.8842 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 69 | Train Loss: 13.6341 | Val Loss: 11.2553 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 70 | Train Loss: 12.6241 | Val Loss: 10.6214 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 71 | Train Loss: 11.4031 | Val Loss: 10.0851 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 72 | Train Loss: 13.0467 | Val Loss: 12.1601 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 73 | Train Loss: 11.5272 | Val Loss: 10.8317 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 74 | Train Loss: 11.2413 | Val Loss: 11.6474 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 75 | Train Loss: 10.3133 | Val Loss: 11.2547 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 76 | Train Loss: 10.9704 | Val Loss: 9.5926 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 77 | Train Loss: 10.8754 | Val Loss: 18.4070 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 78 | Train Loss: 11.2783 | Val Loss: 19.1259 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 79 | Train Loss: 12.7722 | Val Loss: 13.2907 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 80 | Train Loss: 13.5135 | Val Loss: 11.0174 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 81 | Train Loss: 10.5133 | Val Loss: 12.1575 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 82 | Train Loss: 12.4110 | Val Loss: 14.3018 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 83 | Train Loss: 11.9201 | Val Loss: 10.8528 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 84 | Train Loss: 11.4603 | Val Loss: 11.0643 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:43,042] Trial 477 finished with value: 9.59261414287536 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.25185775731601945, 'lr': 0.0006791275964628498, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 9.431764861233023e-05}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 477 | Epoch 85 | Train Loss: 10.7805 | Val Loss: 12.2065 | Optimizer: AdamW\n",
      "Trial 477 | Epoch 86 | Train Loss: 9.5861 | Val Loss: 12.0871 | Optimizer: AdamW\n",
      "Trial 477 - Early stopping triggered at epoch 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:43,219] Trial 478 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 478 | Epoch 01 | Train Loss: 142.9533 | Val Loss: 43.4185 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 01 | Train Loss: 144.5563 | Val Loss: 33.7555 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 02 | Train Loss: 33.5395 | Val Loss: 33.2904 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 03 | Train Loss: 39.2613 | Val Loss: 36.8898 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 04 | Train Loss: 36.8836 | Val Loss: 26.9047 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 05 | Train Loss: 33.4702 | Val Loss: 33.6206 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 06 | Train Loss: 30.5542 | Val Loss: 23.6394 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 07 | Train Loss: 28.4224 | Val Loss: 26.1467 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 08 | Train Loss: 26.7924 | Val Loss: 23.9461 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 09 | Train Loss: 24.8903 | Val Loss: 21.8621 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 10 | Train Loss: 24.1264 | Val Loss: 31.9222 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 11 | Train Loss: 25.9260 | Val Loss: 21.0708 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 12 | Train Loss: 24.1873 | Val Loss: 22.7860 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 13 | Train Loss: 23.5278 | Val Loss: 22.3952 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 14 | Train Loss: 21.9480 | Val Loss: 21.6170 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 15 | Train Loss: 21.9299 | Val Loss: 21.4199 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 16 | Train Loss: 20.7207 | Val Loss: 20.8621 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:44,804] Trial 479 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 479 | Epoch 17 | Train Loss: 20.2937 | Val Loss: 20.9708 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 18 | Train Loss: 21.9015 | Val Loss: 21.7238 | Optimizer: AdamW\n",
      "Trial 479 | Epoch 19 | Train Loss: 21.8857 | Val Loss: 23.7341 | Optimizer: AdamW\n",
      "Trial 480 | Epoch 01 | Train Loss: 176.3561 | Val Loss: 36.3604 | Optimizer: AdamW\n",
      "Trial 480 | Epoch 02 | Train Loss: 54.6429 | Val Loss: 70.0748 | Optimizer: AdamW\n",
      "Trial 480 | Epoch 03 | Train Loss: 52.4979 | Val Loss: 37.9422 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:45,218] Trial 480 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 480 | Epoch 04 | Train Loss: 39.7179 | Val Loss: 34.8679 | Optimizer: AdamW\n",
      "Trial 481 | Epoch 01 | Train Loss: 163.8935 | Val Loss: 38.0842 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:45,392] Trial 481 pruned. \n",
      "[I 2025-09-04 21:26:45,572] Trial 482 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 482 | Epoch 01 | Train Loss: 156.6357 | Val Loss: 57.1259 | Optimizer: AdamW\n",
      "Trial 483 | Epoch 01 | Train Loss: 145.4607 | Val Loss: 46.2845 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:45,748] Trial 483 pruned. \n",
      "[I 2025-09-04 21:26:45,924] Trial 484 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 484 | Epoch 01 | Train Loss: 118.1542 | Val Loss: 59.3058 | Optimizer: AdamW\n",
      "Trial 485 | Epoch 01 | Train Loss: 139.5189 | Val Loss: 46.7475 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:46,100] Trial 485 pruned. \n",
      "[I 2025-09-04 21:26:46,276] Trial 486 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 486 | Epoch 01 | Train Loss: 258.4436 | Val Loss: 255.2442 | Optimizer: AdamW\n",
      "Trial 487 | Epoch 01 | Train Loss: 177.1776 | Val Loss: 75.8540 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:46,450] Trial 487 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 488 | Epoch 01 | Train Loss: 174.9770 | Val Loss: 30.5479 | Optimizer: AdamW\n",
      "Trial 488 | Epoch 02 | Train Loss: 61.9558 | Val Loss: 82.4953 | Optimizer: AdamW\n",
      "Trial 488 | Epoch 03 | Train Loss: 60.3252 | Val Loss: 39.0677 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:46,939] Trial 488 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 488 | Epoch 04 | Train Loss: 47.7641 | Val Loss: 35.5291 | Optimizer: AdamW\n",
      "Trial 488 | Epoch 05 | Train Loss: 39.0186 | Val Loss: 37.1148 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:47,129] Trial 489 pruned. \n",
      "[I 2025-09-04 21:26:47,269] Trial 490 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 489 | Epoch 01 | Train Loss: 172.3082 | Val Loss: 51.9683 | Optimizer: AdamW\n",
      "Trial 490 | Epoch 01 | Train Loss: 113.7817 | Val Loss: 63.4650 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 01 | Train Loss: 140.9913 | Val Loss: 31.8642 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 02 | Train Loss: 37.1969 | Val Loss: 33.6874 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 03 | Train Loss: 40.7591 | Val Loss: 33.9182 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 04 | Train Loss: 33.6946 | Val Loss: 26.4623 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 05 | Train Loss: 33.3183 | Val Loss: 24.0936 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 06 | Train Loss: 28.3431 | Val Loss: 23.1056 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 07 | Train Loss: 29.6268 | Val Loss: 23.5525 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 08 | Train Loss: 28.7721 | Val Loss: 22.1594 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 09 | Train Loss: 24.8906 | Val Loss: 21.7497 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 10 | Train Loss: 26.0002 | Val Loss: 23.0296 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 11 | Train Loss: 26.5789 | Val Loss: 23.8875 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 12 | Train Loss: 24.7943 | Val Loss: 21.8755 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 13 | Train Loss: 24.0253 | Val Loss: 21.0904 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 14 | Train Loss: 27.8087 | Val Loss: 31.0874 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 15 | Train Loss: 26.8200 | Val Loss: 21.4336 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 16 | Train Loss: 28.0934 | Val Loss: 21.4041 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 17 | Train Loss: 23.2808 | Val Loss: 23.9361 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 18 | Train Loss: 24.3010 | Val Loss: 20.0717 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 19 | Train Loss: 22.1356 | Val Loss: 18.6495 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 20 | Train Loss: 22.6892 | Val Loss: 18.8678 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 21 | Train Loss: 22.1737 | Val Loss: 17.0649 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 22 | Train Loss: 20.8920 | Val Loss: 18.3689 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 23 | Train Loss: 22.0850 | Val Loss: 16.9609 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 24 | Train Loss: 22.0058 | Val Loss: 17.0365 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 25 | Train Loss: 24.0479 | Val Loss: 16.6930 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 26 | Train Loss: 22.5306 | Val Loss: 29.3753 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 27 | Train Loss: 22.3940 | Val Loss: 21.7727 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 28 | Train Loss: 19.9956 | Val Loss: 15.9805 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 29 | Train Loss: 17.0508 | Val Loss: 15.2496 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 30 | Train Loss: 19.2691 | Val Loss: 14.8859 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 31 | Train Loss: 19.3732 | Val Loss: 14.6409 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 32 | Train Loss: 22.2535 | Val Loss: 16.0825 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 33 | Train Loss: 21.1249 | Val Loss: 14.9502 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 34 | Train Loss: 18.3277 | Val Loss: 14.9727 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 35 | Train Loss: 19.3745 | Val Loss: 15.5494 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 36 | Train Loss: 21.0541 | Val Loss: 16.2466 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 37 | Train Loss: 17.7633 | Val Loss: 15.8240 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 38 | Train Loss: 19.8945 | Val Loss: 25.0984 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 39 | Train Loss: 18.9697 | Val Loss: 17.1737 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:50,583] Trial 491 finished with value: 14.640862503672034 and parameters: {'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.36865162795464285, 'lr': 0.0006644356119965982, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 1.416624565252356e-05}. Best is trial 272 with value: 8.336457074173097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 491 | Epoch 40 | Train Loss: 17.7475 | Val Loss: 24.5949 | Optimizer: AdamW\n",
      "Trial 491 | Epoch 41 | Train Loss: 19.7746 | Val Loss: 17.8961 | Optimizer: AdamW\n",
      "Trial 491 - Early stopping triggered at epoch 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:50,770] Trial 492 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 492 | Epoch 01 | NaN loss detected so pruning trial\n",
      "Trial 493 | Epoch 01 | Train Loss: 138.4306 | Val Loss: 52.9246 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:50,944] Trial 493 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 494 | Epoch 01 | Train Loss: 229.5785 | Val Loss: 27.4106 | Optimizer: AdamW\n",
      "Trial 494 | Epoch 02 | Train Loss: 68.0015 | Val Loss: 80.8519 | Optimizer: AdamW\n",
      "Trial 494 | Epoch 03 | Train Loss: 72.5176 | Val Loss: 56.8697 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:51,509] Trial 494 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 494 | Epoch 04 | Train Loss: 38.6256 | Val Loss: 29.2827 | Optimizer: AdamW\n",
      "Trial 494 | Epoch 05 | Train Loss: 36.6839 | Val Loss: 32.1156 | Optimizer: AdamW\n",
      "Trial 494 | Epoch 06 | Train Loss: 31.5892 | Val Loss: 29.3956 | Optimizer: AdamW\n",
      "Trial 495 | Epoch 01 | Train Loss: 159.5465 | Val Loss: 37.4053 | Optimizer: AdamW\n",
      "Trial 495 | Epoch 02 | Train Loss: 48.6530 | Val Loss: 53.9292 | Optimizer: AdamW\n",
      "Trial 495 | Epoch 03 | Train Loss: 42.6744 | Val Loss: 36.3573 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:51,913] Trial 495 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 495 | Epoch 04 | Train Loss: 42.3105 | Val Loss: 42.6634 | Optimizer: AdamW\n",
      "Trial 496 | Epoch 01 | Train Loss: 204.5268 | Val Loss: 47.3641 | Optimizer: AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:52,090] Trial 496 pruned. \n",
      "[I 2025-09-04 21:26:52,232] Trial 497 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 497 | Epoch 01 | Train Loss: 181.9246 | Val Loss: 47.7412 | Optimizer: AdamW\n",
      "Trial 498 | Epoch 01 | Train Loss: 178.0417 | Val Loss: 127.8885 | Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-04 21:26:52,408] Trial 498 pruned. \n",
      "[I 2025-09-04 21:26:52,584] Trial 499 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 499 | Epoch 01 | Train Loss: 211.1421 | Val Loss: 78.7468 | Optimizer: AdamW\n",
      "{'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.26835948201850623, 'lr': 0.0006858724864946909, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 1.2164081006161759e-05}\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          10,
          12,
          13,
          15,
          16,
          17,
          20,
          23,
          24,
          26,
          31,
          33,
          37,
          58,
          59,
          60,
          61,
          63,
          79,
          82,
          83,
          84,
          85,
          93,
          94,
          98,
          99,
          101,
          102,
          105,
          106,
          113,
          114,
          121,
          124,
          132,
          143,
          147,
          149,
          154,
          160,
          164,
          184,
          211,
          219,
          227,
          247,
          251,
          261,
          266,
          269,
          270,
          272,
          276,
          278,
          295,
          298,
          299,
          308,
          309,
          314,
          322,
          348,
          433,
          442,
          460,
          464,
          467,
          472,
          477,
          491
         ],
         "y": [
          19.12524386925426,
          34.07929231287018,
          49.5200790777439,
          21.228072864253345,
          23.573560389076793,
          15.720193653571897,
          16.18551901685513,
          16.268691442846283,
          16.11321484945654,
          17.036176603984057,
          16.528479723426386,
          15.11039302794914,
          15.67170184220725,
          17.375625951503352,
          16.52663712385224,
          15.900944097255303,
          16.987479923217276,
          17.419660025495823,
          18.378675825227567,
          15.858131369924157,
          15.320490627754026,
          16.054533741338467,
          15.147421030494256,
          15.908379360912292,
          9.134948959195517,
          11.503930750901137,
          10.371832537457225,
          9.798203173691665,
          15.266971277996776,
          16.736201464645262,
          12.814706848888862,
          15.095547412469134,
          14.861772777588387,
          13.01377438335884,
          10.456899410340844,
          15.308860716780996,
          13.73273016766804,
          11.597237742044092,
          11.842361892141946,
          11.728818932199866,
          11.516944877500457,
          13.458390577052667,
          14.368151478651093,
          14.77558566302788,
          9.124408535841035,
          15.113592093553,
          13.922429937657302,
          13.38084742693397,
          14.338142867979965,
          10.07298573052011,
          9.64368356534136,
          14.444800361385191,
          15.138944393251,
          15.794723882907775,
          15.11982818541488,
          12.392426103111205,
          13.564487077356354,
          8.336457074173097,
          14.019267376845445,
          10.63941675666871,
          10.133446631392813,
          14.044357284297789,
          15.983942737424277,
          19.213915065052063,
          10.053647929090795,
          8.979677227454458,
          10.452346972333707,
          15.11876135725316,
          13.162951577969682,
          11.340287317105425,
          16.17349267199757,
          15.695714229490699,
          13.46592564311454,
          13.429110038571242,
          9.59261414287536,
          14.640862503672034
         ]
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          null,
          19.12524386925426,
          19.12524386925426,
          19.12524386925426,
          19.12524386925426,
          19.12524386925426,
          19.12524386925426,
          19.12524386925426,
          19.12524386925426,
          19.12524386925426,
          15.720193653571897,
          15.720193653571897,
          15.720193653571897,
          15.720193653571897,
          15.720193653571897,
          15.720193653571897,
          15.720193653571897,
          15.720193653571897,
          15.720193653571897,
          15.720193653571897,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          15.11039302794914,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.134948959195517,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          9.124408535841035,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097,
          8.336457074173097
         ]
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [],
         "y": []
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "title": {
          "text": "Trial"
         }
        },
        "yaxis": {
         "title": {
          "text": "Objective Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "cliponaxis": false,
         "hovertemplate": [
          "dropout_rate (FloatDistribution): 0.015144398908947882<extra></extra>",
          "weight_decay (FloatDistribution): 0.016980717756419336<extra></extra>",
          "hidden_dim (CategoricalDistribution): 0.01916690474543807<extra></extra>",
          "activation (CategoricalDistribution): 0.02784379871779251<extra></extra>",
          "gnn_dim (CategoricalDistribution): 0.14884833253746485<extra></extra>",
          "lr (FloatDistribution): 0.30109683767588813<extra></extra>",
          "optimizer (CategoricalDistribution): 0.470919009658049<extra></extra>"
         ],
         "name": "Objective Value",
         "orientation": "h",
         "text": [
          "0.02",
          "0.02",
          "0.02",
          "0.03",
          "0.15",
          "0.30",
          "0.47"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          0.015144398908947882,
          0.016980717756419336,
          0.01916690474543807,
          0.02784379871779251,
          0.14884833253746485,
          0.30109683767588813,
          0.470919009658049
         ],
         "y": [
          "dropout_rate",
          "weight_decay",
          "hidden_dim",
          "activation",
          "gnn_dim",
          "lr",
          "optimizer"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Hyperparameter Importances"
        },
        "xaxis": {
         "title": {
          "text": "Hyperparameter Importance"
         }
        },
        "yaxis": {
         "title": {
          "text": "Hyperparameter"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial0",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial1",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64
         ],
         "y": [
          196.4705298198917,
          110.64569333704506,
          49.16561725275303,
          46.654779046531615,
          43.651205729662884,
          50.723773429064245,
          45.58877817789713,
          35.321241875004965,
          32.41144464074112,
          34.08261868236511,
          31.1624926435269,
          27.53349665509976,
          29.858700511901358,
          26.81124125845064,
          26.616415752627987,
          26.182467390851276,
          26.358774324742757,
          26.508248429957444,
          24.187063449766576,
          26.833434515852268,
          23.052628602438826,
          23.56373740405571,
          25.72992714052278,
          22.910587884546295,
          26.50366355151665,
          23.783674255619204,
          24.29451756361054,
          25.082900365193684,
          22.709200339588694,
          25.053463462891617,
          23.766574177315565,
          26.258172104998334,
          22.898703644915326,
          26.407779693603516,
          22.25839476856759,
          24.470780783552463,
          24.193142883176726,
          23.620134663775683,
          22.696712990117266,
          24.89354221995284,
          22.274054891694853,
          24.141489742248037,
          23.73573991728992,
          23.96503026698663,
          20.816432402385928,
          26.010146722560975,
          20.809458197616948,
          29.053529289679798,
          19.928404490152996,
          23.53892879951291,
          23.944348296498866,
          19.13023715290597,
          23.61056848851646,
          19.12524386925426,
          25.530514073565723,
          20.39390635296581,
          22.04714606060245,
          22.849957846044525,
          20.07295605419128,
          21.5968585130645,
          20.788449620812887,
          20.720297867689677,
          20.768254985654256,
          22.645074394660266
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial2",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          80.85592933592757,
          84.11626409515132,
          70.59127311396405,
          63.00406491659521,
          69.21599293530473,
          62.56842099941843,
          65.31564082944296,
          59.869112278387796,
          65.44151383686841,
          58.78246043755756,
          65.65996129726007,
          57.99320000749293,
          64.55655778714312,
          56.94472624615925,
          66.65667963415626,
          56.50032511765395,
          58.17332343745038,
          55.42975985519285,
          57.04664317185317,
          54.965557842719846,
          57.388139616183146,
          54.11928530437191,
          55.542687361802514,
          54.54385118562032,
          53.9704846638005,
          52.75588130175583,
          52.57686109465312,
          52.16873277493609,
          52.28806639880669,
          54.088211927956685,
          51.368555084476625,
          51.29779918019364,
          51.115587063921176,
          52.35954383912125,
          50.13786402756606,
          50.760316243985805,
          49.52869461803901,
          51.87645036030591,
          49.078051001075806,
          49.491068615176815,
          52.1153904674499,
          49.71061021138013,
          56.90890630086263,
          49.12332122306513,
          54.067974183617565,
          48.715398028614075,
          48.86135228474935,
          46.79089969541968,
          46.53302240759377,
          47.0284140672141,
          45.56362518062436,
          45.20918152971966,
          48.894659181920495,
          46.05897146705689,
          46.83941954325854,
          44.274166665426115,
          44.26488836024835,
          44.33185599102237,
          43.576313173867824,
          43.168595476848324,
          43.44435553822091,
          44.375106377330255,
          42.730477930084476,
          43.76594543457031,
          41.93330155349359,
          41.686555908947454,
          44.89130578389982,
          41.26499946718293,
          46.13897624442248,
          40.83905385955563,
          42.12105926265561,
          40.296220453774055,
          41.60511875927933,
          39.81705803599784,
          40.48535202770698,
          39.89451935620812,
          39.27960864121352,
          38.97910338301,
          38.751470472754505,
          39.506481449778484,
          38.09846415946154,
          37.73495340734963,
          37.998178016848684,
          38.9706576122501,
          37.07810023548157,
          40.949013996899616,
          36.985353779986625,
          39.17963719561817,
          36.560312813859646,
          35.82760007594659,
          35.65697832805355,
          35.326095053820104,
          36.73626505843992,
          34.91904080398684,
          38.181555119956414,
          34.39509069241159,
          37.24627059068137,
          34.07929231287018,
          35.961972430469544
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial3",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          63.21932384832119,
          61.82959427872324,
          71.53004957989948,
          61.844195327138515,
          64.23981376585921,
          62.09304794063413,
          60.74727295666206,
          61.79616599354318,
          60.79879928216702,
          61.3964844990552,
          61.162411759539346,
          59.53064225359661,
          62.57148798500619,
          59.8221348708238,
          59.582548560165776,
          60.353504149894405,
          58.60302138910061,
          59.59616498249333,
          58.621874026166715,
          59.32531784801948,
          58.43868863485693,
          57.63776717147207,
          58.413114284112204,
          57.91393276927917,
          58.16441664656973,
          58.00492924015696,
          57.08526980392332,
          59.16368593045367,
          56.72454300547034,
          56.60888476488067,
          58.196659553341746,
          57.36783593650756,
          57.32742197920636,
          56.11906479626167,
          56.62843279334588,
          56.90611713688548,
          55.76554135578434,
          55.9686335121713,
          56.61697403202212,
          55.86670173086771,
          55.496805020464144,
          56.42841736087954,
          55.07130128193677,
          56.24326423707047,
          55.50570216605334,
          54.75126154829816,
          55.51826939156385,
          54.684132149549036,
          54.517063202896736,
          54.392276298709035,
          55.59896323351356,
          55.351773021666986,
          54.08986018731342,
          54.01742460669541,
          54.2791236939469,
          53.87114352714725,
          56.10512133342464,
          53.76307482835723,
          53.927976065534885,
          53.60714222357525,
          53.733080670116394,
          54.67836938253263,
          52.98278343386767,
          53.148229490450724,
          52.99934052258003,
          52.83537860032989,
          52.7433855010242,
          52.9777123676083,
          53.62950543659489,
          52.54978269871658,
          53.8454306687766,
          52.06340597509369,
          51.94185647731874,
          52.346552561938275,
          51.755787112848544,
          51.64423187379914,
          51.54012317192264,
          51.78923679754986,
          51.47869668355802,
          52.54407802054553,
          51.46062764113511,
          51.76554718637854,
          50.84562546644754,
          51.24567320288681,
          50.96067295229532,
          50.61045514858835,
          50.73707797662998,
          50.35393660630637,
          50.54410183914309,
          51.1476225814199,
          50.527265192047366,
          50.09823971259885,
          49.79789867246054,
          51.780665048738804,
          49.77236833029646,
          49.817164041162506,
          49.5200790777439,
          50.02680383077482,
          49.53084703771079
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial4",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79
         ],
         "y": [
          250.6066727056736,
          187.09940977019022,
          137.35669019745617,
          101.27545643628129,
          77.1391120228341,
          62.36928877791738,
          55.0801376404801,
          52.31419577249667,
          50.6155495992521,
          48.34699720677322,
          45.69738564840177,
          42.97634852029444,
          40.49024262467051,
          37.877887880899074,
          35.54487293522532,
          33.52809564854071,
          32.4311268349004,
          30.807716106011615,
          30.10990727432375,
          29.435911566261353,
          27.916974370072527,
          27.48102492045581,
          27.72766249741965,
          26.57063023636981,
          25.691999807590392,
          25.810117488954127,
          25.12780279454177,
          25.066847312741164,
          24.72335325411665,
          24.70288315439612,
          25.31909717776911,
          24.46992470966122,
          24.037139613453935,
          23.965871841926884,
          24.647592513541866,
          23.927691808561,
          23.702524138659967,
          24.461150797401988,
          24.800188684851175,
          23.558168953996365,
          23.408524986205062,
          24.463660356475085,
          23.243449001777464,
          23.188075817697417,
          23.052377886888458,
          23.295573552449543,
          22.66857198389565,
          23.227533805661086,
          24.311228310189595,
          22.566236976685563,
          22.89462691206273,
          23.9975444514577,
          23.114576463776874,
          22.56256267888759,
          22.481271480157123,
          22.743478433872625,
          22.96471439144476,
          22.5655346381955,
          22.702783026346346,
          22.516805974448598,
          22.171401124659592,
          22.407652583548693,
          22.116769480511426,
          21.810359954833984,
          22.012806683051878,
          22.215179427852476,
          22.045960790742704,
          21.845326276329473,
          21.228072864253345,
          23.314784802072417,
          21.633362188571837,
          21.756059646606445,
          22.404291106433405,
          21.650058854886186,
          21.769372443842695,
          22.765359987088335,
          21.40504241571194,
          21.44673530454558,
          22.138021949830094
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial5",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          286.34718335159425,
          268.4984855341717,
          251.49215301265562,
          235.26064108251555,
          219.8172494531647,
          205.05783329165078,
          191.13584465709158,
          178.02672632729136,
          165.60197523163586,
          153.80422638683785,
          142.81225313016068,
          132.38919774497427,
          122.31292191172034,
          112.6443154559872,
          103.41296064175242,
          94.42005616474927,
          85.76429053826061,
          77.38516806005462,
          69.44397202158362,
          62.00791946659243,
          55.43800357105286,
          49.68770127955491,
          44.86429447081031,
          41.617320704266305,
          39.58364455680537,
          38.2802619934082,
          37.56775434230401,
          37.01303264959072,
          36.52133746263458,
          36.05461725374548,
          35.62791396350396,
          35.19877117436106,
          34.860806255805784,
          34.502245135423614,
          34.0404110109903,
          33.50050803703991,
          33.03331412338629,
          32.64644799581388,
          32.275720162120294,
          32.00755286798245,
          31.618106082202942,
          31.22794898738706,
          30.890177982609448,
          30.55357410461922,
          30.19989925477563,
          29.90291519475177,
          29.69775446449838,
          29.50453334901391,
          29.22505619080086,
          28.90915211623277,
          28.612324210686413,
          28.346840819692225,
          28.081292982023907,
          27.86438495356862,
          27.71130858010393,
          27.453539654491394,
          27.386296404086476,
          27.226189574575038,
          27.112954907300995,
          26.842805040561085,
          26.75127445003851,
          26.67925654775728,
          26.527996916111892,
          26.185249033982192,
          25.948147145713246,
          25.828255940258988,
          25.77477343489484,
          25.75009515033505,
          25.671414677689715,
          25.336224377639894,
          25.213585085985137,
          25.188657512509725,
          25.18651856833357,
          25.155343156519944,
          25.106106223129643,
          25.059522938922168,
          25.004927208753138,
          24.869366514004344,
          24.70361447528126,
          24.616898777039072,
          24.64322339810007,
          24.470598251838993,
          24.33189241672919,
          24.32225877095044,
          24.246121212718933,
          24.20531297699223,
          24.185415578082324,
          24.357645748107412,
          24.38048432512981,
          24.15296746850983,
          24.003462349496235,
          23.9173441941176,
          23.93743099429743,
          23.92504200508924,
          24.10328447915674,
          24.04819085346005,
          23.81589433623523,
          23.678343439489847,
          23.573560389076793
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial6",
         "type": "scatter",
         "x": [
          1,
          2
         ],
         "y": [
          195.21094804469163,
          145.29006623058785
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial7",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          38450.7724847561
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial8",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7
         ],
         "y": [
          70.85744302641085,
          63.258424619349036,
          87.32742129690278,
          62.042571308167,
          71.00566994271627,
          61.852925091255,
          66.85842154278019
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial9",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          312.5605250412856
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial10",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          183.3895745006034,
          91.89328213823521,
          41.63770585719163,
          45.99490012192145,
          43.06772963981318,
          43.37663827291349,
          37.975696222568914,
          33.67098009683252,
          32.95906742995347,
          29.815724132506826,
          28.17889917575247,
          28.530576876508512,
          25.931508986930538,
          28.29259656890621,
          24.773690913750873,
          24.430941217313936,
          24.632447234983367,
          23.335936817696425,
          23.962027541990203,
          22.904517072972244,
          24.657643170860723,
          22.396807848922606,
          25.825360352430888,
          22.011083897536363,
          24.524327146328563,
          21.85275284061587,
          22.942898680524127,
          22.037718330941548,
          22.0304408965072,
          22.419641494750977,
          21.89756684962327,
          21.567757350642506,
          21.95150344352412,
          21.862864471063382,
          21.40830391984645,
          20.462999840092852,
          22.16905662102428,
          20.230516332920974,
          20.95510223822865,
          20.65701481578796,
          21.138567327483884,
          20.060863316543703,
          19.308500483753235,
          22.257776136320782,
          20.049959632439343,
          19.936151496763152,
          19.47540224276907,
          23.22671093204157,
          18.466915316698028,
          21.268892691387393,
          18.48016101558034,
          22.647512513447584,
          18.296476092765,
          21.313541427860415,
          18.152360427670363,
          27.295736979662887,
          17.906981491461032,
          20.750848591812257,
          18.73063336349115,
          19.653883112155324,
          18.286625009242112,
          19.576967999217956,
          17.73268043704149,
          18.331400917797552,
          21.51103802812778,
          17.255109151204426,
          23.9700133005778,
          17.345405384777038,
          20.500309827851087,
          17.19202138544098,
          20.344770384997858,
          17.09431435809872,
          17.989587768306578,
          18.487849654220952,
          17.293576279306798,
          16.760978171495886,
          18.40438238004359,
          16.332421155479864,
          18.081140867093715,
          17.407170791936114,
          16.53386402130127,
          16.329057088712368,
          16.92803278201964,
          19.690649102373822,
          16.20200992987408,
          16.87564524983972,
          17.476124848776717,
          19.16626010677679,
          15.791302425105398,
          20.28623805007314,
          18.108505621189025,
          16.643603115546995,
          16.960485155989485,
          16.25069329021423,
          15.994283575352615,
          19.234521951132674,
          15.720193653571897,
          16.621493595402416,
          19.334889303377974
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial11",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          227.28702489341177
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial12",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90
         ],
         "y": [
          157.18845286795764,
          86.24258336012926,
          43.29248685759257,
          47.739991552461454,
          43.19318368182919,
          44.37457321911323,
          39.50784881715852,
          34.17956510404261,
          32.253926827655576,
          30.133390318087447,
          28.042009632761886,
          27.819491084029035,
          25.741177659693772,
          25.254798299897978,
          26.488074465495785,
          24.010106482156893,
          24.230251513845552,
          24.240182008200545,
          23.149959207550296,
          23.85138288358363,
          23.22980049567494,
          22.607070287068684,
          22.129004517221837,
          23.09681196135234,
          22.568647105519364,
          21.69549617147058,
          23.036859931015385,
          21.146145820617676,
          24.414230687831477,
          20.84748765123569,
          22.527630007363918,
          20.759623868678645,
          23.0998364425287,
          20.215672283637815,
          23.027023827157368,
          19.866644324325932,
          22.4742715727023,
          19.65353380373823,
          22.61385781202859,
          19.256086651871843,
          20.53426971280478,
          22.10934603311182,
          19.078359185195552,
          22.494499097994673,
          18.63631935429767,
          24.70516673142348,
          18.650875564513168,
          22.086482427953705,
          18.480875674302016,
          21.92476185744371,
          18.86834404332851,
          19.706268504383118,
          18.17514225719421,
          21.198862866657535,
          18.287832322159435,
          21.07035309706277,
          17.587103804921718,
          20.184786191800747,
          17.38845436747481,
          19.67288055264853,
          17.671393875184098,
          17.869463579441472,
          19.13787413031105,
          17.241545173210827,
          20.041956575905406,
          16.978504080113357,
          17.799287741746358,
          20.402921327730503,
          17.20197102306335,
          17.40704970631173,
          16.801365146792033,
          16.898297418423784,
          20.06797239257068,
          16.37112154611727,
          16.55887737119101,
          18.719527461664462,
          16.836754186366633,
          16.535360429345108,
          19.5070006362791,
          16.18551901685513,
          22.36008050189755,
          16.623614489547606,
          16.703444705746037,
          17.890668822497858,
          16.733294409465014,
          18.00997795322077,
          17.23398921935539,
          16.47503036405982,
          23.260298860751515,
          16.96945940963621
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial13",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57
         ],
         "y": [
          117.47344958297606,
          45.83998784010972,
          50.8444285819201,
          49.54717915232589,
          48.62516362880304,
          37.01612677225253,
          31.822060499734025,
          32.761970613060925,
          27.686551675563905,
          32.300569084601676,
          25.052243922784076,
          30.069964959369443,
          23.204755860615553,
          24.649867546267625,
          22.441035852199647,
          24.57216962178548,
          21.869427285543303,
          23.276162325851317,
          21.696127263511098,
          24.44046550843774,
          21.075877732377712,
          21.616963332261495,
          21.211101656037616,
          20.829421997070312,
          20.22876568150714,
          27.01736969676444,
          20.256308842480667,
          25.811823449483732,
          19.579558256195813,
          21.360909857401033,
          19.97244672852803,
          20.849386905266986,
          21.205070278508877,
          19.4227152878676,
          20.255415179865146,
          19.554471349328512,
          18.513011730783354,
          19.13534108603873,
          18.881654514529842,
          20.10532447380748,
          17.952458893380513,
          18.26242298033179,
          16.78751170150633,
          19.325999081619386,
          21.44372420582345,
          18.270024539978525,
          16.268691442846283,
          16.561141456045757,
          17.48800839835066,
          18.518805123926178,
          17.9548169267856,
          22.275233958794818,
          16.598322317852237,
          19.180343441846894,
          17.781773652487654,
          16.859312383140004,
          16.538435633589582
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial14",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          249.31497055922097
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial15",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65
         ],
         "y": [
          108.88080212352722,
          28.11306910010857,
          57.27442060641157,
          30.671110664925923,
          30.453054242017792,
          38.38797378540039,
          27.963251501564088,
          27.6787252038475,
          27.42735316889073,
          27.2128168121586,
          24.867244518869292,
          27.20243633859526,
          24.047653012159394,
          23.181152033612012,
          23.42870324607787,
          25.677846598431348,
          22.544365316871705,
          22.39572405621288,
          25.97162921254228,
          23.42921115906258,
          21.671290948138974,
          21.602498837602816,
          20.26965726681841,
          20.765111302941794,
          20.952538063855677,
          19.642300923665363,
          22.105622051207998,
          26.559786308102492,
          22.10347380289217,
          24.600755893118013,
          19.032670160619222,
          18.736417095835616,
          27.70543924967448,
          22.646256113440042,
          19.005873416497455,
          18.676837169057954,
          17.5543978931458,
          26.7497189374474,
          24.938888193146,
          21.826378333859328,
          19.26527354387733,
          18.832383582262487,
          18.23006363225177,
          21.628772689075006,
          20.950626373291016,
          18.827608721043035,
          16.515953707501172,
          16.38625281419211,
          16.634427566838458,
          21.4314987678838,
          22.82910182611729,
          19.293652705060758,
          19.7348802768118,
          16.193251036046966,
          16.11321484945654,
          16.636719633893268,
          17.359766169292172,
          17.52767613263634,
          28.144745400281458,
          27.881086396008,
          18.228339451115307,
          16.401963474304697,
          20.036641175184794,
          18.745719467721333,
          16.540799939535496
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial16",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
         ],
         "y": [
          48.37583312368005,
          52.24217835092932,
          34.471343978633726,
          36.116353849085364,
          28.866317764530336,
          30.39570005153253,
          29.797113310030806,
          25.69928001775974,
          25.13434372878656,
          31.84617777568538,
          25.933217816236542,
          23.590354872912897,
          22.509245322002627,
          27.425577225723888,
          26.44015935572182,
          22.44724199248523,
          21.395539446574887,
          21.318301069058055,
          19.12523533270611,
          19.467334979917943,
          18.480226749327123,
          19.722294132883956,
          17.58942387743694,
          17.79738516923858,
          17.679693586458036,
          17.036176603984057,
          17.920152563389724,
          17.88718340648868,
          17.230532661686098,
          24.27708346669267,
          18.949357218858673,
          19.180157273765502,
          17.969641367594402,
          24.962769841760153,
          20.815102864087113,
          17.75347898839935
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial17",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58
         ],
         "y": [
          93.07664520759893,
          30.98244772500139,
          48.55400094753358,
          29.78753854394928,
          40.32337542278011,
          26.734182109677693,
          30.511524091891157,
          25.286154460131637,
          30.6298431148374,
          23.718657920031042,
          24.445656675633376,
          29.504938342706943,
          23.784556877322313,
          26.49633396737944,
          26.11990937954042,
          22.283575073490297,
          28.15683957231723,
          29.133660944496715,
          20.812592917341526,
          21.363900549043485,
          25.15211210793596,
          19.544963549792282,
          20.17265489624768,
          20.343902781726868,
          23.81419307429616,
          19.960716177777545,
          18.634011532233014,
          21.82497592088653,
          26.189503568943923,
          19.194582047501232,
          18.03055393405077,
          18.234615225132888,
          21.31344651012886,
          23.37463012943423,
          25.621175331798025,
          21.946107538734996,
          17.558995572532094,
          19.579036914236177,
          18.749754874686886,
          20.529181829312954,
          21.75159091484256,
          22.24732716878255,
          20.141166221804735,
          22.495736703640077,
          17.388665160512538,
          20.574401452289365,
          17.9669346460482,
          16.528479723426386,
          16.794122029126175,
          23.82782435223339,
          24.067710519806155,
          26.291305650540483,
          19.992286496046113,
          17.661880128751925,
          20.859175813876515,
          21.452654815301663,
          17.66597305080755,
          16.56697712099649
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial18",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          288.7285982457603
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial19",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          209.1543569177147
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial20",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38
         ],
         "y": [
          107.99664846280726,
          63.712109294364126,
          45.39462928461835,
          67.22607856068184,
          39.80543763075418,
          45.13962911590328,
          29.41919911392336,
          25.631942020199162,
          26.478073430255176,
          24.63005087627628,
          51.309272859154675,
          32.98929619207615,
          22.061619053042033,
          20.5637746671351,
          29.258592729646015,
          25.824065045612613,
          22.812581364701433,
          19.671143748895908,
          19.027056236577227,
          17.486821166868133,
          26.523185032169994,
          29.93580490980691,
          26.0211267083641,
          20.098086814570234,
          20.718687925881486,
          18.987589611270565,
          16.542381193579697,
          15.11039302794914,
          20.310959560115162,
          20.123662483401414,
          17.823246289074905,
          16.90521760490852,
          18.263421624656615,
          26.184895166536656,
          24.349817570632066,
          15.480389641552437,
          15.81308225306069,
          16.723461081342
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial21",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          55.20688843145603,
          87.07523953817724,
          50.794121967098576
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial22",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          145.09512316696043
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial23",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67
         ],
         "y": [
          43.54359270111332,
          37.1717927824191,
          31.425561098548457,
          27.49415116969163,
          29.98751877575386,
          24.461971081369292,
          28.912599703160726,
          29.652626968011624,
          24.77041608918973,
          26.879218124761813,
          20.837151853049672,
          20.463472420607157,
          28.10915269308943,
          19.586007141485446,
          19.43968783743013,
          19.489940495995,
          20.858022976696976,
          18.16090881533739,
          17.829663811660396,
          17.640367050481036,
          17.18485107266806,
          18.374149911771944,
          17.143228693706234,
          17.078326620706697,
          17.273168912748012,
          17.76879197407544,
          20.240589134092254,
          20.05879899156772,
          19.701042857596544,
          24.374766202476934,
          27.21517948988007,
          44.591870842910396,
          23.268896071891476,
          16.756325776014872,
          17.49882413507477,
          19.882343307743227,
          21.617051194353802,
          21.272853246549282,
          20.235642673523447,
          27.494857384906553,
          20.466228981328204,
          17.786038995758304,
          16.333283812049928,
          17.585109664172663,
          21.74815647776534,
          20.226270008862503,
          19.14627777657858,
          27.0016862667673,
          34.636158191091646,
          26.217301205890934,
          18.051139335322187,
          18.81511771969679,
          16.191417391707258,
          15.922394116719564,
          16.478530713213168,
          16.8761483711925,
          15.67170184220725,
          19.994731484389888,
          21.580282195796812,
          23.576465870306745,
          18.726421325187374,
          19.223529877701427,
          25.84549708482696,
          16.357272062844377,
          22.87810798582992,
          21.941732964864592,
          18.447148284291835
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial24",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39
         ],
         "y": [
          58.560155263761196,
          71.36054645321234,
          31.19492978972148,
          40.93058435703681,
          28.329331653874096,
          37.71741476291564,
          25.81097926938437,
          38.983027016244286,
          27.37642393654924,
          23.44345137743446,
          32.04296440806815,
          28.591016815929876,
          25.03120037792175,
          24.536289323636186,
          22.616740529130144,
          27.37802254281393,
          30.846237198124086,
          27.61230130699592,
          24.850482397932346,
          21.114937231792666,
          20.207353653946544,
          21.36216703275355,
          28.714312220007425,
          25.528116986034362,
          21.098193688121267,
          17.50255205573105,
          17.716156075640423,
          17.400633222688505,
          17.375625951503352,
          18.100351822085496,
          20.334086774810544,
          20.158851398685115,
          31.99166037396687,
          31.341699708768022,
          24.833594500533934,
          20.929245088158584,
          31.805282933925227,
          22.370111667043794,
          21.828268252737153
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial25",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          59.39306122694558,
          65.97955164095251,
          63.19757362303695
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial26",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37
         ],
         "y": [
          28.503694890960446,
          77.27394662252287,
          34.286518158951424,
          48.32169599455546,
          30.741933016273066,
          31.003770998822965,
          28.99516280879819,
          29.29908874945912,
          32.90288066088669,
          25.137418095658465,
          27.10375638512092,
          33.03743669463367,
          32.884056292898286,
          24.420430191164094,
          28.08774534085902,
          23.870777951992626,
          35.93566834054342,
          29.546254072732072,
          28.99665779796073,
          20.646855540391876,
          25.945860203688706,
          18.293456542782668,
          16.634992111019972,
          19.38288597168961,
          26.09493861935003,
          22.262128473297366,
          16.52663712385224,
          17.31106020764607,
          16.603149739707387,
          25.048397560429766,
          17.628184698461517,
          17.09619826029956,
          17.824914691894033,
          17.851710606396683,
          19.323854570466327,
          16.882863478931956,
          17.924555902558613
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial27",
         "type": "scatter",
         "x": [
          1,
          2
         ],
         "y": [
          102.43863957102705,
          71.99829585377763
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial28",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          186.24284089871537
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial29",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial30",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          65.22651312603215,
          59.85436828737336,
          60.51884879135504
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial31",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71
         ],
         "y": [
          94.05558640394753,
          29.3706310396272,
          55.04495189635734,
          30.806377627985263,
          32.27815530358291,
          30.939923139122442,
          27.231671418600936,
          28.840412605099562,
          25.72992228686325,
          26.358603702328068,
          23.49148770464145,
          25.070566689095845,
          25.585987385695542,
          25.864914560705667,
          22.023563989778843,
          22.249591145089003,
          26.84225981797629,
          20.66031521897975,
          20.05618443527842,
          20.418151677139406,
          22.414307602052766,
          26.16233002267233,
          23.525143258939913,
          19.280457930836253,
          23.283346284695757,
          21.231438629026336,
          19.07225849182625,
          18.880839146249663,
          19.184424446850286,
          20.827683425531156,
          18.115348629835175,
          19.988847670516346,
          23.950479522953188,
          21.15895969111745,
          22.374319169579483,
          26.617459320440524,
          21.661376270821425,
          16.75069781047542,
          18.021154124562333,
          16.728854830672102,
          18.712096501172073,
          17.704422245180705,
          16.407132358085818,
          16.479216443813915,
          17.000425959021097,
          16.12673084910323,
          18.322367714672552,
          21.94131736445233,
          27.65906586685801,
          18.71168276158775,
          15.961953581833258,
          16.62778972997898,
          15.985206611757356,
          20.48895941323381,
          18.80007260795531,
          16.99608404268094,
          21.407705973803512,
          20.268175264684164,
          18.727785226775378,
          22.902266153475132,
          15.900944097255303,
          18.54026842504982,
          16.944759399910282,
          20.409625030145413,
          22.680096153321305,
          16.36807232368283,
          15.901360178381447,
          16.93468159001048,
          18.59739255517479,
          18.241367293567194,
          18.612624842946122
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial32",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          189.84289712053004
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial33",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49
         ],
         "y": [
          39.92283607110745,
          58.98544128542024,
          37.00015710039837,
          47.893256737933896,
          32.16000178577454,
          42.724407165031124,
          27.048965438594664,
          25.85480637278983,
          33.904726803787355,
          27.453646869194216,
          22.828575025729048,
          23.5579238519436,
          41.13459439781623,
          31.143294512740965,
          21.241289518712982,
          21.17429423138378,
          21.3554318435793,
          20.784124777569033,
          20.446927853716097,
          20.383668519617096,
          28.42731739447369,
          25.79776604194951,
          18.6561988892594,
          18.326079446125807,
          17.717109098667052,
          17.57326711484087,
          18.375038868043482,
          20.020019717332794,
          20.267275476843363,
          21.484656868911372,
          20.09726814332047,
          20.43176533148541,
          21.108856836954754,
          17.56439515245639,
          22.440143988384463,
          32.52210063469119,
          25.404554165475737,
          19.598473727218504,
          16.987479923217276,
          17.657261724394512,
          17.671564846504026,
          24.706085422174716,
          20.158068928292128,
          23.99340485363472,
          19.22932292969246,
          28.275163045743618,
          20.871227543528487,
          28.186362956597552,
          23.12041188061722
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial34",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          135.82832497697535
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial35",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          579.9077341963605
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial36",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          195.57844369779758
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial37",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
         ],
         "y": [
          28.068012035959136,
          97.59591091745268,
          51.370628108823205,
          45.07946901398946,
          44.294699785186026,
          33.4286711312891,
          31.468807530596973,
          26.49161512483426,
          25.91715598687893,
          25.153132803071806,
          28.564898079972927,
          24.59270440078363,
          30.478442339393183,
          28.436529004476903,
          20.309641155770155,
          19.679726011384794,
          20.898885021364787,
          19.867612303757085,
          24.944632088265767,
          25.110283068525113,
          26.16730922605933,
          20.04573131189114,
          22.21813487231247,
          21.617789710440288,
          18.239277467495057,
          17.419660025495823,
          19.397215819940335,
          20.44466381538205,
          17.706591086659007,
          28.239634397553235,
          24.721866018403837,
          22.739560166025548,
          18.66115468498168,
          21.018656521308714,
          22.365020689925526,
          21.899618195324408
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial38",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          239.598235215598
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial39",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          211.19671333126905
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial40",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          311.7246004430259
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial41",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          164.37586739392785
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial42",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          159.12792236824345
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial43",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          190.7773495805942
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial44",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          211.20845279848672
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial45",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          104.04375395736074
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial46",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          64.42875941206769,
          56.474461346137815,
          58.790635147715
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial47",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          169.46449217757558
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial48",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          70.2958389220199,
          59.86140113148263,
          64.43756745501263
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial49",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22
         ],
         "y": [
          62.62028447593131,
          27.1637643953649,
          26.10655745839685,
          29.81901682101614,
          23.93293565269408,
          23.806215487844575,
          24.153258455478078,
          25.60024492527411,
          23.648348180259145,
          24.38766726051889,
          23.2163455001707,
          23.397432978560285,
          27.568149442595196,
          22.895902463091097,
          23.975022385760052,
          23.132940431920492,
          24.0600841646272,
          22.753359678314954,
          22.557153500192534,
          23.807328619608064,
          23.906119912620483,
          23.962885926409466
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial50",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          74.30584248488512,
          60.76326962602817,
          62.322453134428194
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial51",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          229.11370514660348
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial52",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          265.69769981818473
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial53",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          253.8339844990552
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial54",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          245.13516954871696
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial55",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          168.17511110383322
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial56",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial57",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          179.9642418341908
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial58",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25
         ],
         "y": [
          52.083765851772895,
          40.06695758230318,
          49.04755246542334,
          32.14475620859037,
          29.649440222639377,
          24.150971017232756,
          22.4476857224131,
          26.510358267683323,
          20.96191484559842,
          21.561950357948863,
          24.24939337009337,
          21.717652623246355,
          31.322867230671207,
          34.93128317546069,
          18.378675825227567,
          19.589765579719852,
          21.57740614666202,
          22.97728327619351,
          24.53844050275601,
          22.387106639582935,
          20.947310362404924,
          21.259920849063533,
          20.23632841963109,
          20.49191958729814,
          23.817866720804354
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial59",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54
         ],
         "y": [
          51.50419210418453,
          39.59839288975165,
          37.50120122645929,
          30.45931229940275,
          35.54858595375123,
          27.387314245952822,
          29.68334759347807,
          26.066963056238688,
          24.028763871851975,
          28.96730694344373,
          25.04889880544771,
          23.730692995273,
          23.646069007191233,
          22.63069104760643,
          30.20812496712537,
          21.51904375960187,
          21.477810386719742,
          20.2743487939602,
          21.830879397508575,
          24.73499191098097,
          25.237657640038467,
          19.460113595171673,
          20.95220717763513,
          23.153124274277104,
          20.943177324000413,
          18.280478004517594,
          18.714635329517893,
          18.0886102149157,
          25.10348668912562,
          21.18460269090606,
          18.499522604593416,
          18.024189700925252,
          16.87703607915863,
          29.343635466040634,
          30.257991496140395,
          32.6294087975975,
          18.943148605222625,
          17.7543788925419,
          16.784639978796488,
          16.762336033146557,
          17.977349358845533,
          19.848653700293564,
          17.4357385480307,
          15.858131369924157,
          16.18024192592962,
          17.252124910432148,
          19.52475408228432,
          26.079589704187907,
          23.724821509384526,
          18.663566139655384,
          16.40250873565674,
          16.609375310137988,
          19.31535337804779,
          22.917562701837802
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial60",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81
         ],
         "y": [
          36.632062229683726,
          69.03666196993696,
          33.199978588073236,
          38.76309383981596,
          32.75281836346882,
          28.759541441754596,
          29.291803313464655,
          24.89679634280321,
          32.02491788166325,
          26.673244212701068,
          24.66268762728063,
          22.551795943965757,
          34.510953856677546,
          31.28599962374059,
          22.87895959373412,
          21.66272516173076,
          30.497425482525088,
          26.058932854877256,
          19.924499612513596,
          21.900901298212812,
          27.833512329473727,
          22.990053006303988,
          19.389622672786558,
          19.514717768847458,
          23.673104588578386,
          21.33745036861761,
          29.022579193115234,
          23.394237347734652,
          19.130891303705976,
          17.96261134574084,
          17.199788349430737,
          17.94386643510524,
          22.001818245988552,
          18.751892686859378,
          18.134871707699162,
          17.166696160789428,
          21.348654909831723,
          20.285903853129565,
          16.727503164027766,
          18.825507660222247,
          24.66905379876858,
          18.131581965500747,
          17.5254516368959,
          16.639212267185613,
          16.417080917978673,
          16.675737900462575,
          19.312100208871733,
          17.466313602478525,
          18.08593807375528,
          17.949372508661533,
          16.843404684609514,
          17.20907776530196,
          16.951758206375246,
          21.596553213228056,
          16.17111475874738,
          16.45933873091287,
          16.889802389997776,
          16.730559364566957,
          16.176595672359312,
          16.496283872340754,
          19.867282200634964,
          17.519694615185745,
          15.937790614802664,
          16.623667057936753,
          16.962260168742358,
          17.056737892026824,
          16.312318189357356,
          16.311829210296878,
          15.880109779233855,
          16.643472221808704,
          15.320490627754026,
          18.01115129052139,
          18.501694966137894,
          15.725707604633115,
          16.028720809192194,
          15.960813847983756,
          16.379063753577753,
          18.17981636233446,
          16.334479045092575,
          16.59881506508928,
          19.332546017034268
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial61",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48
         ],
         "y": [
          36.01860931830677,
          42.31057270949449,
          32.39427116828236,
          32.48188626669287,
          29.051377304201203,
          24.874482534765228,
          25.67379922013942,
          28.599481163955318,
          27.208452240238344,
          23.677989711606404,
          23.093658679869115,
          21.2473797681855,
          21.393757796869046,
          19.52940508214439,
          19.464212262533543,
          22.20009143178056,
          18.806203059064664,
          20.164188733915005,
          19.87180370237769,
          22.474665122303538,
          22.88917744644289,
          21.886910492811747,
          27.457299472839853,
          19.453319619341595,
          17.60041563297675,
          17.26111323659013,
          18.986951432576994,
          27.481450631366513,
          26.59259676739452,
          23.896726313645278,
          24.26052960341539,
          22.069133138268942,
          17.25768735932141,
          20.545787191003317,
          18.056778442568895,
          21.26019482496308,
          22.491293930425876,
          16.054533741338467,
          20.851167725353704,
          18.730246605911876,
          16.128551079974912,
          21.49227689727535,
          21.25548631001294,
          27.517668871375605,
          16.42792860666911,
          16.11991596221924,
          17.152965917819884,
          17.106307712027697
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial62",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          97.14977959113392
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial63",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67
         ],
         "y": [
          40.58765085732065,
          44.67969695920867,
          35.86845703435138,
          31.08002051686853,
          33.409878847075674,
          28.03625575119887,
          28.192884631273223,
          25.498143203859406,
          29.145846901870357,
          26.731925002927703,
          23.053027874085963,
          24.453801566023166,
          26.939516548218766,
          28.47770824277304,
          26.387844612927942,
          25.57859535527423,
          20.820310352294424,
          20.45291468767616,
          19.806315437565004,
          19.742395749906215,
          24.63384752351094,
          20.4919320238315,
          20.34016454898245,
          20.040903424829,
          28.643155679470155,
          28.255089860621506,
          19.8425420047791,
          19.88530699412028,
          23.998090883580648,
          17.94021196287822,
          18.790362482148456,
          24.027516822504804,
          34.00078664950239,
          17.241098900151446,
          17.472884379751314,
          22.38349889739742,
          19.533622493588826,
          17.271329150936467,
          17.027893756463275,
          17.39703832021574,
          16.804596854419245,
          16.653277071510875,
          19.01494313061722,
          20.181440942655733,
          20.250066912271144,
          20.781784150658584,
          18.366014317768375,
          17.57519444411363,
          17.950940822198138,
          18.945425886448806,
          18.86228400129613,
          16.092328366225328,
          18.58362038930257,
          19.01378685865945,
          21.865824955265698,
          26.19505393795851,
          15.147421030494256,
          16.59414862035736,
          19.66213434110812,
          23.54240327540452,
          15.905912438059241,
          15.55919814691311,
          19.22115788033338,
          21.90441440179096,
          17.868671820415713,
          15.813366572062174,
          15.370837886159013
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial64",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          118.02716560673908
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial65",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          105.83353343436389
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial66",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11
         ],
         "y": [
          35.02694937853309,
          80.5259612788999,
          36.52543264869752,
          36.019858476592276,
          36.531245022285276,
          29.812254184629857,
          26.76077402316458,
          31.200438770821425,
          27.360565247574474,
          26.10151275386655,
          31.38532650955324
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial67",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          41.09282966551742,
          49.48821447729095,
          55.65378669800797,
          38.178802474727476,
          36.98811230233045
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial68",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7
         ],
         "y": [
          37.40351306326021,
          45.47540444474879,
          45.49233363702045,
          31.047388883140997,
          30.963665179120817,
          32.356471022939296,
          35.70998511275625
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial69",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          272.70046947448236
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial70",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          237.9570028413602
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial71",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          104.19619906045557
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial72",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          69.43981406359168
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial73",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          72.5910475816184
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial74",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          123.85781984406758
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial75",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          47.87430991196051,
          55.21196656886155,
          61.5999300886945
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial76",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial77",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          201.8779669040587
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial78",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          65.43524210239813
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial79",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38
         ],
         "y": [
          30.220764082621752,
          56.446977506808146,
          44.27590421350991,
          34.598134761903346,
          32.13035498208147,
          30.456252757126723,
          26.394809025089916,
          27.18901054258269,
          23.500505819553283,
          25.147064829260355,
          22.290489739518826,
          22.44903395427921,
          25.306799973898787,
          21.715157066903462,
          24.155131921535585,
          22.873988143796844,
          20.20584915130119,
          21.40495010313949,
          19.31070551833486,
          23.0451063915966,
          23.349171491173223,
          20.17901377173943,
          19.21245057796075,
          19.27202940374855,
          19.597033314588593,
          17.229737328320013,
          18.685038349492764,
          15.908379360912292,
          16.18696567101207,
          17.565181507327694,
          24.23172180051726,
          18.373356353945848,
          24.750075999314223,
          27.725543278019604,
          20.9581752249865,
          23.202450387846163,
          16.86273199562135,
          16.14675234972946
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial80",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          1816.396698742378
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial81",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          101.05805094649152
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial82",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          39.214516290804234,
          41.93792848664571,
          33.109835958093164,
          29.9683865803044,
          33.95901737368204,
          25.114296114541652,
          23.23194692968353,
          25.18861705500905,
          22.492864934409535,
          23.521601731215068,
          21.89046433301476,
          20.628631320426134,
          19.096702343080104,
          18.870631473820385,
          21.234433321448847,
          19.304654982031845,
          18.094223363612727,
          21.165007955659696,
          21.2263098615941,
          20.925991880215282,
          17.163999154315732,
          15.827643177373622,
          15.383543758857542,
          17.215489178169065,
          15.395047490189715,
          16.042242135458846,
          18.551942484165593,
          19.611005271353374,
          15.663055947156456,
          15.359141799492564,
          15.81461691662548,
          19.062869320070842,
          31.96089967867223,
          15.395739260727797,
          15.273447207319059,
          15.008986876262881,
          15.95804953381298,
          25.473816615779224,
          19.49897914979516,
          16.41437416541867,
          14.187681609052953,
          14.234458031693125,
          14.92939793191305,
          14.712116745429311,
          15.383670023786344,
          14.810743355169528,
          13.908012770055755,
          13.604081215897226,
          14.365807734853853,
          14.229622065536375,
          27.670180250958698,
          20.286227140969377,
          13.513478573744859,
          14.17309285760895,
          13.417576782102508,
          15.338675444688255,
          19.03791530733186,
          13.287032972506392,
          14.968249530327029,
          13.71567326057248,
          18.95756326070646,
          17.04431182582204,
          15.4660970873949,
          14.875490901916008,
          12.94169412008146,
          12.24721878330882,
          12.145099135918345,
          11.984021683049395,
          11.337129011386779,
          14.16834773862265,
          12.380341731435884,
          14.881530420566962,
          17.425196639890594,
          11.586971569836624,
          13.599846560780595,
          12.613597621762656,
          10.809703152354171,
          10.558778700789786,
          10.627341867462407,
          12.615995670721784,
          10.486111299778388,
          10.728102691774446,
          12.036588009780015,
          14.265773385520873,
          11.775562232102805,
          10.436296121860908,
          10.771765949280281,
          11.513281294970009,
          10.16070528728206,
          13.10824681879059,
          11.676945725107581,
          9.531060443661078,
          11.261025839704809,
          9.134948959195517,
          9.199684170203481,
          9.489677801364806,
          9.853969550714261,
          10.070680843136175,
          9.54453467547409
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial83",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86
         ],
         "y": [
          50.37358319662451,
          48.95220187427552,
          32.491665289653994,
          30.09057728837176,
          26.041962321211653,
          26.312305295370457,
          30.44044587670303,
          22.143651016359406,
          22.064584732055664,
          20.777838412339126,
          28.795331148597285,
          26.240045376909457,
          20.974227098914668,
          20.09847075764726,
          20.605053397698132,
          21.974056212882687,
          18.641513149912765,
          18.003539217196828,
          18.076634732688344,
          18.1295534924763,
          20.25305887547935,
          17.92357428480939,
          16.918599927328465,
          16.57705218617509,
          15.945471104567613,
          16.55254901327738,
          16.607609306893696,
          18.34816917946668,
          18.353866049913854,
          21.962664565419765,
          17.973963512637752,
          15.044830423060471,
          15.417062712878716,
          15.94948073906627,
          15.424810874752882,
          14.812270133475947,
          14.64807328945253,
          16.499846489448856,
          20.143074128685928,
          18.118936290585896,
          19.159795551765257,
          18.260842734236057,
          15.439825197545494,
          20.69966530218357,
          19.352634391164393,
          15.299150877851782,
          14.460290598675487,
          14.681795422623797,
          13.830003110373893,
          13.774585568807959,
          13.698060663734994,
          13.246306931100241,
          14.476838111877441,
          14.530819443183217,
          21.59259569741846,
          21.989765508388118,
          24.24356035682244,
          22.829283070758105,
          13.809559573972129,
          12.80458931031266,
          13.609066916675102,
          12.297215593539603,
          13.20797236372785,
          13.032544849364738,
          14.87782439565271,
          13.271614245282926,
          13.973452916959436,
          11.946946353447146,
          13.492335210970747,
          13.048463379464499,
          11.99808806132495,
          11.96438583901258,
          14.362522241545886,
          19.968042187574433,
          12.777613011802115,
          11.503930750901137,
          15.34870547783084,
          18.01109968356001,
          15.846242997704483,
          12.692985852559408,
          13.62046893825376,
          14.272342697391665,
          14.777648258984573,
          14.206407888148858,
          14.030508739192312,
          16.790932376210282
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial84",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73
         ],
         "y": [
          31.454824215028346,
          80.6480993255367,
          36.036004880579505,
          42.76251273426583,
          31.026920551207006,
          30.27512269679124,
          25.89887260809177,
          24.2152270185269,
          22.11697514851888,
          25.90570260644928,
          21.375446908842257,
          20.374044340800463,
          20.191703028795196,
          24.531659816338763,
          21.33247870158374,
          21.909413702119657,
          22.03464895729127,
          23.55570037965852,
          18.152827565262957,
          17.684246016711725,
          19.679938665250454,
          20.68767972496467,
          17.776467718729158,
          15.59987308533211,
          22.573548634847004,
          16.24098277673489,
          14.991882913480929,
          16.663555548443057,
          14.202394361418436,
          15.439986205682523,
          15.406642099706138,
          15.448371770905286,
          14.973103569775093,
          20.254136791074178,
          15.44242171155728,
          13.890786845509599,
          14.40607598932778,
          13.736073269107477,
          15.451652650910665,
          15.668017418403936,
          15.659785542061659,
          14.472324929586271,
          13.9364680856224,
          12.658767258248679,
          12.456016997980878,
          13.811605895437845,
          13.229362976260301,
          19.927886668259536,
          21.931628607152923,
          17.033472929543596,
          14.765493509246081,
          13.425209029903257,
          12.455404653781798,
          12.397798654509753,
          12.854852994283041,
          12.385609556988973,
          12.654914398503497,
          11.301590097628958,
          15.291483297580626,
          14.550938993934693,
          12.50460177320775,
          10.978795012807458,
          10.371832537457225,
          11.828913401782028,
          13.053212080544572,
          14.726458968185797,
          11.099936787675066,
          14.655865537441843,
          11.113059861873223,
          10.788581030155585,
          10.613056492999318,
          10.471133619789185,
          10.93854697157697
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial85",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          43.260030017635685,
          33.61370202971668,
          31.000487955605113,
          30.19735475865806,
          32.16520003962323,
          28.843864580480062,
          26.790104842767484,
          30.253522361197124,
          21.45108190396937,
          24.04964198911093,
          22.088398413929514,
          21.50509376836017,
          19.812631963714352,
          21.66261665220183,
          26.376265425023025,
          27.275233819232724,
          22.445217256623554,
          19.219457913220413,
          16.902424246315064,
          18.816852143140345,
          19.18122331107535,
          17.76449904403066,
          23.16894759201422,
          39.416052066213716,
          25.70497453891165,
          17.571523185667953,
          19.15820909515629,
          15.42878559546742,
          15.548038955626449,
          15.248339435918544,
          18.910500759031713,
          19.93155066172282,
          27.77345680608982,
          21.9195158888654,
          14.78559881691041,
          14.33223082379597,
          15.791283514441513,
          15.220354700476173,
          19.76136420025089,
          16.727802113788883,
          15.70439427073409,
          14.983831793312135,
          14.273693503403083,
          13.283908479582003,
          13.578180762810435,
          18.531618583493117,
          16.796974182128906,
          19.750733243740672,
          19.410186674536728,
          25.881474316604738,
          12.258133477311793,
          13.674380814156882,
          12.255918580342115,
          12.056673964833825,
          12.33334304840584,
          14.761857265379371,
          22.868618321612598,
          13.22813007695888,
          11.609810821409148,
          11.332840198423805,
          11.000624408566855,
          11.892895621012865,
          13.930052160247554,
          11.269689637471021,
          10.544343103238237,
          10.824500688692419,
          11.128210664764653,
          18.068975285785953,
          13.6021643537816,
          10.264442428340757,
          11.883183603364278,
          10.124331040111015,
          12.778868303066346,
          10.433285643414754,
          17.6956219401786,
          13.061397986683419,
          15.355386323075953,
          22.90068272846501,
          10.506142775217691,
          15.18678406195912,
          15.497682315547292,
          9.839929138741843,
          22.41442533043342,
          20.050400478083912,
          13.784282118324342,
          11.499382771127593,
          10.170949757583742,
          12.284637141033885,
          13.981924320624127,
          9.798203173691665,
          14.455685010770472,
          12.105162922928972,
          10.334166522917709,
          15.428922118210211,
          12.83077304343867,
          10.681978915765033,
          10.156193942558474,
          12.735204386517283,
          12.609295953580034
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial86",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          47.04377073582595,
          64.96596415449933,
          50.60670955006669
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial87",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          61.05811170252358
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial88",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          43.68242040494593,
          48.13146606693423,
          57.570914570878195
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial89",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          34.588660402995785,
          96.65777904231373,
          44.02256824524422,
          39.71130292008563,
          35.66323720730417
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial90",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          46.601229908020514,
          55.20547376803266,
          42.463782705911775,
          40.593131243698
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial91",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          132.4667170454816
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial92",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          108.95194114126811
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial93",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37
         ],
         "y": [
          49.24213629621801,
          41.01558061925376,
          35.419860328116066,
          31.96990118569475,
          24.465754795849808,
          23.34595781031663,
          31.93661765742108,
          26.870400390004725,
          20.61574340448147,
          20.74366405145909,
          21.875499306655513,
          20.453957565431672,
          19.41057748717021,
          19.00079202264305,
          20.38403806453798,
          24.187024635997243,
          19.532803605242474,
          33.28182135171038,
          33.5318091322736,
          36.808567543339926,
          25.92026588005748,
          19.111985904414478,
          16.846847068972703,
          17.784503688657185,
          20.09398928696547,
          17.78672523808673,
          15.266971277996776,
          15.599754504071988,
          16.42698119714008,
          16.642921595069453,
          19.649997052138414,
          25.81554555505272,
          26.152859726572423,
          32.54086882118287,
          24.780476360786253,
          24.94340135217682,
          20.74857831195118
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial94",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31
         ],
         "y": [
          50.117825298774534,
          36.24646712512505,
          40.50005734451418,
          35.89610495218417,
          26.546541942813533,
          30.274851775750882,
          31.859823350983905,
          21.2923059153363,
          22.19157491854536,
          31.40928899563425,
          35.099603373829915,
          30.578372877787768,
          20.002616044951647,
          27.65617555137572,
          27.33609213480135,
          23.232071217482652,
          20.0600160583248,
          22.910383736214985,
          23.318357297075472,
          21.13779126919382,
          16.736201464645262,
          18.865786668730944,
          17.93680165453655,
          17.84728688340846,
          17.6346648844277,
          19.55695635322633,
          44.07759723818399,
          19.14267213557794,
          19.320076167098875,
          19.841663763775088,
          24.447453165441996
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial95",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          45.13078376335827,
          37.9205378245532,
          37.95519541918747,
          33.97861842023648,
          32.74307379683828
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial96",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          65.14871141387195
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial97",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          39.64396938851209,
          75.5481894268253,
          44.79049840787562,
          42.567645499376745
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial98",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52
         ],
         "y": [
          34.25237706812417,
          56.03640002739139,
          40.387807303327854,
          39.704928638489264,
          31.880644914580554,
          26.467596007556452,
          23.276205310976604,
          21.86735329201551,
          21.076714314096343,
          26.27095469032846,
          27.414025345468907,
          21.089844641646717,
          20.708754516229398,
          24.700060929709334,
          29.284618377685547,
          21.70988458152709,
          21.90595133711652,
          21.341328256498507,
          18.193323135375977,
          17.132355348850652,
          19.21217840861499,
          18.063647417518183,
          20.636824522561174,
          18.751459067429952,
          16.05660057843216,
          16.276139902874707,
          15.475460936383504,
          19.736685148099575,
          21.677560651205418,
          26.14536252835902,
          24.951905010192373,
          15.096075088997198,
          14.569255689295327,
          14.50892902777447,
          17.2053691197217,
          18.62551504615846,
          19.73664756712875,
          15.445106785471847,
          14.324289321899414,
          14.428705192193753,
          14.915018182459885,
          12.814706848888862,
          17.92434132583742,
          17.382619028168964,
          16.24735732970199,
          19.234454379818303,
          19.120431876764066,
          13.354536630273834,
          13.425353600726865,
          16.22095912840308,
          19.90259189140506,
          25.38486463655301
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial99",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31
         ],
         "y": [
          39.01839972705376,
          38.99752103603952,
          30.028907636316813,
          33.03151800574326,
          25.05345481004172,
          22.764509185542906,
          25.161979489210175,
          31.50750898345699,
          29.382907123100466,
          26.554165940943772,
          21.844349186594894,
          23.29570136031484,
          25.963611525248705,
          19.391914344415433,
          20.29813882781238,
          17.356156232880384,
          18.45290249149974,
          20.0315395293197,
          17.3756600434218,
          18.973254141768788,
          15.095547412469134,
          31.15420515168973,
          28.86777091607815,
          38.69064017815319,
          30.27923692532671,
          15.584157579313448,
          16.23398773069304,
          20.73882886064731,
          17.09490230219151,
          17.642215961363256,
          21.36882180314723
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial100",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          55.95788878153979
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial101",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47
         ],
         "y": [
          30.468776733894657,
          48.39532498615544,
          46.079043535682246,
          41.972651039681786,
          25.97972834207178,
          34.68777974446615,
          30.115994461183625,
          23.242158781222212,
          30.01937604144337,
          28.220163050705825,
          23.554241366502715,
          23.345933479991384,
          20.476134757685468,
          33.39897075125842,
          20.809886219055674,
          20.607142084013155,
          31.517696054970347,
          21.411507645273595,
          18.66331509070668,
          18.68978355376701,
          19.368902966258972,
          16.806922315582028,
          20.849049699984914,
          17.885675376023702,
          18.34410824039118,
          16.229574575656798,
          25.767451170014173,
          22.215877843096973,
          17.41847975661115,
          15.82269914363458,
          15.275995145968306,
          15.713974580532167,
          19.41017067141649,
          23.64318190163713,
          20.211438434879955,
          15.124264786883097,
          14.861772777588387,
          16.896938703893646,
          17.3698375128149,
          15.909280831251687,
          14.962529663147965,
          16.78639174670708,
          23.42139380540305,
          16.84658004791756,
          17.545568884872807,
          18.109695636160005,
          19.86300327332039
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial102",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59
         ],
         "y": [
          38.72752904504295,
          36.817867837301115,
          37.06593713527772,
          37.04936008918576,
          25.974290103447146,
          23.503970743194827,
          29.424695208789856,
          23.08970553700517,
          21.394225391915175,
          27.783364241685323,
          23.05625479783469,
          20.05382107525337,
          23.009696123076647,
          27.492112120961757,
          19.325667086655532,
          21.51126646026363,
          26.19982973734538,
          22.939243812871172,
          17.816192045444396,
          17.503077778389784,
          15.242890768904028,
          16.479891978628267,
          18.000546370095353,
          17.856024377714327,
          14.95790019461779,
          18.638470223279505,
          24.780348723496846,
          22.756043147265427,
          18.542469652687632,
          14.154328617623182,
          21.370056896674924,
          22.876334089573806,
          18.28517160958391,
          20.081320522277334,
          15.441582043965658,
          14.876454764265356,
          16.662627235660707,
          23.753673274342606,
          20.905586955993154,
          13.308858072854639,
          17.945507569041677,
          13.9572285365283,
          17.625367273160112,
          27.296719899991665,
          23.59658168389545,
          14.235487457213363,
          14.896116093891424,
          13.843377322685427,
          13.01377438335884,
          13.23955846026661,
          16.960323395767833,
          21.830506673673305,
          21.51429584162022,
          14.487855391773751,
          13.052341903128275,
          13.733480717108502,
          14.352337775191641,
          15.509032931754259,
          16.772085864369462
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial103",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          60.92646724421804
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial104",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          51.41489106465161
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial105",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88
         ],
         "y": [
          31.100265564957287,
          48.282138421283506,
          44.05140410012346,
          32.31367108104675,
          26.630335458895054,
          32.04566067021067,
          35.991586219973684,
          21.839097139312,
          21.73872377814316,
          35.457600570306546,
          25.429596272910512,
          20.746215564448658,
          24.258040079256382,
          22.48023374293878,
          19.89447983687486,
          25.593457989576386,
          22.490928045133266,
          19.599444497891557,
          20.727359244493933,
          25.405874267826235,
          17.07757397783481,
          16.455074000164746,
          19.270986215855046,
          18.780169362944317,
          17.874251815361706,
          19.82190821422794,
          23.916057245518132,
          22.04690119115318,
          23.155033049544667,
          26.798810183517332,
          21.679859859187427,
          16.136260784738433,
          15.769371474661478,
          15.356543393639045,
          15.629893667329618,
          17.128165128754407,
          19.47573691267308,
          15.059189114144178,
          17.199561274148586,
          16.323874225461385,
          14.93526664981997,
          15.03545257909511,
          14.221802734747165,
          14.304871109442983,
          15.60015558227291,
          13.994193177882249,
          17.82043827646147,
          17.83561297160823,
          21.278709148003802,
          17.122268521688817,
          16.975417075118397,
          16.389152356279574,
          15.837146650484907,
          13.904325376681197,
          14.114125996101194,
          15.841750160465395,
          18.98546083962045,
          17.359046874007557,
          14.914310556117112,
          12.101736456398072,
          24.43179738424658,
          21.16546266447238,
          21.277072720411347,
          14.988388348401077,
          18.298271597885503,
          22.12258926639712,
          15.78453739290315,
          13.259048392133016,
          11.40934747215209,
          11.871043965099304,
          12.91871906683697,
          12.943827419746214,
          11.32010387017475,
          11.238590860754494,
          11.48992721433562,
          11.935946697142066,
          10.55456060704177,
          10.456899410340844,
          12.412596477725641,
          13.046871262837232,
          10.755879952655576,
          11.298586333670267,
          10.56750944959439,
          10.798644856708806,
          10.477024892481362,
          10.95394455514303,
          11.883408980640938,
          11.32107730028106
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial106",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34
         ],
         "y": [
          43.4449064673447,
          53.903644251629586,
          39.68588642957734,
          29.339488719537005,
          26.713239638786007,
          25.025967636728673,
          28.761834695087217,
          28.387021832349824,
          24.55131040743696,
          32.59005335675992,
          26.10357921879466,
          23.047781300738574,
          20.364634405306685,
          26.506710936383502,
          24.943450586582586,
          21.251225417222432,
          18.388592440907548,
          21.005844178238537,
          19.802810606917713,
          16.564240060201506,
          17.250858911653843,
          16.817680498448812,
          17.893941856012113,
          15.308860716780996,
          19.32921135910158,
          21.58544501638025,
          20.04830744983704,
          20.26902932267848,
          17.109883595288284,
          19.11956159079947,
          19.900198006048434,
          20.230106927515045,
          21.380220165097615,
          19.594888532064793
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial107",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          46.12658223097886,
          58.31654088090106,
          46.520142904142055
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial108",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          36.64134151179616,
          37.068589543908594,
          33.81085578794402,
          33.8435057353198,
          32.505794199501594
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial109",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          32.845036948599464,
          70.49550647270388,
          38.345643051271516,
          43.46989301355874,
          34.725159094585635
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial110",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          72.47311339339589
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial111",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          50.77718815377088
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial112",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          67.54921157960969
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial113",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39
         ],
         "y": [
          38.531656311779486,
          35.94438928123412,
          43.513633728027344,
          30.620385844533036,
          25.891359499799528,
          26.671512045511385,
          23.870571524147095,
          32.780992120262084,
          28.035682662715757,
          27.945121315436634,
          20.40777052902594,
          20.459991175953935,
          22.097913044254955,
          18.307855226160065,
          18.322958736884885,
          20.082124733343356,
          19.677933033888902,
          17.505692567282576,
          19.35306060604933,
          19.738555877189327,
          23.315820476873135,
          21.479052690955683,
          23.849716295071733,
          20.361791889841964,
          15.240437972836379,
          16.025250807041076,
          15.384782178615167,
          18.127360064808915,
          13.73273016766804,
          14.108096045207201,
          14.531592431107187,
          19.351850501890105,
          20.570694217836955,
          19.143698072045797,
          15.464390824480754,
          14.79464285935813,
          22.118632231301408,
          30.76219304402669,
          20.559090187879114
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial114",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79
         ],
         "y": [
          30.041675536613155,
          74.68580180842702,
          34.919143893854404,
          47.32377621410339,
          31.73893819979536,
          27.56099314805938,
          25.543866599478374,
          22.95942075465753,
          26.736595743070772,
          29.63479226585326,
          21.403769500856477,
          20.420144871967594,
          25.062271118164062,
          27.752512613932293,
          22.78345125089816,
          19.977186978347902,
          19.16740465551857,
          23.426694730433024,
          18.81367009635863,
          18.29825274149577,
          21.009894611389658,
          24.686849221950624,
          17.012395122186923,
          16.848710432285216,
          17.326188901575602,
          20.43093969763779,
          18.255228236438782,
          20.29161941714403,
          28.05891996864381,
          24.420598766668057,
          16.94405572782687,
          16.112999714486968,
          18.52919865429886,
          28.802465206239283,
          16.926006526481814,
          15.590130705174392,
          19.78654669939987,
          31.426955944154322,
          22.36571268531365,
          15.344736688505343,
          17.887609481811523,
          17.47919637788602,
          15.369817353845612,
          14.900259304821976,
          15.164425702599006,
          15.02246351164531,
          21.265780162035934,
          18.805205655291797,
          14.846400974242668,
          13.927966482271025,
          15.757860346538264,
          14.285361274471128,
          13.992934451839789,
          15.757286001996297,
          18.20237753643253,
          13.360252403631442,
          13.372107125879303,
          11.843549480283164,
          12.965234128440299,
          12.52033070044789,
          12.81570165525607,
          14.875871154350962,
          14.140390737269952,
          14.248556385195352,
          13.533070013775088,
          12.063457558794719,
          11.829692522684732,
          23.56918767603432,
          11.597237742044092,
          15.67290581338774,
          18.22315361053963,
          15.656130720929403,
          14.206579611553408,
          15.922184618507943,
          15.325986427989433,
          15.28355456561577,
          21.081901674348163,
          13.025286620225364,
          11.715073570003355
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial115",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial116",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          60.052004155104726
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial117",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          45.3107569779807,
          57.23496010633019,
          37.86331767570682,
          37.68626110728194
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial118",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          39.98234877547598,
          47.99154904993569,
          36.78726045872138,
          36.6009351373688
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial119",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          43.58263510417163,
          71.59497188164936,
          41.51090612644103
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial120",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          46.35677697406552
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial121",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56
         ],
         "y": [
          44.61845655363749,
          31.375294351965433,
          37.27077722937111,
          29.00608790017725,
          28.354033788045246,
          22.891185496880755,
          24.93567386099963,
          21.79295656157703,
          25.74394331521135,
          35.10801045487567,
          36.30847326139124,
          19.173195319447093,
          19.64377811866078,
          24.26178431317089,
          24.886114802786974,
          18.773304489569934,
          18.43836441660315,
          19.954511324564617,
          20.008719002328267,
          16.978122804223037,
          15.92293840113694,
          16.878059022794893,
          15.602381116975614,
          19.19120526507618,
          21.061425588964447,
          36.74893225693121,
          23.622068141534076,
          16.70854978638936,
          16.23998082168703,
          16.600488654966277,
          14.54598873417552,
          13.907711750123559,
          14.218133903131253,
          13.658225408414514,
          14.194753584822989,
          17.27211949108093,
          14.317397784411423,
          12.847309422686818,
          13.878645292142542,
          14.069505776816268,
          15.895755155299737,
          15.067906992222236,
          15.67221533767576,
          12.788071066383424,
          14.488908581617402,
          11.842361892141946,
          12.482465410620216,
          19.284883390597212,
          15.335328195153213,
          15.204177763403916,
          14.265509101433482,
          14.10619008444189,
          12.132734376240553,
          12.427095870661542,
          14.28330021369748,
          14.026921179236435
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial122",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          44.83443184209064
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial123",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          73.37104251520421
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial124",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63
         ],
         "y": [
          38.91183087108581,
          42.09474758985566,
          35.819834081138055,
          30.079546610514324,
          37.53093263579578,
          26.870540541361986,
          22.999882070029653,
          22.10794648891542,
          25.013593161978374,
          20.894283147362188,
          23.30185325746614,
          20.125604559735553,
          21.516394777995785,
          21.93931914538872,
          22.94527736136584,
          23.779387559348006,
          25.949798227325687,
          18.112595325563014,
          24.425834764310014,
          28.535196350841986,
          24.161319531076323,
          27.876457865645246,
          17.287782878410525,
          15.673448205963384,
          17.66050372084951,
          16.652881521519607,
          15.153302231455237,
          24.16444040313969,
          19.43314323580362,
          21.53434950355592,
          21.16791063014085,
          16.57416615059705,
          19.870456044266863,
          20.58715764487662,
          17.405715081749893,
          13.877055199165655,
          14.0245687825893,
          13.6458186715599,
          13.72035791040436,
          14.309266478065553,
          13.01995342533763,
          13.178521730066315,
          12.831688857660062,
          12.680757165924321,
          14.475284351565973,
          11.769893568705736,
          13.629373635702986,
          21.771046305090433,
          13.990571588035522,
          13.442017384660922,
          12.74141988521669,
          18.39496141139085,
          11.728818932199866,
          13.14446368643908,
          14.094315761473121,
          12.136897366221358,
          13.476461255453467,
          14.317988240621924,
          12.605529358716515,
          13.489750668285339,
          18.232774083207293,
          12.049173432637037,
          15.173901573429262
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial125",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7
         ],
         "y": [
          31.424403399955935,
          76.05439640448346,
          37.57763912231942,
          41.58419371814263,
          33.011905127424534,
          29.21115402283707,
          31.233157863461873
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial126",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          33.593229603961234,
          67.34589361175289,
          37.25921596744196,
          55.16870235039936,
          32.42252343650756
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial127",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          91.0518771535982
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial128",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          113.39667566811165
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial129",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          347.5826845246602
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial130",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          58.6937142969147
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial131",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          60.33200377177417
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial132",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69
         ],
         "y": [
          38.62210844396576,
          42.108904923849956,
          36.806960470308134,
          34.85520407824012,
          27.53582929595699,
          25.722110608728922,
          25.088831552645054,
          21.677786261085572,
          24.512675866848085,
          31.65967762954836,
          20.151790401799893,
          19.397511583033616,
          20.60434260794787,
          29.46164180786629,
          19.070357369213568,
          18.34614538177242,
          18.132711286467266,
          26.968902618904423,
          20.870930183224562,
          25.221735202200044,
          18.941307982778163,
          19.707861148244966,
          19.479367077835207,
          19.573942417051732,
          17.689842084559,
          23.31806755065918,
          42.24235187313421,
          16.52750758039273,
          18.226773983094752,
          22.904044252101,
          16.20764045405194,
          15.606515690563171,
          15.254994849848554,
          14.647857704782874,
          14.813035871924424,
          17.85300127665202,
          16.498719610819002,
          15.456774114593257,
          15.07977633747628,
          13.768097295993712,
          14.480617600727857,
          14.256461980866224,
          22.216507981463177,
          17.729132621268917,
          13.652832248346593,
          15.516296409979098,
          13.372852379713601,
          16.347171488816176,
          22.297079504989995,
          14.082454906246527,
          12.469229682674253,
          12.059448939997976,
          12.4128290269433,
          16.61663311283763,
          12.581928392735923,
          12.776289653002731,
          13.437736976437453,
          17.037688270816957,
          11.516944877500457,
          12.42938771286631,
          12.999488349852523,
          11.871378379139474,
          12.86389191170049,
          13.030712150945895,
          11.7849319582063,
          11.580253414991425,
          12.254875826641795,
          14.386059652499068,
          14.798944403485555
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial133",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          34.408752286337254,
          85.40326237872364,
          41.6603352616473,
          36.119177097227514,
          37.05660802949735
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial134",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          65.12265957467925
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial135",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          47.340924984071314
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial136",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          280.10060752310403
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial137",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          275.0497987328506
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial138",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          64.91361763806847
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial139",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          34.789859322028434,
          87.75348446233485,
          48.65095885982358,
          39.99123843123273,
          40.96357035443066
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial140",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial141",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          49.863113031154725
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial142",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          62.431633057633064
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial143",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57
         ],
         "y": [
          38.982939526317566,
          37.36268837471319,
          41.411645222485554,
          32.5885975845461,
          29.781573210305314,
          25.084728349515093,
          22.224561319118592,
          22.163449620812887,
          21.17532652568042,
          27.214674321616567,
          30.129407277921352,
          31.45556240546994,
          21.600380540863284,
          26.965514206304782,
          28.44401144012203,
          22.38283690785974,
          19.34496978821793,
          19.489797537888936,
          23.554933966659917,
          23.09976728175714,
          22.443178673100665,
          18.957517383544424,
          17.16857373617529,
          16.792092013165234,
          17.638876341222748,
          22.116595244989163,
          22.96343541339161,
          15.054662758741921,
          19.45624232098339,
          16.118145128575765,
          16.93237190711789,
          17.09226702868454,
          16.601600926096847,
          16.440434448118133,
          14.819075359561579,
          18.91308705399676,
          18.192585929622496,
          14.80788856599389,
          14.919651434673526,
          14.164398449223215,
          16.594835521729014,
          18.000815453568126,
          14.139906604115556,
          15.685363467146711,
          13.936279963671677,
          13.59854447744726,
          13.458390577052667,
          13.783660539766638,
          16.91110816428332,
          16.15282545632463,
          19.81669377892967,
          29.02986374521643,
          20.11026097119339,
          17.880780917842213,
          20.649419396873412,
          18.987039728862484,
          23.183885543327023
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial144",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          34.08925509258984,
          65.04947473169342,
          39.13619909829241,
          44.68870749124667,
          31.935500075177448
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial145",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          204.00062623062755
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial146",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          32.356139640497965,
          70.48770662633385,
          41.72566468153543,
          43.67057024947996,
          34.379960672642156
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial147",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44
         ],
         "y": [
          41.69595206656107,
          33.34215362672883,
          34.525755192206155,
          27.5023108226497,
          28.144014823727492,
          27.746142612240178,
          26.689722107677923,
          23.01700212121979,
          23.36993662516276,
          20.844342813259217,
          20.33571527837738,
          19.46399123494218,
          19.022648183310903,
          18.812822931181124,
          31.681821326899335,
          20.800760129602946,
          17.62862718008398,
          18.74742033423447,
          19.72845173657425,
          16.29317736431835,
          22.568792699798337,
          25.889751635915864,
          18.49767155375907,
          29.67873194934876,
          34.998127696959955,
          20.510826824157217,
          24.646312450005755,
          17.99896503851666,
          16.5847734900994,
          16.24930905908104,
          16.10920204379694,
          15.5301073818672,
          23.739864566461826,
          14.368151478651093,
          23.79191611065128,
          18.36005094574719,
          14.94977572681458,
          15.41595687711142,
          15.54061957878795,
          16.29650165588875,
          18.44310223959326,
          23.843245715629763,
          23.000010684253724,
          19.92301398176488
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial148",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          36.07940869215058,
          108.80582198476404,
          50.3703130706539,
          37.41593218237404
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial149",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40
         ],
         "y": [
          34.00880480200295,
          35.857366934055236,
          33.75057107258618,
          28.36600849492763,
          33.26739162352027,
          25.5935370592567,
          27.761818862542874,
          25.16196233857938,
          30.945008936936294,
          21.429769733087802,
          26.834630733583033,
          26.53642975411764,
          21.37034285940775,
          20.368564008697263,
          23.685663269787298,
          19.37468256601473,
          22.987264958823598,
          20.051375226276676,
          17.869300601928213,
          17.46003444795686,
          29.048223061290212,
          23.541131058359532,
          17.046254607720105,
          19.52681373193012,
          17.6291376206933,
          19.878594747403774,
          17.521550868584857,
          17.807325805105815,
          20.15604022266419,
          14.77558566302788,
          16.69879098442512,
          17.01870176268787,
          15.817250298290718,
          15.3248716524946,
          16.99719853129813,
          17.121688113949162,
          19.779050609929776,
          24.53784544099637,
          23.08507024563425,
          22.128610843565404
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial150",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          3654.455548542302
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial151",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          41.61480965653086,
          42.61985214357453,
          30.502308868780368,
          30.189436811741775,
          28.706867388593473,
          27.32108268117517,
          23.56070129270476,
          28.091268585949408,
          29.38465667352444,
          21.82555124236316,
          20.89783081581922,
          21.714613674132803,
          22.470420046550473,
          22.551512772474833,
          25.8672840304491,
          21.543268482859542,
          26.550419799680633,
          22.423780689394572,
          20.738309829215694
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial152",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6
         ],
         "y": [
          34.7629252953258,
          68.69135073529995,
          34.53998281897568,
          34.57566295406683,
          31.306450355343703,
          31.175146273481168
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial153",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          38.545341088519834,
          72.98466063708794,
          42.89137798402368,
          38.15623694319066
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial154",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          28.362582447083017,
          47.89587594629303,
          41.103325944605885,
          50.00762402914404,
          28.64749269563008,
          28.2987628719671,
          26.537519129311168,
          23.91553213538193,
          33.62729992129938,
          43.6996105163078,
          25.938738737649064,
          21.06877810780595,
          24.967806033002653,
          29.714881540313968,
          21.94925931604897,
          19.799232211539415,
          19.537543219279467,
          30.232234939327085,
          23.852961656523913,
          18.21781005704306,
          21.061653695455412,
          19.157006116417364,
          30.95154323422812,
          33.61526723411994,
          19.957392467715877,
          16.906198199202375,
          20.3389891957849,
          19.48353444851511,
          17.98862026958931,
          20.545343073402965,
          18.219620293718044,
          19.419564301405497,
          20.53719424426071,
          19.938446913308244,
          16.727632592364056,
          14.778631675534132,
          14.63063168719532,
          15.306174813247308,
          16.61431428862781,
          14.084535885632523,
          16.998779707807834,
          15.930241716586478,
          13.778727492665857,
          15.500794658815957,
          15.2190835921745,
          13.875915651398946,
          13.00726942512078,
          13.369694616736435,
          14.800852263846048,
          14.372943358692696,
          14.79174294510508,
          13.542378906312027,
          12.871944962478265,
          13.336097151283326,
          13.266323205901354,
          13.159412825979837,
          13.127533602520701,
          13.614688392577133,
          11.949998173287245,
          13.351022991707655,
          12.762174265171454,
          12.426974730763009,
          12.69187099953008,
          11.950450959244394,
          11.029737387246232,
          13.613094632218523,
          11.593018927225252,
          12.337023797073984,
          17.16090061218758,
          11.472252690695166,
          12.440436843934098,
          14.455719273264815,
          11.535535897665877,
          11.54572668308165,
          10.904002608322516,
          10.966929288414436,
          20.040774818358383,
          18.627834924837437,
          12.506259747636996,
          11.822357022665381,
          13.123083548817208,
          11.847988787705336,
          11.25995211872628,
          11.193713738666318,
          10.165032506958257,
          10.488203901585525,
          9.9084543600315,
          11.025064600192435,
          9.471238031619933,
          9.459329814445681,
          10.192104122503016,
          9.188571360053086,
          10.576292014703519,
          10.36728489883547,
          10.584424073134011,
          9.124408535841035,
          9.314184836255826,
          9.598031063389971,
          9.21826085811708
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial155",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          47.138967622586385
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial156",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          34.3508897176603,
          64.58450053765522,
          32.30090254496753,
          32.82857519630494,
          36.728041532563
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial157",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          46.250070897544305
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial158",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          54.411158864091085
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial159",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          298.0680517181148
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial160",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38
         ],
         "y": [
          29.215883673691167,
          48.985733125267956,
          29.819600624766775,
          30.07079057771016,
          28.04523761097978,
          25.29918470615294,
          32.27259042011044,
          26.277103206975674,
          22.392900404891346,
          25.503374720007425,
          23.042447702671453,
          21.498430345116592,
          21.89692892679354,
          27.06178292995546,
          20.030867561092222,
          20.576804750333956,
          20.82086403389287,
          27.481447933166006,
          21.478206836111177,
          24.121299805679943,
          20.04773349296756,
          16.660324685941866,
          20.89309281449977,
          19.88043840920053,
          16.439545266996554,
          15.351824310736928,
          22.41854842891538,
          15.113592093553,
          15.686318932510003,
          16.376162056031266,
          17.111483775503267,
          17.704107113970004,
          18.407934685063555,
          30.495692772593923,
          26.45437332091293,
          16.152436512272534,
          16.596990608587497,
          19.25088687059356
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial161",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          35.307881130435604,
          91.97241043462986,
          49.965865902784394,
          40.266963043833165
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial162",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6
         ],
         "y": [
          33.513691894407195,
          88.80317334430974,
          34.78180841895623,
          49.643642580606105,
          30.894434331878415,
          31.458382970918485
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial163",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          44.89525411962494
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial164",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48
         ],
         "y": [
          34.12537380931823,
          33.98301658010095,
          33.19651104376568,
          36.18367547136012,
          25.995098610234454,
          23.130419878455683,
          22.939232430806975,
          21.7443252191311,
          21.37784046080054,
          26.810121985954968,
          24.384990366493785,
          37.71990997810674,
          32.274491116283386,
          19.074868124674975,
          18.66966745330066,
          29.40146540820114,
          26.2509084561976,
          17.27360410612773,
          18.713912824305094,
          30.843341269144197,
          24.54376585115262,
          16.886854652466813,
          16.360289984602268,
          15.919026359309996,
          15.677066438566378,
          18.728086859230103,
          16.770098849040707,
          15.094951156678238,
          19.382328359092153,
          16.2699130221111,
          15.362884234606735,
          15.427570746196963,
          27.32636068700775,
          31.836126405049146,
          32.602494154519185,
          16.656618296615477,
          14.847463289896647,
          13.922429937657302,
          14.38750197247761,
          18.745921383059123,
          16.80738897246074,
          19.433320014457394,
          15.968005591291723,
          14.275250838054873,
          19.038836952147445,
          22.99858877910831,
          16.63461486692351,
          14.207014316465797
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial165",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8
         ],
         "y": [
          37.127276056181124,
          37.76835415227627,
          34.68159377865675,
          28.322632735337667,
          26.85186203126985,
          37.720243903679574,
          29.10771565321015,
          35.917834630826626
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial166",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          56.5760270095453
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial167",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          42.39886237353814
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial168",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial169",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          33.188248549050435,
          64.30627878700814,
          42.64901147237638,
          43.594331539743315,
          35.29666029147016
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial170",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          52.044638439891784
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial171",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          34.71151199961096,
          49.23457010780893,
          42.548689214194695,
          31.806019930335566,
          33.51746675444812
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial172",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6
         ],
         "y": [
          34.09583629825251,
          34.35316665773469,
          43.90165651523001,
          29.662173340960248,
          31.861708912422987,
          37.23106644793255
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial173",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          270.5057886635385
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial174",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          119.30259084313866
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial175",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          37.17394935793993,
          87.51383761274136,
          48.28961941478698,
          36.74330483413324
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial176",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          63.396519451606565
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial177",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          61.43507512410482
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial178",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          36.925393562006754,
          64.0184271897727,
          37.90487121954197,
          35.96517995508706
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial179",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          66.20010391483461
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial180",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          1651.037866607914
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial181",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          74.96682640013655
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial182",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          56.99638838108962
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial183",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          33.1435999521395,
          63.08360985236439,
          40.44088652075791,
          45.40687526919977,
          32.00137857886834
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial184",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59
         ],
         "y": [
          31.53229245131578,
          50.568999003588665,
          40.90895127087104,
          32.42025339700342,
          28.860977715593044,
          25.56754828662407,
          22.863753280019374,
          25.595560461525025,
          21.40158926180708,
          20.949819029831303,
          23.303089110831905,
          23.669889062400756,
          22.299553956442733,
          25.77503632336128,
          20.463575541488524,
          23.54536639577974,
          21.363346130867313,
          17.457486726404206,
          19.275066887460103,
          27.06210744284033,
          21.698027432449464,
          17.95954613569306,
          16.983966889420177,
          23.554912753221465,
          22.827143785430163,
          18.239911226722285,
          27.970324058842852,
          19.036403749047256,
          15.623406107832746,
          20.525105615941488,
          16.368744532267254,
          16.38325304326003,
          14.739841926388625,
          14.752179525732025,
          15.097513129071492,
          14.476509505171117,
          16.732108123903352,
          15.4565657018646,
          13.80734891813945,
          18.081119909519103,
          17.521294058823003,
          15.47194713499488,
          16.783011622545196,
          14.425909112139445,
          22.5638058670168,
          22.501473822244783,
          14.145779749242271,
          13.723420856444816,
          13.38084742693397,
          13.619557512484915,
          17.464125679760443,
          14.083130828733367,
          19.794643262537516,
          25.249800441710928,
          25.48350938936559,
          22.024769123976792,
          31.389338097921232,
          16.823878133200047,
          19.453387221669765
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial185",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          48.66819933759488
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial186",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          43.944931510987324
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial187",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          32.43120748628446,
          74.20802604861376,
          38.90222397470862,
          53.901925683990726,
          35.61065117130435
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial188",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          58.05889098624873
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial189",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          46.12056360012147
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial190",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          42.89897332540372
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial191",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          65.68519443418921
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial192",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          47.018128402834016
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial193",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          60.90857035939286
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial194",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          34.282368760767994,
          96.27123086820772,
          50.51289659205491,
          39.340086107331565,
          43.420587493152155
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial195",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          55.438835516208556
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial196",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          38.8818019153626,
          85.5966510927774,
          50.79298983938326
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial197",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          64.8323491724526
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial198",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          65.60705209747563
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial199",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          49.724433495746396
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial200",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          41.265407143569576
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial201",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          54.63775603751826
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial202",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial203",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          43.88096984615171
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial204",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          61.75428725451958
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial205",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          39.18059950727758,
          69.98246126252461,
          45.66842415662316
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial206",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          32.727410897976014,
          83.41395872782886,
          36.84836105408707,
          42.49015575501977,
          33.4863905402703
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial207",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17
         ],
         "y": [
          39.110002192055305,
          36.914254335853144,
          26.861237146020905,
          24.71450252067752,
          27.451816713906886,
          28.23313813868577,
          35.647081793808354,
          24.984925929123794,
          26.806797353232778,
          22.725746139278257,
          41.03002008577673,
          26.912312282779354,
          22.20714317880026,
          23.270979811505573,
          25.4398391227412,
          28.44815187531758,
          22.096906724014904
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial208",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          73.50681503419953
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial209",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          52.101015385573476
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial210",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          82.78131029082508
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial211",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42
         ],
         "y": [
          31.205334361006575,
          49.705054926678415,
          40.167700108473866,
          44.91399975908481,
          27.851060122978396,
          35.907818150714164,
          27.069427412699877,
          24.438074701200655,
          23.939220831646182,
          36.079431099620294,
          24.25875328808296,
          21.282976212540294,
          29.584919410023264,
          28.43443774401657,
          21.386699847089567,
          22.89375205931625,
          24.58688509561182,
          23.03078670036502,
          18.326511096179,
          18.204233154048765,
          31.54643841875278,
          21.799305101720297,
          17.800427940802845,
          18.551030849053607,
          17.23647599103974,
          25.505824313900334,
          31.93767967844397,
          23.82990385846394,
          21.54561794870268,
          17.215667809897322,
          16.263753208687636,
          14.338142867979965,
          24.486630261428957,
          20.247439407720798,
          15.84579197178042,
          15.43936571260778,
          22.747323989868164,
          24.230811033791642,
          18.307384870885834,
          15.7071490636686,
          16.441379787476084,
          32.746250648808676
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial212",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          45.82111222181863
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial213",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          40.948206211493265
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial214",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          99.23079284419859
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial215",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          44.54813301272509
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial216",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          56.045640309651695
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial217",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          35.46358150389136,
          70.55304556745824,
          38.17503887269555,
          47.85347800526193
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial218",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          39.26032278789737,
          76.58115759128478,
          42.211988371562185,
          39.02805257037403
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial219",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83
         ],
         "y": [
          40.583880153128774,
          42.94052030982041,
          33.38958502978814,
          29.83231882545037,
          26.390753815813763,
          24.06444549560547,
          23.102141419077306,
          21.71165537640331,
          22.25005932939731,
          25.44936288663042,
          26.53412913500778,
          19.559090156865313,
          19.105625571274174,
          32.24449588806649,
          27.818077226964437,
          19.41545758596281,
          18.540685025657094,
          23.710926055908203,
          27.5023988553179,
          17.792530470747288,
          17.339960935639173,
          30.51108734006804,
          19.367810311356212,
          17.98490704172026,
          18.823927925854193,
          19.59103951803068,
          22.357343394581864,
          19.97162128851666,
          14.995832396716606,
          16.977556259651493,
          15.218332019278673,
          15.554237071091567,
          15.206566655538916,
          20.004552220910544,
          20.07146852384738,
          15.731358659945853,
          14.558491458737754,
          17.38699302828409,
          14.480433068624357,
          20.0941247242253,
          18.91312576697125,
          15.366035701782723,
          13.599872480563032,
          18.706286732743425,
          13.927960736964776,
          14.870601181092301,
          13.729792889540757,
          13.468949395466627,
          15.72976898565525,
          12.944802493583865,
          12.856477721919859,
          12.841926086239699,
          12.802018824631606,
          12.529549846804239,
          13.353246200375441,
          13.024458784398025,
          12.328297157597735,
          13.598210893026213,
          12.73788608768122,
          12.654832785691672,
          13.32126410414533,
          12.348311780914058,
          11.90208823506425,
          11.514413221095635,
          11.57677879953772,
          11.970071591012847,
          16.92967301655591,
          12.396672450430025,
          13.821286891534076,
          10.4541327623817,
          11.14276861175289,
          10.972272283662626,
          10.07298573052011,
          10.747356131794007,
          11.20056305474382,
          14.186058354571584,
          13.124426081897766,
          11.083337729539329,
          13.022607485453287,
          10.234986328497165,
          10.546051393679488,
          10.407323286785342,
          13.462784263176646
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial220",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          44.09991523308483
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial221",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          44.677900562441444
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial222",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          42.41721452542437
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial223",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          32.5861391207067,
          56.967406172093334,
          41.67385997617148,
          39.746138192773834,
          32.323746719980626
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial224",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          38.41528292400081,
          47.93587528011663,
          33.06129323757761,
          31.85581194869871,
          33.90359983986956
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial225",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          41.831614114404694
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial226",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          45.70764239241437
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial227",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75
         ],
         "y": [
          37.907154967145225,
          38.79063696202224,
          46.47882867828617,
          27.802116921277552,
          24.47457702760774,
          22.775600603925504,
          25.824989582464948,
          32.7980504384855,
          23.618139406529867,
          20.78872063489464,
          22.404352591289737,
          23.659369553976912,
          24.822472192407623,
          27.397206717390354,
          19.492812722679076,
          18.477025985717773,
          20.581919832927426,
          23.35355564830749,
          21.990887525604993,
          26.032700654937,
          22.149199509039157,
          17.213036451882463,
          20.49761728736443,
          33.3998683836402,
          32.563303893174584,
          22.386755222227514,
          18.24684315968335,
          21.970846331216457,
          19.278635319655503,
          33.16112881171994,
          29.001489127554546,
          14.697644551595053,
          15.701951057930303,
          19.87158238790869,
          14.973928009591452,
          15.582606276845544,
          18.360836036806184,
          13.256218320955107,
          13.096118461794969,
          15.536698225067884,
          20.01231066385905,
          20.881282201627407,
          24.436358056417326,
          12.721002834599192,
          21.209486162759426,
          18.840954741811366,
          15.41501322800551,
          12.701941412638842,
          13.101618665989822,
          12.663513912418024,
          14.16358812843881,
          15.255379924929239,
          16.091045069500684,
          14.431720004818304,
          13.475495803646925,
          11.374555618782354,
          11.952294760603246,
          11.10504210867533,
          14.489159684840256,
          15.05348357534021,
          12.07943708528348,
          10.724991550290488,
          11.164318092470246,
          10.532937026605374,
          9.64368356534136,
          11.040163350299123,
          9.773029509598647,
          15.130187305977675,
          15.621780651371653,
          15.697789835736033,
          12.690730218964864,
          12.285915382509309,
          14.0801774932117,
          13.452928093390737,
          10.764544859165099
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial228",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          38.263973468687475,
          69.44510278469178,
          41.49214947708254,
          39.86009489230024
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial229",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          34.3748155299241,
          77.0494047335493,
          43.43507171258694,
          39.515363103975126,
          42.02995480173002
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial230",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6
         ],
         "y": [
          33.82494326335628,
          40.5984342001318,
          35.42266320019233,
          31.48179900936964,
          28.49566909355846,
          28.598938143350246
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial231",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          31.941978066917358,
          64.991750577601,
          43.94278896921049,
          33.83355540764041,
          30.853695629088858
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial232",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6
         ],
         "y": [
          34.92468641637787,
          35.23426479246558,
          51.809026857701745,
          30.212104471718394,
          28.80386504506677,
          32.12132224416345
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial233",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          49.67975542021961
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial234",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          139.14192714536094
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial235",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial236",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          247.76773902458874
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial237",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          45.62902599427758
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial238",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          31.27420940244101,
          38.166855246070924,
          34.168543792352445,
          34.07532076331658,
          33.10318314156881
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial239",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          41.1685763413344
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial240",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          339.9692802119061
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial241",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          68.13862768033655
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial242",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          43.40920192439382
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial243",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          50.3530016643245
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial244",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          37.33688072266617,
          74.55629085137592,
          38.811558855258355,
          46.63086275550408
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial245",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          37.07056837934789,
          63.656938537349546,
          31.516713336231263,
          32.788434641148015,
          34.271619145463156
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial246",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          44.853214853178194
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial247",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39
         ],
         "y": [
          31.20677546369351,
          39.9031527139307,
          53.83072432851404,
          33.87330822828339,
          27.697771677156773,
          35.394692847399206,
          25.78148592196829,
          22.312559499973204,
          22.576102202500753,
          24.548937045461763,
          25.63782467105524,
          28.118299050059743,
          21.877031962076824,
          23.033024144366504,
          24.850810896090376,
          19.17945293488541,
          22.98632983075894,
          18.511072957418797,
          16.656719952094846,
          20.29424479724915,
          22.083138768265886,
          27.808479278068233,
          40.02421988510504,
          43.42634718980246,
          18.33042787536373,
          17.85327654737767,
          18.742295753665086,
          15.506114796894353,
          14.444800361385191,
          15.24074056671887,
          17.991230406412264,
          16.41156946352827,
          29.503472289418788,
          17.16081998406387,
          18.620213547373204,
          19.234622102442795,
          20.19976552327474,
          17.24668906762348,
          16.170641325353607
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial248",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          48.18553887343988
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial249",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          54.183947369335144
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial250",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          50.691176747888086
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial251",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42
         ],
         "y": [
          32.73950997019202,
          32.972070709476625,
          35.34053805591614,
          35.829177592828025,
          27.605621865125208,
          35.50965036221636,
          25.383857199816198,
          22.112757659539945,
          28.187702178955078,
          23.586991643517965,
          27.088562911118917,
          22.224120101308436,
          20.436388667036848,
          19.664309749758342,
          22.854419351593265,
          21.810681909080444,
          24.33807945251465,
          24.068718995505233,
          17.960714797663496,
          17.415781850737286,
          21.79225954195348,
          29.875291932889116,
          22.632284350511505,
          17.98735317757459,
          17.542466861445728,
          19.29256870300789,
          16.052856600381496,
          15.227355166179377,
          17.77383241420839,
          24.219386852853667,
          19.218054523312947,
          15.138944393251,
          15.899470042407028,
          16.215838657162053,
          19.59911454983843,
          18.719723546408055,
          20.05953225856874,
          16.545025903034986,
          22.018700452354864,
          29.250018267127558,
          17.409071123696926,
          18.47698608646548
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial252",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          32.753684982051695,
          70.63687586590527,
          41.83509304077645,
          45.19811360428973,
          35.152296546998066
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial253",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          390.8009886702871
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial254",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          48.47310923754684
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial255",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          43.473196029663086
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial256",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          49.64640541386798
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial257",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11
         ],
         "y": [
          39.0992657421081,
          48.593873714043845,
          36.66571434145051,
          31.80112137833262,
          26.310185936408313,
          24.87903232109256,
          26.23675002121344,
          27.946935777741718,
          27.962961832682293,
          29.106365607036807,
          28.629228080191265
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial258",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          59.058771552109135
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial259",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          37.576307110670136,
          60.72880687558554,
          36.84166750093786,
          52.170497956314705
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial260",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          303.8016615456682
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial261",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29
         ],
         "y": [
          38.90074809004621,
          34.26459109298582,
          35.53731676427329,
          31.161611045279155,
          25.72327539397449,
          22.079566412824924,
          36.36968314938429,
          24.794219660565137,
          21.26807953284039,
          22.677765233729914,
          20.66045400572986,
          22.557299404609495,
          18.863896587030673,
          19.384686330469645,
          21.13072902400319,
          26.564531264266346,
          23.222679417307784,
          24.544093480924282,
          15.794723882907775,
          19.127502255323456,
          29.596371611928554,
          29.15270343253283,
          20.43346084036478,
          16.427515533881458,
          23.45772135354639,
          27.28557813070654,
          25.122231940913007,
          19.20343411453371,
          17.759046585579227
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial262",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6
         ],
         "y": [
          35.265092834224546,
          43.375668471421655,
          49.075977883687834,
          33.879636562936675,
          28.521863410143347,
          30.203503337332872
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial263",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          49.379747344226374
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial264",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          78.22285616494776
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial265",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial266",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46
         ],
         "y": [
          30.25233692076148,
          64.3645162815001,
          39.37530399725689,
          36.49954353890768,
          29.75806262628819,
          26.707904365973743,
          24.270050296938518,
          22.09302582779551,
          25.107047864092074,
          30.901223888242146,
          23.06175788631284,
          22.4675123168201,
          24.685253003748453,
          30.186980270757907,
          19.557552810606918,
          21.785781689775668,
          28.700660085290426,
          21.251121474475397,
          21.851956313218526,
          18.097333272298176,
          21.474263726211177,
          27.50488081210997,
          17.89597620615145,
          26.19884317289523,
          19.659086281691142,
          17.60638261810551,
          16.62602652573004,
          15.866599509386512,
          17.542795801550394,
          21.7773381054886,
          18.779136285549257,
          15.18851889633551,
          17.66482590466011,
          26.24396800219528,
          19.56182069700908,
          15.11982818541488,
          18.994511030553802,
          15.328539732025892,
          18.313260621171658,
          17.00582076282036,
          17.09275469741201,
          21.58778327073508,
          16.266141371998362,
          19.290131692963886,
          17.545839937721812,
          20.83653484127386
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial267",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          39.61146671016042
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial268",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          53.38260991786554
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial269",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59
         ],
         "y": [
          37.949678002334224,
          32.01680175657195,
          30.637302290133345,
          27.324715172372212,
          26.13553580617517,
          22.662159818943923,
          21.688931123997136,
          21.826257364536687,
          28.144977569580078,
          20.988577827205503,
          20.829696097025057,
          22.453322495871443,
          20.508464464327183,
          19.90251150363829,
          19.545077827887805,
          21.920259351652813,
          19.159613880684706,
          20.90159289042155,
          17.48221664118573,
          20.7750343617385,
          18.878592475643003,
          20.4087865488316,
          22.839283408188237,
          16.691036852394664,
          19.626751170894963,
          26.079499671129675,
          24.02468518513005,
          16.19878608424489,
          16.44966697692871,
          16.1058576475314,
          14.526473293459512,
          17.14798989334727,
          20.285688121144364,
          19.134535549132803,
          16.906385158135638,
          13.978190468578804,
          14.049865319476865,
          14.198216422786558,
          15.284612105144717,
          14.201225482351411,
          23.081239948427775,
          21.00018792811448,
          18.010628979380538,
          13.894520666541123,
          12.954518418971116,
          14.786353909872412,
          13.403894098793588,
          12.574840227762857,
          12.392426103111205,
          14.600600622533783,
          17.537667150419903,
          20.560576539698655,
          13.02804577059862,
          13.909190387260622,
          16.130761588492046,
          13.77678618392324,
          12.919416691229596,
          14.626718753721656,
          13.276276774522735
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial270",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56
         ],
         "y": [
          32.21282797712621,
          56.89881099918024,
          39.038164123287046,
          43.449333811193945,
          29.612447025330088,
          27.602854333272795,
          24.53440984090169,
          24.567002722887487,
          31.534814586484334,
          22.047099245273,
          20.958371201181798,
          25.815088504698217,
          20.988344991110203,
          21.315160472218583,
          22.63280287021544,
          20.171554914334926,
          21.46783782214653,
          18.983087686988398,
          20.803491755229672,
          21.83393645868069,
          21.748626739998173,
          19.917817255345785,
          20.317702386437393,
          20.11427144306462,
          21.996785388729435,
          20.667477227808014,
          18.0376686313288,
          23.516476545876603,
          16.325621232753846,
          19.104210388369676,
          22.593220175766362,
          22.79785525314207,
          21.770913550524206,
          16.283269703872804,
          15.558863647584992,
          17.174710351277174,
          16.218928639481707,
          17.0193552156774,
          19.169551546980696,
          18.41756819128021,
          15.2878801376839,
          16.92546666928423,
          22.624851025216948,
          21.315574863092685,
          15.789624113377517,
          13.564487077356354,
          17.133577509624203,
          14.322110997952096,
          18.5078196254203,
          23.059051839316762,
          24.592592084310887,
          20.277304362475387,
          13.973928529072584,
          19.28811685825751,
          14.494308060746851,
          14.054547852616968
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial271",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          48.67538359107041
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial272",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94
         ],
         "y": [
          34.575862310766205,
          33.9564038873688,
          36.50495465596517,
          27.456576649735613,
          31.862987394255352,
          22.637578142367726,
          21.90726092578919,
          25.04632087645492,
          21.868616429770864,
          21.345766036491085,
          20.661334456467046,
          21.523873491985043,
          21.04374119518249,
          21.63201361555394,
          23.29904134486749,
          27.01853162874051,
          19.193942721297102,
          30.75665061454463,
          24.568648547661013,
          25.744636209999644,
          23.50387070818645,
          18.548878801547414,
          21.5120282677131,
          19.5278533299764,
          22.29532469772711,
          20.91486923093718,
          21.459398099077426,
          22.273048912606587,
          25.00460520798598,
          23.30003093316303,
          24.51200385985336,
          16.531775932001874,
          16.327333659660525,
          14.484541869745023,
          14.159168809410033,
          15.070785979914472,
          14.806344900673967,
          14.371896953117556,
          15.934748959735158,
          16.5806331246849,
          14.248196221948639,
          14.514766917965277,
          15.493668881858268,
          15.720871219790078,
          13.483892867235634,
          20.853311011461706,
          18.00470653782046,
          14.435139981711783,
          13.024967209110415,
          13.397322026694694,
          14.01165036457341,
          13.215311763732414,
          12.782995402328368,
          14.41939865670553,
          13.208700389396853,
          11.648476344783132,
          12.24401201852938,
          14.601923384317537,
          15.398797981138152,
          21.13033223345997,
          13.251426735544593,
          13.301418630088248,
          11.715978676710671,
          12.495413307251969,
          12.835177444830173,
          10.577183680805733,
          12.44581724182377,
          10.80636483091649,
          16.04949495269031,
          9.802377061146062,
          12.6360800518253,
          10.185696132783967,
          10.25066826983196,
          10.405572429905092,
          11.300301040091165,
          9.848760341241107,
          9.49928981889554,
          12.32391866823522,
          20.691944525493838,
          12.383960483520012,
          12.038860561401863,
          9.706642247797028,
          9.377435377942838,
          8.336457074173097,
          9.699022847462476,
          9.587773536278949,
          10.910852680361367,
          9.04520660493432,
          8.942364541495719,
          9.352870460448226,
          9.74902089436849,
          10.28289201782971,
          9.566809596084967,
          8.881411637717147
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial273",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          42.88207586024835
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial274",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          162.87260759555227
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial275",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          63.90771723181252
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial276",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47
         ],
         "y": [
          34.30533067966864,
          35.87299407400736,
          37.60376573578129,
          28.62770733406873,
          27.314025754851055,
          29.33498565549773,
          25.477342977756408,
          21.58295640712831,
          21.313767169549212,
          21.05203972017862,
          23.413629966053538,
          23.871131199162182,
          21.235067065169172,
          20.980243147873296,
          20.365371331935975,
          21.840794943212494,
          24.287643727248277,
          19.480173103208465,
          19.226809594689346,
          19.794603712190458,
          24.545645938656193,
          23.434904718786722,
          18.534286002802656,
          17.830864123212613,
          17.416332353421343,
          20.052843698641148,
          20.96069787188274,
          23.960696460754892,
          18.160278165243504,
          20.992852466862377,
          15.913517207634158,
          20.178546990805525,
          16.360594392791995,
          17.720267272577054,
          14.978345622861289,
          15.2507820982274,
          14.019267376845445,
          14.140396443808951,
          14.716347895986665,
          14.057542684601575,
          15.183646163320153,
          14.268838704116945,
          20.126526033975246,
          21.128646648996245,
          21.56582565617755,
          19.381465113259914,
          14.636502304697425
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial277",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          41.01052167938977
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial278",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92
         ],
         "y": [
          31.40546060577641,
          52.42888501795327,
          35.133326724292786,
          36.07458962851423,
          27.433885977520205,
          24.792826799842402,
          24.151017212286227,
          22.821446519557053,
          26.906043231002684,
          23.689038563550003,
          21.420341158300882,
          23.47474791363972,
          20.702126999211504,
          20.53469126011298,
          21.11449883623821,
          21.361346066482668,
          19.33023848184725,
          18.875134072652678,
          21.730769893987393,
          20.947184865067644,
          19.515391838259813,
          25.965151127760972,
          22.89492754432244,
          16.588111861934507,
          18.176051039036697,
          16.93910260704475,
          20.548779976077196,
          17.862180182604284,
          16.91915767948802,
          15.878818969416425,
          22.391492378420946,
          19.018818886299442,
          21.795421894972886,
          16.68306936868807,
          17.731122148715382,
          14.591086783060213,
          15.674828661166556,
          15.906746344837716,
          20.569162105157123,
          17.773617054388776,
          14.184698290941192,
          18.994027478908137,
          15.299562702334024,
          14.76607949946954,
          16.23254904708242,
          19.51298805950134,
          16.77373327084673,
          13.995737649560944,
          14.222906143684698,
          14.140832451300893,
          13.159244925026002,
          16.195836679722234,
          13.470144248590238,
          15.637646628589165,
          17.90204340461793,
          12.222023545241937,
          12.869504091216296,
          12.643278114194793,
          16.4738698121978,
          22.691289234936722,
          13.621097929109403,
          13.381383174803199,
          13.99527701711267,
          12.504727650464066,
          15.142767720106171,
          11.949277133476443,
          14.550794244781743,
          14.158586052375112,
          12.167953026003953,
          14.863079753348497,
          13.341311524553996,
          23.557956602515244,
          15.925750267214891,
          12.910529439042254,
          11.2612652390953,
          11.50032990153243,
          10.973215401657228,
          11.783397379929458,
          12.541617122122911,
          12.805122608091773,
          12.007134344519638,
          10.63941675666871,
          15.278164002953506,
          14.587690190571111,
          13.571599146214927,
          11.534689368271247,
          12.131011714780234,
          13.441178771538462,
          11.334284084599192,
          11.564229034795993,
          12.947073789146858,
          10.858372091277827
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial279",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          33.24512821290551,
          67.94399360718766,
          32.794056233351796,
          33.19791451120764,
          31.644937933945073
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial280",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          47.491282982554864
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial281",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8
         ],
         "y": [
          37.24281419583453,
          89.23216142111677,
          49.35046067276621,
          29.634045096916882,
          26.828738328887194,
          25.532853444417317,
          34.754297023866236,
          27.5276134459953
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial282",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          61.59421424555585
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial283",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          42.944778411369015
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial284",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          50.569285478049174
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial285",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          281.34749076036906
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial286",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          45.78303076581257
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial287",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          64.83640946799177
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial288",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          40.30767478012457
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial289",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          44.22214415015244
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial290",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6
         ],
         "y": [
          31.328929947643744,
          55.156243797240215,
          38.21293077236268,
          35.18141085927079,
          28.708627127050384,
          28.941888669642008
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial291",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          44.985967124380714
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial292",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82
         ],
         "y": [
          34.68018776808328,
          35.37141295952526,
          28.597654125554776,
          27.68732574897084,
          24.58892056224792,
          23.454488583696566,
          28.268490023729278,
          25.424995949597864,
          21.11300241268747,
          22.28902894306958,
          26.69565771459564,
          20.350234977598113,
          21.566802079115458,
          20.06466859336791,
          20.50637405674632,
          21.363810453957658,
          20.25576799284152,
          19.68281051976894,
          19.01442303696299,
          21.020541493485613,
          22.14278140494494,
          20.662348351827482,
          17.5114647547404,
          17.533287025079495,
          17.195424529595105,
          16.878418131572445,
          18.822265663767247,
          21.52428090475439,
          20.696430609478213,
          15.538127829388875,
          15.785668489409655,
          15.39835518162425,
          15.139004389444986,
          15.092814127604166,
          23.90935595442609,
          26.377358987079404,
          20.582605734104064,
          17.765104185274947,
          15.133102168881797,
          14.821750726157088,
          15.263062546892863,
          16.2938558609505,
          15.426348337313025,
          14.920420406310539,
          14.453703787268662,
          14.412523215379172,
          15.34595862442885,
          14.31544924945366,
          14.066604924395802,
          15.535620317226503,
          20.271071937995227,
          15.069929464076592,
          14.297012065484271,
          13.87905330968097,
          17.344184689405488,
          16.027149153918756,
          14.09783197418461,
          13.706010585877953,
          15.543006741903662,
          12.443185806274414,
          13.25427156929078,
          14.230938159353364,
          15.564069243950572,
          15.41688682587166,
          13.183630020637823,
          12.878771960250731,
          12.800235980894508,
          16.054747263590496,
          13.117323030301225,
          12.313555322042326,
          13.376764468061246,
          14.22851531486201,
          13.65062081716894,
          15.405449929276132,
          13.467660524011627,
          13.06602815302407,
          14.545522379681346,
          13.362311347713316,
          14.99878327439471,
          12.18002961321575,
          12.028523212525903,
          12.956950241957253
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial293",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          39.026596689611914
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial294",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          62.57756979097196
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial295",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85
         ],
         "y": [
          32.094310791511845,
          38.56232348496352,
          33.045164154797064,
          27.529936084902385,
          27.875124721992307,
          23.24183605163078,
          21.786451587832072,
          21.70005398261838,
          20.70473690343097,
          25.562184077937427,
          20.591580949178557,
          20.714941086807872,
          19.76027397217789,
          22.27085558573405,
          23.15210395130685,
          20.641142713345165,
          18.84423651346346,
          20.39364304581309,
          17.785536455914258,
          17.600809407428027,
          21.905120880623173,
          18.822565474161287,
          23.935005125960682,
          15.201289626641001,
          18.91250879396268,
          20.660470621372625,
          15.547824673536347,
          14.268646899277602,
          22.554023215441198,
          16.646550984886602,
          15.659405010502512,
          14.951049998523743,
          22.33132774849248,
          15.134999887730048,
          13.601004538497305,
          13.45360248069453,
          13.966183639154202,
          17.803107502014658,
          18.83216461708875,
          14.764325428784378,
          18.639986736018482,
          23.379033918303204,
          17.307946639332346,
          13.986450311614245,
          13.519825881089622,
          13.416668597275649,
          14.861209094039793,
          13.146506146686834,
          14.320088906016776,
          14.86299538030857,
          13.78438035453238,
          13.040336655407417,
          12.457682834408148,
          11.329413158137625,
          13.543401454522357,
          12.42120358226745,
          12.203303693755855,
          11.65352127416347,
          11.629615931006951,
          13.981205668875841,
          12.400507671077078,
          10.812911180945916,
          12.333273189823801,
          11.637729854118533,
          11.923405282865694,
          12.874077215427306,
          13.01283840241471,
          12.372983420767435,
          10.750645168428498,
          10.888644730172507,
          11.341092311269868,
          10.423906341800844,
          10.648100023347189,
          10.811304999560845,
          10.133446631392813,
          11.838123647178092,
          13.054112891840742,
          11.604077331418914,
          11.567289096553152,
          12.376848957402919,
          12.77233007477551,
          11.563601424054403,
          10.597490589793136,
          11.288569737256058,
          12.820367216094722
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial296",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial297",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          40.035866590050176
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial298",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51
         ],
         "y": [
          33.53146148309475,
          31.357769400123658,
          31.917177479441573,
          26.435744494926638,
          35.26363442583782,
          24.313981172515124,
          23.41441050583754,
          23.75575093525212,
          22.819045276176638,
          27.355753658263662,
          21.353072670417102,
          21.339877058820026,
          21.60425798679755,
          21.350756079200806,
          19.807612147757677,
          19.515025790144758,
          20.49591821965163,
          21.071774940180585,
          18.156148895015562,
          18.465577055768268,
          27.74132908456694,
          20.469304852369355,
          18.80646915745929,
          20.01083187940644,
          28.976221177636123,
          20.975014965708663,
          16.22980478333264,
          16.469768136497436,
          15.489862015576866,
          16.09806819078399,
          16.32100338664481,
          16.015004181280368,
          15.860169116074477,
          16.192687670389812,
          17.423826271925517,
          14.488888911115446,
          16.14687977767572,
          15.050849767235237,
          14.567665472263243,
          16.790353751764066,
          14.044357284297789,
          14.989756467865735,
          14.304896571771886,
          14.282008721576474,
          19.14812976558034,
          18.155661125493243,
          18.91713297464014,
          20.869784409437724,
          14.28884022410323,
          15.193394203496172,
          15.165117628206083
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial299",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28
         ],
         "y": [
          37.680165345106666,
          35.32404851525779,
          35.972909896354366,
          27.60261630236618,
          28.954955093259734,
          24.107550892403456,
          22.42362990030428,
          24.340899413194112,
          21.55081272900589,
          20.982983643446513,
          20.960942524235424,
          23.462694989956493,
          20.503237235836867,
          20.555055626039582,
          18.52112501811206,
          18.819074056982025,
          19.365118104267896,
          15.983942737424277,
          16.271831776068463,
          21.122721470468413,
          50.60932730077728,
          36.239929695439535,
          28.613541114621047,
          20.22416200870421,
          21.5129423219014,
          22.169960874852126,
          20.034306689006527,
          22.14040129746848
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial300",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          55.085337569073936
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial301",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          40.27729527543231
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial302",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          33.166800010495066,
          68.45116207463954,
          34.197795588795735,
          34.10803754542901
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial303",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          36.17965403611098,
          91.74289256770436,
          44.40075240096426,
          37.39951888914031
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial304",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          58.393357718863136
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial305",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          44.62603703940787
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial306",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          35.95139977990127,
          65.77831966121022,
          38.52407441488126,
          35.20489889625611
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial307",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          219658.1280487805
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial308",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26
         ],
         "y": [
          32.34845234320416,
          32.403819246989926,
          41.40196984764037,
          27.40434342671216,
          30.506397216300655,
          23.553814353012456,
          29.23921352479516,
          21.69022731470868,
          21.87588260619621,
          24.4961220966122,
          23.489929586891236,
          20.824905023342225,
          24.129721866390568,
          20.78486834890474,
          19.60890778487291,
          19.213915065052063,
          20.46370169786903,
          21.247548824403346,
          20.75192301060126,
          22.03938783475054,
          22.522697293661473,
          20.363718218919708,
          22.169559742377057,
          21.136599889615688,
          22.336745642065033,
          21.647784706053695
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial309",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93
         ],
         "y": [
          31.82337898936698,
          60.90862990588677,
          33.59561006809638,
          35.4931356383533,
          27.266752785783474,
          26.178297166901874,
          23.54517652930283,
          30.268636005680737,
          22.19416810632721,
          26.605803931631694,
          21.210171389385938,
          23.325825109714415,
          20.95703859251689,
          22.76721099915543,
          20.6515021052787,
          22.06274955253291,
          23.703973304934618,
          20.159539191703487,
          21.546002783426424,
          19.549131137568775,
          26.257664269548123,
          18.975648492332397,
          19.141425698753295,
          17.19193879569449,
          18.78479310167514,
          18.831232830760925,
          27.902171468347067,
          25.343280575139737,
          18.409772849664456,
          19.824059106470123,
          17.19553252739635,
          19.65185007234899,
          18.01401340670702,
          15.64612072270091,
          15.828248016233367,
          17.199799142232756,
          14.67502697115022,
          15.004208735334196,
          14.760354158354968,
          13.980681783784696,
          14.316425897241608,
          15.838497099837637,
          14.323353806162268,
          16.819498147421736,
          16.694710615204603,
          16.35345493099554,
          22.969158963459293,
          17.090820103156858,
          13.960398821326775,
          14.207760756577903,
          14.624675556896179,
          13.80084114539914,
          13.764153131624548,
          14.074196846504522,
          15.693544248255288,
          18.853056690557217,
          15.250586618252886,
          13.188878090401007,
          12.707937085531592,
          13.699349953876279,
          12.966437037398176,
          13.830513744819456,
          13.299413122781893,
          12.51825178347952,
          13.653228837300123,
          14.228399920269725,
          16.512829454933726,
          16.52203797906395,
          18.69274187475685,
          15.086262571133249,
          13.170806279996546,
          13.94529833444735,
          13.344748310926484,
          11.913092078232184,
          11.432722719704232,
          11.693710311641539,
          13.119030246889688,
          11.946343499470533,
          11.533380958122935,
          11.297329522729889,
          10.968418001159419,
          10.316696775637991,
          10.053647929090795,
          11.00822924792282,
          10.99942587643135,
          12.747563928123412,
          19.398345869731127,
          11.727778682863809,
          10.419781366984049,
          13.517120268286728,
          11.133393760619125,
          10.40677804869365,
          10.587035977743506
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial310",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          114.90805407268245
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial311",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          40.68947319092789
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial312",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          198.16212835544493
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial313",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          49.00056745947861
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial314",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          36.4650311043592,
          52.17021635102063,
          29.034159311434117,
          34.76309793363742,
          25.73181664071432,
          31.18826449014307,
          23.561550574574046,
          25.010390801158376,
          22.252536292967758,
          22.23162357981612,
          30.50381913223887,
          21.109111111338546,
          22.047093042513218,
          22.796057228150406,
          21.39131087016284,
          21.038146716792408,
          22.424431560485342,
          25.239345162864623,
          20.101832932573025,
          19.732568833886123,
          21.56896488840987,
          21.40421278108426,
          18.481120745340984,
          18.918322214266148,
          18.743924233971573,
          21.997092471859318,
          26.022673645639806,
          20.286184256638936,
          17.165155224683808,
          16.49932611667044,
          16.597538304522754,
          17.552960713704426,
          17.255665802374118,
          19.406916052345338,
          26.52917367268384,
          21.571509849734422,
          16.502670683511873,
          14.841703081518654,
          14.257017631840899,
          15.603990198150884,
          15.972499281410279,
          17.18021043141683,
          18.068862550626925,
          17.428936221735263,
          13.58638408319737,
          14.169720145745005,
          14.913989191132833,
          14.644758488104596,
          14.61295564387872,
          14.036135425412558,
          13.411656813892892,
          15.694722974203467,
          17.959220413270035,
          13.693730509378076,
          13.661990491355338,
          14.617522573083397,
          14.442131755797844,
          13.15327655978319,
          13.614866326494914,
          12.941787448355822,
          13.962554195062902,
          13.110720068458619,
          13.723004914880768,
          13.568404740434351,
          12.115914709199735,
          13.392383544425654,
          14.14468394643892,
          12.655131704438993,
          12.184277015003731,
          11.812616200951057,
          12.766181728704188,
          14.14874235788981,
          11.537087959971855,
          12.946721286308474,
          11.282262267135993,
          12.769054250019353,
          11.463941039108649,
          9.995540882513776,
          9.955809453638588,
          9.829624086860719,
          11.281068848400581,
          10.052553006303988,
          9.982389601265512,
          9.4795489737658,
          9.618577115904024,
          9.998034182602797,
          9.890846992895856,
          9.785168267847077,
          11.000141539224764,
          10.978492252225799,
          11.047940498445092,
          10.687959965651597,
          11.9160582069459,
          8.979677227454458,
          9.638474359744933,
          9.932968767677865,
          10.726380991741893,
          11.708252612168227,
          10.45559888932763
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial315",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          248.90790762552402
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial316",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          56.68914004070003
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial317",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          52.02466753827847
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial318",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          124.00166128515228
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial319",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial320",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          67.19160014827077
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial321",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          53.3258426635246
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial322",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91
         ],
         "y": [
          34.63881399573349,
          34.533607731020545,
          41.10436472078649,
          27.450344488872744,
          27.367986989215137,
          22.893356214693892,
          27.944869452375706,
          22.91868074928842,
          21.88670795719798,
          25.07699629930946,
          22.59918219093385,
          22.171814616133528,
          20.48681333588391,
          19.983934836659007,
          21.66297357450656,
          22.022415409243205,
          21.559154603539444,
          20.28862253049525,
          26.05569617728877,
          26.358394622802734,
          29.369400737731436,
          23.940598542128154,
          24.359215402990824,
          19.23412232282685,
          18.231250313239368,
          18.615071854940275,
          24.383291508124127,
          20.787981281435588,
          22.502441731894887,
          22.84573463501969,
          22.179010670359542,
          16.67623968046855,
          16.236359456690348,
          16.97740109761556,
          15.66525596525611,
          16.0632423385372,
          18.361680658852183,
          18.022199871094248,
          18.63289151540617,
          15.155243726280647,
          19.776904113893586,
          15.336043063218032,
          19.123586646909636,
          18.02810428588371,
          15.642453387500794,
          15.343493818267573,
          20.49256613196396,
          13.713048213865699,
          21.911069621884725,
          17.660439940972058,
          19.722207829235046,
          13.933877239382364,
          18.73353478191345,
          15.481791566057902,
          13.605037937319375,
          13.129948336903642,
          13.951642943591606,
          13.931232537680525,
          15.084530140326276,
          13.878314940910029,
          14.524900583716912,
          14.203049155754771,
          14.039471478966194,
          14.722855226780341,
          11.584025375242156,
          12.388431835949905,
          14.152781323688787,
          13.684280566083707,
          12.143016582582055,
          13.721653806484811,
          11.293954794969016,
          12.155502738022223,
          13.257192340323595,
          13.88714515096773,
          14.609388987223307,
          23.25655088967424,
          14.316352627141688,
          11.58325078235409,
          11.4485075958376,
          12.36356743758287,
          10.452346972333707,
          10.801912959029035,
          10.816981183804147,
          16.353390274978267,
          16.516943489632954,
          12.245184014483195,
          13.193011392422807,
          15.389677334607132,
          11.87382218895889,
          13.218981091569109,
          12.101332051967217
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial323",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          49.37628164523986
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial324",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          35.15752820270817,
          54.92425906173582,
          38.540833480959016,
          41.70148585870014
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial325",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          35.250948541532686,
          67.36280221086207,
          34.84369409762747,
          39.160816409723544
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial326",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          43.08359183334723
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial327",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          56.81147300906298
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial328",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          46.046950363531344
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial329",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6
         ],
         "y": [
          27.676065274370394,
          60.28486977553949,
          42.674539488505545,
          34.547872047114176,
          34.89758117799836,
          28.094701038143498
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial330",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          11487.05380621189
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial331",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          39.54825436972021
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial332",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          38.089158050413054,
          87.89920000525994,
          46.84253466226221
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial333",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          124.73416745565771
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial334",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          38.66318152202823
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial335",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          36.10400589113313,
          50.85904519926242,
          60.691544602556924,
          46.617124262864024
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial336",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          46.41115734441494
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial337",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          42.36272627357545
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial338",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          34.8204232502759,
          72.64629891248254,
          34.631808505794865,
          42.64579326350515
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial339",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          38.28299196755014,
          61.44009383907163,
          34.00768917362864,
          32.37721173356219,
          30.19774767247642
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial340",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          43.87644843745038
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial341",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          66.10356313813993
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial342",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          50.97136219924058
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial343",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          38.13751248615544,
          78.58304676583143,
          44.3376833287681
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial344",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          33.488789550657195,
          77.14747743684102,
          32.914608808067754,
          41.56811970036204,
          30.256465400137554
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial345",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial346",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          118.85502388806847
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial347",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          104.80222202704205
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial348",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43
         ],
         "y": [
          34.38716973715681,
          36.926309166885005,
          33.652224951643284,
          27.507534786937683,
          25.054816532910355,
          25.776336685428774,
          21.8818700371719,
          29.41712077071027,
          27.816442257020533,
          21.85069692037939,
          22.928090708042546,
          21.802538445325403,
          22.826048176463058,
          21.76828472788741,
          19.82309347633424,
          19.995716723000132,
          22.539681457891696,
          28.18597463282143,
          19.729236455467657,
          19.46904014184223,
          25.218146843638845,
          24.75457797787054,
          19.081961817857696,
          16.339181636407123,
          18.560919226669682,
          19.986421476534712,
          19.758260246214828,
          15.422867526852988,
          17.08201625482823,
          17.62856786619357,
          17.34351372137302,
          16.450391110365953,
          15.11876135725316,
          16.908437178386905,
          15.88709574598607,
          16.287568061332394,
          15.720183170907866,
          15.4537580226495,
          15.985222692412089,
          15.681116406510515,
          15.655347087518956,
          15.774400121797392,
          16.96083531728605
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial349",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          283.4967616631733
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial350",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          49.28382293577117
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial351",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          48.04543468816494
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial352",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          50.67895926498785
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial353",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          216.32326885936706
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial354",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          33.243362519799206,
          61.27227829723823,
          34.18551113159676,
          36.26044453256498
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial355",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          37.338369633124124,
          55.13815106027494,
          66.42119585983153
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial356",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          32.99958639997777,
          94.35265654277026,
          33.726805586155834,
          48.19505762860058
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial357",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          85.89552009396436
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial358",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          62.97872692201196
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial359",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          36.46171884614278,
          34.12581355397294,
          39.35043133371244,
          35.55049911747134
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial360",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          50.4888746370145
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial361",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          41.92617391570797
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial362",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          35.092838411408714,
          82.85325721802751,
          36.737952395183285,
          42.35602858008408
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial363",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          156.49583993306973
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial364",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          59.837182114763955
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial365",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          61.00403325150653
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial366",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          317.81013724474406
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial367",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          246.01632752457286
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial368",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial369",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          54.784298408322215
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial370",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          259.3395102896341
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial371",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          67.84021914102198
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial372",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          44.41239575641911
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial373",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          37.639950248284066,
          80.77142960463112,
          42.79926852094449
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial374",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          47.45492395540563
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial375",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          63.80058719666024
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial376",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          242.00782664229232
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial377",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          77.52292977309808
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial378",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          44.95283133033814
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial379",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          74.13873960913682
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial380",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          41.83696349849546
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial381",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          48.661388676340984
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial382",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          43.7314254326549
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial383",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          41.572327730132315
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial384",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          44.902002598212015
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial385",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          56.382133763010906
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial386",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          212.32566213220116
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial387",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6
         ],
         "y": [
          34.47082463706412,
          39.06961807002866,
          35.80164923319003,
          27.89660703457468,
          30.81023146466511,
          37.70577252395754
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial388",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          48.33995893524914
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial389",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          40.33283494158489
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial390",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          30.910253865932063,
          67.93217542694836,
          40.19805964027963,
          36.80139578842535,
          30.449959855738694
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial391",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial392",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          107.28375845808324
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial393",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          43.830184967537235
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial394",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          122.1235947027439
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial395",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          31.64974758489345,
          36.01073235612574,
          30.026791394241457,
          34.25008711776113,
          37.82285922911109
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial396",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          51.043105955046364
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial397",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          49.53592638465447
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial398",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          64.84961408909743
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial399",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          59.35407232269039
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial400",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          50.04140354559674
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial401",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          60.13603833826577
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial402",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          397580.1239837398
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial403",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          146.36530310157838
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial404",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          53.79427691203792
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial405",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          43.21287172208957
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial406",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6
         ],
         "y": [
          31.971347777824093,
          62.84953770211072,
          40.62200671870534,
          35.235594974300724,
          28.957156034019903,
          30.050029646090376
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial407",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          54.53042264488654
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial408",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          35.377479010481174,
          73.46549503977705,
          41.55646449763601,
          38.45895537709802
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial409",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          52.982836281381005
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial410",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          40.94432213635949
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial411",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          50.16810418725983
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial412",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          34.01878864009206,
          70.18068279483454,
          40.6299026613313,
          41.14382494174368
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial413",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          185.52344122165587
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial414",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          31.92884412625941,
          81.12260536255876,
          35.28910992009853,
          41.97109058038975,
          32.879302978515625
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial415",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          61.05613714698853
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial416",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          41.82444346047998
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial417",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial418",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          48.15414099964669
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial419",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          54.16755254481866
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial420",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          48.93330929143642
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial421",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          167.13254404455665
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial422",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          66.22915844800995
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial423",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          59.62909794629105
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial424",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          31.554339028955475,
          72.8965352996578,
          32.05283966684729,
          31.21472479657429,
          34.218173003778226
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial425",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          45.05582198476404
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial426",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          59.378573968158506
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial427",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          312.28701856659677
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial428",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          36.04816077007511,
          43.98139503913197,
          30.855070967015212,
          35.171591564891784,
          31.681276538507724
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial429",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          61.3854840286379
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial430",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          49.793651952976134
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial431",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6
         ],
         "y": [
          28.970234273895016,
          65.71185767941358,
          36.81336190448544,
          34.06270959125302,
          39.355159170259306,
          29.08561475490167
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial432",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          55.28706545946075
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial433",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65
         ],
         "y": [
          34.37795921263656,
          35.82073914132467,
          44.269784694764674,
          28.693323988255447,
          26.222675928255406,
          25.793846487029782,
          27.830346285812254,
          21.8742788671478,
          22.02298417130137,
          20.96130164851987,
          30.369851089105374,
          35.29822575949072,
          20.8391808843225,
          19.966415296725142,
          25.44195784592047,
          25.373119943510225,
          19.532862926886335,
          18.172820184288955,
          19.659156543452564,
          21.581401080619997,
          17.156152872535273,
          19.059532227554943,
          22.26531110934126,
          20.372182427383052,
          23.360782685318615,
          16.53824657347144,
          19.255857475404817,
          20.096618621329952,
          17.505787748631423,
          24.703968854454473,
          26.786650231214075,
          21.728842463919786,
          15.166446484201323,
          16.75152358388513,
          15.241480486179755,
          15.693344372074778,
          14.68729447155464,
          14.474803932313996,
          16.781965914780532,
          19.646116117151774,
          20.599047373949997,
          20.945865569075917,
          21.36719374928048,
          17.995624371660433,
          15.465625607870459,
          15.133434218119799,
          14.177041798103147,
          19.84791110589252,
          20.520843722955966,
          27.393862251343766,
          15.262707849828208,
          15.160149085812453,
          13.467099391347993,
          15.149434601388327,
          13.162951577969682,
          19.08651630277556,
          15.654060743688568,
          13.587152147680763,
          15.145531600083762,
          13.226225186169632,
          14.00966030213891,
          19.12140484941684,
          24.60248339273096,
          21.421950890765928,
          20.067649577691302
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial434",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          73.7450236808963
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial435",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          111.76305984869236
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial436",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          38.716891854759155
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial437",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          111.03921074595878
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial438",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          51.108245601498986
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial439",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          252.53708022202903
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial440",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial441",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          41.879948003505305
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial442",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61
         ],
         "y": [
          37.640631419856376,
          33.98567098912185,
          29.687456782271223,
          30.590911601617083,
          27.02851106287018,
          26.051252085988114,
          21.47356977695372,
          21.1475261905329,
          23.564178947510758,
          31.73257264858339,
          35.665444831538004,
          20.655403866031307,
          20.039766931921488,
          35.640260572355935,
          21.298204468517767,
          19.37847221188429,
          28.101862403435437,
          20.164826292332595,
          18.388605551991038,
          19.786105078410326,
          20.682759277219695,
          24.755275276618274,
          18.67761539831394,
          17.896621618813615,
          16.500607451772304,
          20.474955318419912,
          16.595235669515965,
          15.822004620621843,
          17.927436813106382,
          14.59405546265889,
          20.533599915543224,
          16.722279657193315,
          14.522186209515828,
          16.00784431240423,
          14.693402158535593,
          16.484062590250154,
          16.78478837207081,
          19.92402208529837,
          18.24035238250484,
          15.082515266852651,
          14.236483007911744,
          16.16124636177125,
          15.417408919915921,
          13.474628138348338,
          16.614633552427215,
          13.993541469418906,
          12.695670593075636,
          18.188667421418476,
          13.972396261323759,
          12.508949915568033,
          11.340287317105425,
          12.056194119337128,
          20.45803984975427,
          17.26024485022072,
          20.389394481007646,
          19.94167510862273,
          23.08594550156012,
          21.09211817795668,
          21.39531101443903,
          20.120937091548267,
          12.094565003868041
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial443",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          59.27754999176273
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial444",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          204.21930551916603
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial445",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          93.41950740659139
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial446",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6
         ],
         "y": [
          34.20966939228337,
          31.690507346052463,
          29.561202274105412,
          27.384997670243425,
          35.439794183746585,
          31.92490574596374
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial447",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          36.47714283020516,
          93.57543089331651,
          46.95031679355032,
          39.02552727179798
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial448",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          40.11401779671026
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial449",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          80.50963865450727
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial450",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          64.13583355415159
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial451",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          38984.004795477646
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial452",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          65.43482034574679
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial453",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          32.74500620462061,
          55.57738879444153,
          37.789636177745294,
          30.900171341934826,
          33.50278381409684
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial454",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          39.78197364496991
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial455",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          32.62704419702049,
          44.006998542847676,
          36.92926368093103,
          31.090863096035594,
          29.897170973987116
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial456",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          52.381586493515385
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial457",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          32.075123034841646,
          76.50715848100864,
          35.05932697823377,
          37.8314577893513,
          33.12430423643531
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial458",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8
         ],
         "y": [
          34.9551036493565,
          59.04675063466638,
          34.67308543755756,
          29.97572593378827,
          28.66308970567657,
          25.345855185656042,
          25.110533629006486,
          35.28867254799944
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial459",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          56.38795620057641
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial460",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          27.266374324395404,
          33.62168609805224,
          32.31285034737936,
          25.779022744031458,
          24.96334651233704,
          23.636590352872524,
          27.550947111796557,
          22.419206836359287,
          21.37892730836946,
          20.66817491422824,
          24.89549788808435,
          20.597756393556672,
          23.378934736174298,
          33.38837001769523,
          31.05487224920009,
          22.339442167824846,
          20.05732415362102,
          18.379438020349518,
          18.801766294774,
          22.014253104605327,
          20.657615025838215,
          16.91318553056174,
          17.77626131414398,
          16.354525790951115,
          16.17349267199757,
          18.175593244351024,
          28.611486202333033,
          22.194212688663143,
          16.99053135538489,
          18.412249960550447,
          19.92573491538443,
          20.01258206561329,
          26.74368862989472,
          22.116100016648208,
          16.471856466153774
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial461",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          51.41195486425384
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial462",
         "type": "scatter",
         "x": [
          1,
          2,
          3
         ],
         "y": [
          37.69194669645976,
          65.21179527965019,
          37.83802573661494
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial463",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          52.16180410617736
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial464",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39
         ],
         "y": [
          33.62126044916913,
          38.109442951233405,
          41.77141555538022,
          29.450202864360033,
          25.28778187821551,
          25.842918116871903,
          31.110446790369547,
          21.528898223628843,
          21.077584258909148,
          21.162356694539387,
          23.0380688644037,
          23.23039444093782,
          30.528108100580976,
          21.293188048572077,
          19.69763694546087,
          18.20034548906776,
          18.344527174786823,
          17.871038266313754,
          16.95985790966003,
          22.1887547407693,
          22.92584205255276,
          33.0475421223214,
          22.27228426739452,
          21.65249787307367,
          16.84531455311349,
          27.325449842747634,
          28.356506518232145,
          19.02770486692103,
          15.695714229490699,
          19.31007388355286,
          22.92573240326672,
          19.857257649181335,
          17.599330452399524,
          17.504033910549754,
          24.129120648391847,
          23.86334107561809,
          18.188912717307485,
          29.228289518899064,
          21.46221747049471
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial465",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          38.547750984750145
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial466",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial467",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51
         ],
         "y": [
          36.03901743695019,
          38.87517206455634,
          36.733608633522095,
          28.92924525873448,
          23.761193376246506,
          21.820215101164532,
          21.154538108081354,
          20.953592633813376,
          28.54832650781647,
          23.366411720834126,
          20.22034203909277,
          19.42422924196817,
          19.463808222514828,
          22.337022641809977,
          17.90759927082837,
          17.483709715246185,
          16.49063125858462,
          17.23701207230731,
          21.235515734044515,
          17.804116722044906,
          21.39837823263029,
          24.38157901918985,
          29.332911747257885,
          36.99438727774271,
          16.56421013188556,
          16.389255608969588,
          21.108590567984233,
          23.658961582959183,
          17.41352935728988,
          14.88277074767322,
          15.619134678104059,
          20.192911535743775,
          22.386489573532973,
          19.271502456044765,
          17.404755421770297,
          19.34952334272183,
          17.34552853281905,
          17.613503432855374,
          20.994720458984375,
          14.330493950262301,
          13.46592564311454,
          14.06694250959691,
          14.584899979878248,
          18.16417802640093,
          18.408345020883452,
          13.947044108941302,
          17.13818218262215,
          21.824805965268514,
          18.399383281304583,
          25.872217519496516,
          22.0837885383668
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial468",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          62.07793901024795
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial469",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          51.215962417726594
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial470",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          58.05392608022302
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial471",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          109.60460179026533
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial472",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45
         ],
         "y": [
          37.03575452168783,
          36.00125860198727,
          32.37334842216678,
          27.393476470699156,
          24.22079757752457,
          22.611360565433657,
          23.124267469576704,
          24.022901643582475,
          21.19135316988317,
          24.095068753250246,
          22.77169788949858,
          21.163820561354722,
          21.037309165892562,
          19.505601518522433,
          18.303393170116394,
          25.87188069413348,
          26.407913347569906,
          18.380076516934526,
          18.74104123774583,
          16.919560556489277,
          15.478562417069101,
          15.38852621093998,
          15.401621384349296,
          17.3790723366466,
          25.303072564970186,
          14.74333895706549,
          18.10003890060797,
          15.061020323900673,
          16.959211357240754,
          20.45156872757082,
          25.29496250307657,
          16.996305271862,
          14.688580450972891,
          18.269834611474014,
          13.429110038571242,
          13.9115259666753,
          15.53789943602027,
          15.670272904682935,
          13.91418456255905,
          15.392854977429398,
          14.530731185664976,
          13.657410257230929,
          14.637306445982398,
          17.855693181355793,
          18.655327176660055
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial473",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          56.33578584252334
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial474",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          55.32761801742926
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial475",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          62.32797033418485
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial476",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          300.2363846941692
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial477",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86
         ],
         "y": [
          33.81285089012084,
          40.54650934731088,
          35.14484385358609,
          26.988580750256048,
          25.600933059444273,
          22.97289699461402,
          22.37807362254073,
          28.795598828695653,
          22.885800508948847,
          20.31126958180249,
          23.020178259872807,
          20.15390476754041,
          21.684355541942566,
          19.722857591582507,
          20.460569094836227,
          24.016955321397237,
          19.683529109489626,
          18.822461655469446,
          18.530873694070955,
          18.24202573977835,
          21.360333900141523,
          20.641731479303623,
          17.418834283099912,
          16.72766511033221,
          17.584450675220026,
          16.04245007522707,
          18.70943803709697,
          18.90288278533191,
          27.13801723573266,
          18.714061892129543,
          15.663502987807359,
          19.63527684095429,
          24.011311182161656,
          23.22447651188548,
          16.537913361215978,
          15.24729061126709,
          15.341547679125778,
          16.036826459372914,
          14.99156665026657,
          18.827990849812824,
          17.958462575586832,
          15.71475237559497,
          19.076310460160418,
          17.13209394129311,
          14.436699316753604,
          14.534894586578618,
          13.371944481764382,
          13.833967875658981,
          13.214345110141165,
          13.931079585377763,
          13.345873886976785,
          15.233250540446459,
          17.2686071861081,
          15.442840839789167,
          13.977775310113177,
          14.218281265196762,
          12.1297720622241,
          12.326106970872337,
          13.178885839818939,
          19.75851820348724,
          17.850263820431095,
          16.83823949147046,
          14.451840772861388,
          12.727552522488725,
          12.030951872104552,
          11.770355899159501,
          11.596760261349562,
          11.884151489754034,
          11.255332117158224,
          10.621395855415159,
          10.085142883828016,
          12.16008582541613,
          10.831701836934904,
          11.647358537689458,
          11.254698412205146,
          9.59261414287536,
          18.406956533106364,
          19.125904021224354,
          13.290685552891677,
          11.017412239943093,
          12.157545989121848,
          14.301837665278738,
          10.8528137323333,
          11.06425923448268,
          12.206533400993036,
          12.08708521990272
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial478",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          43.41854920426035
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial479",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          33.75552631781353,
          33.29039131722799,
          36.88981295019631,
          26.90470194622753,
          33.62064674811634,
          23.6394412032957,
          26.14674050245828,
          23.946056443501295,
          21.862131630502095,
          31.922234449929338,
          21.0708433166752,
          22.785966950703443,
          22.39523895387727,
          21.616987197379757,
          21.41988102982684,
          20.862089157104492,
          20.970804873520766,
          21.72378772642554,
          23.73408339275577
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial480",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          36.36042494114822,
          70.07475317978277,
          37.94221191096112,
          34.867920542150976
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial481",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          38.084159339346535
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial482",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          57.12592573088359
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial483",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          46.28449515986249
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial484",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          59.305752296757895
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial485",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          46.747470700643895
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial486",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          255.24422274953952
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial487",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          75.85401947517705
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial488",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          30.547899060133027,
          82.49525836231263,
          39.067714024365436,
          35.52913028437917,
          37.11475397125492
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial489",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          51.96833562463279
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial490",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          63.46498877052369
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial491",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41
         ],
         "y": [
          31.864195195640004,
          33.68739914312595,
          33.91817720924936,
          26.462253679104936,
          24.09361180251207,
          23.105589021512163,
          23.552464647990902,
          22.159437629265515,
          21.749736351695486,
          23.029649858552265,
          23.887528318699783,
          21.875475875730437,
          21.09037266708002,
          31.08742025421887,
          21.433621437568974,
          21.40412400408489,
          23.936112861323164,
          20.071722728450123,
          18.649461498105428,
          18.867801898863256,
          17.064943499681426,
          18.368913162045363,
          16.96086988991838,
          17.036541721685147,
          16.692990178984356,
          29.37527566615159,
          21.772674653588272,
          15.98046671859617,
          15.249645496771588,
          14.885903575556066,
          14.640862503672034,
          16.082546536515398,
          14.950214037081091,
          14.972728527658354,
          15.549421124342011,
          16.246638088691526,
          15.823981548712506,
          25.09843299059364,
          17.173718747084703,
          24.59492731482033,
          17.896112434263152
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial492",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          null
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial493",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          52.92464248533172
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial494",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6
         ],
         "y": [
          27.41062581442236,
          80.85189217668238,
          56.869719993777395,
          29.28272813316283,
          32.11558470687246,
          29.395638799279684
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial495",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4
         ],
         "y": [
          37.40534883204514,
          53.929164979516,
          36.357305232102306,
          42.66335960326156
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial496",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          47.36405724626246
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial497",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          47.74123888093282
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial498",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          127.88851333245998
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial499",
         "type": "scatter",
         "x": [
          1
         ],
         "y": [
          78.74676352400121
         ]
        }
       ],
       "layout": {
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Intermediate Values Plot"
        },
        "xaxis": {
         "title": {
          "text": "Step"
         }
        },
        "yaxis": {
         "title": {
          "text": "Intermediate Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "dimensions": [
          {
           "label": "Objective Value",
           "range": [
            34.07929231287018,
            49.5200790777439
           ],
           "values": [
            49.5200790777439,
            34.07929231287018
           ]
          },
          {
           "label": "activation",
           "range": [
            0,
            0
           ],
           "ticktext": [
            "Swish"
           ],
           "tickvals": [
            0
           ],
           "values": [
            0,
            0
           ]
          },
          {
           "label": "dropout_rate",
           "range": [
            0.3138445295264759,
            0.3452449144029921
           ],
           "values": [
            0.3138445295264759,
            0.3452449144029921
           ]
          },
          {
           "label": "gnn_dim",
           "range": [
            0,
            1
           ],
           "ticktext": [
            "512",
            "1024"
           ],
           "tickvals": [
            0,
            1
           ],
           "values": [
            0,
            1
           ]
          },
          {
           "label": "hidden_dim",
           "range": [
            0,
            0
           ],
           "ticktext": [
            "384"
           ],
           "tickvals": [
            0
           ],
           "values": [
            0,
            0
           ]
          },
          {
           "label": "lr",
           "range": [
            -5.026089036530513,
            -4.604942377892336
           ],
           "ticktext": [
            "9.42e-06",
            "1e-05",
            "2.48e-05"
           ],
           "tickvals": [
            -5.026089036530513,
            -5,
            -4.604942377892336
           ],
           "values": [
            -5.026089036530513,
            -4.604942377892336
           ]
          },
          {
           "label": "momentum",
           "range": [
            -0.07115139553136464,
            -0.06073082490818521
           ],
           "ticktext": [
            "0.849",
            "0.869"
           ],
           "tickvals": [
            -0.07115139553136464,
            -0.06073082490818521
           ],
           "values": [
            -0.07115139553136464,
            -0.06073082490818521
           ]
          },
          {
           "label": "optimizer",
           "range": [
            0,
            0
           ],
           "ticktext": [
            "SGD"
           ],
           "tickvals": [
            0
           ],
           "values": [
            0,
            0
           ]
          },
          {
           "label": "weight_decay",
           "range": [
            -4.875091539667371,
            -4.319927012384718
           ],
           "ticktext": [
            "1.33e-05",
            "4.79e-05"
           ],
           "tickvals": [
            -4.875091539667371,
            -4.319927012384718
           ],
           "values": [
            -4.319927012384718,
            -4.875091539667371
           ]
          }
         ],
         "labelangle": 30,
         "labelside": "bottom",
         "line": {
          "color": [
           49.5200790777439,
           34.07929231287018
          ],
          "colorbar": {
           "title": {
            "text": "Objective Value"
           }
          },
          "colorscale": [
           [
            0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1,
            "rgb(8,48,107)"
           ]
          ],
          "reversescale": true,
          "showscale": true
         },
         "type": "parcoords"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Parallel Coordinate Plot"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           1,
           2,
           3,
           4,
           5,
           10,
           12,
           13,
           15,
           16,
           17,
           20,
           23,
           24,
           26,
           31,
           33,
           37,
           58,
           59,
           60,
           61,
           63,
           79,
           82,
           83,
           84,
           85,
           93,
           94,
           98,
           99,
           101,
           102,
           105,
           106,
           113,
           114,
           121,
           124,
           132,
           143,
           147,
           149,
           154,
           160,
           164,
           184,
           211,
           219,
           227,
           247,
           251,
           261,
           266,
           269,
           270,
           272,
           276,
           278,
           295,
           298,
           299,
           308,
           309,
           314,
           322,
           348,
           433,
           442,
           460,
           464,
           467,
           472,
           477,
           491
          ],
          "colorbar": {
           "title": {
            "text": "Trial"
           },
           "x": 1,
           "xpad": 40
          },
          "colorscale": [
           [
            0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1,
            "rgb(8,48,107)"
           ]
          ],
          "line": {
           "color": "Grey",
           "width": 0.5
          },
          "showscale": true
         },
         "mode": "markers",
         "name": "Feasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "Swish",
          "Swish",
          "Swish",
          "GELU",
          "Swish",
          "Swish",
          "Swish",
          "Swish",
          "ReLU",
          "ReLU",
          "ReLU",
          "ReLU",
          "ReLU",
          "ReLU",
          "ReLU",
          "ReLU",
          "ReLU",
          "ReLU",
          "GELU",
          "ReLU",
          "ReLU",
          "ReLU",
          "ReLU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "Swish",
          "Swish",
          "Swish",
          "Swish",
          "Swish",
          "Swish",
          "Swish",
          "Swish",
          "Swish",
          "Swish",
          "Swish",
          "Swish",
          "Swish",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "GELU",
          "Swish",
          "Swish"
         ],
         "xaxis": "x",
         "y": [
          19.12524386925426,
          34.07929231287018,
          49.5200790777439,
          21.228072864253345,
          23.573560389076793,
          15.720193653571897,
          16.18551901685513,
          16.268691442846283,
          16.11321484945654,
          17.036176603984057,
          16.528479723426386,
          15.11039302794914,
          15.67170184220725,
          17.375625951503352,
          16.52663712385224,
          15.900944097255303,
          16.987479923217276,
          17.419660025495823,
          18.378675825227567,
          15.858131369924157,
          15.320490627754026,
          16.054533741338467,
          15.147421030494256,
          15.908379360912292,
          9.134948959195517,
          11.503930750901137,
          10.371832537457225,
          9.798203173691665,
          15.266971277996776,
          16.736201464645262,
          12.814706848888862,
          15.095547412469134,
          14.861772777588387,
          13.01377438335884,
          10.456899410340844,
          15.308860716780996,
          13.73273016766804,
          11.597237742044092,
          11.842361892141946,
          11.728818932199866,
          11.516944877500457,
          13.458390577052667,
          14.368151478651093,
          14.77558566302788,
          9.124408535841035,
          15.113592093553,
          13.922429937657302,
          13.38084742693397,
          14.338142867979965,
          10.07298573052011,
          9.64368356534136,
          14.444800361385191,
          15.138944393251,
          15.794723882907775,
          15.11982818541488,
          12.392426103111205,
          13.564487077356354,
          8.336457074173097,
          14.019267376845445,
          10.63941675666871,
          10.133446631392813,
          14.044357284297789,
          15.983942737424277,
          19.213915065052063,
          10.053647929090795,
          8.979677227454458,
          10.452346972333707,
          15.11876135725316,
          13.162951577969682,
          11.340287317105425,
          16.17349267199757,
          15.695714229490699,
          13.46592564311454,
          13.429110038571242,
          9.59261414287536,
          14.640862503672034
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": [
           1,
           2,
           3,
           4,
           5,
           10,
           12,
           13,
           15,
           16,
           17,
           20,
           23,
           24,
           26,
           31,
           33,
           37,
           58,
           59,
           60,
           61,
           63,
           79,
           82,
           83,
           84,
           85,
           93,
           94,
           98,
           99,
           101,
           102,
           105,
           106,
           113,
           114,
           121,
           124,
           132,
           143,
           147,
           149,
           154,
           160,
           164,
           184,
           211,
           219,
           227,
           247,
           251,
           261,
           266,
           269,
           270,
           272,
           276,
           278,
           295,
           298,
           299,
           308,
           309,
           314,
           322,
           348,
           433,
           442,
           460,
           464,
           467,
           472,
           477,
           491
          ],
          "colorbar": {
           "title": {
            "text": "Trial"
           },
           "x": 1,
           "xpad": 40
          },
          "colorscale": [
           [
            0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1,
            "rgb(8,48,107)"
           ]
          ],
          "line": {
           "color": "Grey",
           "width": 0.5
          },
          "showscale": false
         },
         "mode": "markers",
         "name": "Feasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.32279148171296423,
          0.3452449144029921,
          0.3138445295264759,
          0.3576019968562552,
          0.32672051933279234,
          0.2514117597604075,
          0.2513605709326827,
          0.2539114663014676,
          0.27724349003807336,
          0.2750336719354182,
          0.298421958322783,
          0.3966604263049831,
          0.3996290969690424,
          0.3986804230594082,
          0.38380299543850616,
          0.2620629079167512,
          0.39068129452156414,
          0.3238815843153331,
          0.37470298453969536,
          0.2636706864969312,
          0.275506892609232,
          0.2731898641278501,
          0.27332450928665775,
          0.2615298760987676,
          0.26234125716523543,
          0.26100251167272137,
          0.2531696766329015,
          0.3562465266073639,
          0.3934287640738944,
          0.39322284558999004,
          0.38645954341635763,
          0.3868917770803063,
          0.38493067355770816,
          0.37747008134327,
          0.38400594715175823,
          0.38523942467790323,
          0.3768374139214,
          0.37807162727718013,
          0.37704609872846584,
          0.3773631447522626,
          0.37190771369349446,
          0.3868556342903326,
          0.3825658562457436,
          0.37098292733399957,
          0.3807823079171701,
          0.39119426891928016,
          0.3031341789373579,
          0.380250945452507,
          0.3934249503905161,
          0.30503452797449326,
          0.39967898982478123,
          0.3873689472754691,
          0.3843876814470079,
          0.3668839580006545,
          0.3789455284721415,
          0.2614910288587241,
          0.2629713632294253,
          0.26835948201850623,
          0.2590970855716058,
          0.25655551254389586,
          0.26376792880323235,
          0.26672149928890154,
          0.2618979494451215,
          0.2551677674660391,
          0.2751109485606933,
          0.3037109072828516,
          0.31278802667276084,
          0.261161479720435,
          0.3774428959934768,
          0.3536126077063912,
          0.38752265473680564,
          0.3847369190905062,
          0.32753050280126955,
          0.25593973143533477,
          0.25185775731601945,
          0.36865162795464285
         ],
         "xaxis": "x2",
         "y": [
          19.12524386925426,
          34.07929231287018,
          49.5200790777439,
          21.228072864253345,
          23.573560389076793,
          15.720193653571897,
          16.18551901685513,
          16.268691442846283,
          16.11321484945654,
          17.036176603984057,
          16.528479723426386,
          15.11039302794914,
          15.67170184220725,
          17.375625951503352,
          16.52663712385224,
          15.900944097255303,
          16.987479923217276,
          17.419660025495823,
          18.378675825227567,
          15.858131369924157,
          15.320490627754026,
          16.054533741338467,
          15.147421030494256,
          15.908379360912292,
          9.134948959195517,
          11.503930750901137,
          10.371832537457225,
          9.798203173691665,
          15.266971277996776,
          16.736201464645262,
          12.814706848888862,
          15.095547412469134,
          14.861772777588387,
          13.01377438335884,
          10.456899410340844,
          15.308860716780996,
          13.73273016766804,
          11.597237742044092,
          11.842361892141946,
          11.728818932199866,
          11.516944877500457,
          13.458390577052667,
          14.368151478651093,
          14.77558566302788,
          9.124408535841035,
          15.113592093553,
          13.922429937657302,
          13.38084742693397,
          14.338142867979965,
          10.07298573052011,
          9.64368356534136,
          14.444800361385191,
          15.138944393251,
          15.794723882907775,
          15.11982818541488,
          12.392426103111205,
          13.564487077356354,
          8.336457074173097,
          14.019267376845445,
          10.63941675666871,
          10.133446631392813,
          14.044357284297789,
          15.983942737424277,
          19.213915065052063,
          10.053647929090795,
          8.979677227454458,
          10.452346972333707,
          15.11876135725316,
          13.162951577969682,
          11.340287317105425,
          16.17349267199757,
          15.695714229490699,
          13.46592564311454,
          13.429110038571242,
          9.59261414287536,
          14.640862503672034
         ],
         "yaxis": "y2"
        },
        {
         "marker": {
          "color": [
           1,
           2,
           3,
           4,
           5,
           10,
           12,
           13,
           15,
           16,
           17,
           20,
           23,
           24,
           26,
           31,
           33,
           37,
           58,
           59,
           60,
           61,
           63,
           79,
           82,
           83,
           84,
           85,
           93,
           94,
           98,
           99,
           101,
           102,
           105,
           106,
           113,
           114,
           121,
           124,
           132,
           143,
           147,
           149,
           154,
           160,
           164,
           184,
           211,
           219,
           227,
           247,
           251,
           261,
           266,
           269,
           270,
           272,
           276,
           278,
           295,
           298,
           299,
           308,
           309,
           314,
           322,
           348,
           433,
           442,
           460,
           464,
           467,
           472,
           477,
           491
          ],
          "colorbar": {
           "title": {
            "text": "Trial"
           },
           "x": 1,
           "xpad": 40
          },
          "colorscale": [
           [
            0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1,
            "rgb(8,48,107)"
           ]
          ],
          "line": {
           "color": "Grey",
           "width": 0.5
          },
          "showscale": false
         },
         "mode": "markers",
         "name": "Feasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [
          384,
          1024,
          512,
          384,
          384,
          384,
          384,
          384,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024,
          1024
         ],
         "xaxis": "x3",
         "y": [
          19.12524386925426,
          34.07929231287018,
          49.5200790777439,
          21.228072864253345,
          23.573560389076793,
          15.720193653571897,
          16.18551901685513,
          16.268691442846283,
          16.11321484945654,
          17.036176603984057,
          16.528479723426386,
          15.11039302794914,
          15.67170184220725,
          17.375625951503352,
          16.52663712385224,
          15.900944097255303,
          16.987479923217276,
          17.419660025495823,
          18.378675825227567,
          15.858131369924157,
          15.320490627754026,
          16.054533741338467,
          15.147421030494256,
          15.908379360912292,
          9.134948959195517,
          11.503930750901137,
          10.371832537457225,
          9.798203173691665,
          15.266971277996776,
          16.736201464645262,
          12.814706848888862,
          15.095547412469134,
          14.861772777588387,
          13.01377438335884,
          10.456899410340844,
          15.308860716780996,
          13.73273016766804,
          11.597237742044092,
          11.842361892141946,
          11.728818932199866,
          11.516944877500457,
          13.458390577052667,
          14.368151478651093,
          14.77558566302788,
          9.124408535841035,
          15.113592093553,
          13.922429937657302,
          13.38084742693397,
          14.338142867979965,
          10.07298573052011,
          9.64368356534136,
          14.444800361385191,
          15.138944393251,
          15.794723882907775,
          15.11982818541488,
          12.392426103111205,
          13.564487077356354,
          8.336457074173097,
          14.019267376845445,
          10.63941675666871,
          10.133446631392813,
          14.044357284297789,
          15.983942737424277,
          19.213915065052063,
          10.053647929090795,
          8.979677227454458,
          10.452346972333707,
          15.11876135725316,
          13.162951577969682,
          11.340287317105425,
          16.17349267199757,
          15.695714229490699,
          13.46592564311454,
          13.429110038571242,
          9.59261414287536,
          14.640862503672034
         ],
         "yaxis": "y3"
        },
        {
         "marker": {
          "color": [
           1,
           2,
           3,
           4,
           5,
           10,
           12,
           13,
           15,
           16,
           17,
           20,
           23,
           24,
           26,
           31,
           33,
           37,
           58,
           59,
           60,
           61,
           63,
           79,
           82,
           83,
           84,
           85,
           93,
           94,
           98,
           99,
           101,
           102,
           105,
           106,
           113,
           114,
           121,
           124,
           132,
           143,
           147,
           149,
           154,
           160,
           164,
           184,
           211,
           219,
           227,
           247,
           251,
           261,
           266,
           269,
           270,
           272,
           276,
           278,
           295,
           298,
           299,
           308,
           309,
           314,
           322,
           348,
           433,
           442,
           460,
           464,
           467,
           472,
           477,
           491
          ],
          "colorbar": {
           "title": {
            "text": "Trial"
           },
           "x": 1,
           "xpad": 40
          },
          "colorscale": [
           [
            0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1,
            "rgb(8,48,107)"
           ]
          ],
          "line": {
           "color": "Grey",
           "width": 0.5
          },
          "showscale": false
         },
         "mode": "markers",
         "name": "Feasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [
          256,
          384,
          384,
          512,
          384,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          384,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256,
          256
         ],
         "xaxis": "x4",
         "y": [
          19.12524386925426,
          34.07929231287018,
          49.5200790777439,
          21.228072864253345,
          23.573560389076793,
          15.720193653571897,
          16.18551901685513,
          16.268691442846283,
          16.11321484945654,
          17.036176603984057,
          16.528479723426386,
          15.11039302794914,
          15.67170184220725,
          17.375625951503352,
          16.52663712385224,
          15.900944097255303,
          16.987479923217276,
          17.419660025495823,
          18.378675825227567,
          15.858131369924157,
          15.320490627754026,
          16.054533741338467,
          15.147421030494256,
          15.908379360912292,
          9.134948959195517,
          11.503930750901137,
          10.371832537457225,
          9.798203173691665,
          15.266971277996776,
          16.736201464645262,
          12.814706848888862,
          15.095547412469134,
          14.861772777588387,
          13.01377438335884,
          10.456899410340844,
          15.308860716780996,
          13.73273016766804,
          11.597237742044092,
          11.842361892141946,
          11.728818932199866,
          11.516944877500457,
          13.458390577052667,
          14.368151478651093,
          14.77558566302788,
          9.124408535841035,
          15.113592093553,
          13.922429937657302,
          13.38084742693397,
          14.338142867979965,
          10.07298573052011,
          9.64368356534136,
          14.444800361385191,
          15.138944393251,
          15.794723882907775,
          15.11982818541488,
          12.392426103111205,
          13.564487077356354,
          8.336457074173097,
          14.019267376845445,
          10.63941675666871,
          10.133446631392813,
          14.044357284297789,
          15.983942737424277,
          19.213915065052063,
          10.053647929090795,
          8.979677227454458,
          10.452346972333707,
          15.11876135725316,
          13.162951577969682,
          11.340287317105425,
          16.17349267199757,
          15.695714229490699,
          13.46592564311454,
          13.429110038571242,
          9.59261414287536,
          14.640862503672034
         ],
         "yaxis": "y4"
        },
        {
         "marker": {
          "color": [
           1,
           2,
           3,
           4,
           5,
           10,
           12,
           13,
           15,
           16,
           17,
           20,
           23,
           24,
           26,
           31,
           33,
           37,
           58,
           59,
           60,
           61,
           63,
           79,
           82,
           83,
           84,
           85,
           93,
           94,
           98,
           99,
           101,
           102,
           105,
           106,
           113,
           114,
           121,
           124,
           132,
           143,
           147,
           149,
           154,
           160,
           164,
           184,
           211,
           219,
           227,
           247,
           251,
           261,
           266,
           269,
           270,
           272,
           276,
           278,
           295,
           298,
           299,
           308,
           309,
           314,
           322,
           348,
           433,
           442,
           460,
           464,
           467,
           472,
           477,
           491
          ],
          "colorbar": {
           "title": {
            "text": "Trial"
           },
           "x": 1,
           "xpad": 40
          },
          "colorscale": [
           [
            0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1,
            "rgb(8,48,107)"
           ]
          ],
          "line": {
           "color": "Grey",
           "width": 0.5
          },
          "showscale": false
         },
         "mode": "markers",
         "name": "Feasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.00016821076514418284,
          0.00002483462588772003,
          0.000009416965155849704,
          0.00004529522026341851,
          0.000015108839485547171,
          0.00019778176533884577,
          0.0001794071563571159,
          0.00029119051158418147,
          0.0002580131746406671,
          0.00038829019374009174,
          0.00028655500652053083,
          0.0009734992713466208,
          0.00046646849425856896,
          0.0004559478574456893,
          0.0006533321327397485,
          0.0002736701533782797,
          0.0005077854774388577,
          0.0006981973202746956,
          0.0007871754763646908,
          0.00029811082792742207,
          0.0005061254008883885,
          0.0004420920209902722,
          0.0003303636905644971,
          0.00038716004266474186,
          0.0005228448618646409,
          0.0005478646358976391,
          0.0006970885506031969,
          0.0007307017675073904,
          0.0008572762567986945,
          0.0008371586120191461,
          0.0006836145941956344,
          0.0009842844443434418,
          0.0006715705678767695,
          0.0006815932289247624,
          0.0006942532388223274,
          0.0006846229917331851,
          0.0006639319983501419,
          0.0006501435270073714,
          0.0007638339834854481,
          0.0006777804388163587,
          0.0006672913433432091,
          0.0007072870720599108,
          0.0007242441800119607,
          0.0005267826646317277,
          0.0007112525279861455,
          0.0004458455035443059,
          0.0007123986253912668,
          0.0007535970200714215,
          0.000591119540759259,
          0.0005340323929366291,
          0.000617809494718854,
          0.0006890112003441306,
          0.0006175535159740012,
          0.0007572733685600326,
          0.000713418694813003,
          0.0006687300139298326,
          0.0006605755371672007,
          0.0006858724864946909,
          0.0007112098494546964,
          0.0006682737417009588,
          0.000651568224844297,
          0.0006566251260670098,
          0.000682469968481782,
          0.0007116512787633273,
          0.0006221742482883665,
          0.0006250957284443225,
          0.0006854282937101316,
          0.0006203305576102897,
          0.0005704155988689003,
          0.0006620236950166221,
          0.0004657913848936423,
          0.0006927562043496528,
          0.0005965819640688847,
          0.0006734500184595535,
          0.0006791275964628498,
          0.0006644356119965982
         ],
         "xaxis": "x5",
         "y": [
          19.12524386925426,
          34.07929231287018,
          49.5200790777439,
          21.228072864253345,
          23.573560389076793,
          15.720193653571897,
          16.18551901685513,
          16.268691442846283,
          16.11321484945654,
          17.036176603984057,
          16.528479723426386,
          15.11039302794914,
          15.67170184220725,
          17.375625951503352,
          16.52663712385224,
          15.900944097255303,
          16.987479923217276,
          17.419660025495823,
          18.378675825227567,
          15.858131369924157,
          15.320490627754026,
          16.054533741338467,
          15.147421030494256,
          15.908379360912292,
          9.134948959195517,
          11.503930750901137,
          10.371832537457225,
          9.798203173691665,
          15.266971277996776,
          16.736201464645262,
          12.814706848888862,
          15.095547412469134,
          14.861772777588387,
          13.01377438335884,
          10.456899410340844,
          15.308860716780996,
          13.73273016766804,
          11.597237742044092,
          11.842361892141946,
          11.728818932199866,
          11.516944877500457,
          13.458390577052667,
          14.368151478651093,
          14.77558566302788,
          9.124408535841035,
          15.113592093553,
          13.922429937657302,
          13.38084742693397,
          14.338142867979965,
          10.07298573052011,
          9.64368356534136,
          14.444800361385191,
          15.138944393251,
          15.794723882907775,
          15.11982818541488,
          12.392426103111205,
          13.564487077356354,
          8.336457074173097,
          14.019267376845445,
          10.63941675666871,
          10.133446631392813,
          14.044357284297789,
          15.983942737424277,
          19.213915065052063,
          10.053647929090795,
          8.979677227454458,
          10.452346972333707,
          15.11876135725316,
          13.162951577969682,
          11.340287317105425,
          16.17349267199757,
          15.695714229490699,
          13.46592564311454,
          13.429110038571242,
          9.59261414287536,
          14.640862503672034
         ],
         "yaxis": "y5"
        },
        {
         "marker": {
          "color": [
           2,
           3
          ],
          "colorbar": {
           "title": {
            "text": "Trial"
           },
           "x": 1,
           "xpad": 40
          },
          "colorscale": [
           [
            0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1,
            "rgb(8,48,107)"
           ]
          ],
          "line": {
           "color": "Grey",
           "width": 0.5
          },
          "showscale": false
         },
         "mode": "markers",
         "name": "Feasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.8694991766438684,
          0.8488845013853628
         ],
         "xaxis": "x6",
         "y": [
          34.07929231287018,
          49.5200790777439
         ],
         "yaxis": "y6"
        },
        {
         "marker": {
          "color": [
           1,
           2,
           3,
           4,
           5,
           10,
           12,
           13,
           15,
           16,
           17,
           20,
           23,
           24,
           26,
           31,
           33,
           37,
           58,
           59,
           60,
           61,
           63,
           79,
           82,
           83,
           84,
           85,
           93,
           94,
           98,
           99,
           101,
           102,
           105,
           106,
           113,
           114,
           121,
           124,
           132,
           143,
           147,
           149,
           154,
           160,
           164,
           184,
           211,
           219,
           227,
           247,
           251,
           261,
           266,
           269,
           270,
           272,
           276,
           278,
           295,
           298,
           299,
           308,
           309,
           314,
           322,
           348,
           433,
           442,
           460,
           464,
           467,
           472,
           477,
           491
          ],
          "colorbar": {
           "title": {
            "text": "Trial"
           },
           "x": 1,
           "xpad": 40
          },
          "colorscale": [
           [
            0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1,
            "rgb(8,48,107)"
           ]
          ],
          "line": {
           "color": "Grey",
           "width": 0.5
          },
          "showscale": false
         },
         "mode": "markers",
         "name": "Feasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "Adam",
          "SGD",
          "SGD",
          "AdamW",
          "AdamW",
          "Adam",
          "Adam",
          "Adam",
          "Adam",
          "Adam",
          "Adam",
          "Adam",
          "Adam",
          "Adam",
          "Adam",
          "Adam",
          "Adam",
          "AdamW",
          "Adam",
          "Adam",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "Adam",
          "AdamW",
          "Adam",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW",
          "AdamW"
         ],
         "xaxis": "x7",
         "y": [
          19.12524386925426,
          34.07929231287018,
          49.5200790777439,
          21.228072864253345,
          23.573560389076793,
          15.720193653571897,
          16.18551901685513,
          16.268691442846283,
          16.11321484945654,
          17.036176603984057,
          16.528479723426386,
          15.11039302794914,
          15.67170184220725,
          17.375625951503352,
          16.52663712385224,
          15.900944097255303,
          16.987479923217276,
          17.419660025495823,
          18.378675825227567,
          15.858131369924157,
          15.320490627754026,
          16.054533741338467,
          15.147421030494256,
          15.908379360912292,
          9.134948959195517,
          11.503930750901137,
          10.371832537457225,
          9.798203173691665,
          15.266971277996776,
          16.736201464645262,
          12.814706848888862,
          15.095547412469134,
          14.861772777588387,
          13.01377438335884,
          10.456899410340844,
          15.308860716780996,
          13.73273016766804,
          11.597237742044092,
          11.842361892141946,
          11.728818932199866,
          11.516944877500457,
          13.458390577052667,
          14.368151478651093,
          14.77558566302788,
          9.124408535841035,
          15.113592093553,
          13.922429937657302,
          13.38084742693397,
          14.338142867979965,
          10.07298573052011,
          9.64368356534136,
          14.444800361385191,
          15.138944393251,
          15.794723882907775,
          15.11982818541488,
          12.392426103111205,
          13.564487077356354,
          8.336457074173097,
          14.019267376845445,
          10.63941675666871,
          10.133446631392813,
          14.044357284297789,
          15.983942737424277,
          19.213915065052063,
          10.053647929090795,
          8.979677227454458,
          10.452346972333707,
          15.11876135725316,
          13.162951577969682,
          11.340287317105425,
          16.17349267199757,
          15.695714229490699,
          13.46592564311454,
          13.429110038571242,
          9.59261414287536,
          14.640862503672034
         ],
         "yaxis": "y7"
        },
        {
         "marker": {
          "color": [
           1,
           2,
           3,
           4,
           5,
           10,
           12,
           13,
           15,
           16,
           17,
           20,
           23,
           24,
           26,
           31,
           33,
           37,
           58,
           59,
           60,
           61,
           63,
           79,
           82,
           83,
           84,
           85,
           93,
           94,
           98,
           99,
           101,
           102,
           105,
           106,
           113,
           114,
           121,
           124,
           132,
           143,
           147,
           149,
           154,
           160,
           164,
           184,
           211,
           219,
           227,
           247,
           251,
           261,
           266,
           269,
           270,
           272,
           276,
           278,
           295,
           298,
           299,
           308,
           309,
           314,
           322,
           348,
           433,
           442,
           460,
           464,
           467,
           472,
           477,
           491
          ],
          "colorbar": {
           "title": {
            "text": "Trial"
           },
           "x": 1,
           "xpad": 40
          },
          "colorscale": [
           [
            0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1,
            "rgb(8,48,107)"
           ]
          ],
          "line": {
           "color": "Grey",
           "width": 0.5
          },
          "showscale": false
         },
         "mode": "markers",
         "name": "Feasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.000005684834094844966,
          0.000013332403849718848,
          0.00004787105377488972,
          0.000004047981477928513,
          0.0000013791000051589796,
          0.000001698272497464436,
          0.000002494254836160617,
          0.0000023133401851093756,
          0.000002615960502236875,
          0.0000010260527049464841,
          0.000003515884716355651,
          0.00008868914988497863,
          0.00006989393781240468,
          0.00009896011996506398,
          0.00003196270937358899,
          0.0000029607970760683065,
          0.000003921292704647082,
          0.000012257465278792643,
          0.0000018084526585737036,
          0.0000061920954181468565,
          0.0000067314672712150585,
          0.000006455272411962794,
          0.000006332752365633072,
          0.000004141982647370079,
          0.000006454206158060796,
          0.0000037468630112209242,
          0.00000797796721138696,
          0.000011164779854151833,
          0.000005892695867520759,
          0.000007141185921307453,
          0.00008910044784234928,
          0.00007426758100569708,
          0.00007731096998695225,
          0.00008028786810548833,
          0.00007746962586906582,
          0.00007367672970630745,
          0.000047829257875961916,
          0.00003101483668701196,
          0.00008927379680514828,
          0.00008577853595890161,
          0.0000756833045550914,
          0.00006320898803235578,
          0.00009933016191772933,
          0.000016894197144251622,
          0.00002164797185648682,
          0.000011140857978727634,
          0.00008114652457430634,
          0.0000462890573820893,
          0.000011551553281367087,
          0.000011684772228773459,
          0.00007738304830222328,
          0.000016256341623589214,
          0.0000033512939319704674,
          0.000002941374843469981,
          0.00008871961149756802,
          0.000012833561902519631,
          0.000012363843556305364,
          0.000012164081006161759,
          0.000012722438934304967,
          0.000009866546858624851,
          0.000012896784380230221,
          0.000012333161038689704,
          0.00001322437152747346,
          0.000007690613374340105,
          0.000015395768087303467,
          0.000013411681277405122,
          0.00008347373789789642,
          0.0000678308636067942,
          0.0000044917467255026655,
          0.000005803814532956909,
          0.000021692543046116824,
          0.00009481155802701844,
          0.0000043582446833661025,
          0.00006759399315542203,
          0.00009431764861233023,
          0.00001416624565252356
         ],
         "xaxis": "x8",
         "y": [
          19.12524386925426,
          34.07929231287018,
          49.5200790777439,
          21.228072864253345,
          23.573560389076793,
          15.720193653571897,
          16.18551901685513,
          16.268691442846283,
          16.11321484945654,
          17.036176603984057,
          16.528479723426386,
          15.11039302794914,
          15.67170184220725,
          17.375625951503352,
          16.52663712385224,
          15.900944097255303,
          16.987479923217276,
          17.419660025495823,
          18.378675825227567,
          15.858131369924157,
          15.320490627754026,
          16.054533741338467,
          15.147421030494256,
          15.908379360912292,
          9.134948959195517,
          11.503930750901137,
          10.371832537457225,
          9.798203173691665,
          15.266971277996776,
          16.736201464645262,
          12.814706848888862,
          15.095547412469134,
          14.861772777588387,
          13.01377438335884,
          10.456899410340844,
          15.308860716780996,
          13.73273016766804,
          11.597237742044092,
          11.842361892141946,
          11.728818932199866,
          11.516944877500457,
          13.458390577052667,
          14.368151478651093,
          14.77558566302788,
          9.124408535841035,
          15.113592093553,
          13.922429937657302,
          13.38084742693397,
          14.338142867979965,
          10.07298573052011,
          9.64368356534136,
          14.444800361385191,
          15.138944393251,
          15.794723882907775,
          15.11982818541488,
          12.392426103111205,
          13.564487077356354,
          8.336457074173097,
          14.019267376845445,
          10.63941675666871,
          10.133446631392813,
          14.044357284297789,
          15.983942737424277,
          19.213915065052063,
          10.053647929090795,
          8.979677227454458,
          10.452346972333707,
          15.11876135725316,
          13.162951577969682,
          11.340287317105425,
          16.17349267199757,
          15.695714229490699,
          13.46592564311454,
          13.429110038571242,
          9.59261414287536,
          14.640862503672034
         ],
         "yaxis": "y8"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Slice Plot"
        },
        "width": 2400,
        "xaxis": {
         "anchor": "y",
         "categoryarray": [
          "ReLU",
          "GELU",
          "Swish"
         ],
         "categoryorder": "array",
         "domain": [
          0,
          0.103125
         ],
         "title": {
          "text": "activation"
         },
         "type": "category"
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.128125,
          0.23124999999999998
         ],
         "title": {
          "text": "dropout_rate"
         }
        },
        "xaxis3": {
         "anchor": "y3",
         "categoryarray": [
          384,
          512,
          1024
         ],
         "categoryorder": "array",
         "domain": [
          0.25625,
          0.359375
         ],
         "title": {
          "text": "gnn_dim"
         },
         "type": "category"
        },
        "xaxis4": {
         "anchor": "y4",
         "categoryarray": [
          256,
          384,
          512
         ],
         "categoryorder": "array",
         "domain": [
          0.38437499999999997,
          0.48749999999999993
         ],
         "title": {
          "text": "hidden_dim"
         },
         "type": "category"
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0.5125,
          0.615625
         ],
         "title": {
          "text": "lr"
         },
         "type": "log"
        },
        "xaxis6": {
         "anchor": "y6",
         "domain": [
          0.640625,
          0.74375
         ],
         "title": {
          "text": "momentum"
         },
         "type": "log"
        },
        "xaxis7": {
         "anchor": "y7",
         "categoryarray": [
          "Adam",
          "AdamW",
          "SGD",
          "RMSprop"
         ],
         "categoryorder": "array",
         "domain": [
          0.76875,
          0.8718750000000001
         ],
         "title": {
          "text": "optimizer"
         },
         "type": "category"
        },
        "xaxis8": {
         "anchor": "y8",
         "domain": [
          0.8968750000000001,
          1
         ],
         "title": {
          "text": "weight_decay"
         },
         "type": "log"
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Objective Value"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis6": {
         "anchor": "x6",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis7": {
         "anchor": "x7",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis8": {
         "anchor": "x8",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # hyperparameter search space\n",
    "    gnn_dim = trial.suggest_categorical(\"gnn_dim\", [384, 512, 1024])\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [256, 384, 512])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.25, 0.4)\n",
    "    lr = trial.suggest_float(\"lr\", 8e-6, 1e-3, log=True)\n",
    "    activation = trial.suggest_categorical(\"activation\", ['ReLU', 'GELU', 'Swish'])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\", \"SGD\", \"RMSprop\"])\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.8, 0.99, log=True) if optimizer_name == \"SGD\" else None\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True)\n",
    "\n",
    "    # Corrected Data Splitting for Polymer Data \n",
    "    # Split the full data_list into train, validation, and test sets.\n",
    "    # Note: `data_list` should be created in a previous cell.\n",
    "    train_val_set, test_set = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "    train_set, val_set = train_test_split(train_val_set, test_size=0.25, random_state=42)\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    valid_loader = DataLoader(val_set, batch_size=64)\n",
    "    \n",
    "    # model instantiation\n",
    "    # The rdkit_dim is dynamically taken from the pre-processed features.\n",
    "    model = HybridGNN(\n",
    "        gnn_dim=gnn_dim,\n",
    "        rdkit_dim=rdkit_features.shape[1],\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=activation\n",
    "    )\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # optimizer instantiation\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{optimizer_name}' not supported\")\n",
    "\n",
    "    # training loop with NaN check and early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, 100):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch)\n",
    "            loss = F.mse_loss(pred, batch.y.view(-1, 1))\n",
    "\n",
    "            if torch.isnan(loss).any():\n",
    "                print(f\"Trial {trial.number} | Epoch {epoch:02d} | NaN loss detected so pruning trial\")\n",
    "                trial.report(float('inf'), epoch)\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch.num_graphs\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                batch = batch.to(device)\n",
    "                pred = model(batch)\n",
    "                val_loss += F.mse_loss(pred, batch.y.view(-1, 1)).item() * batch.num_graphs\n",
    "        val_loss /= len(valid_loader.dataset)\n",
    "\n",
    "        # logging, reporting, pruning, early stopping\n",
    "        print(f\"Trial {trial.number} | Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Optimizer: {optimizer_name}\")\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Trial {trial.number} - Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    study_name = \"final_2d_gnn_study_Rg_2\"\n",
    "    storage_name = f\"sqlite:///{study_name}.db\"\n",
    "    study = optuna.create_study(study_name=study_name, storage=storage_name, direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "    def save_study_callback(study, trial):\n",
    "        pass\n",
    "\n",
    "    study.optimize(objective, n_trials=500, callbacks=[save_study_callback])\n",
    "    print(study.best_params)\n",
    "    joblib.dump(study, f\"{study_name}_final.pkl\")\n",
    "    \n",
    "    # final plots\n",
    "    vis = optuna.visualization\n",
    "    fig = vis.plot_optimization_history(study)\n",
    "    fig.show()\n",
    "    fig_params = vis.plot_param_importances(study)\n",
    "    fig_params.show()\n",
    "    fig_intermediate = vis.plot_intermediate_values(study)\n",
    "    fig_intermediate.show()\n",
    "    fig_parallel_coordinate = vis.plot_parallel_coordinate(study)\n",
    "    fig_parallel_coordinate.show()\n",
    "    fig_slice = vis.plot_slice(study)\n",
    "    fig_slice.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0b2e9bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gnn_dim': 1024, 'hidden_dim': 256, 'dropout_rate': 0.26835948201850623, 'lr': 0.0006858724864946909, 'activation': 'Swish', 'optimizer': 'AdamW', 'weight_decay': 1.2164081006161759e-05}\n"
     ]
    }
   ],
   "source": [
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad23af4",
   "metadata": {},
   "source": [
    "# Step 9: Retrain with best prameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bdff0035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 238.0840 | Val Loss: 254.5446"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning:\n",
      "\n",
      "'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02 | Train Loss: 216.8862 | Val Loss: 207.5458\n",
      "Epoch 03 | Train Loss: 144.0035 | Val Loss: 58.6246\n",
      "Epoch 04 | Train Loss: 83.2378 | Val Loss: 55.3066\n",
      "Epoch 05 | Train Loss: 42.0978 | Val Loss: 37.3129\n",
      "Epoch 06 | Train Loss: 40.9842 | Val Loss: 29.5151\n",
      "Epoch 07 | Train Loss: 30.5856 | Val Loss: 25.0098\n",
      "Epoch 08 | Train Loss: 29.5081 | Val Loss: 23.7346\n",
      "Epoch 09 | Train Loss: 29.7961 | Val Loss: 27.3702\n",
      "Epoch 10 | Train Loss: 26.4361 | Val Loss: 27.2392\n",
      "Epoch 11 | Train Loss: 23.9520 | Val Loss: 21.6992\n",
      "Epoch 12 | Train Loss: 21.6257 | Val Loss: 19.7837\n",
      "Epoch 13 | Train Loss: 23.4030 | Val Loss: 19.7664\n",
      "Epoch 14 | Train Loss: 24.0455 | Val Loss: 20.2861\n",
      "Epoch 15 | Train Loss: 20.8455 | Val Loss: 28.1146\n",
      "Epoch 16 | Train Loss: 23.9554 | Val Loss: 26.1087\n",
      "Epoch 17 | Train Loss: 24.3614 | Val Loss: 32.1632\n",
      "Epoch 18 | Train Loss: 22.0810 | Val Loss: 33.6542\n",
      "Epoch 19 | Train Loss: 21.9528 | Val Loss: 21.7166\n",
      "Epoch 20 | Train Loss: 19.6516 | Val Loss: 19.5627\n",
      "Epoch 21 | Train Loss: 19.0851 | Val Loss: 21.9380\n",
      "Epoch 22 | Train Loss: 21.4032 | Val Loss: 27.3699\n",
      "Epoch 23 | Train Loss: 19.2409 | Val Loss: 22.6582\n",
      "Epoch 24 | Train Loss: 17.6096 | Val Loss: 16.7980\n",
      "Epoch 25 | Train Loss: 18.5516 | Val Loss: 20.4230\n",
      "Epoch 26 | Train Loss: 19.5996 | Val Loss: 20.4508\n",
      "Epoch 27 | Train Loss: 16.8243 | Val Loss: 23.1757\n",
      "Epoch 28 | Train Loss: 16.5824 | Val Loss: 15.5332\n",
      "Epoch 29 | Train Loss: 17.6938 | Val Loss: 15.4046\n",
      "Epoch 30 | Train Loss: 16.2459 | Val Loss: 17.0402\n",
      "Epoch 31 | Train Loss: 15.9355 | Val Loss: 13.3931\n",
      "Epoch 32 | Train Loss: 15.3574 | Val Loss: 14.5581\n",
      "Epoch 33 | Train Loss: 15.6438 | Val Loss: 14.0713\n",
      "Epoch 34 | Train Loss: 15.9048 | Val Loss: 14.3221\n",
      "Epoch 35 | Train Loss: 16.9366 | Val Loss: 18.8264\n",
      "Epoch 36 | Train Loss: 14.9942 | Val Loss: 16.8720\n",
      "Epoch 37 | Train Loss: 14.8798 | Val Loss: 15.9298\n",
      "Epoch 38 | Train Loss: 15.1636 | Val Loss: 17.3819\n",
      "Epoch 39 | Train Loss: 14.5783 | Val Loss: 17.5149\n",
      "Epoch 40 | Train Loss: 14.0520 | Val Loss: 17.0666\n",
      "Epoch 41 | Train Loss: 15.8312 | Val Loss: 13.7619\n",
      "Early stopping triggered at epoch 41\n",
      "\n",
      "Final Test Set Evaluation:\n",
      "        MAE      RMSE  r_squared\n",
      "0  2.739251  3.480396   0.408193\n",
      "Final Test Loss: 12.1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattg\\AppData\\Local\\Temp\\ipykernel_16204\\471462254.py:105: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The best_params dictionary is available from your Optuna study\n",
    "best_params = study.best_params\n",
    "\n",
    "# Use the same train/val/test sets as the Optuna objective function.\n",
    "# You need to make sure `data_list` is available in this scope.\n",
    "train_val_set, test_set = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "train_set, val_set = train_test_split(train_val_set, test_size=0.25, random_state=42)\n",
    " \n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(val_set, batch_size=64)\n",
    "test_loader = DataLoader(test_set, batch_size=64)\n",
    "\n",
    "# reinitialize model\n",
    "model = HybridGNN(\n",
    "    gnn_dim=best_params['gnn_dim'],\n",
    "    rdkit_dim=rdkit_features.shape[1],\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    activation=best_params['activation']\n",
    ")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# instantiate Optimizer based on Optuna's choice\n",
    "optimizer_name = best_params['optimizer']\n",
    "lr = best_params['lr']\n",
    "weight_decay = best_params['weight_decay']\n",
    "\n",
    "if optimizer_name == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "elif optimizer_name == \"AdamW\":\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "elif optimizer_name == \"SGD\":\n",
    "    momentum = best_params['momentum']\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "elif optimizer_name == \"RMSprop\":\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "else:\n",
    "    raise ValueError(f\"Optimizer '{optimizer_name}' not supported.\")\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "num_warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# early stopping training loop w loss tracking and plotting\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            loss = F.mse_loss(pred, batch.y.view(-1, 1))\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "            preds.append(pred.cpu())\n",
    "            targets.append(batch.y.view(-1, 1).cpu())\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    return avg_loss, preds, targets\n",
    "\n",
    "for epoch in range(1, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        loss = F.mse_loss(pred, batch.y.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    val_loss, val_preds, val_targets = evaluate(model, valid_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_hybridgnn.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# load best model and final evaluation on the TEST set\n",
    "model.load_state_dict(torch.load(\"best_hybridgnn.pt\"))\n",
    "model.eval()\n",
    "final_test_loss, test_preds, test_targets = evaluate(model, test_loader)\n",
    "metrics = regression_metrics(test_targets.numpy(), test_preds.numpy())\n",
    "print(\"\\nFinal Test Set Evaluation:\")\n",
    "print(metrics[['MAE', 'RMSE', 'r_squared']])\n",
    "print(f\"Final Test Loss: {final_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef0f88",
   "metadata": {},
   "source": [
    "# Step 10: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9437faf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting training and validation loss \n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c13e8b",
   "metadata": {},
   "source": [
    "| Model Type             | Featurization        |   MAE |  RMSE |   R² | Notes             |\n",
    "|------------------------|----------------------|-------|-------|------|-------------------|\n",
    "| Hybrid GNN (Tuned)| OGB smiles2graph + RDKit descriptors | 0.159 | 0.234 | 0.965 | Best   |\n",
    "| Hybrid GNN (Untuned) | OGB smiles2graph + RDKit descriptors | 0.223 | 0.308 | 0.939 | 2nd best|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130ba5c",
   "metadata": {},
   "source": [
    "# Step 11: Evaluate on test-dev and save the predictions to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map subset back to original dataset indices\n",
    "test_indices = split_idx['test-dev']\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "all_preds = []\n",
    "all_ids = [] # store original molecule indices\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_loader, desc=\"Predicting\")):\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "\n",
    "# stack predictions\n",
    "all_preds = np.vstack(all_preds)\n",
    "\n",
    "# match predictions to original indices\n",
    "submission_df = pd.DataFrame({'mol_index': test_indices,  # original indices\n",
    "                              'prediction': all_preds.flatten()  # flatten to 1D\n",
    "                              })\n",
    "\n",
    "# sort by original molecule ID \n",
    "submission_df = submission_df.sort_values('mol_index').reset_index(drop=True)\n",
    "\n",
    "# save to CSV\n",
    "submission_df.to_csv(\"hybridgnn_testdev_predictions_with_ids.csv\", index=False)\n",
    "print(\"Predictions with molecule IDs saved to 'hybridgnn_testdev_predictions_with_ids.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673460",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656cce6e",
   "metadata": {},
   "source": [
    "## Model Performance Summary\n",
    "\n",
    "All baseline models were initially trained and evaluated on a 5,000 molecule subset of the full dataset. Below is a comparison of results across different featurization strategies and model types:\n",
    "\n",
    "### 2D Baseline Models\n",
    "\n",
    "| Model Type    | Featurization      | MAE   | RMSE  | R²    | Notes                                 |\n",
    "| ------------- | ------------------ | ----- | ----- | ----- | ------------------------------------- |\n",
    "| MLP (Tuned)   | RDKit Fingerprints | 0.426 | 0.574 | 0.798 | Strong performance across all metrics |\n",
    "| KRR (Tuned)   | RDKit Fingerprints | 0.454 | 0.593 | 0.784 | Good overall, slightly lower R²       |\n",
    "| RF (Tuned)    | RDKit Fingerprints | 0.423 | 0.583 | 0.791 | Best MAE, very competitive overall    |\n",
    "| MLP (Tuned)   | Coulomb Matrix     | 0.636 | 0.819 | 0.588 | Significantly weaker performance      |\n",
    "| MLP (Untuned) | RDKit Fingerprints | 0.467 | 0.609 | 0.772 | Solid untuned baseline                |\n",
    "| KRR (Untuned) | RDKit Fingerprints | 0.519 | 0.668 | 0.726 | Notable drop from tuned version       |\n",
    "| RF (Untuned)  | RDKit Fingerprints | 0.426 | 0.587 | 0.788 | Surprisingly close to tuned RF        |\n",
    "| MLP (Untuned) | Coulomb Matrix     | 0.663 | 0.847 | 0.559 | Consistently underperforms            |\n",
    "\n",
    "### Graph Neural Network Models (ChemML)\n",
    "\n",
    "| Model Type    | Featurization               | MAE   | RMSE  | R²    | Notes                                |\n",
    "| ------------- | --------------------------- | ----- | ----- | ----- | ------------------------------------ |\n",
    "| GNN (Tuned)   | `tensorise_molecules` Graph | 0.302 | 0.411 | 0.900 | Best results from ChemML experiments |\n",
    "| GNN (Untuned) | `tensorise_molecules` Graph | 0.400 | 0.519 | 0.841 | Strong but less optimized            |\n",
    "\n",
    "### Final Hybrid GNN Model Trained on Full Dataset (OGB-Compatible)\n",
    "\n",
    "| Model Type           | Featurization                          | MAE   | RMSE  | R²    | Notes                              |\n",
    "| -------------------- | -------------------------------------- | ----- | ----- | ----- | ---------------------------------- |\n",
    "| Hybrid GNN (Tuned)   | OGB `smiles2graph` + RDKit descriptors | 0.159 | 0.234 | 0.965 | State-of-the-art level performance |\n",
    "| Hybrid GNN (Untuned) | OGB `smiles2graph` + RDKit descriptors | 0.223 | 0.308 | 0.939 | Still very strong pre-tuning       |\n",
    "\n",
    "---\n",
    "\n",
    "## Model Error Analysis\n",
    "\n",
    "I performed qualitative evaluation by comparing predicted vs. true HOMO–LUMO gaps for both randomly selected and poorly predicted molecules. The worst performing molecules often showed rare or complex structures likely underrepresented in the training set. This highlights the importance of structural diversity and potentially more expressive 3D information to improve generalization.\n",
    "\n",
    "## Next Steps: Integrating 3D Molecular Information\n",
    "\n",
    "To push performance even further and overcome limitations of 2D graphs and hand crafted descriptors, my next step will involve:\n",
    "\n",
    "* Using **3D molecular geometries** \n",
    "* Incorporating **interatomic distances**, angles, and **spatial encoding** (SchNet, DimeNet, or SE(3)-equivariant models)\n",
    "* Comparing results against the current best MAE (\\~0.159)\n",
    "\n",
    "This direction aligns with trends in molecular property prediction where 3D aware models often outperform purely 2D approaches, especially for quantum properties like HOMO–LUMO gaps.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
