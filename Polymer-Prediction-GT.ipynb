{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61979795",
   "metadata": {},
   "source": [
    "# HOMO-LUMO Gap Predictions\n",
    "\n",
    "### Problem Statement & Motivation\n",
    "\n",
    "Accurately predicting quantum chemical properties like the HOMO–LUMO energy gap is essential for advancing materials science, drug discovery, and electronic design. The HOMO–LUMO gap is particularly informative for assessing molecular reactivity and stability. While Density Functional Theory (DFT) provides precise estimates, its high computational cost makes it impractical for large-scale screening of molecular libraries. This notebook explores machine learning alternatives that are fast, scalable, and interpretable, offering solutions that are accessible even on modest hardware.\n",
    "\n",
    "### Related Work & Key Gap\n",
    "\n",
    "Past work has shown that:\n",
    "\n",
    "* DFT is accurate but computationally intensive\n",
    "* ML models like kernel methods and GNNs show promise, but often require large models and expensive hardware\n",
    "\n",
    "Key Gap: A need for lightweight, high-performing models that can run locally and integrate with user-friendly tools for deployment in research or education.\n",
    "\n",
    "### Methodology & Evaluation\n",
    "\n",
    "This notebook:\n",
    "\n",
    "* Benchmarks a variety of 2D-based models using RDKit descriptors, Coulomb matrices, and graph neural networks (GNNs) on a 5k molecule subset\n",
    "* Progresses to a hybrid GNN architecture combining OGB-standard graphs with SMILES-derived cheminformatics features\n",
    "* Achieves **MAE = 0.159 eV**\n",
    "* Visualizes results using parity plots, error inspection, and predicted-vs-true comparisons\n",
    "* Evaluates both random and high-error cases to better understand model behavior\n",
    "\n",
    "| Metric   | Best Model (Hybrid GNN) |\n",
    "| -------- | ----------------------- |\n",
    "| **MAE**  | 0.159 eV                |\n",
    "| **RMSE** | 0.234 eV                |\n",
    "| **R²**   | 0.965                   |\n",
    "\n",
    "\n",
    "### Deployment & Accessibility\n",
    "\n",
    "To make the model practically useful, an **interactive web app** was developed:\n",
    "\n",
    "**Live App**: [HOMO–LUMO Gap Predictor on Hugging Face](https://huggingface.co/spaces/MooseML/homo-lumo-gap-predictor)\n",
    "\n",
    "Features:\n",
    "\n",
    "* **SMILES input** for any organic molecule\n",
    "* **Real-time prediction** of the HOMO–LUMO gap\n",
    "* **Molecular visualization**\n",
    "* Simple **CSV logging** for result tracking\n",
    "\n",
    "GitHub Repository: [MooseML/homo-lumo-gap-models](https://github.com/MooseML/homo-lumo-gap-models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ace_tools_open as tools\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import pickle\n",
    "import joblib\n",
    "import os \n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Module, Sequential, Dropout\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# OGB dataset \n",
    "from ogb.lsc import PygPCQM4Mv2Dataset, PCQM4Mv2Dataset\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# RDKit\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "\n",
    "# ChemML\n",
    "from chemml.chem import Molecule, RDKitFingerprint, CoulombMatrix, tensorise_molecules\n",
    "from chemml.models import MLP, NeuralGraphHidden, NeuralGraphOutput\n",
    "from chemml.utils import regression_metrics\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589db70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Built with CUDA: True\n",
      "CUDA available: True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device: /physical_device:GPU:0\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: data\n",
      "LMDB directory: data\\processed_chunks\n",
      "Train LMDB: data\\processed_chunks\\polymer_train3d_dist.lmdb\n",
      "Test LMDB: data\\processed_chunks\\polymer_test3d_dist.lmdb\n",
      "LMDBs already exist.\n"
     ]
    }
   ],
   "source": [
    "# Paths - Fixed for Kaggle environment\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  # Writable directory\n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/best_gnn_transformer_hybrid.pt'\n",
    "else:\n",
    "    DATA_ROOT = 'data'\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = 'best_gnn_transformer_hybrid.pt'\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"LMDB directory: {CHUNK_DIR}\")\n",
    "print(f\"Train LMDB: {TRAIN_LMDB}\")\n",
    "print(f\"Test LMDB: {TEST_LMDB}\")\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    # Run the LMDB builders\n",
    "    !python build_polymer_lmdb_fixed.py train\n",
    "    !python build_polymer_lmdb_fixed.py test\n",
    "    print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c34b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDB contains 7,973 train graphs\n",
      "Global pools -> train_pool=7,175  val_pool=798\n",
      "     Tg:    511 rows with labels (pre-intersection with pools)\n",
      "    FFV:   7030 rows with labels (pre-intersection with pools)\n",
      "     Tc:    737 rows with labels (pre-intersection with pools)\n",
      "Density:    613 rows with labels (pre-intersection with pools)\n",
      "     Rg:    614 rows with labels (pre-intersection with pools)\n"
     ]
    }
   ],
   "source": [
    "# LMDB+CSV wiring \n",
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "# 1) Columns / index mapping\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "\n",
    "# 2) Read the training labels (CSV is only used to know which IDs have labels)\n",
    "train_path = os.path.join(DATA_ROOT, 'train.csv')\n",
    "train_df   = pd.read_csv(train_path)\n",
    "assert {'id','SMILES'}.issubset(train_df.columns), \"train.csv must have id and SMILES\"\n",
    "train_df['id'] = train_df['id'].astype(int)\n",
    "\n",
    "# 3) Read the actual IDs that exist in the LMDB\n",
    "def read_lmdb_ids(lmdb_path: str) -> np.ndarray:\n",
    "    ids_txt = lmdb_path + \".ids.txt\"\n",
    "    if not os.path.exists(ids_txt):\n",
    "        raise FileNotFoundError(f\"Missing {ids_txt}. Rebuild LMDB or confirm paths.\")\n",
    "    ids = np.loadtxt(ids_txt, dtype=np.int64)\n",
    "    if ids.ndim == 0:  # single id edge case\n",
    "        ids = ids.reshape(1)\n",
    "    return ids\n",
    "\n",
    "lmdb_ids = read_lmdb_ids(TRAIN_LMDB)\n",
    "print(f\"LMDB contains {len(lmdb_ids):,} train graphs\")\n",
    "\n",
    "# 4) Helper: IDs that have a label for a given task (intersection with LMDB ids)\n",
    "def ids_with_label(task: str) -> np.ndarray:\n",
    "    col = task\n",
    "    have_label = train_df.loc[~train_df[col].isna(), 'id'].astype(int).values\n",
    "    # Only keep those that were actually written to the LMDB\n",
    "    keep = np.intersect1d(have_label, lmdb_ids, assume_unique=False)\n",
    "    return keep\n",
    "\n",
    "# 5) Make a global pool split once (reused for each task)\n",
    "rng = np.random.default_rng(123)\n",
    "perm = rng.permutation(len(lmdb_ids))\n",
    "split = int(0.9 * len(lmdb_ids))\n",
    "train_pool_ids = lmdb_ids[perm[:split]]\n",
    "val_pool_ids   = lmdb_ids[perm[split:]]\n",
    "\n",
    "print(f\"Global pools -> train_pool={len(train_pool_ids):,}  val_pool={len(val_pool_ids):,}\")\n",
    "\n",
    "# 6) Quick sanity: show available counts per task\n",
    "for t in label_cols:\n",
    "    n_task_ids = len(ids_with_label(t))\n",
    "    print(f\"{t:>7}: {n_task_ids:6d} rows with labels (pre-intersection with pools)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125f621",
   "metadata": {},
   "source": [
    "The only property that appears will succeed with a simple imputation strategy is FFV. All other properties contain very high percent missing. Therefore, I will impute median for FFV, train a model for FFV, and train separate models for other properties. I will attempt to filter out missing values for each property. If this yields uncessful, I may explore sampling techniques or use the trained model to impute values to train a secondaery model. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd3c3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def _safe_numpy(x, default_shape=None, dtype=np.float32):\n",
    "    try:\n",
    "        return torch.as_tensor(x).detach().cpu().numpy().astype(dtype)\n",
    "    except Exception:\n",
    "        if default_shape is None:\n",
    "            return np.array([], dtype=dtype)\n",
    "        return np.zeros(default_shape, dtype=dtype)\n",
    "\n",
    "def geom_features_from_rec(rec, rdkit_dim_expected=15, rbf_K=32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a fixed-length vector from a single LMDB record:\n",
    "      [rdkit(15), n_atoms, n_bonds, deg_mean, deg_max, has_xyz,\n",
    "       eig3(3), bbox_extents(3), radius_stats(3), hop_hist(3), extra_atom_mean(5),\n",
    "       edge_rbf_mean(32)]\n",
    "    ~ total len = 15 + 5 + 3 + 3 + 3 + 3 + 5 + 32 = 69\n",
    "    \"\"\"\n",
    "    # 15 RDKit descriptors stored in LMDB (your rebuilt version)\n",
    "    rd = getattr(rec, \"rdkit_feats\", None)\n",
    "    rd = _safe_numpy(rd, default_shape=(1, rdkit_dim_expected)).reshape(-1)\n",
    "    if rd.size != rdkit_dim_expected:\n",
    "        rd = np.zeros((rdkit_dim_expected,), dtype=np.float32)\n",
    "\n",
    "    # basic graph sizes & degree\n",
    "    x = torch.as_tensor(rec.x)             # [N, ...]\n",
    "    ei = torch.as_tensor(rec.edge_index)   # [2, E]\n",
    "    n = x.shape[0]\n",
    "    e = ei.shape[1] if ei.ndim == 2 else 0\n",
    "    deg = torch.bincount(ei[0], minlength=n) if e > 0 else torch.zeros(n, dtype=torch.long)\n",
    "    deg_mean = deg.float().mean().item() if n > 0 else 0.0\n",
    "    deg_max  = deg.max().item() if n > 0 else 0.0\n",
    "\n",
    "    # has_xyz flag\n",
    "    has_xyz = int(bool(getattr(rec, \"has_xyz\", torch.zeros(1, dtype=torch.bool))[0].item())) if hasattr(rec, \"has_xyz\") else 0\n",
    "\n",
    "    # pos-based features\n",
    "    eig3 = np.zeros(3, dtype=np.float32)\n",
    "    extents = np.zeros(3, dtype=np.float32)\n",
    "    rad_stats = np.zeros(3, dtype=np.float32)\n",
    "    pos = getattr(rec, \"pos\", None)\n",
    "    if pos is not None and n > 0 and has_xyz:\n",
    "        P = torch.as_tensor(pos).float()                     # [N,3]\n",
    "        center = P.mean(dim=0, keepdim=True)\n",
    "        C = P - center\n",
    "        cov = (C.T @ C) / max(1, n-1)                       # [3,3]\n",
    "        vals = torch.linalg.eigvalsh(cov).clamp_min(0).sqrt()  # length scales\n",
    "        eig3 = vals.detach().cpu().numpy()\n",
    "        mn, mx = P.min(0).values, P.max(0).values\n",
    "        extents = (mx - mn).detach().cpu().numpy()\n",
    "        r = C.norm(dim=1)\n",
    "        rad_stats = np.array([r.mean().item(), r.std().item(), r.max().item()], dtype=np.float32)\n",
    "\n",
    "    # hop-distance histogram (1,2,3 hops)\n",
    "    hop_hist = np.zeros(3, dtype=np.float32)\n",
    "    D = getattr(rec, \"dist\", None)\n",
    "    if D is not None and n > 0:\n",
    "        Dn = torch.as_tensor(D).float()[:n, :n]\n",
    "        hop_hist = np.array([\n",
    "            (Dn == 1).float().mean().item(),\n",
    "            (Dn == 2).float().mean().item(),\n",
    "            (Dn == 3).float().mean().item()\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    # extra atom features (mean over atoms, 5 dims if present)\n",
    "    extra_atom = getattr(rec, \"extra_atom_feats\", None)\n",
    "    extra_mean = np.zeros(5, dtype=np.float32)\n",
    "    if extra_atom is not None and hasattr(extra_atom, \"shape\") and extra_atom.shape[-1] == 5:\n",
    "        extra_mean = torch.as_tensor(extra_atom).float().mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    # edge RBF (last 32 channels of edge_attr were RBF(d))\n",
    "    rbf_mean = np.zeros(rbf_K, dtype=np.float32)\n",
    "    ea = getattr(rec, \"edge_attr\", None)\n",
    "    if ea is not None:\n",
    "        EA = torch.as_tensor(ea)\n",
    "        if EA.ndim == 2 and EA.shape[1] >= (3 + rbf_K):\n",
    "            rbf = EA[:, -rbf_K:].float()\n",
    "            rbf_mean = rbf.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    scalars = np.array([n, e, deg_mean, deg_max, has_xyz], dtype=np.float32)\n",
    "    return np.concatenate([rd, scalars, eig3, extents, rad_stats, hop_hist, extra_mean, rbf_mean], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e663914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors as rdmd, DataStructs\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def morgan_bits(smiles_list, n_bits=1024, radius=3):\n",
    "    X = np.zeros((len(smiles_list), n_bits), dtype=np.uint8)\n",
    "    for i, s in enumerate(smiles_list):\n",
    "        arr = np.zeros((n_bits,), dtype=np.uint8)\n",
    "        m = Chem.MolFromSmiles(s)\n",
    "        if m is not None:\n",
    "            fp = rdmd.GetMorganFingerprintAsBitVect(m, radius=radius, nBits=n_bits)\n",
    "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        X[i] = arr\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "def build_rf_features_from_lmdb(ids: np.ndarray, lmdb_path: str, smiles_list: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns X = [Morgan1024 | LMDB-3D-global(69)] for each id/smiles.\n",
    "    Assumes ids and smiles_list are aligned with the CSV used to build LMDB.\n",
    "    \"\"\"\n",
    "    base = LMDBDataset(ids, lmdb_path)\n",
    "    # 3D/global block\n",
    "    feats3d = []\n",
    "    for i in range(len(base)):\n",
    "        rec = base[i]\n",
    "        feats3d.append(geom_features_from_rec(rec))  # shape (69,)\n",
    "    X3d = np.vstack(feats3d).astype(np.float32) if feats3d else np.zeros((0, 69), dtype=np.float32)\n",
    "\n",
    "    # Morgan FP block (2D)\n",
    "    Xfp = morgan_bits(smiles_list, n_bits=1024, radius=3)   # (N,1024)\n",
    "\n",
    "    # concat\n",
    "    X = np.hstack([Xfp, X3d]).astype(np.float32)            # (N, 1024+69)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe69f3",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47dc5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Tg shape: (7973, 2)\n",
      "Initial Tg missing:\n",
      "SMILES       0\n",
      "Tg        7462\n",
      "dtype: int64\n",
      "Cleaned Tg shape: (511, 2)\n",
      "Cleaned Tg missing:\n",
      "SMILES    0\n",
      "Tg        0\n",
      "dtype: int64\n",
      "\n",
      "Initial Density shape: (7973, 2)\n",
      "Initial Density missing:\n",
      "SMILES        0\n",
      "Density    7360\n",
      "dtype: int64\n",
      "Cleaned Density shape: (613, 2)\n",
      "Cleaned Density missing:\n",
      "SMILES     0\n",
      "Density    0\n",
      "dtype: int64\n",
      "\n",
      "Initial FFV shape: (7973, 2)\n",
      "Initial FFV missing:\n",
      "SMILES      0\n",
      "FFV       943\n",
      "dtype: int64\n",
      "Cleaned FFV shape: (7030, 2)\n",
      "Cleaned FFV missing:\n",
      "SMILES    0\n",
      "FFV       0\n",
      "dtype: int64\n",
      "\n",
      "Initial Tc shape: (7973, 2)\n",
      "Initial Tc missing:\n",
      "SMILES       0\n",
      "Tc        7236\n",
      "dtype: int64\n",
      "Cleaned Tc shape: (737, 2)\n",
      "Cleaned Tc missing:\n",
      "SMILES    0\n",
      "Tc        0\n",
      "dtype: int64\n",
      "\n",
      "Initial Rg shape: (7973, 2)\n",
      "Initial Rg missing:\n",
      "SMILES       0\n",
      "Rg        7359\n",
      "dtype: int64\n",
      "Cleaned Rg shape: (614, 2)\n",
      "Cleaned Rg missing:\n",
      "SMILES    0\n",
      "Rg        0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the CSV only to know which rows have labels; keep 'id' here.\n",
    "train_df = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_df[\"id\"] = train_df[\"id\"].astype(int)\n",
    "\n",
    "def build_target_df_from_ids(df: pd.DataFrame, target_col: str, keep_ids: np.ndarray):\n",
    "    \"\"\"\n",
    "    Return DataFrame with only SMILES + target, restricted to IDs present in the LMDB\n",
    "    and dropping missing targets.\n",
    "    \"\"\"\n",
    "    out = df.loc[df[\"id\"].isin(keep_ids), [\"SMILES\", target_col]].copy()\n",
    "    print(f\"Initial {target_col} shape:\", out.shape)\n",
    "    print(f\"Initial {target_col} missing:\\n{out.isnull().sum()}\")\n",
    "    out = out.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    print(f\"Cleaned {target_col} shape:\", out.shape)\n",
    "    print(f\"Cleaned {target_col} missing:\\n{out.isnull().sum()}\\n\")\n",
    "    return out\n",
    "\n",
    "# Build all five (use same LMDB id set so we only keep rows that exist in LMDB)\n",
    "df_tg      = build_target_df_from_ids(train_df, \"Tg\",      lmdb_ids)\n",
    "df_density = build_target_df_from_ids(train_df, \"Density\", lmdb_ids)\n",
    "df_ffv     = build_target_df_from_ids(train_df, \"FFV\",     lmdb_ids)\n",
    "df_tc      = build_target_df_from_ids(train_df, \"Tc\",      lmdb_ids)\n",
    "df_rg      = build_target_df_from_ids(train_df, \"Rg\",      lmdb_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff48e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morgan FP utilities (no 3D, no external descriptors) \n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def smiles_to_morgan_fp(\n",
    "    smi: str,\n",
    "    n_bits: int = 1024,\n",
    "    radius: int = 3,\n",
    "    use_counts: bool = False,\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"Return a 1D numpy array Morgan fingerprint; None if SMILES invalid.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    if use_counts:\n",
    "        fp = rdMolDescriptors.GetMorganFingerprint(mol, radius)\n",
    "        # convert to dense count vector\n",
    "        arr = np.zeros((n_bits,), dtype=np.int32)\n",
    "        for bit_id, count in fp.GetNonzeroElements().items():\n",
    "            arr[bit_id % n_bits] += count\n",
    "        return arr.astype(np.float32)\n",
    "    else:\n",
    "        bv = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "        arr = np.zeros((n_bits,), dtype=np.int8)\n",
    "        Chem.DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "        return arr.astype(np.float32)\n",
    "\n",
    "def prepare_fp_for_target(\n",
    "    df_target: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    *,\n",
    "    fp_bits: int = 1024,\n",
    "    fp_radius: int = 3,\n",
    "    use_counts: bool = False,\n",
    "    save_csv_path: Optional[str] = None,\n",
    "    show_progress: bool = True,\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Drop missing targets, compute Morgan FPs from SMILES only.\n",
    "    Returns (df_clean, y, X_fp) where:\n",
    "      df_clean: ['SMILES', target_col]\n",
    "      y: (N,)\n",
    "      X_fp: (N, fp_bits)\n",
    "    \"\"\"\n",
    "    assert {\"SMILES\", target_col}.issubset(df_target.columns)\n",
    "\n",
    "    # 1) drop missing targets (no imputation)\n",
    "    work = df_target[[\"SMILES\", target_col]].copy()\n",
    "    before = len(work)\n",
    "    work = work.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    after = len(work)\n",
    "    print(f\"[{target_col}] dropped {before - after} missing; kept {after}\")\n",
    "\n",
    "    # 2) compute FPs; skip invalid SMILES\n",
    "    fps, ys, keep_smiles = [], [], []\n",
    "    it = work.itertuples(index=False)\n",
    "    if show_progress:\n",
    "        it = tqdm(it, total=len(work), desc=f\"FPs for {target_col}\")\n",
    "\n",
    "    for row in it:\n",
    "        smi = row.SMILES\n",
    "        yv  = getattr(row, target_col)\n",
    "        arr = smiles_to_morgan_fp(smi, n_bits=fp_bits, radius=fp_radius, use_counts=use_counts)\n",
    "        if arr is None:\n",
    "            continue\n",
    "        fps.append(arr)\n",
    "        ys.append(float(yv))\n",
    "        keep_smiles.append(smi)\n",
    "\n",
    "    X_fp = np.stack(fps, axis=0) if fps else np.zeros((0, fp_bits), dtype=np.float32)\n",
    "    y = np.asarray(ys, dtype=float)\n",
    "    df_clean = pd.DataFrame({\"SMILES\": keep_smiles, target_col: y})\n",
    "\n",
    "    if save_csv_path:\n",
    "        df_clean.to_csv(save_csv_path, index=False)\n",
    "        print(f\"[{target_col}] saved cleaned CSV -> {save_csv_path}\")\n",
    "\n",
    "    print(f\"[{target_col}] X_fp: {X_fp.shape} | y: {y.shape}\")\n",
    "    return df_clean, y, X_fp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f37942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] dropped 0 missing; kept 511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555e8f114aa948bc96cf0da8b6cc5c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Tg:   0%|          | 0/511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] saved cleaned CSV -> cleaned_tg_fp.csv\n",
      "[Tg] X_fp: (511, 1024) | y: (511,)\n",
      "[Density] dropped 0 missing; kept 613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e41bc670b074cb8b2b1e20227e51334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Density:   0%|          | 0/613 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Density] saved cleaned CSV -> cleaned_density_fp.csv\n",
      "[Density] X_fp: (613, 1024) | y: (613,)\n",
      "[FFV] dropped 0 missing; kept 7030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f999f21ac84ac1a8bad32a3ef6ce9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for FFV:   0%|          | 0/7030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FFV] saved cleaned CSV -> cleaned_ffv_fp.csv\n",
      "[FFV] X_fp: (7030, 1024) | y: (7030,)\n",
      "[Tc] dropped 0 missing; kept 737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b63b1324eb74c6a8dba4d5bd3511d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Tc:   0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tc] saved cleaned CSV -> cleaned_tc_fp.csv\n",
      "[Tc] X_fp: (737, 1024) | y: (737,)\n",
      "[Rg] dropped 0 missing; kept 614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e135e85a8c2458ab8681455c579035c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Rg:   0%|          | 0/614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rg] saved cleaned CSV -> cleaned_rg_fp.csv\n",
      "[Rg] X_fp: (614, 1024) | y: (614,)\n"
     ]
    }
   ],
   "source": [
    "# Bit vectors (1024, r=3) \n",
    "df_clean_tg,      y_tg,      X_tg      = prepare_fp_for_target(df_tg,      \"Tg\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_tg_fp.csv\")\n",
    "df_clean_density, y_density, X_density = prepare_fp_for_target(df_density, \"Density\", fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_density_fp.csv\")\n",
    "df_clean_ffv,     y_ffv,     X_ffv     = prepare_fp_for_target(df_ffv,     \"FFV\",     fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_ffv_fp.csv\")\n",
    "df_clean_tc,      y_tc,      X_tc      = prepare_fp_for_target(df_tc,      \"Tc\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_tc_fp.csv\")\n",
    "df_clean_rg,      y_rg,      X_rg      = prepare_fp_for_target(df_rg,      \"Rg\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_rg_fp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff620911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "@dataclass\n",
    "class TabularSplits:\n",
    "    # unscaled (for RF)\n",
    "    X_train: np.ndarray\n",
    "    X_test:  np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    y_test:  np.ndarray\n",
    "    # scaled (for KRR/MLP)\n",
    "    X_train_scaled: Optional[np.ndarray] = None\n",
    "    X_test_scaled:  Optional[np.ndarray] = None\n",
    "    y_train_scaled: Optional[np.ndarray] = None  # shape (N,1)\n",
    "    y_test_scaled:  Optional[np.ndarray] = None\n",
    "    x_scaler: Optional[StandardScaler] = None\n",
    "    y_scaler: Optional[StandardScaler] = None\n",
    "\n",
    "def _make_regression_stratify_bins(y: np.ndarray, n_bins: int = 10) -> np.ndarray:\n",
    "    \"\"\"Return integer bins for approximate stratification in regression.\"\"\"\n",
    "    y = y.ravel()\n",
    "    # handle degenerate case\n",
    "    if np.unique(y).size < n_bins:\n",
    "        n_bins = max(2, np.unique(y).size)\n",
    "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "    bins = np.unique(np.quantile(y, quantiles))\n",
    "    # ensure strictly increasing\n",
    "    bins = np.unique(bins)\n",
    "    # np.digitize expects right-open intervals by default\n",
    "    strat = np.digitize(y, bins[1:-1], right=False)\n",
    "    return strat\n",
    "\n",
    "def make_tabular_splits(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    scale_X: bool = True,\n",
    "    scale_y: bool = True,\n",
    "    stratify_regression: bool = False,\n",
    "    n_strat_bins: int = 10,\n",
    "    # if you already decided splits (e.g., scaffold split), pass indices:\n",
    "    train_idx: Optional[np.ndarray] = None,\n",
    "    test_idx: Optional[np.ndarray] = None,\n",
    ") -> TabularSplits:\n",
    "    \"\"\"\n",
    "    Split and (optionally) scale tabular features/targets for a single target.\n",
    "    Returns both scaled and unscaled arrays, plus fitted scalers.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    if train_idx is not None and test_idx is not None:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "    else:\n",
    "        strat = None\n",
    "        if stratify_regression:\n",
    "            strat = _make_regression_stratify_bins(y, n_bins=n_strat_bins)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "        )\n",
    "\n",
    "    # Unscaled outputs (for RF, tree models)\n",
    "    splits = TabularSplits(\n",
    "        X_train=X_train, X_test=X_test,\n",
    "        y_train=y_train, y_test=y_test\n",
    "    )\n",
    "\n",
    "    # Scaled versions (for KRR/MLP)\n",
    "    if scale_X:\n",
    "        xscaler = StandardScaler()\n",
    "        splits.X_train_scaled = xscaler.fit_transform(X_train)\n",
    "        splits.X_test_scaled  = xscaler.transform(X_test)\n",
    "        splits.x_scaler = xscaler\n",
    "    if scale_y:\n",
    "        yscaler = StandardScaler()\n",
    "        splits.y_train_scaled = yscaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        splits.y_test_scaled  = yscaler.transform(y_test.reshape(-1, 1))\n",
    "        splits.y_scaler = yscaler\n",
    "\n",
    "    # Shapes summary\n",
    "    print(\"Splits:\")\n",
    "    print(\"X_train:\", splits.X_train.shape, \"| X_test:\", splits.X_test.shape)\n",
    "    if splits.X_train_scaled is not None:\n",
    "        print(\"X_train_scaled:\", splits.X_train_scaled.shape, \"| X_test_scaled:\", splits.X_test_scaled.shape)\n",
    "    print(\"y_train:\", splits.y_train.shape, \"| y_test:\", splits.y_test.shape)\n",
    "    if splits.y_train_scaled is not None:\n",
    "        print(\"y_train_scaled:\", splits.y_train_scaled.shape, \"| y_test_scaled:\", splits.y_test_scaled.shape)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c284cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def train_eval_rf(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    rf_params: Dict[str, Any],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    save_dir: str = \"saved_models/rf\",\n",
    "    tag: str = \"model\",\n",
    ") -> Tuple[RandomForestRegressor, Dict[str, float], TabularSplits, str]:\n",
    "    \"\"\"\n",
    "    Trains a RandomForest on unscaled features; returns (model, metrics, splits, path).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # Pick a safe number of bins based on dataset size\n",
    "    if stratify_regression:\n",
    "        adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "    else:\n",
    "        adaptive_bins = n_strat_bins\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False,                 # RF doesn't need scaling\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=adaptive_bins\n",
    "    )\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=random_state, n_jobs=-1, **rf_params)\n",
    "    rf.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "    pred_tr = rf.predict(splits.X_train)\n",
    "    pred_te = rf.predict(splits.X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_MAE\": mean_absolute_error(splits.y_train, pred_tr),\n",
    "        \"train_RMSE\": mean_squared_error(splits.y_train, pred_tr, squared=False),\n",
    "        \"train_R2\": r2_score(splits.y_train, pred_tr),\n",
    "        \"val_MAE\": mean_absolute_error(splits.y_test, pred_te),\n",
    "        \"val_RMSE\": mean_squared_error(splits.y_test, pred_te, squared=False),\n",
    "        \"val_R2\": r2_score(splits.y_test, pred_te),\n",
    "    }\n",
    "    print(f\"[RF/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"rf_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": rf, \"metrics\": metrics, \"rf_params\": rf_params}, path)\n",
    "    return rf, metrics, splits, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d95126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:\n",
      "X_train: (5624, 1024) | X_test: (1406, 1024)\n",
      "y_train: (5624,) | y_test: (1406,)\n",
      "[RF/FFV] val_MAE=0.009095  val_RMSE=0.019753  val_R2=0.5701\n",
      "Splits:\n",
      "X_train: (589, 1024) | X_test: (148, 1024)\n",
      "y_train: (589,) | y_test: (148,)\n",
      "[RF/Tc] val_MAE=0.029866  val_RMSE=0.045109  val_R2=0.7304\n",
      "Splits:\n",
      "X_train: (491, 1024) | X_test: (123, 1024)\n",
      "y_train: (491,) | y_test: (123,)\n",
      "[RF/Rg] val_MAE=1.715067  val_RMSE=2.664982  val_R2=0.6916\n",
      "Splits:\n",
      "X_train: (408, 1024) | X_test: (103, 1024)\n",
      "y_train: (408,) | y_test: (103,)\n",
      "[RF/Tg] val_MAE=61.738193  val_RMSE=78.750171  val_R2=0.5333\n",
      "Splits:\n",
      "X_train: (490, 1024) | X_test: (123, 1024)\n",
      "y_train: (490,) | y_test: (123,)\n",
      "[RF/Density] val_MAE=0.054697  val_RMSE=0.092855  val_R2=0.6311\n"
     ]
    }
   ],
   "source": [
    "rf_cfg = {\n",
    "    \"FFV\": {\"n_estimators\": 100, \"max_depth\": 60},\n",
    "    \"Tc\":  {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "    \"Rg\":  {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "}\n",
    "\n",
    "rf_ffv, m_ffv, splits_ffv, p_ffv = train_eval_rf(X_ffv, y_ffv, rf_params=rf_cfg[\"FFV\"], tag=\"FFV\")\n",
    "rf_tc,  m_tc,  splits_tc,  p_tc  = train_eval_rf(X_tc,  y_tc,  rf_params=rf_cfg[\"Tc\"],  tag=\"Tc\")\n",
    "rf_rg,  m_rg,  splits_rg,  p_rg  = train_eval_rf(X_rg,  y_rg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Rg\")\n",
    "rf_tg,  m_tg,  splits_tg,  p_tg  = train_eval_rf(X_tg,  y_tg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Tg\")\n",
    "rf_density,  m_density,  splits_density,  p_density  = train_eval_rf(X_density,  y_density,  rf_params=rf_cfg[\"Rg\"],  tag=\"Density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7492bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Training RF(+3D) for FFV\n",
      "Splits:\n",
      "X_train: (5624, 1093) | X_test: (1406, 1093)\n",
      "y_train: (5624,) | y_test: (1406,)\n",
      "[RF/FFV_aug3D] val_MAE=0.007621  val_RMSE=0.017553  val_R2=0.6605\n",
      "[RF+3D/FFV]  val_MAE=0.007621  val_RMSE=0.017553  val_R2=0.6605\n",
      "\n",
      ">>> Training RF(+3D) for Tc\n",
      "Splits:\n",
      "X_train: (589, 1093) | X_test: (148, 1093)\n",
      "y_train: (589,) | y_test: (148,)\n",
      "[RF/Tc_aug3D] val_MAE=0.029937  val_RMSE=0.045036  val_R2=0.7313\n",
      "[RF+3D/Tc]  val_MAE=0.029937  val_RMSE=0.045036  val_R2=0.7313\n",
      "\n",
      ">>> Training RF(+3D) for Rg\n",
      "Splits:\n",
      "X_train: (491, 1093) | X_test: (123, 1093)\n",
      "y_train: (491,) | y_test: (123,)\n",
      "[RF/Rg_aug3D] val_MAE=1.648818  val_RMSE=2.493712  val_R2=0.7299\n",
      "[RF+3D/Rg]  val_MAE=1.648818  val_RMSE=2.493712  val_R2=0.7299\n",
      "\n",
      ">>> Training RF(+3D) for Tg\n",
      "Splits:\n",
      "X_train: (408, 1093) | X_test: (103, 1093)\n",
      "y_train: (408,) | y_test: (103,)\n",
      "[RF/Tg_aug3D] val_MAE=58.315801  val_RMSE=74.296699  val_R2=0.5846\n",
      "[RF+3D/Tg]  val_MAE=58.315801  val_RMSE=74.296699  val_R2=0.5846\n",
      "\n",
      ">>> Training RF(+3D) for Density\n",
      "Splits:\n",
      "X_train: (490, 1093) | X_test: (123, 1093)\n",
      "y_train: (490,) | y_test: (123,)\n",
      "[RF/Density_aug3D] val_MAE=0.037793  val_RMSE=0.070932  val_R2=0.7847\n",
      "[RF+3D/Density]  val_MAE=0.037793  val_RMSE=0.070932  val_R2=0.7847\n"
     ]
    }
   ],
   "source": [
    "# === helpers (uses the LMDB feature builders you already added) ===\n",
    "def train_rf_aug3d_for_target(\n",
    "    target_col: str,\n",
    "    rf_params: dict,\n",
    "    *,\n",
    "    train_csv_path: str,\n",
    "    lmdb_path: str,\n",
    "    save_dir: str = \"saved_models/rf_aug3d\",\n",
    "    tag_prefix: str = \"aug3D\",\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "):\n",
    "    \"\"\"Load rows with target, build X=[FP|3D], train RF via your train_eval_rf().\"\"\"\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    mask = ~df[target_col].isna()\n",
    "    ids_tr    = df.loc[mask, 'id'].astype(int).values\n",
    "    smiles_tr = df.loc[mask, 'SMILES'].astype(str).tolist()\n",
    "    y         = df.loc[mask, target_col].astype(float).values\n",
    "\n",
    "    X_aug = build_rf_features_from_lmdb(ids_tr, lmdb_path, smiles_tr)  # (N, 1024 + 69)\n",
    "\n",
    "    model, metrics, splits, path = train_eval_rf(\n",
    "        X_aug, y,\n",
    "        rf_params=rf_params,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=n_strat_bins,\n",
    "        save_dir=save_dir,\n",
    "        tag=f\"{target_col}_{tag_prefix}\"\n",
    "    )\n",
    "    return model, metrics, splits, path\n",
    "\n",
    "# === per-target configs (start with what worked; tweak later) ===\n",
    "rf_cfg_aug = {\n",
    "    \"FFV\":     {\"n_estimators\": 800, \"max_depth\": 30, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "    \"Tc\":      {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "    \"Rg\":      {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "    # reasonable first passes for the two GNN targets (just to A/B):\n",
    "    \"Tg\":      {\"n_estimators\": 600, \"max_depth\": 60, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "    \"Density\": {\"n_estimators\": 600, \"max_depth\": 40, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "}\n",
    "\n",
    "# === train all five with augmented features ===\n",
    "TRAIN_CSV = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "rf_models, rf_metrics, rf_splits, rf_paths = {}, {}, {}, {}\n",
    "\n",
    "for t in [\"FFV\", \"Tc\", \"Rg\", \"Tg\", \"Density\"]:\n",
    "    print(f\"\\n>>> Training RF(+3D) for {t}\")\n",
    "    m, met, sp, p = train_rf_aug3d_for_target(\n",
    "        t, rf_cfg_aug[t],\n",
    "        train_csv_path=TRAIN_CSV,\n",
    "        lmdb_path=TRAIN_LMDB,\n",
    "        save_dir=\"saved_models/rf_aug3d\",\n",
    "        tag_prefix=\"aug3D\",\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify_regression=True,\n",
    "        n_strat_bins=10,\n",
    "    )\n",
    "    rf_models[t], rf_metrics[t], rf_splits[t], rf_paths[t] = m, met, sp, p\n",
    "    print(f\"[RF+3D/{t}]  val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d0915ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df  = pd.read_csv(os.path.join(DATA_ROOT, 'test.csv'))\n",
    "# test_ids = test_df['id'].astype(int).values\n",
    "# test_smiles = test_df['SMILES'].astype(str).tolist()\n",
    "\n",
    "# X_test_aug = build_rf_features_from_lmdb(test_ids, TEST_LMDB, test_smiles)  # (M, 1093)\n",
    "\n",
    "# # Example: swap in RF(+3D) for FFV and Tc (and optionally Tg/Density/Rg if better)\n",
    "# rf_ffv = joblib.load(rf_paths[\"FFV\"])['model']\n",
    "# rf_tc  = joblib.load(rf_paths[\"Tc\"])['model']\n",
    "\n",
    "# pred_ffv = rf_ffv.predict(X_test_aug).astype(float)\n",
    "# pred_tc  = rf_tc.predict(X_test_aug).astype(float)\n",
    "\n",
    "# # If you want to try RF(+3D) for Tg/Density/Rg too:\n",
    "# # pred_tg = joblib.load(rf_paths[\"Tg\"])['model'].predict(X_test_aug).astype(float)\n",
    "# # pred_density = joblib.load(rf_paths[\"Density\"])['model'].predict(X_test_aug).astype(float)\n",
    "# # pred_rg = joblib.load(rf_paths[\"Rg\"])['model'].predict(X_test_aug).astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77f7ec",
   "metadata": {},
   "source": [
    "## ChemML GNN Model Results\n",
    "| Model Type             | Featurization        |   MAE |  RMSE |   R² | Notes             |\n",
    "|------------------------|----------------------|-------|-------|------|-------------------|\n",
    "| GNN (Tuned)            | tensorise_molecules Graph   | 0.302 | 0.411 | 0.900 | Best performance across all metrics   |\n",
    "| GNN (Untuned)          | tensorise_molecules Graph   | 0.400 | 0.519 | 0.841 | Good overall|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42db218",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Model Training\n",
    "\n",
    "Having explored different molecular graph representations and model architectures, I am now moving to training what is expected to be the best-performing model using the full dataset. The earlier GNN model was based on `tensorise_molecules` (ChemML) graphs and had strong performance with a **mean absolute error (MAE) around 0.30**. These graphs are based on RDKit's internal descriptors and do not reflect the original PCQM4Mv2 graph structure used in the Open Graph Benchmark (OGB). Therefore, I will shift focus to the `smiles2graph` representation provided by OGB, which aligns more directly with the benchmark's evaluation setup and top-performing models on the leaderboard.\n",
    "\n",
    "\n",
    "| Source                         | Atom/Bond Features                                                 | Format                                          | Customizable?     | Alignment with PCQM4Mv2?  |\n",
    "| ------------------------------ | ------------------------------------------------------------------ | ----------------------------------------------- | ----------------- | ---------------------- |\n",
    "| `tensorise_molecules` (ChemML) | RDKit-based descriptors (ex: atom number, degree, hybridization) | NumPy tensors (`X_atoms`, `X_bonds`, `X_edges`) | Limited           |  Not aligned          |\n",
    "| `smiles2graph` (OGB / PyG)     | Predefined categorical features from PCQM4Mv2                      | PyTorch Geometric `Data` objects                |  Highly flexible |  Matches OGB standard |\n",
    "\n",
    "By using `smiles2graph`, we:\n",
    "\n",
    "* Use OGB-standard graph construction and feature encoding for fair comparisons with leaderboard models\n",
    "* Include learnable AtomEncoder and BondEncoder embeddings from `ogb.graphproppred.mol_encoder`, which improve model expressiveness\n",
    "* Maintain compatibility with PyTorch Geometric, DGL, and OGB tools\n",
    "\n",
    "I will also concatenate GNN-derived embeddings with SMILES-based RDKit descriptors, feeding this hybrid representation into MLP head. This allows you to combine structural and cheminformatics perspectives for improved prediction accuracy. With this setup, I aim to improve upon the MAE of \\~0.30 achieved earlier and push closer toward state-of-the-art performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d599b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tg ids: (511,) Density ids: (613,)\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))  # keep 'id'!\n",
    "lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "if os.path.exists(lmdb_ids_path):\n",
    "    with open(lmdb_ids_path) as f:\n",
    "        kept_ids = set(int(x.strip()) for x in f if x.strip())\n",
    "else:\n",
    "    kept_ids = set(train_csv['id'].astype(int).tolist())\n",
    "\n",
    "def ids_for_task(task):\n",
    "    t = task2idx[task]\n",
    "    col = label_cols[t]\n",
    "    ids = train_csv.loc[~train_csv[col].isna(), 'id'].astype(int).tolist()\n",
    "    # only those that actually exist in LMDB\n",
    "    return np.array([i for i in ids if i in kept_ids], dtype=int)\n",
    "\n",
    "ids_tg  = ids_for_task(\"Tg\")\n",
    "ids_den = ids_for_task(\"Density\")\n",
    "ids_tc = ids_for_task(\"Tc\")\n",
    "ids_rg = ids_for_task(\"Rg\")\n",
    "ids_ffv = ids_for_task(\"FFV\")\n",
    "print(\"Tg ids:\", ids_tg.shape, \"Density ids:\", ids_den.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3efce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import torch, numpy as np\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def _get_rdkit_feats_from_record(rec):\n",
    "    arr = getattr(rec, \"rdkit_feats\", None)\n",
    "    if arr is None:\n",
    "        return torch.zeros(15, dtype=torch.float32)   # or 6 if that’s your build\n",
    "    v = torch.as_tensor(np.asarray(arr, np.float32).reshape(-1), dtype=torch.float32)\n",
    "    return v.unsqueeze(0)  # <<< IMPORTANT: (1, D) so batch -> (B, D)\n",
    "\n",
    "\n",
    "class LMDBtoPyGSingleTask(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ids,\n",
    "        lmdb_path,\n",
    "        target_index=None,\n",
    "        *,\n",
    "        use_mixed_edges: bool = True,      # <— enables 3 cat + 32 RBF continuous\n",
    "        include_extra_atom_feats: bool = True,  # <— attach per-atom extras\n",
    "    ):\n",
    "        self.base = LMDBDataset(ids, lmdb_path)\n",
    "        self.t = target_index\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        self.include_extra_atom_feats = include_extra_atom_feats\n",
    "\n",
    "    def __len__(self): return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.base[idx]\n",
    "\n",
    "        x  = torch.as_tensor(rec.x, dtype=torch.long)\n",
    "        ei = torch.as_tensor(rec.edge_index, dtype=torch.long)\n",
    "\n",
    "        ea = torch.as_tensor(rec.edge_attr)              # (E, 3 + 32)\n",
    "        if self.use_mixed_edges:\n",
    "            # keep all columns; EdgeEncoderMixed will split cat vs cont\n",
    "            edge_attr = ea.to(torch.float32)\n",
    "        else:\n",
    "            # categorical-only for vanilla BondEncoder\n",
    "            edge_attr = ea[:, :3].to(torch.long)\n",
    "\n",
    "        # rdkit globals: KEEP AS (1, D) so PyG collates to (B, D)\n",
    "        rdkit_feats = _get_rdkit_feats_from_record(rec)  # (1, D)\n",
    "        d = Data(x=x, edge_index=ei, edge_attr=edge_attr, rdkit_feats=rdkit_feats)\n",
    "\n",
    "        if self.include_extra_atom_feats and hasattr(rec, \"extra_atom_feats\"):\n",
    "            d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32)  # (N,5)\n",
    "\n",
    "        if hasattr(rec, \"has_xyz\"):\n",
    "            # collates to (B,1); handy as a gating/global indicator\n",
    "            hz = np.asarray(rec.has_xyz, np.uint8).reshape(-1)\n",
    "            d.has_xyz = torch.from_numpy(hz.astype(np.float32))\n",
    "\n",
    "        if (self.t is not None) and hasattr(rec, \"y\"):\n",
    "            yv = torch.as_tensor(rec.y, dtype=torch.float32).view(-1)\n",
    "            if self.t < yv.numel():\n",
    "                d.y = yv[self.t:self.t+1]  # (1,)\n",
    "\n",
    "        # geometry & extras from LMDB (if present)\n",
    "        if hasattr(rec, \"pos\"):              # (N,3) float\n",
    "            d.pos = torch.as_tensor(rec.pos, dtype=torch.float32)\n",
    "        if hasattr(rec, \"extra_atom_feats\"): # (N,5) float\n",
    "            d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32)\n",
    "        if hasattr(rec, \"has_xyz\"):          # (1,) bool/uint8\n",
    "            d.has_xyz = torch.as_tensor(rec.has_xyz, dtype=torch.float32)\n",
    "        # LMDBtoPyGSingleTask.__getitem__  (add this near the end, after you create Data d)\n",
    "        if hasattr(rec, \"dist\"):\n",
    "            # rec.dist is (L, L) (uint8) in your LMDB\n",
    "            d.hops = torch.as_tensor(rec.dist, dtype=torch.long).unsqueeze(0)  # (1, L, L)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "694612d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "def make_loaders_for_task(task, ids, *, batch_size=64, seed=42,\n",
    "                          use_mixed_edges=True, include_extra_atom_feats=True):\n",
    "    t = task2idx[task]\n",
    "    tr_ids, va_ids = train_test_split(ids, test_size=0.2, random_state=seed)\n",
    "    tr_ds = LMDBtoPyGSingleTask(tr_ids, TRAIN_LMDB, target_index=t,\n",
    "                                use_mixed_edges=use_mixed_edges,\n",
    "                                include_extra_atom_feats=include_extra_atom_feats)\n",
    "    va_ds = LMDBtoPyGSingleTask(va_ids, TRAIN_LMDB, target_index=t,\n",
    "                                use_mixed_edges=use_mixed_edges,\n",
    "                                include_extra_atom_feats=include_extra_atom_feats)\n",
    "    tr = GeoDataLoader(tr_ds, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "    va = GeoDataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    return tr, va\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983db98",
   "metadata": {},
   "source": [
    "## Step 5: Define the Hybrid GNN Model\n",
    "\n",
    "The final architecture uses both structural and cheminformatics data by combining GNN-learned graph embeddings with SMILES-derived RDKit descriptors. This Hybrid GNN model uses `smiles2graph` for graph construction and augments it with RDKit-based molecular features for improved prediction accuracy.\n",
    "\n",
    "### Model Components:\n",
    "\n",
    "* **AtomEncoder / BondEncoder**\n",
    "  Transforms categorical atom and bond features (provided by OGB) into learnable embeddings using the encoders from `ogb.graphproppred.mol_encoder`. These provide a strong foundation for expressive graph learning.\n",
    "\n",
    "* **GINEConv Layers (x2)**\n",
    "  I use two stacked GINEConv layers (Graph Isomorphism Network with Edge features). These layers perform neighborhood aggregation based on edge attributes, allowing the model to capture localized chemical environments.\n",
    "\n",
    "* **Global Mean Pooling**\n",
    "  After message passing, node level embeddings are aggregated into a fixed size graph level representation using `global_mean_pool`.\n",
    "\n",
    "* **Concatenation with RDKit Descriptors**\n",
    "  The pooled GNN embedding is concatenated with external RDKit descriptors, which capture global molecular properties not easily inferred from graph data alone.\n",
    "\n",
    "* **MLP Prediction Head**\n",
    "  A multilayer perceptron processes the combined feature vector with ReLU activations, dropout regularization, and linear layers to predict the HOMO–LUMO gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc992041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numpy as np, torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, RMSprop\n",
    "from torch.amp import GradScaler, autocast\n",
    "from copy import deepcopy\n",
    "\n",
    "def train_hybrid_gnn_sota(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    *,\n",
    "    lr: float = 5e-4,\n",
    "    optimizer: str = \"AdamW\",\n",
    "    weight_decay: float = 1e-5,\n",
    "    epochs: int = 120,\n",
    "    warmup_epochs: int = 5,\n",
    "    patience: int = 15,\n",
    "    clip_norm: float = 1.0,\n",
    "    amp: bool = True,\n",
    "    loss_name: str = \"mse\",   # \"mse\" or \"huber\"\n",
    "    save_dir: str = \"saved_models/gnn\",\n",
    "    tag: str = \"model_sota\",\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    opt_name = optimizer.lower()\n",
    "    if opt_name == \"rmsprop\":\n",
    "        opt = RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.0)\n",
    "    else:\n",
    "        opt = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # cosine schedule w/ warmup\n",
    "    def lr_factor(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / max(1, warmup_epochs)\n",
    "        t = (epoch - warmup_epochs) / max(1, (epochs - warmup_epochs))\n",
    "        return 0.5 * (1 + math.cos(math.pi * t))\n",
    "    scaler = GradScaler(\"cuda\", enabled=amp)\n",
    "\n",
    "    def loss_fn(pred, target):\n",
    "        if loss_name.lower() == \"huber\":\n",
    "            return F.huber_loss(pred, target, delta=1.0)\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_once(loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        for b in loader:\n",
    "            b = b.to(device)\n",
    "            p = model(b)\n",
    "            preds.append(p.detach().cpu())\n",
    "            trues.append(b.y.view(-1,1).cpu())\n",
    "        preds = torch.cat(preds).numpy(); trues = torch.cat(trues).numpy()\n",
    "        mae = np.mean(np.abs(preds - trues))\n",
    "        rmse = float(np.sqrt(np.mean((preds - trues)**2)))\n",
    "        r2 = float(1 - np.sum((preds - trues)**2) / np.sum((trues - trues.mean())**2))\n",
    "        return mae, rmse, r2\n",
    "\n",
    "    best_mae = float(\"inf\")\n",
    "    best = None\n",
    "    best_path = os.path.join(save_dir, f\"{tag}.pt\")\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # schedule\n",
    "        for g in opt.param_groups:\n",
    "            g[\"lr\"] = lr * lr_factor(ep-1)\n",
    "\n",
    "        model.train()\n",
    "        total, count = 0.0, 0\n",
    "        for b in train_loader:\n",
    "            b = b.to(device)\n",
    "            with autocast(\"cuda\", enabled=amp):\n",
    "                pred = model(b)\n",
    "                loss = loss_fn(pred, b.y.view(-1,1))\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            if clip_norm is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "            scaler.step(opt); scaler.update()\n",
    "\n",
    "            total += loss.item() * b.num_graphs\n",
    "            count += b.num_graphs\n",
    "\n",
    "        tr_mse = total / max(1, count)\n",
    "        mae, rmse, r2 = eval_once(val_loader)\n",
    "        print(f\"Epoch {ep:03d} | tr_MSE {tr_mse:.5f} | val_MAE {mae:.5f} | val_RMSE {rmse:.5f} | R2 {r2:.4f}\")\n",
    "\n",
    "        if mae < best_mae - 1e-6:\n",
    "            best_mae = mae\n",
    "            best = deepcopy(model.state_dict())\n",
    "            torch.save(best, best_path)\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best is not None:\n",
    "        model.load_state_dict(best)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "    final_mae, final_rmse, final_r2 = eval_once(val_loader)\n",
    "    print(f\"[{tag}] Best Val — MAE {final_mae:.6f} | RMSE {final_rmse:.6f} | R2 {final_r2:.4f}\")\n",
    "    return model, best_path, {\"MAE\": final_mae, \"RMSE\": final_rmse, \"R2\": final_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aba07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "\n",
    "# ---------- helpers ----------\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, p=0.0): super().__init__(); self.p=float(p)\n",
    "    def forward(self, x):\n",
    "        if self.p==0.0 or not self.training: return x\n",
    "        keep = 1.0 - self.p\n",
    "        shape = (x.size(0),) + (1,)*(x.ndim-1)\n",
    "        rnd = keep + torch.rand(shape, device=x.device, dtype=x.dtype)\n",
    "        rnd.floor_()\n",
    "        return x * rnd / keep\n",
    "\n",
    "def _act(name): \n",
    "    name=(name or \"relu\").lower()\n",
    "    return nn.GELU() if name==\"gelu\" else nn.SiLU() if name in (\"swish\",\"silu\") else nn.ReLU()\n",
    "\n",
    "# ---------- attention layer that accepts additive attn_bias ----------\n",
    "class MHALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps torch.nn.MultiheadAttention but allows a per-head additive bias:\n",
    "      attn_bias: (B, H, L, L) float, added to scaled dot-product before softmax.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def forward(self, x, key_padding_mask, attn_bias):\n",
    "        # x: (B,L,D), key_padding_mask: (B,L) True=PAD, attn_bias: (B,H,L,L)\n",
    "        B, L, D = x.shape\n",
    "        H = self.nhead\n",
    "        # PyTorch accepts attn_mask shaped (B*H, L, L) additive\n",
    "        mask_add = attn_bias.reshape(B*H, L, L)\n",
    "        out, _ = self.mha(\n",
    "            x, x, x,\n",
    "            key_padding_mask=key_padding_mask,  # True at pads\n",
    "            attn_mask=mask_add,\n",
    "            need_weights=False\n",
    "        )\n",
    "        return out\n",
    "\n",
    "# ---------- Transformer block (pre-norm, bias-aware attention, FFN, drop-path) ----------\n",
    "class TFBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, mlp_ratio=4.0, act=\"gelu\", dropout=0.1, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn  = MHALayer(d_model, nhead, dropout=dropout)\n",
    "        self.dp1   = DropPath(drop_path)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        hidden = int(d_model * mlp_ratio)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden), _act(act), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, d_model), nn.Dropout(dropout),\n",
    "        )\n",
    "        self.dp2   = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, x, key_padding_mask, attn_bias):\n",
    "        h = self.attn(self.norm1(x), key_padding_mask, attn_bias)\n",
    "        x = x + self.dp1(h)\n",
    "        h = self.ffn(self.norm2(x))\n",
    "        x = x + self.dp2(h)\n",
    "        return x\n",
    "\n",
    "# ---------- build geometry/graph attention bias ----------\n",
    "class AttnBiasBuilder(nn.Module):\n",
    "    def __init__(self, n_heads, rbf_k=32, dmin=0.0, dmax=10.0, beta=5.0, act=\"silu\"):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.register_buffer(\"centers\", torch.linspace(dmin, dmax, rbf_k).view(1,1,rbf_k))\n",
    "        self.beta = beta\n",
    "        self.geo_mlp = nn.Sequential(\n",
    "            nn.Linear(rbf_k, 2*n_heads), _act(act), nn.Linear(2*n_heads, n_heads)\n",
    "        )\n",
    "        self.adj_bias = nn.Parameter(torch.zeros(n_heads))\n",
    "        nn.init.normal_(self.adj_bias, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, pos, edge_index, batch, key_padding_mask):\n",
    "        \"\"\"\n",
    "        Build additive attention bias of shape (B, H, L0, L0) before CLS.\n",
    "        key_padding_mask: (B, L0)   True == PAD   (no CLS here)\n",
    "        Uses:\n",
    "        - self.n_heads (int)\n",
    "        - self.centers (K,)  float tensor for RBF centers\n",
    "        - self.beta    (float) scalar for RBF width\n",
    "        - self.geo_mlp : nn.Sequential(K -> H) maps RBF to per-head geometry bias\n",
    "        - self.adj_bias: nn.Parameter(H,) per-head adjacency bias scale\n",
    "        \"\"\"\n",
    "        # 1) dense adjacency  (B, L0, L0)\n",
    "        A = to_dense_adj(edge_index, batch=batch)          # (B, 1, L0, L0) or (B, L0, L0) depending on version\n",
    "        if A.dim() == 4:\n",
    "            A = A.squeeze(1)                               # -> (B, L0, L0)\n",
    "\n",
    "        B, L0, _ = A.shape\n",
    "        H = self.n_heads\n",
    "        device = A.device\n",
    "\n",
    "        # 2) geometry bias (zeros if pos is None)\n",
    "        if pos is not None:\n",
    "            pad_pos, _valid = to_dense_batch(pos, batch)   # (B, L0, 3)\n",
    "            # pairwise distances\n",
    "            diff = pad_pos.unsqueeze(2) - pad_pos.unsqueeze(1)         # (B, L0, L0, 3)\n",
    "            dist = torch.sqrt(torch.clamp((diff**2).sum(-1), min=0.0)) # (B, L0, L0)\n",
    "\n",
    "            centers = self.centers.to(dist.device)  # (K,)\n",
    "            rbf = torch.exp(-self.beta * (dist.unsqueeze(-1) - centers)**2)  # (B, L0, L0, K)\n",
    "            geo_bias = self.geo_mlp(rbf).permute(0, 3, 1, 2).contiguous()    # (B, H, L0, L0)\n",
    "            geo_bias = geo_bias.to(torch.float32)\n",
    "        else:\n",
    "            geo_bias = torch.zeros((B, H, L0, L0), device=device, dtype=torch.float32)\n",
    "\n",
    "        # 3) adjacency bias per head\n",
    "        #     A is (B, L0, L0) float; scale each head with learned scalar\n",
    "        adj_bias = self.adj_bias.view(1, H, 1, 1) * A.unsqueeze(1)     # (B, H, L0, L0)\n",
    "\n",
    "        # 4) PAD masking (rows from PAD, cols to PAD) with big negative\n",
    "        #    (we keep diagonal as-is here; model will final-fix diagonals if needed)\n",
    "        pad = key_padding_mask.to(torch.bool)                           # (B, L0)\n",
    "        big_neg = geo_bias.new_tensor(-1e4)\n",
    "\n",
    "        # rows FROM PAD\n",
    "        geo_bias = geo_bias.masked_fill(pad.view(B, 1, L0, 1), big_neg)\n",
    "        adj_bias = adj_bias.masked_fill(pad.view(B, 1, L0, 1), 0.0)\n",
    "        # cols TO PAD\n",
    "        geo_bias = geo_bias.masked_fill(pad.view(B, 1, 1, L0), big_neg)\n",
    "        adj_bias = adj_bias.masked_fill(pad.view(B, 1, 1, L0), 0.0)\n",
    "\n",
    "        # 5) return head-wise additive bias\n",
    "        return geo_bias + adj_bias                                     # (B, H, L0, L0)\n",
    "\n",
    "# ---------- the Graph Transformer ----------\n",
    "class GraphTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 8,\n",
    "        nlayers: int = 6,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        dropout: float = 0.1,\n",
    "        drop_path: float = 0.1,\n",
    "        atom_emb_dim: int = 256,          # kept equal to d_model\n",
    "        rdkit_dim: int = 15,              # from LMDB (6 or 15)\n",
    "        extra_atom_dim: int = 5,          # LMDB extra per-atom feats\n",
    "        activation: str = \"silu\",\n",
    "        use_extra_atom_feats: bool = True,\n",
    "        use_cls_token: bool = True,\n",
    "        use_has_xyz: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_extra_atom_feats = use_extra_atom_feats\n",
    "        self.use_cls_token = use_cls_token\n",
    "        self.use_has_xyz = use_has_xyz\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.atom_enc = AtomEncoder(emb_dim=atom_emb_dim)\n",
    "        if use_extra_atom_feats:\n",
    "            self.extra_proj = nn.Sequential(\n",
    "                nn.Linear(extra_atom_dim, d_model), _act(activation),\n",
    "                nn.Linear(d_model, d_model)\n",
    "            )\n",
    "            self.extra_gate = nn.Sequential(\n",
    "                nn.Linear(2*d_model, d_model), _act(activation)\n",
    "            )\n",
    "\n",
    "        # class token appended at end of each sequence\n",
    "        if use_cls_token:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, d_model))\n",
    "            nn.init.normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        # blocks with linearly increasing drop-path\n",
    "        dprs = [drop_path * i / max(1,(nlayers-1)) for i in range(nlayers)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TFBlock(d_model, nhead, mlp_ratio, activation, dropout, dprs[i]) for i in range(nlayers)\n",
    "        ])\n",
    "\n",
    "        self.bias_builder = AttnBiasBuilder(n_heads=nhead, rbf_k=32, dmin=0.0, dmax=10.0, beta=5.0, act=activation)\n",
    "\n",
    "        head_in = d_model + rdkit_dim + (1 if use_has_xyz else 0)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(head_in),\n",
    "            nn.Linear(head_in, 2*d_model), _act(activation), nn.Dropout(dropout),\n",
    "            nn.Linear(2*d_model, d_model//2), _act(activation), nn.Dropout(dropout),\n",
    "            nn.Linear(d_model//2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.atom_enc(data.x)\n",
    "        if self.use_extra_atom_feats and hasattr(data, \"extra_atom_feats\"):\n",
    "            xa = self.extra_proj(data.extra_atom_feats)\n",
    "            x  = self.extra_gate(torch.cat([x, xa], dim=1))\n",
    "\n",
    "        # Pack (no CLS yet)\n",
    "        x_pad, valid_mask = to_dense_batch(x, data.batch)  # (B,L0,D)\n",
    "        B, L0, D = x_pad.shape\n",
    "        key_padding_mask = ~valid_mask                     # (B,L0)  True=PAD\n",
    "\n",
    "        # Build bias on L0 tokens (no CLS)\n",
    "        attn_bias = self.bias_builder(\n",
    "            pos=data.pos if hasattr(data, \"pos\") else None,\n",
    "            edge_index=data.edge_index, batch=data.batch,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )  # (B,H,L0,L0)\n",
    "\n",
    "        # Append CLS and pad bias\n",
    "        if self.use_cls_token:\n",
    "            cls = self.cls_token.expand(B, 1, D)\n",
    "            x_pad = torch.cat([x_pad, cls], dim=1)                        # (B,L0+1,D)\n",
    "            key_padding_mask = torch.cat(\n",
    "                [key_padding_mask, torch.zeros(B,1, dtype=torch.bool, device=x_pad.device)],\n",
    "                dim=1\n",
    "            )                                                              # (B,L0+1)\n",
    "            # pad attn_bias by one row and one col (zeros) for CLS\n",
    "            attn_bias = F.pad(attn_bias, (0,1, 0,1))                       # (B,H,L0+1,L0+1)\n",
    "            L = L0 + 1\n",
    "        else:\n",
    "            L = L0\n",
    "\n",
    "        # Transformer\n",
    "        h = x_pad\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h, key_padding_mask[:, :L], attn_bias)\n",
    "\n",
    "        # Readout\n",
    "        if self.use_cls_token:\n",
    "            gvec = h[:, L-1, :]\n",
    "        else:\n",
    "            gvec = (h * valid_mask.unsqueeze(-1)).sum(1) / valid_mask.sum(1, keepdim=True)\n",
    "\n",
    "        rd = data.rdkit_feats.view(B, -1).float()\n",
    "        pieces = [gvec, rd]\n",
    "        if self.use_has_xyz and hasattr(data, \"has_xyz\"):\n",
    "            pieces.append(data.has_xyz.view(B,1).float())\n",
    "        out = torch.cat(pieces, dim=1)\n",
    "        return self.head(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e524006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loaders with mixed edges ON (so edge_attr keeps the 3 cat + 32 RBF),\n",
    "# and with extra_atom_feats so the model can use them.\n",
    "train_loader_tg,  val_loader_tg  = make_loaders_for_task(\"Tg\",      ids_tg,  batch_size=512,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "train_loader_den, val_loader_den = make_loaders_for_task(\"Density\", ids_den, batch_size=512,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "train_loader_rg,  val_loader_rg  = make_loaders_for_task(\"Rg\",      ids_rg,  batch_size=512,\n",
    "                                                         use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bee54662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | tr_MSE 22792.74805 | val_MAE 100.59009 | val_RMSE 131.10902 | R2 -0.8007\n",
      "Epoch 002 | tr_MSE 22792.79492 | val_MAE 100.59009 | val_RMSE 131.10902 | R2 -0.8007\n",
      "Epoch 003 | tr_MSE 22792.75000 | val_MAE 100.59009 | val_RMSE 131.10902 | R2 -0.8007\n",
      "Epoch 004 | tr_MSE 22792.78320 | val_MAE 100.59009 | val_RMSE 131.10902 | R2 -0.8007\n",
      "Epoch 005 | tr_MSE 22793.02539 | val_MAE 100.59009 | val_RMSE 131.10902 | R2 -0.8007\n",
      "Epoch 006 | tr_MSE 22793.52344 | val_MAE 100.59009 | val_RMSE 131.10902 | R2 -0.8007\n",
      "Epoch 007 | tr_MSE 22792.68555 | val_MAE 100.59009 | val_RMSE 131.10902 | R2 -0.8007\n",
      "Epoch 008 | tr_MSE 22793.31445 | val_MAE 100.59009 | val_RMSE 131.10902 | R2 -0.8007\n",
      "Epoch 009 | tr_MSE 22791.89062 | val_MAE 100.42054 | val_RMSE 130.91714 | R2 -0.7954\n",
      "Epoch 010 | tr_MSE 22738.57227 | val_MAE 100.25423 | val_RMSE 130.73039 | R2 -0.7903\n",
      "Epoch 011 | tr_MSE 22684.26367 | val_MAE 100.08749 | val_RMSE 130.54160 | R2 -0.7851\n",
      "Epoch 012 | tr_MSE 22630.24023 | val_MAE 99.90849 | val_RMSE 130.33475 | R2 -0.7795\n",
      "Epoch 013 | tr_MSE 22569.49609 | val_MAE 99.70316 | val_RMSE 130.09790 | R2 -0.7730\n",
      "Epoch 014 | tr_MSE 22504.88086 | val_MAE 99.46233 | val_RMSE 129.82074 | R2 -0.7655\n",
      "Epoch 015 | tr_MSE 22425.59180 | val_MAE 99.18456 | val_RMSE 129.49323 | R2 -0.7566\n",
      "Epoch 016 | tr_MSE 22331.11133 | val_MAE 98.85835 | val_RMSE 129.10551 | R2 -0.7461\n",
      "Epoch 017 | tr_MSE 22223.58008 | val_MAE 98.85835 | val_RMSE 129.10551 | R2 -0.7461\n",
      "Epoch 018 | tr_MSE 22224.66797 | val_MAE 98.47402 | val_RMSE 128.65111 | R2 -0.7338\n",
      "Epoch 019 | tr_MSE 22089.33398 | val_MAE 98.02366 | val_RMSE 128.12202 | R2 -0.7196\n",
      "Epoch 020 | tr_MSE 21936.26562 | val_MAE 97.50240 | val_RMSE 127.51438 | R2 -0.7033\n",
      "Epoch 021 | tr_MSE 21770.49805 | val_MAE 96.90788 | val_RMSE 126.82720 | R2 -0.6850\n",
      "Epoch 022 | tr_MSE 21580.25977 | val_MAE 96.90788 | val_RMSE 126.82720 | R2 -0.6850\n",
      "Epoch 023 | tr_MSE 21573.93750 | val_MAE 96.25671 | val_RMSE 126.06715 | R2 -0.6648\n",
      "Epoch 024 | tr_MSE 21358.62891 | val_MAE 95.54821 | val_RMSE 125.23425 | R2 -0.6429\n",
      "Epoch 025 | tr_MSE 21147.76953 | val_MAE 94.79021 | val_RMSE 124.33322 | R2 -0.6194\n",
      "Epoch 026 | tr_MSE 20866.12891 | val_MAE 94.01105 | val_RMSE 123.36860 | R2 -0.5943\n",
      "Epoch 027 | tr_MSE 20599.16797 | val_MAE 93.19965 | val_RMSE 122.34443 | R2 -0.5680\n",
      "Epoch 028 | tr_MSE 20283.78516 | val_MAE 93.19965 | val_RMSE 122.34443 | R2 -0.5680\n",
      "Epoch 029 | tr_MSE 20275.99219 | val_MAE 92.37755 | val_RMSE 121.27430 | R2 -0.5407\n",
      "Epoch 030 | tr_MSE 19992.39453 | val_MAE 91.51234 | val_RMSE 120.15397 | R2 -0.5123\n",
      "Epoch 031 | tr_MSE 19740.19531 | val_MAE 90.62524 | val_RMSE 118.98758 | R2 -0.4831\n",
      "Epoch 032 | tr_MSE 19389.47461 | val_MAE 89.75882 | val_RMSE 117.77995 | R2 -0.4532\n",
      "Epoch 033 | tr_MSE 19042.22852 | val_MAE 88.86190 | val_RMSE 116.53729 | R2 -0.4227\n",
      "Epoch 034 | tr_MSE 18695.19727 | val_MAE 87.98914 | val_RMSE 115.26661 | R2 -0.3918\n",
      "Epoch 035 | tr_MSE 18282.33984 | val_MAE 87.18352 | val_RMSE 113.97527 | R2 -0.3608\n",
      "Epoch 036 | tr_MSE 18046.69727 | val_MAE 86.40682 | val_RMSE 112.67202 | R2 -0.3298\n",
      "Epoch 037 | tr_MSE 17683.96680 | val_MAE 85.60046 | val_RMSE 111.36591 | R2 -0.2992\n",
      "Epoch 038 | tr_MSE 17261.43945 | val_MAE 84.78113 | val_RMSE 110.06623 | R2 -0.2690\n",
      "Epoch 039 | tr_MSE 17046.01562 | val_MAE 84.00896 | val_RMSE 108.78306 | R2 -0.2396\n",
      "Epoch 040 | tr_MSE 16622.97852 | val_MAE 83.29169 | val_RMSE 107.52650 | R2 -0.2112\n",
      "Epoch 041 | tr_MSE 16252.84277 | val_MAE 82.63406 | val_RMSE 106.30661 | R2 -0.1838\n",
      "Epoch 042 | tr_MSE 15983.31152 | val_MAE 82.00434 | val_RMSE 105.13325 | R2 -0.1578\n",
      "Epoch 043 | tr_MSE 15616.81348 | val_MAE 81.43317 | val_RMSE 104.01637 | R2 -0.1334\n",
      "Epoch 044 | tr_MSE 15332.08301 | val_MAE 80.90331 | val_RMSE 102.96554 | R2 -0.1106\n",
      "Epoch 045 | tr_MSE 15063.99121 | val_MAE 80.42636 | val_RMSE 101.99053 | R2 -0.0897\n",
      "Epoch 046 | tr_MSE 14874.13281 | val_MAE 79.94466 | val_RMSE 101.09933 | R2 -0.0707\n",
      "Epoch 047 | tr_MSE 14579.91504 | val_MAE 79.48463 | val_RMSE 100.30090 | R2 -0.0539\n",
      "Epoch 048 | tr_MSE 14228.95605 | val_MAE 79.11886 | val_RMSE 99.60224 | R2 -0.0392\n",
      "Epoch 049 | tr_MSE 13899.63086 | val_MAE 78.82079 | val_RMSE 99.00954 | R2 -0.0269\n",
      "Epoch 050 | tr_MSE 13888.79492 | val_MAE 78.57083 | val_RMSE 98.52877 | R2 -0.0169\n",
      "Epoch 051 | tr_MSE 13688.27051 | val_MAE 78.58318 | val_RMSE 98.16432 | R2 -0.0094\n",
      "Epoch 052 | tr_MSE 13582.87695 | val_MAE 78.77155 | val_RMSE 97.91982 | R2 -0.0044\n",
      "Epoch 053 | tr_MSE 13379.98047 | val_MAE 79.01691 | val_RMSE 97.79731 | R2 -0.0019\n",
      "Epoch 054 | tr_MSE 13200.35938 | val_MAE 79.45198 | val_RMSE 97.79718 | R2 -0.0019\n",
      "Epoch 055 | tr_MSE 13434.49805 | val_MAE 80.02800 | val_RMSE 97.91772 | R2 -0.0044\n",
      "Epoch 056 | tr_MSE 13148.16992 | val_MAE 80.65958 | val_RMSE 98.15390 | R2 -0.0092\n",
      "Epoch 057 | tr_MSE 13065.18164 | val_MAE 81.32932 | val_RMSE 98.49392 | R2 -0.0162\n",
      "Epoch 058 | tr_MSE 12960.32422 | val_MAE 81.96294 | val_RMSE 98.91170 | R2 -0.0249\n",
      "Epoch 059 | tr_MSE 13112.12500 | val_MAE 82.51167 | val_RMSE 99.31261 | R2 -0.0332\n",
      "Epoch 060 | tr_MSE 13046.94531 | val_MAE 82.98316 | val_RMSE 99.65750 | R2 -0.0404\n",
      "Epoch 061 | tr_MSE 13266.96387 | val_MAE 83.33128 | val_RMSE 99.92302 | R2 -0.0459\n",
      "Epoch 062 | tr_MSE 13177.56152 | val_MAE 83.55331 | val_RMSE 100.10139 | R2 -0.0497\n",
      "Epoch 063 | tr_MSE 13197.08105 | val_MAE 83.65940 | val_RMSE 100.18900 | R2 -0.0515\n",
      "Epoch 064 | tr_MSE 12964.75195 | val_MAE 83.66637 | val_RMSE 100.19450 | R2 -0.0516\n",
      "Epoch 065 | tr_MSE 13117.17773 | val_MAE 83.58641 | val_RMSE 100.12730 | R2 -0.0502\n",
      "Epoch 066 | tr_MSE 13042.11328 | val_MAE 83.43098 | val_RMSE 99.99934 | R2 -0.0475\n",
      "Epoch 067 | tr_MSE 12991.01270 | val_MAE 83.21009 | val_RMSE 99.82304 | R2 -0.0438\n",
      "Epoch 068 | tr_MSE 13140.42285 | val_MAE 82.93235 | val_RMSE 99.61062 | R2 -0.0394\n",
      "Epoch 069 | tr_MSE 13318.85547 | val_MAE 82.60707 | val_RMSE 99.37450 | R2 -0.0345\n",
      "Epoch 070 | tr_MSE 12964.79199 | val_MAE 82.28367 | val_RMSE 99.12837 | R2 -0.0294\n",
      "Early stopping.\n",
      "[graphtransformer_tg] Best Val — MAE 78.570831 | RMSE 98.528770 | R2 -0.0169\n",
      "Epoch 001 | tr_MSE 1.25057 | val_MAE 1.10460 | val_RMSE 1.11239 | R2 -69.4664\n",
      "Epoch 002 | tr_MSE 1.24742 | val_MAE 1.10460 | val_RMSE 1.11239 | R2 -69.4664\n",
      "Epoch 003 | tr_MSE 1.24893 | val_MAE 0.96152 | val_RMSE 0.97059 | R2 -52.6467\n",
      "Epoch 004 | tr_MSE 0.96797 | val_MAE 0.79018 | val_RMSE 0.80176 | R2 -35.6066\n",
      "Epoch 005 | tr_MSE 0.65548 | val_MAE 0.57217 | val_RMSE 0.58976 | R2 -18.8071\n",
      "Epoch 006 | tr_MSE 0.36694 | val_MAE 0.29665 | val_RMSE 0.33172 | R2 -5.2665\n",
      "Epoch 007 | tr_MSE 0.12989 | val_MAE 0.14934 | val_RMSE 0.20029 | R2 -1.2845\n",
      "Epoch 008 | tr_MSE 0.05432 | val_MAE 0.28321 | val_RMSE 0.32279 | R2 -4.9335\n",
      "Epoch 009 | tr_MSE 0.10941 | val_MAE 0.29290 | val_RMSE 0.31957 | R2 -4.8158\n",
      "Epoch 010 | tr_MSE 0.11225 | val_MAE 0.20084 | val_RMSE 0.22357 | R2 -1.8465\n",
      "Epoch 011 | tr_MSE 0.06048 | val_MAE 0.13049 | val_RMSE 0.16328 | R2 -0.5183\n",
      "Epoch 012 | tr_MSE 0.03824 | val_MAE 0.16725 | val_RMSE 0.22202 | R2 -1.8072\n",
      "Epoch 013 | tr_MSE 0.05827 | val_MAE 0.15529 | val_RMSE 0.20496 | R2 -1.3922\n",
      "Epoch 014 | tr_MSE 0.04960 | val_MAE 0.12388 | val_RMSE 0.15681 | R2 -0.4002\n",
      "Epoch 015 | tr_MSE 0.03585 | val_MAE 0.13365 | val_RMSE 0.15935 | R2 -0.4460\n",
      "Epoch 016 | tr_MSE 0.03397 | val_MAE 0.13893 | val_RMSE 0.16359 | R2 -0.5239\n",
      "Epoch 017 | tr_MSE 0.03759 | val_MAE 0.11613 | val_RMSE 0.13938 | R2 -0.1063\n",
      "Epoch 018 | tr_MSE 0.03047 | val_MAE 0.10290 | val_RMSE 0.12850 | R2 0.0596\n",
      "Epoch 019 | tr_MSE 0.02688 | val_MAE 0.10266 | val_RMSE 0.13973 | R2 -0.1118\n",
      "Epoch 020 | tr_MSE 0.02979 | val_MAE 0.10602 | val_RMSE 0.14796 | R2 -0.2468\n",
      "Epoch 021 | tr_MSE 0.02869 | val_MAE 0.10234 | val_RMSE 0.14202 | R2 -0.1487\n",
      "Epoch 022 | tr_MSE 0.02751 | val_MAE 0.09646 | val_RMSE 0.12908 | R2 0.0511\n",
      "Epoch 023 | tr_MSE 0.02573 | val_MAE 0.09689 | val_RMSE 0.12225 | R2 0.1489\n",
      "Epoch 024 | tr_MSE 0.02453 | val_MAE 0.10422 | val_RMSE 0.12698 | R2 0.0819\n",
      "Epoch 025 | tr_MSE 0.02549 | val_MAE 0.11047 | val_RMSE 0.13340 | R2 -0.0134\n",
      "Epoch 026 | tr_MSE 0.02815 | val_MAE 0.10901 | val_RMSE 0.13202 | R2 0.0074\n",
      "Epoch 027 | tr_MSE 0.02790 | val_MAE 0.10185 | val_RMSE 0.12476 | R2 0.1136\n",
      "Epoch 028 | tr_MSE 0.02596 | val_MAE 0.09469 | val_RMSE 0.11963 | R2 0.1850\n",
      "Epoch 029 | tr_MSE 0.02210 | val_MAE 0.09110 | val_RMSE 0.12079 | R2 0.1691\n",
      "Epoch 030 | tr_MSE 0.02371 | val_MAE 0.09046 | val_RMSE 0.12392 | R2 0.1255\n",
      "Epoch 031 | tr_MSE 0.02408 | val_MAE 0.09013 | val_RMSE 0.12400 | R2 0.1244\n",
      "Epoch 032 | tr_MSE 0.02350 | val_MAE 0.08882 | val_RMSE 0.12090 | R2 0.1676\n",
      "Epoch 033 | tr_MSE 0.02338 | val_MAE 0.08960 | val_RMSE 0.11682 | R2 0.2228\n",
      "Epoch 034 | tr_MSE 0.02237 | val_MAE 0.09249 | val_RMSE 0.11621 | R2 0.2309\n",
      "Epoch 035 | tr_MSE 0.02394 | val_MAE 0.09591 | val_RMSE 0.11824 | R2 0.2039\n",
      "Epoch 036 | tr_MSE 0.02266 | val_MAE 0.09703 | val_RMSE 0.11896 | R2 0.1941\n",
      "Epoch 037 | tr_MSE 0.02217 | val_MAE 0.09487 | val_RMSE 0.11684 | R2 0.2226\n",
      "Epoch 038 | tr_MSE 0.02171 | val_MAE 0.09100 | val_RMSE 0.11386 | R2 0.2618\n",
      "Epoch 039 | tr_MSE 0.02105 | val_MAE 0.08700 | val_RMSE 0.11209 | R2 0.2846\n",
      "Epoch 040 | tr_MSE 0.02129 | val_MAE 0.08455 | val_RMSE 0.11211 | R2 0.2843\n",
      "Epoch 041 | tr_MSE 0.02057 | val_MAE 0.08366 | val_RMSE 0.11178 | R2 0.2885\n",
      "Epoch 042 | tr_MSE 0.02047 | val_MAE 0.08344 | val_RMSE 0.11052 | R2 0.3044\n",
      "Epoch 043 | tr_MSE 0.01985 | val_MAE 0.08438 | val_RMSE 0.10941 | R2 0.3184\n",
      "Epoch 044 | tr_MSE 0.02026 | val_MAE 0.08671 | val_RMSE 0.10948 | R2 0.3175\n",
      "Epoch 045 | tr_MSE 0.01896 | val_MAE 0.08845 | val_RMSE 0.11016 | R2 0.3089\n",
      "Epoch 046 | tr_MSE 0.01897 | val_MAE 0.08751 | val_RMSE 0.10934 | R2 0.3192\n",
      "Epoch 047 | tr_MSE 0.01965 | val_MAE 0.08455 | val_RMSE 0.10744 | R2 0.3427\n",
      "Epoch 048 | tr_MSE 0.02002 | val_MAE 0.08129 | val_RMSE 0.10608 | R2 0.3592\n",
      "Epoch 049 | tr_MSE 0.01911 | val_MAE 0.07906 | val_RMSE 0.10554 | R2 0.3657\n",
      "Epoch 050 | tr_MSE 0.01833 | val_MAE 0.07779 | val_RMSE 0.10529 | R2 0.3687\n",
      "Epoch 051 | tr_MSE 0.01771 | val_MAE 0.07761 | val_RMSE 0.10414 | R2 0.3824\n",
      "Epoch 052 | tr_MSE 0.01902 | val_MAE 0.07859 | val_RMSE 0.10336 | R2 0.3917\n",
      "Epoch 053 | tr_MSE 0.01772 | val_MAE 0.08001 | val_RMSE 0.10342 | R2 0.3909\n",
      "Epoch 054 | tr_MSE 0.01803 | val_MAE 0.07973 | val_RMSE 0.10303 | R2 0.3955\n",
      "Epoch 055 | tr_MSE 0.01550 | val_MAE 0.07755 | val_RMSE 0.10163 | R2 0.4118\n",
      "Epoch 056 | tr_MSE 0.01723 | val_MAE 0.07493 | val_RMSE 0.10028 | R2 0.4273\n",
      "Epoch 057 | tr_MSE 0.01677 | val_MAE 0.07277 | val_RMSE 0.09933 | R2 0.4382\n",
      "Epoch 058 | tr_MSE 0.01739 | val_MAE 0.07148 | val_RMSE 0.09844 | R2 0.4482\n",
      "Epoch 059 | tr_MSE 0.01541 | val_MAE 0.07093 | val_RMSE 0.09750 | R2 0.4587\n",
      "Epoch 060 | tr_MSE 0.01678 | val_MAE 0.06950 | val_RMSE 0.09638 | R2 0.4710\n",
      "Epoch 061 | tr_MSE 0.01506 | val_MAE 0.06862 | val_RMSE 0.09531 | R2 0.4827\n",
      "Epoch 062 | tr_MSE 0.01632 | val_MAE 0.06635 | val_RMSE 0.09361 | R2 0.5010\n",
      "Epoch 063 | tr_MSE 0.01655 | val_MAE 0.06405 | val_RMSE 0.09184 | R2 0.5197\n",
      "Epoch 064 | tr_MSE 0.01415 | val_MAE 0.06167 | val_RMSE 0.08996 | R2 0.5391\n",
      "Epoch 065 | tr_MSE 0.01300 | val_MAE 0.06040 | val_RMSE 0.08843 | R2 0.5547\n",
      "Epoch 066 | tr_MSE 0.01441 | val_MAE 0.06103 | val_RMSE 0.08835 | R2 0.5554\n",
      "Epoch 067 | tr_MSE 0.01387 | val_MAE 0.05556 | val_RMSE 0.08777 | R2 0.5613\n",
      "Epoch 068 | tr_MSE 0.01446 | val_MAE 0.06788 | val_RMSE 0.09489 | R2 0.4872\n",
      "Epoch 069 | tr_MSE 0.01453 | val_MAE 0.05320 | val_RMSE 0.08344 | R2 0.6035\n",
      "Epoch 070 | tr_MSE 0.01255 | val_MAE 0.05172 | val_RMSE 0.08158 | R2 0.6210\n",
      "Epoch 071 | tr_MSE 0.01280 | val_MAE 0.05568 | val_RMSE 0.08493 | R2 0.5892\n",
      "Epoch 072 | tr_MSE 0.01120 | val_MAE 0.05157 | val_RMSE 0.08152 | R2 0.6216\n",
      "Epoch 073 | tr_MSE 0.01137 | val_MAE 0.05042 | val_RMSE 0.08022 | R2 0.6335\n",
      "Epoch 074 | tr_MSE 0.01118 | val_MAE 0.05801 | val_RMSE 0.08814 | R2 0.5576\n",
      "Epoch 075 | tr_MSE 0.01215 | val_MAE 0.06186 | val_RMSE 0.08958 | R2 0.5430\n",
      "Epoch 076 | tr_MSE 0.01149 | val_MAE 0.05070 | val_RMSE 0.08193 | R2 0.6178\n",
      "Epoch 077 | tr_MSE 0.01097 | val_MAE 0.04762 | val_RMSE 0.07803 | R2 0.6532\n",
      "Epoch 078 | tr_MSE 0.00908 | val_MAE 0.05067 | val_RMSE 0.07901 | R2 0.6445\n",
      "Epoch 079 | tr_MSE 0.00978 | val_MAE 0.04898 | val_RMSE 0.07919 | R2 0.6429\n",
      "Epoch 080 | tr_MSE 0.00833 | val_MAE 0.05269 | val_RMSE 0.08457 | R2 0.5927\n",
      "Epoch 081 | tr_MSE 0.00875 | val_MAE 0.05102 | val_RMSE 0.08047 | R2 0.6313\n",
      "Epoch 082 | tr_MSE 0.00908 | val_MAE 0.04703 | val_RMSE 0.07728 | R2 0.6599\n",
      "Epoch 083 | tr_MSE 0.00838 | val_MAE 0.05938 | val_RMSE 0.09089 | R2 0.5296\n",
      "Epoch 084 | tr_MSE 0.01007 | val_MAE 0.06620 | val_RMSE 0.09652 | R2 0.4695\n",
      "Epoch 085 | tr_MSE 0.01193 | val_MAE 0.04635 | val_RMSE 0.07879 | R2 0.6465\n",
      "Epoch 086 | tr_MSE 0.00927 | val_MAE 0.07811 | val_RMSE 0.10623 | R2 0.3574\n",
      "Epoch 087 | tr_MSE 0.01188 | val_MAE 0.04649 | val_RMSE 0.07881 | R2 0.6463\n",
      "Epoch 088 | tr_MSE 0.00732 | val_MAE 0.06277 | val_RMSE 0.09402 | R2 0.4966\n",
      "Epoch 089 | tr_MSE 0.01047 | val_MAE 0.04951 | val_RMSE 0.08155 | R2 0.6213\n",
      "Epoch 090 | tr_MSE 0.00870 | val_MAE 0.05855 | val_RMSE 0.09067 | R2 0.5319\n",
      "Epoch 091 | tr_MSE 0.00831 | val_MAE 0.06748 | val_RMSE 0.09851 | R2 0.4473\n",
      "Epoch 092 | tr_MSE 0.01004 | val_MAE 0.04511 | val_RMSE 0.07761 | R2 0.6570\n",
      "Epoch 093 | tr_MSE 0.00805 | val_MAE 0.05482 | val_RMSE 0.08545 | R2 0.5842\n",
      "Epoch 094 | tr_MSE 0.00979 | val_MAE 0.05138 | val_RMSE 0.08226 | R2 0.6146\n",
      "Epoch 095 | tr_MSE 0.00911 | val_MAE 0.04459 | val_RMSE 0.07728 | R2 0.6599\n",
      "Epoch 096 | tr_MSE 0.00770 | val_MAE 0.05648 | val_RMSE 0.08813 | R2 0.5577\n",
      "Epoch 097 | tr_MSE 0.00874 | val_MAE 0.05401 | val_RMSE 0.08707 | R2 0.5682\n",
      "Epoch 098 | tr_MSE 0.00883 | val_MAE 0.04415 | val_RMSE 0.07617 | R2 0.6696\n",
      "Epoch 099 | tr_MSE 0.00730 | val_MAE 0.05254 | val_RMSE 0.08064 | R2 0.6297\n",
      "Epoch 100 | tr_MSE 0.00890 | val_MAE 0.05068 | val_RMSE 0.07945 | R2 0.6405\n",
      "Epoch 101 | tr_MSE 0.00892 | val_MAE 0.04424 | val_RMSE 0.07682 | R2 0.6639\n",
      "Epoch 102 | tr_MSE 0.00853 | val_MAE 0.04642 | val_RMSE 0.08021 | R2 0.6337\n",
      "Epoch 103 | tr_MSE 0.00872 | val_MAE 0.04359 | val_RMSE 0.07752 | R2 0.6578\n",
      "Epoch 104 | tr_MSE 0.00705 | val_MAE 0.04209 | val_RMSE 0.07577 | R2 0.6731\n",
      "Epoch 105 | tr_MSE 0.00753 | val_MAE 0.04431 | val_RMSE 0.07695 | R2 0.6628\n",
      "Epoch 106 | tr_MSE 0.00752 | val_MAE 0.04258 | val_RMSE 0.07630 | R2 0.6685\n",
      "Epoch 107 | tr_MSE 0.00801 | val_MAE 0.04241 | val_RMSE 0.07675 | R2 0.6645\n",
      "Epoch 108 | tr_MSE 0.00757 | val_MAE 0.04267 | val_RMSE 0.07720 | R2 0.6606\n",
      "Epoch 109 | tr_MSE 0.00662 | val_MAE 0.04311 | val_RMSE 0.07731 | R2 0.6596\n",
      "Epoch 110 | tr_MSE 0.00755 | val_MAE 0.04340 | val_RMSE 0.07695 | R2 0.6628\n",
      "Epoch 111 | tr_MSE 0.00698 | val_MAE 0.04310 | val_RMSE 0.07672 | R2 0.6648\n",
      "Epoch 112 | tr_MSE 0.00723 | val_MAE 0.04234 | val_RMSE 0.07643 | R2 0.6674\n",
      "Epoch 113 | tr_MSE 0.00708 | val_MAE 0.04184 | val_RMSE 0.07638 | R2 0.6678\n",
      "Epoch 114 | tr_MSE 0.00707 | val_MAE 0.04148 | val_RMSE 0.07594 | R2 0.6716\n",
      "Epoch 115 | tr_MSE 0.00803 | val_MAE 0.04142 | val_RMSE 0.07581 | R2 0.6727\n",
      "Epoch 116 | tr_MSE 0.00662 | val_MAE 0.04129 | val_RMSE 0.07584 | R2 0.6724\n",
      "Epoch 117 | tr_MSE 0.00651 | val_MAE 0.04162 | val_RMSE 0.07611 | R2 0.6702\n",
      "Epoch 118 | tr_MSE 0.00690 | val_MAE 0.04264 | val_RMSE 0.07802 | R2 0.6533\n",
      "Epoch 119 | tr_MSE 0.00663 | val_MAE 0.04242 | val_RMSE 0.07756 | R2 0.6574\n",
      "Epoch 120 | tr_MSE 0.00647 | val_MAE 0.04316 | val_RMSE 0.07678 | R2 0.6643\n",
      "Epoch 121 | tr_MSE 0.00713 | val_MAE 0.04386 | val_RMSE 0.07699 | R2 0.6625\n",
      "Epoch 122 | tr_MSE 0.00756 | val_MAE 0.04157 | val_RMSE 0.07629 | R2 0.6686\n",
      "Epoch 123 | tr_MSE 0.00656 | val_MAE 0.04345 | val_RMSE 0.07920 | R2 0.6428\n",
      "Epoch 124 | tr_MSE 0.00727 | val_MAE 0.04286 | val_RMSE 0.07819 | R2 0.6519\n",
      "Epoch 125 | tr_MSE 0.00794 | val_MAE 0.04114 | val_RMSE 0.07483 | R2 0.6811\n",
      "Epoch 126 | tr_MSE 0.00662 | val_MAE 0.04439 | val_RMSE 0.07679 | R2 0.6642\n",
      "Epoch 127 | tr_MSE 0.00778 | val_MAE 0.04340 | val_RMSE 0.07620 | R2 0.6693\n",
      "Epoch 128 | tr_MSE 0.00701 | val_MAE 0.04154 | val_RMSE 0.07662 | R2 0.6656\n",
      "Epoch 129 | tr_MSE 0.00679 | val_MAE 0.04317 | val_RMSE 0.07936 | R2 0.6413\n",
      "Epoch 130 | tr_MSE 0.00754 | val_MAE 0.04193 | val_RMSE 0.07731 | R2 0.6596\n",
      "Epoch 131 | tr_MSE 0.00703 | val_MAE 0.04174 | val_RMSE 0.07522 | R2 0.6778\n",
      "Epoch 132 | tr_MSE 0.00651 | val_MAE 0.04168 | val_RMSE 0.07482 | R2 0.6813\n",
      "Epoch 133 | tr_MSE 0.00697 | val_MAE 0.04074 | val_RMSE 0.07437 | R2 0.6850\n",
      "Epoch 134 | tr_MSE 0.00626 | val_MAE 0.04070 | val_RMSE 0.07589 | R2 0.6720\n",
      "Epoch 135 | tr_MSE 0.00633 | val_MAE 0.04205 | val_RMSE 0.07747 | R2 0.6582\n",
      "Epoch 136 | tr_MSE 0.00710 | val_MAE 0.04005 | val_RMSE 0.07488 | R2 0.6807\n",
      "Epoch 137 | tr_MSE 0.00643 | val_MAE 0.03967 | val_RMSE 0.07293 | R2 0.6971\n",
      "Epoch 138 | tr_MSE 0.00645 | val_MAE 0.04027 | val_RMSE 0.07308 | R2 0.6958\n",
      "Epoch 139 | tr_MSE 0.00571 | val_MAE 0.03989 | val_RMSE 0.07288 | R2 0.6975\n",
      "Epoch 140 | tr_MSE 0.00700 | val_MAE 0.03883 | val_RMSE 0.07332 | R2 0.6938\n",
      "Epoch 141 | tr_MSE 0.00661 | val_MAE 0.04106 | val_RMSE 0.07608 | R2 0.6704\n",
      "Epoch 142 | tr_MSE 0.00640 | val_MAE 0.04282 | val_RMSE 0.07803 | R2 0.6533\n",
      "Epoch 143 | tr_MSE 0.00655 | val_MAE 0.04010 | val_RMSE 0.07518 | R2 0.6781\n",
      "Epoch 144 | tr_MSE 0.00661 | val_MAE 0.03948 | val_RMSE 0.07321 | R2 0.6948\n",
      "Epoch 145 | tr_MSE 0.00656 | val_MAE 0.04132 | val_RMSE 0.07387 | R2 0.6893\n",
      "Epoch 146 | tr_MSE 0.00740 | val_MAE 0.04018 | val_RMSE 0.07321 | R2 0.6948\n",
      "Epoch 147 | tr_MSE 0.00669 | val_MAE 0.03805 | val_RMSE 0.07321 | R2 0.6948\n",
      "Epoch 148 | tr_MSE 0.00626 | val_MAE 0.04024 | val_RMSE 0.07577 | R2 0.6731\n",
      "Epoch 149 | tr_MSE 0.00727 | val_MAE 0.04086 | val_RMSE 0.07644 | R2 0.6672\n",
      "Epoch 150 | tr_MSE 0.00684 | val_MAE 0.03954 | val_RMSE 0.07513 | R2 0.6785\n",
      "Epoch 151 | tr_MSE 0.00632 | val_MAE 0.03813 | val_RMSE 0.07364 | R2 0.6912\n",
      "Epoch 152 | tr_MSE 0.00603 | val_MAE 0.03811 | val_RMSE 0.07299 | R2 0.6966\n",
      "Epoch 153 | tr_MSE 0.00694 | val_MAE 0.03909 | val_RMSE 0.07318 | R2 0.6950\n",
      "Epoch 154 | tr_MSE 0.00737 | val_MAE 0.03903 | val_RMSE 0.07331 | R2 0.6939\n",
      "Epoch 155 | tr_MSE 0.00598 | val_MAE 0.03844 | val_RMSE 0.07324 | R2 0.6945\n",
      "Epoch 156 | tr_MSE 0.00610 | val_MAE 0.03838 | val_RMSE 0.07396 | R2 0.6885\n",
      "Epoch 157 | tr_MSE 0.00660 | val_MAE 0.03894 | val_RMSE 0.07475 | R2 0.6818\n",
      "Epoch 158 | tr_MSE 0.00646 | val_MAE 0.03871 | val_RMSE 0.07446 | R2 0.6843\n",
      "Epoch 159 | tr_MSE 0.00677 | val_MAE 0.03851 | val_RMSE 0.07423 | R2 0.6862\n",
      "Epoch 160 | tr_MSE 0.00688 | val_MAE 0.03830 | val_RMSE 0.07392 | R2 0.6888\n",
      "Epoch 161 | tr_MSE 0.00627 | val_MAE 0.03791 | val_RMSE 0.07335 | R2 0.6936\n",
      "Epoch 162 | tr_MSE 0.00604 | val_MAE 0.03785 | val_RMSE 0.07294 | R2 0.6970\n",
      "Epoch 163 | tr_MSE 0.00619 | val_MAE 0.03786 | val_RMSE 0.07261 | R2 0.6998\n",
      "Epoch 164 | tr_MSE 0.00625 | val_MAE 0.03794 | val_RMSE 0.07253 | R2 0.7004\n",
      "Epoch 165 | tr_MSE 0.00658 | val_MAE 0.03781 | val_RMSE 0.07257 | R2 0.7001\n",
      "Epoch 166 | tr_MSE 0.00625 | val_MAE 0.03776 | val_RMSE 0.07290 | R2 0.6974\n",
      "Epoch 167 | tr_MSE 0.00598 | val_MAE 0.03780 | val_RMSE 0.07322 | R2 0.6947\n",
      "Epoch 168 | tr_MSE 0.00615 | val_MAE 0.03783 | val_RMSE 0.07337 | R2 0.6935\n",
      "Epoch 169 | tr_MSE 0.00651 | val_MAE 0.03782 | val_RMSE 0.07334 | R2 0.6937\n",
      "Epoch 170 | tr_MSE 0.00526 | val_MAE 0.03781 | val_RMSE 0.07313 | R2 0.6954\n",
      "Epoch 171 | tr_MSE 0.00563 | val_MAE 0.03786 | val_RMSE 0.07307 | R2 0.6959\n",
      "Epoch 172 | tr_MSE 0.00605 | val_MAE 0.03789 | val_RMSE 0.07331 | R2 0.6940\n",
      "Epoch 173 | tr_MSE 0.00704 | val_MAE 0.03789 | val_RMSE 0.07347 | R2 0.6926\n",
      "Epoch 174 | tr_MSE 0.00667 | val_MAE 0.03793 | val_RMSE 0.07357 | R2 0.6917\n",
      "Epoch 175 | tr_MSE 0.00611 | val_MAE 0.03798 | val_RMSE 0.07372 | R2 0.6905\n",
      "Epoch 176 | tr_MSE 0.00635 | val_MAE 0.03807 | val_RMSE 0.07386 | R2 0.6894\n",
      "Epoch 177 | tr_MSE 0.00595 | val_MAE 0.03821 | val_RMSE 0.07404 | R2 0.6878\n",
      "Epoch 178 | tr_MSE 0.00637 | val_MAE 0.03842 | val_RMSE 0.07432 | R2 0.6855\n",
      "Epoch 179 | tr_MSE 0.00629 | val_MAE 0.03861 | val_RMSE 0.07457 | R2 0.6833\n",
      "Epoch 180 | tr_MSE 0.00586 | val_MAE 0.03864 | val_RMSE 0.07463 | R2 0.6828\n",
      "Epoch 181 | tr_MSE 0.00595 | val_MAE 0.03855 | val_RMSE 0.07447 | R2 0.6842\n",
      "Epoch 182 | tr_MSE 0.00573 | val_MAE 0.03841 | val_RMSE 0.07420 | R2 0.6865\n",
      "Epoch 183 | tr_MSE 0.00675 | val_MAE 0.03830 | val_RMSE 0.07396 | R2 0.6885\n",
      "Epoch 184 | tr_MSE 0.00571 | val_MAE 0.03823 | val_RMSE 0.07381 | R2 0.6898\n",
      "Epoch 185 | tr_MSE 0.00587 | val_MAE 0.03821 | val_RMSE 0.07368 | R2 0.6908\n",
      "Epoch 186 | tr_MSE 0.00611 | val_MAE 0.03818 | val_RMSE 0.07358 | R2 0.6917\n",
      "Epoch 187 | tr_MSE 0.00604 | val_MAE 0.03815 | val_RMSE 0.07353 | R2 0.6921\n",
      "Epoch 188 | tr_MSE 0.00674 | val_MAE 0.03813 | val_RMSE 0.07347 | R2 0.6926\n",
      "Epoch 189 | tr_MSE 0.00624 | val_MAE 0.03811 | val_RMSE 0.07341 | R2 0.6931\n",
      "Epoch 190 | tr_MSE 0.00642 | val_MAE 0.03809 | val_RMSE 0.07338 | R2 0.6934\n",
      "Epoch 191 | tr_MSE 0.00575 | val_MAE 0.03807 | val_RMSE 0.07333 | R2 0.6937\n",
      "Epoch 192 | tr_MSE 0.00581 | val_MAE 0.03804 | val_RMSE 0.07332 | R2 0.6939\n",
      "Epoch 193 | tr_MSE 0.00575 | val_MAE 0.03802 | val_RMSE 0.07330 | R2 0.6941\n",
      "Epoch 194 | tr_MSE 0.00635 | val_MAE 0.03800 | val_RMSE 0.07329 | R2 0.6941\n",
      "Epoch 195 | tr_MSE 0.00651 | val_MAE 0.03799 | val_RMSE 0.07330 | R2 0.6941\n",
      "Epoch 196 | tr_MSE 0.00597 | val_MAE 0.03797 | val_RMSE 0.07330 | R2 0.6940\n",
      "Early stopping.\n",
      "[graphtransformer_density] Best Val — MAE 0.037757 | RMSE 0.072899 | R2 0.6974\n",
      "Epoch 001 | tr_MSE 15.73872 | val_MAE 16.60133 | val_RMSE 17.25141 | R2 -12.5224\n",
      "Epoch 002 | tr_MSE 15.74000 | val_MAE 16.45274 | val_RMSE 17.10927 | R2 -12.3005\n",
      "Epoch 003 | tr_MSE 15.59750 | val_MAE 16.26244 | val_RMSE 16.92699 | R2 -12.0186\n",
      "Epoch 004 | tr_MSE 15.41234 | val_MAE 15.99969 | val_RMSE 16.67607 | R2 -11.6355\n",
      "Epoch 005 | tr_MSE 15.15213 | val_MAE 15.64154 | val_RMSE 16.33571 | R2 -11.1250\n",
      "Epoch 006 | tr_MSE 14.80899 | val_MAE 15.14325 | val_RMSE 15.86546 | R2 -10.4369\n",
      "Epoch 007 | tr_MSE 14.31748 | val_MAE 14.53780 | val_RMSE 15.29895 | R2 -9.6348\n",
      "Epoch 008 | tr_MSE 13.73058 | val_MAE 13.78815 | val_RMSE 14.60415 | R2 -8.6907\n",
      "Epoch 009 | tr_MSE 12.99968 | val_MAE 12.86018 | val_RMSE 13.75300 | R2 -7.5941\n",
      "Epoch 010 | tr_MSE 12.11700 | val_MAE 11.72498 | val_RMSE 12.72455 | R2 -6.3568\n",
      "Epoch 011 | tr_MSE 10.97588 | val_MAE 10.36130 | val_RMSE 11.50969 | R2 -5.0191\n",
      "Epoch 012 | tr_MSE 9.62646 | val_MAE 10.36130 | val_RMSE 11.50969 | R2 -5.0191\n",
      "Epoch 013 | tr_MSE 9.61215 | val_MAE 8.75746 | val_RMSE 10.11887 | R2 -3.6523\n",
      "Epoch 014 | tr_MSE 8.08546 | val_MAE 6.91778 | val_RMSE 8.59271 | R2 -2.3548\n",
      "Epoch 015 | tr_MSE 6.15131 | val_MAE 5.16521 | val_RMSE 7.03733 | R2 -1.2502\n",
      "Epoch 016 | tr_MSE 4.63035 | val_MAE 4.34909 | val_RMSE 5.70243 | R2 -0.4775\n",
      "Epoch 017 | tr_MSE 3.81882 | val_MAE 4.44830 | val_RMSE 5.10678 | R2 -0.1849\n",
      "Epoch 018 | tr_MSE 3.85117 | val_MAE 4.73316 | val_RMSE 5.37636 | R2 -0.3134\n",
      "Epoch 019 | tr_MSE 4.30372 | val_MAE 5.02347 | val_RMSE 5.85624 | R2 -0.5583\n",
      "Epoch 020 | tr_MSE 4.64510 | val_MAE 5.24448 | val_RMSE 6.17807 | R2 -0.7342\n",
      "Epoch 021 | tr_MSE 4.89451 | val_MAE 5.27681 | val_RMSE 6.22672 | R2 -0.7617\n",
      "Epoch 022 | tr_MSE 4.96242 | val_MAE 5.08432 | val_RMSE 6.00767 | R2 -0.6399\n",
      "Epoch 023 | tr_MSE 4.85287 | val_MAE 4.74535 | val_RMSE 5.58853 | R2 -0.4191\n",
      "Epoch 024 | tr_MSE 4.59727 | val_MAE 4.36190 | val_RMSE 5.09042 | R2 -0.1774\n",
      "Epoch 025 | tr_MSE 4.13316 | val_MAE 4.02343 | val_RMSE 4.69946 | R2 -0.0035\n",
      "Epoch 026 | tr_MSE 3.59425 | val_MAE 3.76962 | val_RMSE 4.65956 | R2 0.0135\n",
      "Epoch 027 | tr_MSE 3.41435 | val_MAE 3.77919 | val_RMSE 5.15330 | R2 -0.2066\n",
      "Epoch 028 | tr_MSE 3.54477 | val_MAE 4.11471 | val_RMSE 5.69438 | R2 -0.4733\n",
      "Epoch 029 | tr_MSE 3.67638 | val_MAE 4.38274 | val_RMSE 5.99798 | R2 -0.6346\n",
      "Epoch 030 | tr_MSE 3.81754 | val_MAE 4.40673 | val_RMSE 6.02512 | R2 -0.6494\n",
      "Epoch 031 | tr_MSE 3.90012 | val_MAE 4.22003 | val_RMSE 5.81827 | R2 -0.5381\n",
      "Epoch 032 | tr_MSE 3.79949 | val_MAE 3.93067 | val_RMSE 5.45259 | R2 -0.3509\n",
      "Epoch 033 | tr_MSE 3.56018 | val_MAE 3.72169 | val_RMSE 5.03022 | R2 -0.1497\n",
      "Epoch 034 | tr_MSE 3.34054 | val_MAE 3.70180 | val_RMSE 4.68853 | R2 0.0012\n",
      "Epoch 035 | tr_MSE 3.39574 | val_MAE 3.76018 | val_RMSE 4.58950 | R2 0.0429\n",
      "Epoch 036 | tr_MSE 3.37838 | val_MAE 3.80353 | val_RMSE 4.57613 | R2 0.0485\n",
      "Epoch 037 | tr_MSE 3.38886 | val_MAE 3.79567 | val_RMSE 4.57375 | R2 0.0495\n",
      "Epoch 038 | tr_MSE 3.53121 | val_MAE 3.74448 | val_RMSE 4.58501 | R2 0.0448\n",
      "Epoch 039 | tr_MSE 3.32392 | val_MAE 3.68801 | val_RMSE 4.66177 | R2 0.0126\n",
      "Epoch 040 | tr_MSE 3.25207 | val_MAE 3.66686 | val_RMSE 4.86919 | R2 -0.0773\n",
      "Epoch 041 | tr_MSE 3.29574 | val_MAE 3.70570 | val_RMSE 5.04161 | R2 -0.1549\n",
      "Epoch 042 | tr_MSE 3.23148 | val_MAE 3.72252 | val_RMSE 5.09863 | R2 -0.1812\n",
      "Epoch 043 | tr_MSE 3.40006 | val_MAE 3.69701 | val_RMSE 5.03474 | R2 -0.1518\n",
      "Epoch 044 | tr_MSE 3.52113 | val_MAE 3.65257 | val_RMSE 4.88107 | R2 -0.0825\n",
      "Epoch 045 | tr_MSE 3.17670 | val_MAE 3.64340 | val_RMSE 4.69907 | R2 -0.0033\n",
      "Epoch 046 | tr_MSE 3.38214 | val_MAE 3.67628 | val_RMSE 4.59682 | R2 0.0399\n",
      "Epoch 047 | tr_MSE 3.31489 | val_MAE 3.68476 | val_RMSE 4.57235 | R2 0.0501\n",
      "Epoch 048 | tr_MSE 3.35712 | val_MAE 3.66457 | val_RMSE 4.58650 | R2 0.0442\n",
      "Epoch 049 | tr_MSE 3.26066 | val_MAE 3.61901 | val_RMSE 4.65967 | R2 0.0135\n",
      "Epoch 050 | tr_MSE 3.19118 | val_MAE 3.59903 | val_RMSE 4.78678 | R2 -0.0411\n",
      "Epoch 051 | tr_MSE 3.24641 | val_MAE 3.59804 | val_RMSE 4.85393 | R2 -0.0705\n",
      "Epoch 052 | tr_MSE 3.30600 | val_MAE 3.58540 | val_RMSE 4.82681 | R2 -0.0586\n",
      "Epoch 053 | tr_MSE 3.21717 | val_MAE 3.56746 | val_RMSE 4.72859 | R2 -0.0159\n",
      "Epoch 054 | tr_MSE 3.24848 | val_MAE 3.56863 | val_RMSE 4.64429 | R2 0.0200\n",
      "Epoch 055 | tr_MSE 3.23139 | val_MAE 3.55911 | val_RMSE 4.61879 | R2 0.0307\n",
      "Epoch 056 | tr_MSE 3.29169 | val_MAE 3.53928 | val_RMSE 4.63384 | R2 0.0244\n",
      "Epoch 057 | tr_MSE 3.22625 | val_MAE 3.51399 | val_RMSE 4.73095 | R2 -0.0170\n",
      "Epoch 058 | tr_MSE 3.19884 | val_MAE 3.50524 | val_RMSE 4.77579 | R2 -0.0363\n",
      "Epoch 059 | tr_MSE 3.26945 | val_MAE 3.48357 | val_RMSE 4.72501 | R2 -0.0144\n",
      "Epoch 060 | tr_MSE 3.16550 | val_MAE 3.46888 | val_RMSE 4.61344 | R2 0.0329\n",
      "Epoch 061 | tr_MSE 3.17275 | val_MAE 3.45426 | val_RMSE 4.60040 | R2 0.0384\n",
      "Epoch 062 | tr_MSE 3.12192 | val_MAE 3.43246 | val_RMSE 4.64947 | R2 0.0178\n",
      "Epoch 063 | tr_MSE 3.19361 | val_MAE 3.41981 | val_RMSE 4.66688 | R2 0.0104\n",
      "Epoch 064 | tr_MSE 3.17910 | val_MAE 3.41397 | val_RMSE 4.63290 | R2 0.0248\n",
      "Epoch 065 | tr_MSE 3.15819 | val_MAE 3.40061 | val_RMSE 4.66422 | R2 0.0115\n",
      "Epoch 066 | tr_MSE 3.12488 | val_MAE 3.39119 | val_RMSE 4.65260 | R2 0.0165\n",
      "Epoch 067 | tr_MSE 3.11184 | val_MAE 3.38184 | val_RMSE 4.65543 | R2 0.0153\n",
      "Epoch 068 | tr_MSE 3.13900 | val_MAE 3.37549 | val_RMSE 4.67573 | R2 0.0066\n",
      "Epoch 069 | tr_MSE 3.19970 | val_MAE 3.36425 | val_RMSE 4.68562 | R2 0.0024\n",
      "Epoch 070 | tr_MSE 3.13015 | val_MAE 3.36565 | val_RMSE 4.74113 | R2 -0.0213\n",
      "Epoch 071 | tr_MSE 3.08829 | val_MAE 10.39524 | val_RMSE 14.66243 | R2 -8.7683\n",
      "Epoch 072 | tr_MSE 10.08343 | val_MAE 14.55135 | val_RMSE 17.36652 | R2 -12.7035\n",
      "Epoch 073 | tr_MSE 12.74655 | val_MAE 14.00085 | val_RMSE 16.62677 | R2 -11.5609\n",
      "Epoch 074 | tr_MSE 12.39202 | val_MAE 13.29409 | val_RMSE 15.81902 | R2 -10.3701\n",
      "Epoch 075 | tr_MSE 11.71961 | val_MAE 12.74362 | val_RMSE 15.12539 | R2 -9.3949\n",
      "Epoch 076 | tr_MSE 11.12921 | val_MAE 12.33475 | val_RMSE 14.60804 | R2 -8.6959\n",
      "Epoch 077 | tr_MSE 10.82835 | val_MAE 11.95208 | val_RMSE 14.20935 | R2 -8.1739\n",
      "Epoch 078 | tr_MSE 10.47580 | val_MAE 11.16832 | val_RMSE 13.54727 | R2 -7.3389\n",
      "Epoch 079 | tr_MSE 10.05650 | val_MAE 11.02007 | val_RMSE 13.34634 | R2 -7.0934\n",
      "Epoch 080 | tr_MSE 9.83042 | val_MAE 8.12646 | val_RMSE 10.86966 | R2 -4.3683\n",
      "Epoch 081 | tr_MSE 7.86893 | val_MAE 4.20192 | val_RMSE 5.76320 | R2 -0.5092\n",
      "Epoch 082 | tr_MSE 3.49497 | val_MAE 3.58535 | val_RMSE 4.99207 | R2 -0.1323\n",
      "Epoch 083 | tr_MSE 3.18588 | val_MAE 3.46028 | val_RMSE 4.92551 | R2 -0.1023\n",
      "Epoch 084 | tr_MSE 3.13319 | val_MAE 3.37853 | val_RMSE 4.72501 | R2 -0.0144\n",
      "Epoch 085 | tr_MSE 3.17323 | val_MAE 3.49689 | val_RMSE 4.72732 | R2 -0.0154\n",
      "Epoch 086 | tr_MSE 3.27202 | val_MAE 3.53489 | val_RMSE 4.73758 | R2 -0.0198\n",
      "Epoch 087 | tr_MSE 3.37803 | val_MAE 3.46181 | val_RMSE 4.71612 | R2 -0.0106\n",
      "Epoch 088 | tr_MSE 3.32365 | val_MAE 3.37237 | val_RMSE 4.73511 | R2 -0.0187\n",
      "Epoch 089 | tr_MSE 3.09304 | val_MAE 3.44135 | val_RMSE 4.92602 | R2 -0.1025\n",
      "Early stopping.\n",
      "[gt_rg] Best Val — MAE 3.364249 | RMSE 4.685619 | R2 0.0024\n"
     ]
    }
   ],
   "source": [
    "# infer rdkit_dim from a batch (6 or 15 depending on your LMDB)\n",
    "b = next(iter(train_loader_tg))\n",
    "rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "model_tg = GraphTransformer(d_model=256, nhead=8, nlayers=6,\n",
    "                            dropout=0.2, drop_path=0.1,\n",
    "                            rdkit_dim=rd_dim, use_extra_atom_feats=True,\n",
    "                            activation=\"silu\").to(b.x.device)\n",
    "\n",
    "model_tg, ckpt_tg, metrics_tg = train_hybrid_gnn_sota(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=120, warmup_epochs=5, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_tg\", tag=\"graphtransformer_tg\"\n",
    ")\n",
    "\n",
    "\n",
    "# Density (use your tuned dims if you like larger backbones)\n",
    "model_den = GraphTransformer(d_model=256,nhead=8, nlayers=6,\n",
    "                             dropout=0.2, drop_path=0.1,\n",
    "                             rdkit_dim=rd_dim, use_extra_atom_feats=True,\n",
    "                             activation=\"silu\").to(b.x.device)\n",
    "\n",
    "model_den, ckpt_den, metrics_den = train_hybrid_gnn_sota(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=8, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_density\", tag=\"graphtransformer_density\"\n",
    ")\n",
    "\n",
    "\n",
    "# Rg (your tuned gnn_dim + swish + RMSprop work fine here)\n",
    "model_rg = GraphTransformer(d_model=256, nhead=8, nlayers=6,\n",
    "                            dropout=0.2, drop_path=0.1,\n",
    "                            rdkit_dim=rd_dim, use_extra_atom_feats=True,\n",
    "                            activation=\"silu\").to(b.x.device)\n",
    "\n",
    "model_rg, ckpt_rg, metrics_rg = train_hybrid_gnn_sota(\n",
    "    model_rg, train_loader_rg, val_loader_rg,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=6, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"huber\",  # Huber often helps Rg\n",
    "    save_dir=\"saved_models/gt_rg\", tag=\"gt_rg\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673460",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656cce6e",
   "metadata": {},
   "source": [
    "## Model Performance Summary\n",
    "\n",
    "All baseline models were initially trained and evaluated on a 5,000 molecule subset of the full dataset. Below is a comparison of results across different featurization strategies and model types:\n",
    "\n",
    "### 2D Baseline Models\n",
    "\n",
    "| Model Type    | Featurization      | MAE   | RMSE  | R²    | Notes                                 |\n",
    "| ------------- | ------------------ | ----- | ----- | ----- | ------------------------------------- |\n",
    "| MLP (Tuned)   | RDKit Fingerprints | 0.426 | 0.574 | 0.798 | Strong performance across all metrics |\n",
    "| KRR (Tuned)   | RDKit Fingerprints | 0.454 | 0.593 | 0.784 | Good overall, slightly lower R²       |\n",
    "| RF (Tuned)    | RDKit Fingerprints | 0.423 | 0.583 | 0.791 | Best MAE, very competitive overall    |\n",
    "| MLP (Tuned)   | Coulomb Matrix     | 0.636 | 0.819 | 0.588 | Significantly weaker performance      |\n",
    "| MLP (Untuned) | RDKit Fingerprints | 0.467 | 0.609 | 0.772 | Solid untuned baseline                |\n",
    "| KRR (Untuned) | RDKit Fingerprints | 0.519 | 0.668 | 0.726 | Notable drop from tuned version       |\n",
    "| RF (Untuned)  | RDKit Fingerprints | 0.426 | 0.587 | 0.788 | Surprisingly close to tuned RF        |\n",
    "| MLP (Untuned) | Coulomb Matrix     | 0.663 | 0.847 | 0.559 | Consistently underperforms            |\n",
    "\n",
    "### Graph Neural Network Models (ChemML)\n",
    "\n",
    "| Model Type    | Featurization               | MAE   | RMSE  | R²    | Notes                                |\n",
    "| ------------- | --------------------------- | ----- | ----- | ----- | ------------------------------------ |\n",
    "| GNN (Tuned)   | `tensorise_molecules` Graph | 0.302 | 0.411 | 0.900 | Best results from ChemML experiments |\n",
    "| GNN (Untuned) | `tensorise_molecules` Graph | 0.400 | 0.519 | 0.841 | Strong but less optimized            |\n",
    "\n",
    "### Final Hybrid GNN Model Trained on Full Dataset (OGB-Compatible)\n",
    "\n",
    "| Model Type           | Featurization                          | MAE   | RMSE  | R²    | Notes                              |\n",
    "| -------------------- | -------------------------------------- | ----- | ----- | ----- | ---------------------------------- |\n",
    "| Hybrid GNN (Tuned)   | OGB `smiles2graph` + RDKit descriptors | 0.159 | 0.234 | 0.965 | State-of-the-art level performance |\n",
    "| Hybrid GNN (Untuned) | OGB `smiles2graph` + RDKit descriptors | 0.223 | 0.308 | 0.939 | Still very strong pre-tuning       |\n",
    "\n",
    "---\n",
    "\n",
    "## Model Error Analysis\n",
    "\n",
    "I performed qualitative evaluation by comparing predicted vs. true HOMO–LUMO gaps for both randomly selected and poorly predicted molecules. The worst performing molecules often showed rare or complex structures likely underrepresented in the training set. This highlights the importance of structural diversity and potentially more expressive 3D information to improve generalization.\n",
    "\n",
    "## Next Steps: Integrating 3D Molecular Information\n",
    "\n",
    "To push performance even further and overcome limitations of 2D graphs and hand crafted descriptors, my next step will involve:\n",
    "\n",
    "* Using **3D molecular geometries** \n",
    "* Incorporating **interatomic distances**, angles, and **spatial encoding** (SchNet, DimeNet, or SE(3)-equivariant models)\n",
    "* Comparing results against the current best MAE (\\~0.159)\n",
    "\n",
    "This direction aligns with trends in molecular property prediction where 3D aware models often outperform purely 2D approaches, especially for quantum properties like HOMO–LUMO gaps.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
