{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d12becd",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-09-14T21:52:25.145822Z",
     "iopub.status.busy": "2025-09-14T21:52:25.145522Z",
     "iopub.status.idle": "2025-09-14T21:52:59.578542Z",
     "shell.execute_reply": "2025-09-14T21:52:59.576607Z"
    },
    "papermill": {
     "duration": 34.444509,
     "end_time": "2025-09-14T21:52:59.581226",
     "exception": false,
     "start_time": "2025-09-14T21:52:25.136717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: --quiet, /kaggle/input/autogluon-package\r\n",
      "Processing ./autogluon.tabular-1.3.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy<2.3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular==1.3.1) (1.26.4)\r\n",
      "Requirement already satisfied: scipy<1.16,>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular==1.3.1) (1.15.3)\r\n",
      "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular==1.3.1) (2.2.3)\r\n",
      "\u001b[33mWARNING: Location '--quiet' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/autogluon-package/scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from autogluon.tabular==1.3.1)\r\n",
      "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular==1.3.1) (3.5)\r\n",
      "\u001b[33mWARNING: Location '--quiet' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/autogluon-package/autogluon.core-1.3.1-py3-none-any.whl (from autogluon.tabular==1.3.1)\r\n",
      "\u001b[33mWARNING: Location '--quiet' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/autogluon-package/autogluon.features-1.3.1-py3-none-any.whl (from autogluon.tabular==1.3.1)\r\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.tabular==1.3.1) (4.67.1)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.tabular==1.3.1) (2.32.4)\r\n",
      "Requirement already satisfied: matplotlib<3.11,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.tabular==1.3.1) (3.7.2)\r\n",
      "Requirement already satisfied: boto3<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.tabular==1.3.1) (1.39.1)\r\n",
      "\u001b[33mWARNING: Location '--quiet' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/autogluon-package/autogluon.common-1.3.1-py3-none-any.whl (from autogluon.core==1.3.1->autogluon.tabular==1.3.1)\r\n",
      "Requirement already satisfied: psutil<7.1.0,>=5.7.3 in /usr/local/lib/python3.11/dist-packages (from autogluon.common==1.3.1->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (7.0.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular==1.3.1) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular==1.3.1) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular==1.3.1) (2025.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7.0,>=1.4.0->autogluon.tabular==1.3.1) (1.5.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7.0,>=1.4.0->autogluon.tabular==1.3.1) (3.6.0)\r\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (1.39.1)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (0.13.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (4.58.4)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (1.4.8)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (25.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (11.2.1)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (3.0.9)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=2.0.0->autogluon.tabular==1.3.1) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2024.2.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.3.1->autogluon.tabular==1.3.1) (2025.6.15)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.3.0,>=1.25.0->autogluon.tabular==1.3.1) (2024.2.0)\r\n",
      "Installing collected packages: scikit-learn, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed autogluon.common-1.3.1 autogluon.core-1.3.1 autogluon.features-1.3.1 autogluon.tabular-1.3.1 scikit-learn-1.6.1\r\n"
     ]
    }
   ],
   "source": [
    "!cp -r /kaggle/input/autogluon-package/* /kaggle/working/\n",
    "!pip install -f --quiet --no-index --find-links='/kaggle/input/autogluon-package' 'autogluon.tabular-1.3.1-py3-none-any.whl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce013725",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-09-14T21:52:59.621982Z",
     "iopub.status.busy": "2025-09-14T21:52:59.621617Z",
     "iopub.status.idle": "2025-09-14T21:53:05.777708Z",
     "shell.execute_reply": "2025-09-14T21:53:05.776742Z"
    },
    "papermill": {
     "duration": 6.178229,
     "end_time": "2025-09-14T21:53:05.779262",
     "exception": false,
     "start_time": "2025-09-14T21:52:59.601033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: --quiet, /kaggle/input/scikit-package\r\n",
      "Processing ./scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.5.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.5.2) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.5.2) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.5.2) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.5.2) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn==1.5.2) (2024.2.0)\r\n",
      "Installing collected packages: scikit-learn\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.6.1\r\n",
      "    Uninstalling scikit-learn-1.6.1:\r\n",
      "      Successfully uninstalled scikit-learn-1.6.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.5.2\r\n"
     ]
    }
   ],
   "source": [
    "!cp -r /kaggle/input/scikit-package/* /kaggle/working/\n",
    "!pip install -f --quiet --no-index --find-links='/kaggle/input/scikit-package' 'scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea1c8a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T21:53:05.797605Z",
     "iopub.status.busy": "2025-09-14T21:53:05.797148Z",
     "iopub.status.idle": "2025-09-14T21:53:10.929862Z",
     "shell.execute_reply": "2025-09-14T21:53:10.929102Z"
    },
    "papermill": {
     "duration": 5.143599,
     "end_time": "2025-09-14T21:53:10.931282",
     "exception": false,
     "start_time": "2025-09-14T21:53:05.787683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3a29a6",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-09-14T21:53:10.953593Z",
     "iopub.status.busy": "2025-09-14T21:53:10.953223Z",
     "iopub.status.idle": "2025-09-14T21:53:15.763675Z",
     "shell.execute_reply": "2025-09-14T21:53:15.762919Z"
    },
    "papermill": {
     "duration": 4.825345,
     "end_time": "2025-09-14T21:53:15.765213",
     "exception": false,
     "start_time": "2025-09-14T21:53:10.939868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\r\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.2.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\r\n",
      "Installing collected packages: rdkit\r\n",
      "Successfully installed rdkit-2025.3.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72cfc02a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T21:53:15.782014Z",
     "iopub.status.busy": "2025-09-14T21:53:15.781681Z",
     "iopub.status.idle": "2025-09-14T21:53:20.995093Z",
     "shell.execute_reply": "2025-09-14T21:53:20.994321Z"
    },
    "papermill": {
     "duration": 5.223091,
     "end_time": "2025-09-14T21:53:20.996509",
     "exception": false,
     "start_time": "2025-09-14T21:53:15.773418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/input/mordred-1-2-0-py3-none-any/\r\n",
      "Processing /kaggle/input/mordred-1-2-0-py3-none-any/mordred-1.2.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: six==1.* in /usr/local/lib/python3.11/dist-packages (from mordred) (1.17.0)\r\n",
      "Requirement already satisfied: numpy==1.* in /usr/local/lib/python3.11/dist-packages (from mordred) (1.26.4)\r\n",
      "Processing /kaggle/input/mordred-1-2-0-py3-none-any/networkx-2.8.8-py3-none-any.whl (from mordred)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy==1.*->mordred) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy==1.*->mordred) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy==1.*->mordred) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy==1.*->mordred) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy==1.*->mordred) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy==1.*->mordred) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.*->mordred) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.*->mordred) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy==1.*->mordred) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy==1.*->mordred) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy==1.*->mordred) (2024.2.0)\r\n",
      "Installing collected packages: networkx, mordred\r\n",
      "  Attempting uninstall: networkx\r\n",
      "    Found existing installation: networkx 3.5\r\n",
      "    Uninstalling networkx-3.5:\r\n",
      "      Successfully uninstalled networkx-3.5\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "autogluon-core 1.3.1 requires networkx<4,>=3.0, but you have networkx 2.8.8 which is incompatible.\r\n",
      "autogluon-tabular 1.3.1 requires networkx<4,>=3.0, but you have networkx 2.8.8 which is incompatible.\r\n",
      "scikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\r\n",
      "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed mordred-1.2.0 networkx-2.8.8\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mordred --no-index --find-links=file:///kaggle/input/mordred-1-2-0-py3-none-any/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57220825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T21:53:21.014417Z",
     "iopub.status.busy": "2025-09-14T21:53:21.014177Z",
     "iopub.status.idle": "2025-09-14T21:53:21.899696Z",
     "shell.execute_reply": "2025-09-14T21:53:21.898602Z"
    },
    "papermill": {
     "duration": 0.89627,
     "end_time": "2025-09-14T21:53:21.901342",
     "exception": false,
     "start_time": "2025-09-14T21:53:21.005072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f9ce56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T21:53:21.918877Z",
     "iopub.status.busy": "2025-09-14T21:53:21.918594Z",
     "iopub.status.idle": "2025-09-14T21:53:21.922779Z",
     "shell.execute_reply": "2025-09-14T21:53:21.922073Z"
    },
    "papermill": {
     "duration": 0.01411,
     "end_time": "2025-09-14T21:53:21.924047",
     "exception": false,
     "start_time": "2025-09-14T21:53:21.909937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5921016f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T21:53:21.941621Z",
     "iopub.status.busy": "2025-09-14T21:53:21.941424Z",
     "iopub.status.idle": "2025-09-14T21:53:21.944655Z",
     "shell.execute_reply": "2025-09-14T21:53:21.944130Z"
    },
    "papermill": {
     "duration": 0.012713,
     "end_time": "2025-09-14T21:53:21.945744",
     "exception": false,
     "start_time": "2025-09-14T21:53:21.933031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dfs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d503d",
   "metadata": {
    "papermill": {
     "duration": 0.007651,
     "end_time": "2025-09-14T21:53:21.961295",
     "exception": false,
     "start_time": "2025-09-14T21:53:21.953644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f50360e",
   "metadata": {
    "papermill": {
     "duration": 0.007471,
     "end_time": "2025-09-14T21:53:21.976394",
     "exception": false,
     "start_time": "2025-09-14T21:53:21.968923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Notebook Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1db05",
   "metadata": {
    "papermill": {
     "duration": 0.007538,
     "end_time": "2025-09-14T21:53:21.991579",
     "exception": false,
     "start_time": "2025-09-14T21:53:21.984041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook implements a modular and robust machine learning pipeline for polymer property prediction. Key features include:\n",
    "\n",
    "- Config-driven pipeline: All model and preprocessing parameters are controlled via a config object for reproducibility and flexibility.\n",
    "- Data preparation: SMILES strings are canonicalized, deduplicated, and optionally augmented. Features are generated using molecular fingerprints and descriptors.\n",
    "- Feature engineering: Includes robust feature selection, correlation pruning, variance thresholding, and optional PCA.\n",
    "- Modeling: Supports stacking with XGBoost, LightGBM, and CatBoost as base models, and XGBoost as the meta-model. Neural network and traditional ML models are also supported.\n",
    "- Cross-validation: Stratified and standard CV splits are available, with holdout sets for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a58cf51f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T21:53:22.015238Z",
     "iopub.status.busy": "2025-09-14T21:53:22.015031Z",
     "iopub.status.idle": "2025-09-14T21:53:54.698691Z",
     "shell.execute_reply": "2025-09-14T21:53:54.697719Z"
    },
    "papermill": {
     "duration": 32.700448,
     "end_time": "2025-09-14T21:53:54.699887",
     "exception": false,
     "start_time": "2025-09-14T21:53:21.999439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading competition data...\n",
      "Training data shape: (7973, 7), Test data shape: (3, 2)\n",
      "Cleaning and validating SMILES...\n",
      "   Removed 0 invalid SMILES from training data\n",
      "   Removed 0 invalid SMILES from test data\n",
      "   Final training samples: 7973\n",
      "   Final test samples: 3\n",
      "\n",
      "üìÇ Loading external datasets...\n",
      "   ‚úÖ Tc data: 874 samples\n",
      "   ‚ö†Ô∏è TgSS enriched data failed: [Errno 2] No such file or directory: '/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleane\n",
      "   ‚úÖ JCIM Tg data: 662 samples\n",
      "   ‚úÖ Xlsx Tg data: 501 samples\n",
      "   ‚úÖ Density data: 786 samples\n",
      "   ‚úÖ dataset 4: 862 samples\n",
      "\n",
      "üîÑ Integrating external data...\n",
      "   Processing Tc data...\n",
      "      Processing 874 Tc samples...\n",
      "      Kept 874/874 valid samples\n",
      "      Tc: +129 samples, +129 unique SMILES\n",
      "      Filled 0 missing entries in train for Tc\n",
      "      Added 129 new entries for Tc\n",
      "   Processing Tg data...\n",
      "      Processing 662 Tg samples...\n",
      "      Kept 662/662 valid samples\n",
      "      Tg: +151 samples, +136 unique SMILES\n",
      "      Filled 15 missing entries in train for Tg\n",
      "      Added 136 new entries for Tg\n",
      "   Processing Tg data...\n",
      "      Processing 501 Tg samples...\n",
      "      Kept 501/501 valid samples\n",
      "      Tg: +499 samples, +499 unique SMILES\n",
      "      Filled 0 missing entries in train for Tg\n",
      "      Added 499 new entries for Tg\n",
      "   Processing Density data...\n",
      "      Processing 786 Density samples...\n",
      "      Kept 780/786 valid samples\n",
      "      Density: +634 samples, +524 unique SMILES\n",
      "      Filled 110 missing entries in train for Density\n",
      "      Added 524 new entries for Density\n",
      "   Processing FFV data...\n",
      "      Processing 862 FFV samples...\n",
      "      Kept 862/862 valid samples\n",
      "      FFV: +862 samples, +819 unique SMILES\n",
      "      Filled 43 missing entries in train for FFV\n",
      "      Added 819 new entries for FFV\n",
      "\n",
      "üìä Final training data:\n",
      "   Original samples: 7973\n",
      "   Extended samples: 10080\n",
      "   Gain: +2107 samples\n",
      "   Tg: 1,161 samples (+650)\n",
      "   FFV: 7,892 samples (+862)\n",
      "   Tc: 866 samples (+129)\n",
      "   Density: 1,247 samples (+634)\n",
      "   Rg: 614 samples (+0)\n",
      "\n",
      "‚úÖ Data integration complete with clean SMILES!\n",
      "Tg NaNs per column:\n",
      "SMILES    0\n",
      "Tg        0\n",
      "dtype: int64\n",
      "(1161, 2)\n",
      "----------------------------------------\n",
      "FFV NaNs per column:\n",
      "SMILES    0\n",
      "FFV       0\n",
      "dtype: int64\n",
      "(7892, 2)\n",
      "----------------------------------------\n",
      "Tc NaNs per column:\n",
      "SMILES    0\n",
      "Tc        0\n",
      "dtype: int64\n",
      "(866, 2)\n",
      "----------------------------------------\n",
      "Density NaNs per column:\n",
      "SMILES     0\n",
      "Density    0\n",
      "dtype: int64\n",
      "(1247, 2)\n",
      "----------------------------------------\n",
      "Rg NaNs per column:\n",
      "SMILES    0\n",
      "Rg        0\n",
      "dtype: int64\n",
      "(614, 2)\n",
      "----------------------------------------\n",
      "\n",
      "=== Loading models and data for label: Tg ===\n",
      "Loaded 10 models for label 'Tg'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping correlated columns from test: ['ATS3d', 'ATS5d', 'ATS1Z', 'Xp-2d', 'ETA_eta_L', 'ETA_eta_FL', 'IC4']\n",
      "Model 1 holdout MAE: 15.50679819998558\n",
      "Model 2 holdout MAE: 15.132775055537877\n",
      "Model 3 holdout MAE: 14.746208292911291\n",
      "Model 4 holdout MAE: 18.796538719059015\n",
      "Model 5 holdout MAE: 16.259336387111773\n",
      "Model 6 holdout MAE: 17.838669646433686\n",
      "Model 7 holdout MAE: 18.303901659413537\n",
      "Model 8 holdout MAE: 17.322172298437305\n",
      "Model 9 holdout MAE: 15.448651658787695\n",
      "Model 10 holdout MAE: 15.791739486828485\n",
      "Tg Holdout MAE (mean ¬± std over all models): 16.51468 ¬± 1.36347\n",
      "[167.22568  171.49619  112.769226]\n",
      "\n",
      "=== Loading models and data for label: FFV ===\n",
      "Loaded 10 models for label 'FFV'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping correlated columns from test: ['SpDiam_A', 'VR2_A', 'ATS6d', 'ATS8Z', 'AATS2dv', 'AATS1d', 'Xpc-5d', 'Xpc-6d', 'Xp-4d', 'AXp-0d', 'SsCH3', 'StsC', 'SaaaC', 'SssNH', 'SaaN', 'SaasN', 'SssO', 'AETA_beta_ns', 'AETA_dBeta', 'SIC5', 'BIC0', 'CIC2', 'CIC4', 'CIC5', 'ZMIC4', 'ZMIC5', 'MID_h', 'MID_N', 'MID_O', 'MPC9', 'piPC8', 'piPC10', 'TpiPC10', 'TopoPSA', 'GGI2', 'GGI3', 'GGI9', 'AMW']\n",
      "Model 1 holdout MAE: 0.003181394273829\n",
      "Model 2 holdout MAE: 0.0030051946982283034\n",
      "Model 3 holdout MAE: 0.002603549133033655\n",
      "Model 4 holdout MAE: 0.0028116787887928563\n",
      "Model 5 holdout MAE: 0.003015804047563911\n",
      "Model 6 holdout MAE: 0.002573779564237642\n",
      "Model 7 holdout MAE: 0.0025629417617373776\n",
      "Model 8 holdout MAE: 0.0025816899973995187\n",
      "Model 9 holdout MAE: 0.002434586068605529\n",
      "Model 10 holdout MAE: 0.003021486554341292\n",
      "FFV Holdout MAE (mean ¬± std over all models): 0.00278 ¬± 0.00025\n",
      "[0.3750729  0.37614182 0.35092032]\n",
      "\n",
      "=== Loading models and data for label: Tc ===\n",
      "Loaded 10 models for label 'Tc'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping correlated columns from test: ['ATS2dv', 'AATS1d', 'MZ', 'AETA_beta_ns', 'CIC5', 'VSA_EState7']\n",
      "Model 1 holdout MAE: 0.029072056111796144\n",
      "Model 2 holdout MAE: 0.02525132977473781\n",
      "Model 3 holdout MAE: 0.03151671406674659\n",
      "Model 4 holdout MAE: 0.029781155687776107\n",
      "Model 5 holdout MAE: 0.03319188211964465\n",
      "Model 6 holdout MAE: 0.031186267680683357\n",
      "Model 7 holdout MAE: 0.0287859162002231\n",
      "Model 8 holdout MAE: 0.034352132459344535\n",
      "Model 9 holdout MAE: 0.029098924963090616\n",
      "Model 10 holdout MAE: 0.032117810254672474\n",
      "Tc Holdout MAE (mean ¬± std over all models): 0.03044 ¬± 0.00247\n",
      "[0.18172295 0.24017832 0.2527067 ]\n",
      "\n",
      "=== Loading models and data for label: Density ===\n",
      "Loaded 10 models for label 'Density'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping correlated columns from test: ['ATS5d', 'ATS7d', 'ATS7Z', 'AATS2dv', 'nBondsS', 'nBondsKS', 'FCSP3', 'MZ', 'SsNH2', 'TIC2', 'TIC3', 'TIC5', 'BIC0', 'SlogP_VSA6', 'MPC8', 'SMR', 'AMW']\n",
      "Model 1 holdout MAE: 0.015373099922109373\n",
      "Model 2 holdout MAE: 0.016884704879880856\n",
      "Model 3 holdout MAE: 0.02180963844608985\n",
      "Model 4 holdout MAE: 0.02268799879478906\n",
      "Model 5 holdout MAE: 0.012700415204003907\n",
      "Model 6 holdout MAE: 0.018609349198783688\n",
      "Model 7 holdout MAE: 0.01659094873413842\n",
      "Model 8 holdout MAE: 0.016453451721263668\n",
      "Model 9 holdout MAE: 0.013605503480093265\n",
      "Model 10 holdout MAE: 0.01586205507145117\n",
      "Density Holdout MAE (mean ¬± std over all models): 0.01706 ¬± 0.00304\n",
      "[1.1703981 1.0836802 1.113959 ]\n",
      "\n",
      "=== Loading models and data for label: Rg ===\n",
      "Loaded 10 models for label 'Rg'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping correlated columns from test: ['ATS6d', 'nBondsKS', 'Xp-1d', 'SsCH3', 'ZMIC1']\n",
      "Model 1 holdout MAE: 0.854962744812051\n",
      "Model 2 holdout MAE: 0.6686672602067715\n",
      "Model 3 holdout MAE: 0.7273912048275264\n",
      "Model 4 holdout MAE: 0.9728035707123394\n",
      "Model 5 holdout MAE: 0.7460436389433437\n",
      "Model 6 holdout MAE: 0.7648310415865206\n",
      "Model 7 holdout MAE: 0.7340207549049034\n",
      "Model 8 holdout MAE: 0.6858005633369298\n",
      "Model 9 holdout MAE: 0.5996891318617299\n",
      "Model 10 holdout MAE: 0.7776748795477888\n",
      "Rg Holdout MAE (mean ¬± std over all models): 0.75319 ¬± 0.09780\n",
      "[20.529217 19.965155 21.623934]\n",
      "\n",
      "Mean Absolute Error for each label:\n",
      "     label fold_mae_mean fold_mae_std  holdout_mae_mean  holdout_mae_std\n",
      "0       Tg          None         None         16.514679         1.363467\n",
      "1      FFV          None         None          0.002779         0.000246\n",
      "2       Tc          None         None          0.030435         0.002473\n",
      "3  Density          None         None          0.017058         0.003041\n",
      "4       Rg          None         None          0.753188         0.097803\n",
      "           id          Tg       FFV        Tc   Density         Rg\n",
      "0  1109053969  167.225677  0.375073  0.181723  1.170398  20.529217\n",
      "1  1422188626  171.496185  0.376142  0.240178  1.083680  19.965155\n",
      "2  2032016830  112.769226  0.350920  0.252707  1.113959  21.623934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import hashlib\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, StackingRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, MACCSkeys, rdmolops, Lipinski, Crippen\n",
    "from rdkit.Chem.rdMolDescriptors import CalcNumRotatableBonds\n",
    "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator\n",
    "\n",
    "from mordred import Calculator, descriptors as mordred_descriptors\n",
    "\n",
    "import shap\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# torchinfo is optional, only used in show_model_summary\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except ImportError:\n",
    "    summary = None\n",
    "\n",
    "# Data paths\n",
    "RDKIT_AVAILABLE = True\n",
    "TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "# Create the NeurIPS directory if it does not exist\n",
    "os.makedirs(\"NeurIPS\", exist_ok=True)\n",
    "class Config:\n",
    "    useAllDataForTraining = False\n",
    "    use_standard_scaler = True  # Set to True to use StandardScaler, False to skip scaling\n",
    "    # Set to True to calculate Mordred descriptors in featurization\n",
    "    use_least_important_features_all_methods = True  # Set to True to call get_least_important_features_all_methods\n",
    "    use_variance_threshold = False  # Set to True to enable VarianceThreshold feature selection\n",
    "    enable_param_tuning = False  # Set to True to enable XGB hyperparameter tuning\n",
    "    debug = False\n",
    "\n",
    "    use_descriptors = False  # Set to True to include RDKit descriptors, False to skip\n",
    "    use_mordred = True\n",
    "    # Control inclusion of AtomPair and TopologicalTorsion fingerprints\n",
    "    use_maccs_fp = False\n",
    "    use_morgan_fp = False\n",
    "    use_atom_pair_fp = False\n",
    "    use_torsion_fp = False\n",
    "    use_chemberta = False\n",
    "    chemberta_pooling = 'max'  # can be 'mean', 'max', 'cls', or 'pooler'\n",
    "    # Include MACCS keys fingerprint\n",
    "\n",
    "    search_nn = False\n",
    "    use_stacking = False\n",
    "    model_name = 'xgb'  \n",
    "    # Options: ['autogluon', xgb', 'catboost', 'lgbm', 'extratrees', 'randomforest', 'tabnet', 'hgbm', 'nn']\n",
    "\n",
    "    # Don't change\n",
    "    # Choose importance method: 'feature_importances_' or 'permutation_importance'\n",
    "    feature_importance_method = 'permutation_importance'\n",
    "    use_cross_validation = True  # Set to False to use a single split for speed\n",
    "    use_pca = False  # Set to True to enable PCA\n",
    "    pca_variance = 0.9999  # Fraction of variance to keep in PCA\n",
    "    use_external_data = True  # Set to True to use external datasets\n",
    "    use_augmentation = False  # Set to True to use augment_dataset\n",
    "    add_gaussian = False  # Set to True to enable Gaussian Mixture Model-based augmentation\n",
    "    random_state = 42\n",
    "\n",
    "\n",
    "    # Number of least important features to drop\n",
    "    # Use a nested dictionary for model- and label-specific n_least_important_features\n",
    "    n_least_important_features = {\n",
    "        'xgb':     {'Tg': 20, 'FFV': 20, 'Tc': 22, 'Density': 19, 'Rg': 19},\n",
    "        'catboost':{'Tg': 15, 'FFV': 15, 'Tc': 18, 'Density': 15, 'Rg': 15},\n",
    "        'lgbm':    {'Tg': 18, 'FFV': 18, 'Tc': 20, 'Density': 17, 'Rg': 17},\n",
    "        'extratrees':{'Tg': 22, 'FFV': 15, 'Tc': 10, 'Density': 25, 'Rg': 5},\n",
    "        'randomforest':{'Tg': 21, 'FFV': 19, 'Tc': 21, 'Density': 18, 'Rg': 18},\n",
    "        'balancedrf':{'Tg': 20, 'FFV': 20, 'Tc': 20, 'Density': 20, 'Rg': 20},\n",
    "    }\n",
    "\n",
    "    # Path for permutation importance log file\n",
    "    permutation_importance_log_path = \"log/permutation_importance_log.xlsx\"\n",
    "\n",
    "    correlation_threshold_value = 0.96\n",
    "    correlation_thresholds = {\n",
    "        \"Tg\": correlation_threshold_value,\n",
    "        \"FFV\": correlation_threshold_value,\n",
    "        \"Tc\": correlation_threshold_value,\n",
    "        \"Density\": correlation_threshold_value,\n",
    "        \"Rg\": correlation_threshold_value\n",
    "    }\n",
    "\n",
    "# Create a single config instance to use everywhere\n",
    "config = Config()\n",
    "\n",
    "if config.debug or config.search_nn:\n",
    "    config.use_cross_validation = False\n",
    "\n",
    "# --- XGB Hyperparameter Tuning DB Utilities ---\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "def init_chemberta():\n",
    "    model_name = \"/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM\"\n",
    "    chemberta_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    chemberta_model = AutoModel.from_pretrained(model_name)\n",
    "    chemberta_model.eval()\n",
    "    return chemberta_tokenizer, chemberta_model\n",
    "\n",
    "def get_chemberta_embedding(smiles, embedding_dim=384):\n",
    "    \"\"\"\n",
    "    Returns ChemBERTa embedding for a single SMILES string.\n",
    "    Pads/truncates to embedding_dim if needed.\n",
    "    \"\"\"\n",
    "    if smiles is None or not isinstance(smiles, str) or len(smiles) == 0:\n",
    "        return np.zeros(embedding_dim)\n",
    "    try:\n",
    "        # Add pooling argument with default 'mean'\n",
    "        pooling = getattr(config, 'chemberta_pooling', 'mean')  # can be 'mean', 'max', 'cls', 'pooler'\n",
    "        chemberta_tokenizer, chemberta_model = init_chemberta()\n",
    "        inputs = chemberta_tokenizer([smiles], padding=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = chemberta_model(**inputs)\n",
    "            if pooling == 'pooler' and hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "                emb = outputs.pooler_output.squeeze(0)\n",
    "            elif pooling == 'cls' and hasattr(outputs, 'last_hidden_state'):\n",
    "                emb = outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
    "            elif pooling == 'max' and hasattr(outputs, 'last_hidden_state'):\n",
    "                emb = outputs.last_hidden_state.max(dim=1).values.squeeze(0)\n",
    "            elif pooling == 'mean' and hasattr(outputs, 'last_hidden_state'):\n",
    "                emb = outputs.last_hidden_state.mean(dim=1).squeeze(0)\n",
    "            else:\n",
    "                raise ValueError(\"Cannot extract embedding from model output\")\n",
    "            emb_np = emb.cpu().numpy()\n",
    "            # Pad or truncate if needed\n",
    "            if emb_np.shape[0] < embedding_dim:\n",
    "                emb_np = np.pad(emb_np, (0, embedding_dim - emb_np.shape[0]))\n",
    "            elif emb_np.shape[0] > embedding_dim:\n",
    "                emb_np = emb_np[:embedding_dim]\n",
    "            return emb_np\n",
    "    except Exception as e:\n",
    "        print(f\"ChemBERTa embedding failed for SMILES '{smiles}': {e}\")\n",
    "        return np.zeros(embedding_dim)\n",
    "    \n",
    "def init_xgb_tuning_db(db_path=\"xgb_tuning.db\"):\n",
    "    \"\"\"Initialize the XGB tuning database and return all existing results.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS xgb_tuning\n",
    "                 (param_hash TEXT PRIMARY KEY, params TEXT, score REAL)''')\n",
    "    c.execute('SELECT params, score FROM xgb_tuning')\n",
    "    results = c.fetchall()\n",
    "    conn.close()\n",
    "    return [(json.loads(params), score) for params, score in results]\n",
    "\n",
    "def get_param_hash(params):\n",
    "    param_str = json.dumps(params, sort_keys=True)\n",
    "    return hashlib.md5(param_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "def check_db_for_params(db_path, param_hash):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT score FROM xgb_tuning WHERE param_hash=?', (param_hash,))\n",
    "    result = c.fetchone()\n",
    "    conn.close()\n",
    "    return result is not None\n",
    "\n",
    "def save_result_to_db(db_path, param_hash, params, score):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''INSERT OR REPLACE INTO xgb_tuning (param_hash, params, score)\n",
    "                 VALUES (?, ?, ?)''', (param_hash, json.dumps(params, sort_keys=True), score))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# --- XGB Hyperparameter Grid Search with DB Caching ---\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def xgb_grid_search_with_db(X, y, param_grid, db_path=\"xgb_tuning.db\"):\n",
    "    \"\"\"\n",
    "    For each param set in grid, check DB. If not present, train and save result.\n",
    "    param_grid: dict of param lists, e.g. {'max_depth':[3,5], 'learning_rate':[0.01,0.1]}\n",
    "    \"\"\"\n",
    "    tried = 0\n",
    "    best_score = None\n",
    "    best_params = None\n",
    "    # Split X, y into train/val for early stopping\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        param_hash = get_param_hash(params)\n",
    "        if check_db_for_params(db_path, param_hash):\n",
    "            print(f\"Skipping already tried params: {params}\")\n",
    "            continue\n",
    "        # print(f\"Trying params: {json.dumps(params, sort_keys=True)}\")\n",
    "        model = XGBRegressor(**params)\n",
    "        # Provide eval_set for early stopping\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        y_pred = model.predict(X_val)\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        score = mean_absolute_error(y_val, y_pred)\n",
    "        print(f\"Result: MAE={score:.6f} for params: {json.dumps(params, sort_keys=True)}\")\n",
    "        # For MAE, lower is better\n",
    "        if (best_score is None) or (score < best_score):\n",
    "            best_score = score\n",
    "            best_params = params.copy()\n",
    "            print(f\"New best MAE: {best_score:.6f} with params: {json.dumps(best_params, sort_keys=True)}\")\n",
    "        save_result_to_db(db_path, param_hash, params, score)\n",
    "        tried += 1\n",
    "    print(f\"Tried {tried} new parameter sets.\")\n",
    "    if best_score is not None:\n",
    "        print(f\"Best score overall: {best_score:.6f} with params: {json.dumps(best_params, sort_keys=True)}\")\n",
    "\n",
    "from sklearn.linear_model import RidgeCV, ElasticNetCV\n",
    "\n",
    "def drop_correlated_features(df, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Drops columns in a DataFrame that are highly correlated with other columns.\n",
    "    Only one of each pair of correlated columns is kept.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        threshold (float): Correlation threshold for dropping columns (default 0.95).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with correlated columns dropped.\n",
    "        list: List of dropped column names.\n",
    "    \"\"\"\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    return df.drop(columns=to_drop), to_drop\n",
    "\n",
    "def get_canonical_smiles(smiles):\n",
    "        \"\"\"Convert SMILES to canonical form for consistency\"\"\"\n",
    "        if not RDKIT_AVAILABLE:\n",
    "            return smiles\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "        except:\n",
    "            pass\n",
    "        return smiles\n",
    "\n",
    "\"\"\"\n",
    "Load competition data with complete filtering of problematic polymer notation\n",
    "\"\"\"\n",
    "\n",
    "print(\"Loading competition data...\")\n",
    "train = pd.read_csv(BASE_PATH + 'train.csv')\n",
    "test = pd.read_csv(BASE_PATH + 'test.csv')\n",
    "\n",
    "if config.debug:\n",
    "    print(\"   Debug mode: sampling 1000 training examples\")\n",
    "    train = train.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Training data shape: {train.shape}, Test data shape: {test.shape}\")\n",
    "\n",
    "def clean_and_validate_smiles(smiles):\n",
    "    \"\"\"Completely clean and validate SMILES, removing all problematic patterns\"\"\"\n",
    "    if not isinstance(smiles, str) or len(smiles) == 0:\n",
    "        return None\n",
    "    \n",
    "    # List of all problematic patterns we've seen\n",
    "    bad_patterns = [\n",
    "        '[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]', \n",
    "        \"[R']\", '[R\"]', 'R1', 'R2', 'R3', 'R4', 'R5',\n",
    "        # Additional patterns that cause issues\n",
    "        '([R])', '([R1])', '([R2])', \n",
    "    ]\n",
    "    \n",
    "    # Check for any bad patterns\n",
    "    for pattern in bad_patterns:\n",
    "        if pattern in smiles:\n",
    "            return None\n",
    "    \n",
    "    # Additional check: if it contains ] followed by [ without valid atoms, likely polymer notation\n",
    "    if '][' in smiles and any(x in smiles for x in ['[R', 'R]']):\n",
    "        return None\n",
    "    \n",
    "    # Try to parse with RDKit if available\n",
    "    if RDKIT_AVAILABLE:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # If RDKit not available, return cleaned SMILES\n",
    "    return smiles\n",
    "\n",
    "# Clean and validate all SMILES\n",
    "print(\"Cleaning and validating SMILES...\")\n",
    "train['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)\n",
    "test['SMILES'] = test['SMILES'].apply(clean_and_validate_smiles)\n",
    "\n",
    "# Remove invalid SMILES\n",
    "invalid_train = train['SMILES'].isnull().sum()\n",
    "invalid_test = test['SMILES'].isnull().sum()\n",
    "\n",
    "print(f\"   Removed {invalid_train} invalid SMILES from training data\")\n",
    "print(f\"   Removed {invalid_test} invalid SMILES from test data\")\n",
    "\n",
    "train = train[train['SMILES'].notnull()].reset_index(drop=True)\n",
    "test = test[test['SMILES'].notnull()].reset_index(drop=True)\n",
    "\n",
    "print(f\"   Final training samples: {len(train)}\")\n",
    "print(f\"   Final test samples: {len(test)}\")\n",
    "\n",
    "def add_extra_data_clean(df_train, df_extra, target):\n",
    "    \"\"\"Add external data with thorough SMILES cleaning\"\"\"\n",
    "    n_samples_before = len(df_train[df_train[target].notnull()])\n",
    "    \n",
    "    print(f\"      Processing {len(df_extra)} {target} samples...\")\n",
    "    \n",
    "    # Clean external SMILES\n",
    "    df_extra['SMILES'] = df_extra['SMILES'].apply(clean_and_validate_smiles)\n",
    "    \n",
    "    # Remove invalid SMILES and missing targets\n",
    "    before_filter = len(df_extra)\n",
    "    df_extra = df_extra[df_extra['SMILES'].notnull()]\n",
    "    df_extra = df_extra.dropna(subset=[target])\n",
    "    after_filter = len(df_extra)\n",
    "    \n",
    "    print(f\"      Kept {after_filter}/{before_filter} valid samples\")\n",
    "    \n",
    "    if len(df_extra) == 0:\n",
    "        print(f\"      No valid data remaining for {target}\")\n",
    "        return df_train\n",
    "    \n",
    "    # Group by canonical SMILES and average duplicates\n",
    "    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n",
    "    \n",
    "    cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n",
    "    unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n",
    "\n",
    "    # Fill missing values\n",
    "    filled_count = 0\n",
    "    for smile in df_train[df_train[target].isnull()]['SMILES'].tolist():\n",
    "        if smile in cross_smiles:\n",
    "            df_train.loc[df_train['SMILES']==smile, target] = \\\n",
    "                df_extra[df_extra['SMILES']==smile][target].values[0]\n",
    "            filled_count += 1\n",
    "    \n",
    "    # Add unique SMILES\n",
    "    extra_to_add = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)].copy()\n",
    "    if len(extra_to_add) > 0:\n",
    "        for col in TARGETS:\n",
    "            if col not in extra_to_add.columns:\n",
    "                extra_to_add[col] = np.nan\n",
    "        \n",
    "        extra_to_add = extra_to_add[['SMILES'] + TARGETS]\n",
    "        df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)\n",
    "\n",
    "    n_samples_after = len(df_train[df_train[target].notnull()])\n",
    "    print(f'      {target}: +{n_samples_after-n_samples_before} samples, +{len(unique_smiles_extra)} unique SMILES')\n",
    "    print(f\"      Filled {filled_count} missing entries in train for {target}\")\n",
    "    print(f\"      Added {len(extra_to_add)} new entries for {target}\")\n",
    "    return df_train\n",
    "\n",
    "# Load external datasets with robust error handling\n",
    "print(\"\\nüìÇ Loading external datasets...\")\n",
    "\n",
    "external_datasets = []\n",
    "\n",
    "# Function to safely load datasets\n",
    "def safe_load_dataset(path, target, processor_func, description):\n",
    "    try:\n",
    "        if path.endswith('.xlsx'):\n",
    "            data = pd.read_excel(path)\n",
    "        else:\n",
    "            data = pd.read_csv(path)\n",
    "        \n",
    "        data = processor_func(data)\n",
    "        external_datasets.append((target, data))\n",
    "        print(f\"   ‚úÖ {description}: {len(data)} samples\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è {description} failed: {str(e)[:100]}\")\n",
    "        return False\n",
    "\n",
    "# Load each dataset\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/tc-smiles/Tc_SMILES.csv',\n",
    "    'Tc',\n",
    "    lambda df: df.rename(columns={'TC_mean': 'Tc'}),\n",
    "    'Tc data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv',\n",
    "    'Tg', \n",
    "    lambda df: df[['SMILES', 'Tg']] if 'Tg' in df.columns else df,\n",
    "    'TgSS enriched data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv',\n",
    "    'Tg',\n",
    "    lambda df: df[['SMILES', 'Tg (C)']].rename(columns={'Tg (C)': 'Tg'}),\n",
    "    'JCIM Tg data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/smiles-extra-data/data_tg3.xlsx',\n",
    "    'Tg',\n",
    "    lambda df: df.rename(columns={'Tg [K]': 'Tg'}).assign(Tg=lambda x: x['Tg'] - 273.15),\n",
    "    'Xlsx Tg data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/smiles-extra-data/data_dnst1.xlsx',\n",
    "    'Density',\n",
    "    lambda df: df.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\n",
    "                .query('SMILES.notnull() and Density.notnull() and Density != \"nylon\"')\n",
    "                .assign(Density=lambda x: x['Density'].astype(float) - 0.118),\n",
    "    'Density data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    BASE_PATH + 'train_supplement/dataset4.csv',\n",
    "    'FFV', \n",
    "    lambda df: df[['SMILES', 'FFV']] if 'FFV' in df.columns else df,\n",
    "    'dataset 4'\n",
    ")\n",
    "\n",
    "# Integrate external data\n",
    "print(\"\\nüîÑ Integrating external data...\")\n",
    "train_extended = train[['SMILES'] + TARGETS].copy()\n",
    "\n",
    "if getattr(config, \"use_external_data\", True) and  not config.debug:\n",
    "    for target, dataset in external_datasets:\n",
    "        print(f\"   Processing {target} data...\")\n",
    "        train_extended = add_extra_data_clean(train_extended, dataset, target)\n",
    "\n",
    "print(f\"\\nüìä Final training data:\")\n",
    "print(f\"   Original samples: {len(train)}\")\n",
    "print(f\"   Extended samples: {len(train_extended)}\")\n",
    "print(f\"   Gain: +{len(train_extended) - len(train)} samples\")\n",
    "\n",
    "for target in TARGETS:\n",
    "    count = train_extended[target].notna().sum()\n",
    "    original_count = train[target].notna().sum() if target in train.columns else 0\n",
    "    gain = count - original_count\n",
    "    print(f\"   {target}: {count:,} samples (+{gain})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data integration complete with clean SMILES!\")\n",
    "\n",
    "def separate_subtables(train_df):\n",
    "    labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "    subtables = {}\n",
    "    for label in labels:\n",
    "        # Filter out NaNs, select columns, reset index\n",
    "        subtables[label] = train_df[train_df[label].notna()][['SMILES', label]].reset_index(drop=True)\n",
    "\n",
    "    # Optional: Debugging\n",
    "    for label in subtables:\n",
    "        print(f\"{label} NaNs per column:\")\n",
    "        print(subtables[label].isna().sum())\n",
    "        print(subtables[label].shape)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    return subtables\n",
    "\n",
    "def augment_smiles_dataset(smiles_list, labels, num_augments=3):\n",
    "    \"\"\"\n",
    "    Augments a list of SMILES strings by generating randomized versions.\n",
    "\n",
    "    Parameters:\n",
    "        smiles_list (list of str): Original SMILES strings.\n",
    "        labels (list or np.array): Corresponding labels.\n",
    "        num_augments (int): Number of augmentations per SMILES.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (augmented_smiles, augmented_labels)\n",
    "    \"\"\"\n",
    "    augmented_smiles = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for smiles, label in zip(smiles_list, labels):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "        # Add original\n",
    "        augmented_smiles.append(smiles)\n",
    "        augmented_labels.append(label)\n",
    "        # Add randomized versions\n",
    "        for _ in range(num_augments):\n",
    "            rand_smiles = Chem.MolToSmiles(mol, doRandom=True)\n",
    "            augmented_smiles.append(rand_smiles)\n",
    "            augmented_labels.append(label)\n",
    "\n",
    "    return augmented_smiles, np.array(augmented_labels)\n",
    "\n",
    "mordred_calc = Calculator(mordred_descriptors, ignore_3D=True)\n",
    "def build_mordred_descriptors(smiles_list):\n",
    "    # Build Mordred descriptors for test\n",
    "    mols_test = [Chem.MolFromSmiles(s) for s in smiles_list]\n",
    "    desc_test = mordred_calc.pandas(mols_test, nproc=1)\n",
    "\n",
    "    # Make columns string & numeric only (no dropping beyond that)\n",
    "    desc_test.columns = desc_test.columns.map(str)\n",
    "    desc_test = desc_test.select_dtypes(include=[np.number]).copy()\n",
    "    desc_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return desc_test\n",
    "\n",
    "from rdkit.Chem import Crippen, Lipinski\n",
    "\n",
    "def smiles_to_combined_fingerprints_with_descriptors(smiles_list):\n",
    "    # Set fingerprint parameters inside the function\n",
    "    radius = 2\n",
    "    n_bits = 128\n",
    "\n",
    "    generator = GetMorganGenerator(radius=radius, fpSize=n_bits) if getattr(Config, \"use_morgan_fp\", True) else None\n",
    "    atom_pair_gen = GetAtomPairGenerator(fpSize=n_bits) if getattr(Config, 'use_atom_pair_fp', False) else None\n",
    "    torsion_gen = GetTopologicalTorsionGenerator(fpSize=n_bits) if getattr(Config, 'use_torsion_fp', False) else None\n",
    "    \n",
    "    fp_len = (n_bits if getattr(Config, 'use_morgan_fp', False) else 0) \\\n",
    "           + (n_bits if getattr(Config, 'use_atom_pair_fp', False) else 0) \\\n",
    "           + (n_bits if getattr(Config, 'use_torsion_fp', False) else 0) \\\n",
    "           + (167 if getattr(Config, 'use_maccs_fp', True) else 0)\n",
    "    if getattr(Config, 'use_chemberta', False):\n",
    "        fp_len += 384\n",
    "        \n",
    "    fingerprints = []\n",
    "    descriptors = []\n",
    "    valid_smiles = []\n",
    "    invalid_indices = []\n",
    "    use_any_fp = getattr(Config, \"use_morgan_fp\", False) or getattr(Config, \"use_atom_pair_fp\", False) or getattr(Config, \"use_torsion_fp\", False) or getattr(Config, \"use_maccs_fp\", False) or getattr(Config, 'use_chemberta', False)\n",
    "\n",
    "    for i, smiles in enumerate(smiles_list):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            # Fingerprints (No change from your code)\n",
    "            if use_any_fp:\n",
    "                fps = []\n",
    "                if getattr(Config, \"use_morgan_fp\", True) and generator is not None:\n",
    "                    fps.append(np.array(generator.GetFingerprint(mol)))\n",
    "                if atom_pair_gen:\n",
    "                    fps.append(np.array(atom_pair_gen.GetFingerprint(mol)))\n",
    "                if torsion_gen:\n",
    "                    fps.append(np.array(torsion_gen.GetFingerprint(mol)))\n",
    "                if getattr(Config, 'use_maccs_fp', True):\n",
    "                    fps.append(np.array(MACCSkeys.GenMACCSKeys(mol)))\n",
    "                if getattr(Config, \"use_chemberta\", False):\n",
    "                    emb = get_chemberta_embedding(smiles)\n",
    "                    fps.append(emb)\n",
    "                \n",
    "                combined_fp = np.concatenate(fps)\n",
    "                fingerprints.append(combined_fp)\n",
    "\n",
    "            if getattr(Config, 'use_descriptors', True):\n",
    "                descriptor_values = {}\n",
    "                for name, func in Descriptors.descList:\n",
    "                    try:\n",
    "                        descriptor_values[name] = func(mol)\n",
    "                    except:\n",
    "                        print(f\"Descriptor {name} failed for SMILES at index {i}\")\n",
    "                        descriptor_values[name] = None\n",
    "\n",
    "                # try:\n",
    "                # --- Features for Rigidity and Complexity (for Tg, FFV) ---\n",
    "                try:\n",
    "                    num_heavy_atoms = mol.GetNumHeavyAtoms()\n",
    "                except Exception as e:\n",
    "                    num_heavy_atoms = 0\n",
    "\n",
    "                # --- Features for Rigidity and Complexity (for Tg, FFV) ---\n",
    "                try:\n",
    "                    descriptor_values['NumAromaticRings'] = Lipinski.NumAromaticRings(mol)\n",
    "                except Exception as e:\n",
    "                    descriptor_values['NumAromaticRings'] = None\n",
    "                try:\n",
    "                    num_sp3_carbons = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 6 and atom.GetHybridization() == Chem.rdchem.HybridizationType.SP3)\n",
    "                    descriptor_values['FractionCSP3'] = num_sp3_carbons / num_heavy_atoms if num_heavy_atoms > 0 else 0\n",
    "                except Exception as e:\n",
    "                    descriptor_values['FractionCSP3'] = None\n",
    "\n",
    "                # --- Features for Bulkiness and Shape (for FFV) ---\n",
    "                try:\n",
    "                    descriptor_values['MolMR'] = Crippen.MolMR(mol) # Molar Refractivity (volume)\n",
    "                except Exception as e:\n",
    "                    descriptor_values['MolMR'] = None\n",
    "                try:\n",
    "                    descriptor_values['LabuteASA'] = Descriptors.LabuteASA(mol) # Accessible surface area\n",
    "                except Exception as e:\n",
    "                    descriptor_values['LabuteASA'] = None\n",
    "\n",
    "                # --- Features for Heavy Atoms (for Density) ---\n",
    "                try:\n",
    "                    descriptor_values['NumFluorine'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 9)\n",
    "                except Exception as e:\n",
    "                    descriptor_values['NumFluorine'] = None\n",
    "                try:\n",
    "                    descriptor_values['NumChlorine'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 17)\n",
    "                except Exception as e:\n",
    "                    descriptor_values['NumChlorine'] = None\n",
    "\n",
    "                # --- Features for Intermolecular Forces (for Tc) ---\n",
    "                try:\n",
    "                    descriptor_values['NumHDonors'] = Lipinski.NumHDonors(mol)\n",
    "                except Exception as e:\n",
    "                    descriptor_values['NumHDonors'] = None\n",
    "                try:\n",
    "                    descriptor_values['NumHAcceptors'] = Lipinski.NumHAcceptors(mol)\n",
    "                except Exception as e:\n",
    "                    descriptor_values['NumHAcceptors'] = None\n",
    "\n",
    "                # --- Features for Branching and Flexibility (for Rg) ---\n",
    "                try:\n",
    "                    descriptor_values['BalabanJ'] = Descriptors.BalabanJ(mol) # Topological index sensitive to branching\n",
    "                except Exception as e:\n",
    "                    descriptor_values['BalabanJ'] = None\n",
    "                try:\n",
    "                    descriptor_values['Kappa2'] = Descriptors.Kappa2(mol) # Molecular shape index\n",
    "                except Exception as e:\n",
    "                    descriptor_values['Kappa2'] = None\n",
    "                try:\n",
    "                    descriptor_values['NumRotatableBonds'] = CalcNumRotatableBonds(mol) # Flexibility\n",
    "                except Exception as e:\n",
    "                    descriptor_values['NumRotatableBonds'] = None\n",
    "                \n",
    "                # Graph-based features\n",
    "                try:\n",
    "                    adj = rdmolops.GetAdjacencyMatrix(mol)\n",
    "                    G = nx.from_numpy_array(adj)\n",
    "                    if nx.is_connected(G):\n",
    "                        descriptor_values['graph_diameter'] = nx.diameter(G)\n",
    "                        descriptor_values['avg_shortest_path'] = nx.average_shortest_path_length(G)\n",
    "                    else:\n",
    "                        descriptor_values['graph_diameter'], descriptor_values['avg_shortest_path'] = 0, 0\n",
    "                    descriptor_values['num_cycles'] = len(list(nx.cycle_basis(G)))\n",
    "                except:\n",
    "                    print(f\"Graph features failed for SMILES at index {i}\")\n",
    "                    descriptor_values['graph_diameter'], descriptor_values['avg_shortest_path'], descriptor_values['num_cycles'] = None, None, None\n",
    "\n",
    "                descriptors.append(descriptor_values)\n",
    "            else:\n",
    "                descriptors.append(None)\n",
    "            valid_smiles.append(smiles)\n",
    "        else:\n",
    "            if use_any_fp: fingerprints.append(np.zeros(fp_len))\n",
    "            if getattr(Config, \"use_chemberta\", False):\n",
    "                descriptors.append({f'chemberta_emb_{j}': 0.0 for j in range(384)})\n",
    "            else:\n",
    "                descriptors.append(None)\n",
    "            valid_smiles.append(None)\n",
    "            invalid_indices.append(i)\n",
    "\n",
    "    fingerprints_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fp_len)]) if use_any_fp else pd.DataFrame()\n",
    "    descriptors_df = pd.DataFrame([d for d in descriptors if d is not None]) if any(d is not None for d in descriptors) else pd.DataFrame()\n",
    "\n",
    "    if getattr(Config, 'use_mordred', False):\n",
    "        mordred_df = build_mordred_descriptors(smiles_list)\n",
    "        if descriptors_df.empty:\n",
    "            descriptors_df = mordred_df\n",
    "        else:\n",
    "            descriptors_df = pd.concat([descriptors_df.reset_index(drop=True), mordred_df], axis=1)\n",
    "    \n",
    "    # Keep only unique columns in descriptors_df\n",
    "    if not descriptors_df.empty:\n",
    "        descriptors_df = descriptors_df.loc[:, ~descriptors_df.columns.duplicated()]\n",
    "    return fingerprints_df, descriptors_df, valid_smiles, invalid_indices\n",
    "\n",
    "required_descriptors = {'graph_diameter','num_cycles','avg_shortest_path','MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}\n",
    "\n",
    "# Utility function to combine train and val sets into X_all and y_all\n",
    "def combine_train_val(X_train, X_val, y_train, y_val):\n",
    "    X_train = pd.DataFrame(X_train) if isinstance(X_train, np.ndarray) else X_train\n",
    "    X_val = pd.DataFrame(X_val) if isinstance(X_val, np.ndarray) else X_val\n",
    "    y_train = pd.Series(y_train) if isinstance(y_train, np.ndarray) else y_train\n",
    "    y_val = pd.Series(y_val) if isinstance(y_val, np.ndarray) else y_val\n",
    "    X_all = pd.concat([X_train, X_val], axis=0)\n",
    "    y_all = pd.concat([y_train, y_val], axis=0)\n",
    "    return X_all, y_all\n",
    "\n",
    "# --- PCA utility for train/test transformation ---\n",
    "def apply_pca(X_train, X_test=None, verbose=True):\n",
    "    pca = PCA(n_components=config.pca_variance, svd_solver='full', random_state=getattr(config, 'random_state', 42))\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test) if X_test is not None else None\n",
    "    if verbose:\n",
    "        print(f\"[PCA] Reduced train shape: {X_train.shape} -> {X_train_pca.shape} (kept {pca.n_components_} components, {100*pca.explained_variance_ratio_.sum():.4f}% variance)\")\n",
    "    return X_train_pca, X_test_pca, pca\n",
    "\n",
    "def augment_dataset(X, y, n_samples=1000, n_components=5, random_state=None):\n",
    "    \"\"\"\n",
    "    Augments a dataset using Gaussian Mixture Models.\n",
    "\n",
    "    Parameters:\n",
    "    - X: pd.DataFrame or np.ndarray ‚Äî feature matrix\n",
    "    - y: pd.Series or np.ndarray ‚Äî target values\n",
    "    - n_samples: int ‚Äî number of synthetic samples to generate\n",
    "    - n_components: int ‚Äî number of GMM components\n",
    "    - random_state: int ‚Äî random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - X_augmented: pd.DataFrame ‚Äî augmented feature matrix\n",
    "    - y_augmented: pd.Series ‚Äî augmented target values\n",
    "    \"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    elif not isinstance(X, pd.DataFrame):\n",
    "        raise ValueError(\"X must be a pandas DataFrame or a NumPy array\")\n",
    "\n",
    "    X.columns = X.columns.astype(str)\n",
    "\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = pd.Series(y)\n",
    "    elif not isinstance(y, pd.Series):\n",
    "        raise ValueError(\"y must be a pandas Series or a NumPy array\")\n",
    "\n",
    "    df = X.copy()\n",
    "    df['Target'] = y.values\n",
    "\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "    gmm.fit(df)\n",
    "\n",
    "    synthetic_data, _ = gmm.sample(n_samples)\n",
    "    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "\n",
    "    augmented_df = pd.concat([df, synthetic_df], ignore_index=True)\n",
    "\n",
    "    X_augmented = augmented_df.drop(columns='Target')\n",
    "    y_augmented = augmented_df['Target']\n",
    "\n",
    "    return X_augmented, y_augmented\n",
    "\n",
    "# --- Outlier Detection Summary Function ---\n",
    "def display_outlier_summary(y, X=None, name=\"target\", z_thresh=3, iqr_factor=1.5, iso_contamination=0.01, lof_contamination=0.01):\n",
    "    \"\"\"\n",
    "    Display the percentage of data flagged as outlier by Z-score, IQR, Isolation Forest, and LOF.\n",
    "    y: 1D array-like (target or feature)\n",
    "    X: 2D array-like (feature matrix, required for Isolation Forest/LOF)\n",
    "    name: str, name of the variable being checked\n",
    "    \"\"\"\n",
    "    print(f\"\\nOutlier summary for: {name}\")\n",
    "    y = np.asarray(y)\n",
    "    n = len(y)\n",
    "    # Z-score\n",
    "    z_scores = (y - np.mean(y)) / np.std(y)\n",
    "    z_outliers = np.abs(z_scores) > z_thresh\n",
    "    print(f\"Z-score > {z_thresh}: {np.sum(z_outliers)} / {n} ({100*np.mean(z_outliers):.2f}%)\")\n",
    "\n",
    "    # IQR\n",
    "    Q1 = np.percentile(y, 25)\n",
    "    Q3 = np.percentile(y, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - iqr_factor * IQR\n",
    "    upper = Q3 + iqr_factor * IQR\n",
    "    iqr_outliers = (y < lower) | (y > upper)\n",
    "    print(f\"IQR (factor {iqr_factor}): {np.sum(iqr_outliers)} / {n} ({100*np.mean(iqr_outliers):.2f}%)\")\n",
    "\n",
    "    # Isolation Forest (if X provided)\n",
    "    if X is not None:\n",
    "        try:\n",
    "            from sklearn.ensemble import IsolationForest\n",
    "            iso = IsolationForest(contamination=iso_contamination, random_state=42)\n",
    "            iso_out = iso.fit_predict(X)\n",
    "            iso_outliers = iso_out == -1\n",
    "            print(f\"Isolation Forest (contamination={iso_contamination}): {np.sum(iso_outliers)} / {len(iso_outliers)} ({100*np.mean(iso_outliers):.2f}%)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Isolation Forest failed: {e}\")\n",
    "        # Local Outlier Factor\n",
    "        try:\n",
    "            from sklearn.neighbors import LocalOutlierFactor\n",
    "            lof = LocalOutlierFactor(n_neighbors=20, contamination=lof_contamination)\n",
    "            lof_out = lof.fit_predict(X)\n",
    "            lof_outliers = lof_out == -1\n",
    "            print(f\"Local Outlier Factor (contamination={lof_contamination}): {np.sum(lof_outliers)} / {len(lof_outliers)} ({100*np.mean(lof_outliers):.2f}%)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Local Outlier Factor failed: {e}\")\n",
    "    else:\n",
    "        print(\"Isolation Forest/LOF skipped (X not provided)\")\n",
    "\n",
    "\n",
    "train_df=train_extended\n",
    "test_df=test\n",
    "subtables = separate_subtables(train_df)\n",
    "\n",
    "test_smiles = test_df['SMILES'].tolist()\n",
    "test_ids = test_df['id'].values\n",
    "labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "#labels = ['Tc']\n",
    "\n",
    "# Save importance_df to Excel log file, one sheet per label\n",
    "def save_importance_to_excel(importance_df, label, log_path):\n",
    "    import os\n",
    "    from openpyxl import load_workbook\n",
    "    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "    if os.path.exists(log_path):\n",
    "        with pd.ExcelWriter(log_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "            importance_df.to_excel(writer, sheet_name=label, index=False)\n",
    "    else:\n",
    "        with pd.ExcelWriter(log_path, engine='openpyxl') as writer:\n",
    "            importance_df.to_excel(writer, sheet_name=label, index=False)\n",
    "\n",
    "def get_least_important_features_all_methods(X, y, label, model_name=None):\n",
    "    \"\"\"\n",
    "    Remove features in three steps:\n",
    "    1. Remove features with model.feature_importances_ <= 0\n",
    "    2. Remove features with permutation_importance <= 0\n",
    "    3. Remove features with SHAP importance <= 0\n",
    "    Returns a list of features to remove (union of all three criteria).\n",
    "    \"\"\"    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=config.random_state)\n",
    "    model_type = (model_name or getattr(config, 'model_name', 'xgb'))\n",
    "    if model_type == 'xgb':\n",
    "        model = XGBRegressor(random_state=config.random_state, n_jobs=-1, verbosity=0, early_stopping_rounds=50, eval_metric=\"mae\", objective=\"reg:absoluteerror\")\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "    elif model_type == 'catboost':\n",
    "        model = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6, loss_function='MAE', eval_metric='MAE', random_seed=config.random_state, verbose=False)\n",
    "        model.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=50, use_best_model=True)\n",
    "    elif model_type == 'lgbm':\n",
    "        model = LGBMRegressor(n_estimators=1000, learning_rate=0.05, max_depth=6, reg_lambda=1.0, objective='mae', random_state=config.random_state, verbose=-1, verbosity=-1)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric='mae', callbacks=[lgb.early_stopping(stopping_rounds=50)])\n",
    "    else:\n",
    "        model = XGBRegressor(random_state=config.random_state, n_jobs=-1, verbosity=0, early_stopping_rounds=50, eval_metric=\"rmse\")\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "    # 1. Remove features with model.feature_importances_ <= 0\n",
    "    fi_mask = model.feature_importances_ <= 0\n",
    "    fi_features = set(feature_names[fi_mask])\n",
    "    # Save feature_importances_ to Excel, sorted by importance\n",
    "    fi_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance_mean': model.feature_importances_,\n",
    "        'importance_std': [0]*len(feature_names)\n",
    "    }).sort_values('importance_mean', ascending=False)\n",
    "    save_importance_to_excel(fi_importance_df, label + '_fi', getattr(Config, 'permutation_importance_log_path', 'log/permutation_importance_log.xlsx'))\n",
    "\n",
    "    # 2. Remove features with permutation_importance <= 0\n",
    "    perm_result = permutation_importance(\n",
    "        model, X_valid, y_valid,\n",
    "        n_repeats=1 if config.debug else 10,\n",
    "        random_state=config.random_state,\n",
    "        scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    # Save permutation importance to Excel, sorted by mean importance descending\n",
    "    perm_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance_mean': perm_result.importances_mean,\n",
    "        'importance_std': perm_result.importances_std\n",
    "    }).sort_values('importance_mean', ascending=False)\n",
    "    save_importance_to_excel(perm_importance_df, label + '_perm', getattr(Config, 'permutation_importance_log_path', 'log/permutation_importance_log.xlsx'))\n",
    "    perm_mask = perm_result.importances_mean <= 0\n",
    "    perm_features = set(feature_names[perm_mask])\n",
    "\n",
    "    # 3. Remove features with SHAP importance <= 0\n",
    "    explainer = shap.Explainer(model, X_valid)\n",
    "    # For LGBM, disable additivity check to avoid ExplainerError\n",
    "    if model_type == 'lgbm':\n",
    "        shap_values = explainer(X_valid, check_additivity=False)\n",
    "    else:\n",
    "        shap_values = explainer(X_valid)\n",
    "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    shap_mask = shap_importance <= 0\n",
    "    shap_features = set(feature_names[shap_mask])\n",
    "    # Save SHAP importance to Excel, sorted by importance\n",
    "    shap_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance_mean': shap_importance,\n",
    "        'importance_std': [0]*len(feature_names)\n",
    "    }).sort_values('importance_mean', ascending=False)\n",
    "    save_importance_to_excel(shap_importance_df, label + '_shap', getattr(Config, 'permutation_importance_log_path', 'log/permutation_importance_log.xlsx'))\n",
    "\n",
    "    # Union of all features to remove\n",
    "    features_to_remove = fi_features | perm_features | shap_features\n",
    "    print(f\"Removed {len(features_to_remove)} features for {label} using all methods (fi: {len(fi_features)}, perm: {len(perm_features)}, shap: {len(shap_features)})\")\n",
    "\n",
    "    return list(features_to_remove)\n",
    "\n",
    "def get_least_important_features(X, y, label, model_name=None):\n",
    "    # Correct unpacking of train_test_split\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=config.random_state)\n",
    "\n",
    "    if (model_name or getattr(config, 'model_name', 'xgb')) == 'xgb':\n",
    "        model = XGBRegressor(random_state=config.random_state, n_jobs=-1, verbosity=0, early_stopping_rounds=50, eval_metric=\"mae\", objective=\"reg:absoluteerror\")\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "    else:\n",
    "        model = ExtraTreesRegressor(random_state=config.random_state, criterion='absolute_error')\n",
    "        model.fit(X, y)\n",
    "\n",
    "    # Use config.feature_importance_method to choose method\n",
    "    importance_method = getattr(config, 'feature_importance_method', 'feature_importances_')\n",
    "    if importance_method == 'feature_importances_':\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance_mean': model.feature_importances_,\n",
    "            'importance_std': [0]*len(X_train.columns)\n",
    "        })\n",
    "    else:\n",
    "    # model = ExtraTreesRegressor(random_state=config.random_state)\n",
    "        result = permutation_importance(\n",
    "            model, X_valid, y_valid,\n",
    "            n_repeats=30,\n",
    "            random_state=Config.random_state,\n",
    "            scoring='r2'\n",
    "        )\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance_mean': result.importances_mean,\n",
    "            'importance_std': result.importances_std\n",
    "        })\n",
    "\n",
    "    # Use model- and label-specific n_least_important_features\n",
    "    if model_name is None:\n",
    "        model_name_used = getattr(config, 'model_name', 'xgb')\n",
    "    else:\n",
    "        model_name_used = model_name\n",
    "    n = config.n_least_important_features.get(model_name_used, {}).get(label, 5)\n",
    "    # Sort by importance (ascending) and return n least important features\n",
    "    # Remove all features with importance_mean < 0 first\n",
    "    negative_importance = importance_df[importance_df['importance_mean'] <= 0]\n",
    "    num_negative = len(negative_importance)\n",
    "    least_important = negative_importance\n",
    "\n",
    "    # If less than n features removed, remove more to reach n\n",
    "    if num_negative < n:\n",
    "        # Exclude already selected features\n",
    "        remaining = importance_df[~importance_df['feature'].isin(negative_importance['feature'])]\n",
    "        additional = remaining.sort_values(by='importance_mean').head(n - num_negative)\n",
    "        least_important = pd.concat([least_important, additional], ignore_index=True)\n",
    "    else:\n",
    "        # If already removed n or more, just keep the negative ones\n",
    "        least_important = negative_importance\n",
    "\n",
    "    print(f\"Removed {len(least_important)} least important features for {label} (with {num_negative} <= 0)\")\n",
    "\n",
    "    importance_df = importance_df.sort_values(by='importance_mean', ascending=True)\n",
    "\n",
    "    # Mark features to be removed\n",
    "    importance_df['removed'] = importance_df['feature'].isin(least_important['feature'])\n",
    "\n",
    "    save_importance_to_excel(importance_df, label, Config.permutation_importance_log_path)\n",
    "\n",
    "    return least_important['feature'].tolist()\n",
    "\n",
    "# Save model to disk for this fold using a helper function\n",
    "def save_model(Model, label, fold, model_name):\n",
    "    model_path = f\"models/{label}_fold{fold+1}_{model_name}\"\n",
    "    try:\n",
    "        if 'torch' in str(type(Model)).lower():\n",
    "            # Save PyTorch model state_dict\n",
    "            model_path += \".pt\"\n",
    "            torch.save(Model.state_dict(), model_path)\n",
    "        else:\n",
    "            # Save scikit-learn model\n",
    "            model_path += \".joblib\"\n",
    "            joblib.dump(Model, model_path)\n",
    "        print(f\"Saved model for {label} fold {fold+1} to {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save model for {label} fold {fold+1}: {e}\")\n",
    "\n",
    "def train_with_other_models(model_name, label, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train a regression model using the specified model_name, with hyperparameters\n",
    "    adapted to the data size of the target label.\n",
    "    \"\"\"\n",
    "    print(f\"Training {model_name} model for label: {label}\")\n",
    "    if model_name == 'tabnet':\n",
    "        try:\n",
    "            from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "        except ImportError:\n",
    "            raise ImportError(\"pytorch-tabnet is not installed. Please install it with 'pip install pytorch-tabnet'.\")\n",
    "        \n",
    "        # --- Define TabNet parameters based on label ---\n",
    "        if label in ['Rg', 'Tc']: # Low Data\n",
    "            params = {'n_d': 8, 'n_a': 8, 'n_steps': 3, 'gamma': 1.3, 'lambda_sparse': 1e-4}\n",
    "        elif label == 'FFV': # High Data\n",
    "            params = {'n_d': 24, 'n_a': 24, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 1e-5}\n",
    "        else: # Medium Data\n",
    "            params = {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 1e-5}\n",
    "\n",
    "        Model = TabNetRegressor(**params, seed=42, verbose=0)\n",
    "        Model.fit(\n",
    "            X_train.values, y_train.values.reshape(-1, 1),\n",
    "            eval_set=[(X_val.values, y_val.values.reshape(-1, 1))],\n",
    "            eval_metric=['mae'], # ACTION: Changed from 'rmse' to 'mae'\n",
    "            max_epochs=200, patience=20, batch_size=1024, virtual_batch_size=128\n",
    "        )\n",
    "\n",
    "    elif model_name == 'catboost':\n",
    "        # --- Define CatBoost parameters based on label ---\n",
    "        params = {'iterations': 3000, 'learning_rate': 0.05, 'loss_function': 'MAE', 'eval_metric': 'MAE', 'random_seed': Config.random_state, 'verbose': False}\n",
    "        if label in ['Rg', 'Tc']: # Low Data\n",
    "            params.update({'depth': 5, 'l2_leaf_reg': 7})\n",
    "        elif label == 'FFV': # High Data\n",
    "            params.update({'depth': 7, 'l2_leaf_reg': 2})\n",
    "        else: # Medium Data\n",
    "            params.update({'depth': 6, 'l2_leaf_reg': 3})\n",
    "\n",
    "        Model = CatBoostRegressor(**params)\n",
    "        Model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, use_best_model=True)\n",
    "\n",
    "    elif model_name == 'lgbm':\n",
    "        # --- Define LightGBM parameters based on label ---\n",
    "        params = {'n_estimators': 3000, 'learning_rate': 0.05, 'objective': 'mae', 'random_state': Config.random_state, 'verbose': -1, 'verbosity': -1}\n",
    "        if label in ['Rg', 'Tc']: # Low Data\n",
    "            params.update({'max_depth': 4, 'num_leaves': 20, 'reg_lambda': 5.0})\n",
    "        elif label == 'FFV': # High Data\n",
    "            params.update({'max_depth': 7, 'num_leaves': 40, 'reg_lambda': 1.0})\n",
    "        else: # Medium Data\n",
    "            params.update({'max_depth': 6, 'num_leaves': 31, 'reg_lambda': 1.0})\n",
    "\n",
    "        Model = LGBMRegressor(**params)\n",
    "        Model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='mae', callbacks=[lgb.early_stopping(stopping_rounds=50)])\n",
    "\n",
    "    elif model_name == 'extratrees':\n",
    "        # --- Define ExtraTrees parameters based on label ---\n",
    "        params = {'n_estimators': 300, 'criterion': 'absolute_error', 'random_state': Config.random_state, 'n_jobs': -1}\n",
    "        if label in ['Rg', 'Tc']: # Low Data: Prevent overfitting by requiring more samples per leaf\n",
    "            params.update({'min_samples_leaf': 3, 'max_features': 0.8})\n",
    "        else: # High/Medium Data\n",
    "            params.update({'min_samples_leaf': 1, 'max_features': 1.0})\n",
    "            \n",
    "        Model = ExtraTreesRegressor(**params)\n",
    "        X_all, y_all = combine_train_val(X_train, X_val, y_train, y_val)\n",
    "        Model.fit(X_all, y_all)\n",
    "\n",
    "    elif model_name == 'randomforest':\n",
    "        # --- Define RandomForest parameters based on label ---\n",
    "        params = {'n_estimators': 1000, 'criterion': 'absolute_error', 'random_state': Config.random_state, 'n_jobs': -1}\n",
    "        if label in ['Rg', 'Tc']: # Low Data\n",
    "            params.update({'min_samples_leaf': 3, 'max_features': 0.8, 'max_depth': 15})\n",
    "        else: # High/Medium Data\n",
    "            params.update({'min_samples_leaf': 1, 'max_features': 1.0, 'max_depth': None})\n",
    "\n",
    "        Model = RandomForestRegressor(**params)\n",
    "        X_all, y_all = combine_train_val(X_train, X_val, y_train, y_val)\n",
    "        Model.fit(X_all, y_all)\n",
    "\n",
    "    elif model_name == 'hgbm':\n",
    "        from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "        # --- Define HGBM parameters based on label ---\n",
    "        params = {'max_iter': 1000, 'learning_rate': 0.05, 'loss': 'absolute_error', 'early_stopping': True, 'random_state': 42} # ACTION: Changed loss to 'absolute_error'\n",
    "        if label in ['Rg', 'Tc']: # Low Data\n",
    "            params.update({'max_depth': 4, 'l2_regularization': 1.0})\n",
    "        elif label == 'FFV': # High Data\n",
    "            params.update({'max_depth': 7, 'l2_regularization': 0.1})\n",
    "        else: # Medium Data\n",
    "            params.update({'max_depth': 6, 'l2_regularization': 0.5})\n",
    "\n",
    "        Model = HistGradientBoostingRegressor(**params)\n",
    "        X_all, y_all = combine_train_val(X_train, X_val, y_train, y_val)\n",
    "        Model.fit(X_all, y_all)\n",
    "\n",
    "    elif model_name == 'nn':\n",
    "        # The 'train_with_nn' function already uses different configs per label, which is excellent!\n",
    "        # Just ensure the loss function inside it is nn.L1Loss()\n",
    "        Model = train_with_nn(label, X_train, X_val, y_train, y_val)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown or unavailable model: {model_name}\")\n",
    "    \n",
    "    return Model\n",
    "\n",
    "def train_with_autogluon(label, X_train, y_train):\n",
    "    try:\n",
    "        from autogluon.tabular import TabularPredictor\n",
    "    except ImportError:\n",
    "        raise ImportError(\"AutoGluon is not installed. Please install it with 'pip install autogluon'.\")\n",
    "    import pandas as pd\n",
    "    import uuid\n",
    "    # Prepare data for AutoGluon (must be DataFrame with column names)\n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    y_train_series = pd.Series(y_train, name=label)\n",
    "    train_data = X_train_df.copy()\n",
    "    train_data[label] = y_train_series.values\n",
    "\n",
    "    unique_path = f\"autogluon_{label}_{int(time.time())}_{uuid.uuid4().hex}\"\n",
    "\n",
    "    hyperparameters = {\n",
    "        \"GBM\": {},\n",
    "        \"CAT\": {},\n",
    "        \"XGB\": {},\n",
    "        \"NN_TORCH\": {},\n",
    "        \"RF\": {},\n",
    "        \"XT\": {}\n",
    "    }\n",
    "\n",
    "    hyperparameter_tune_kwargs = {\n",
    "        \"num_trials\": 50,\n",
    "        \"scheduler\": \"local\",\n",
    "        \"searcher\": \"auto\"\n",
    "    }\n",
    "\n",
    "    time_limit = 300 if getattr(Config, 'debug', False) else 3600\n",
    "\n",
    "    predictor = TabularPredictor(\n",
    "        label=label,\n",
    "        eval_metric=\"mae\",  # Use 'mae' for regression\n",
    "        path=unique_path\n",
    "    ).fit(\n",
    "        train_data,\n",
    "        presets=\"best_quality\",\n",
    "        hyperparameters=hyperparameters,\n",
    "        hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    "        num_bag_folds=5,\n",
    "        num_stack_levels=2,\n",
    "        time_limit=time_limit\n",
    "    )\n",
    "\n",
    "    print(\"\\n[AutoGluon] Leaderboard:\")\n",
    "    leaderboard = predictor.leaderboard(silent=False)\n",
    "\n",
    "    print(\"\\n[AutoGluon] Model Info:\")\n",
    "    print(predictor.info())\n",
    "\n",
    "    print(\"\\n[AutoGluon] Model Names:\")\n",
    "    model_names = leaderboard[\"model\"].tolist()   # <- FIXED here\n",
    "    print(model_names)\n",
    "\n",
    "    # Save feature importance to CSV\n",
    "    fi_df = predictor.feature_importance(train_data)\n",
    "    fi_path = f\"NeurIPS/autogluon_feature_importance_{label}.csv\"  \n",
    "    fi_df.to_csv(fi_path)\n",
    "    print(f\"[AutoGluon] Feature importance saved to {fi_path}\")\n",
    "\n",
    "    return predictor\n",
    "\n",
    "def train_with_stacking(label, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Trains XGBoost, ExtraTrees, and CatBoost using sklearn's StackingRegressor.\n",
    "    Returns the fitted stacking model and base models.\n",
    "    \"\"\"\n",
    "    estimators = [\n",
    "    ('xgb', XGBRegressor(n_estimators=10000, learning_rate=0.01, max_depth=5, subsample=0.8, colsample_bytree=0.8, gamma=0.1, reg_lambda=1.0, objective=\"reg:absoluteerror\", random_state=Config.random_state, n_jobs=-1)),\n",
    "    ('lgbm', LGBMRegressor(n_estimators=10000, learning_rate=0.01, num_leaves=64, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=5, reg_lambda=1.0, max_depth=-1, objective=\"mae\", random_state=Config.random_state, n_jobs=-1, verbose=-1)),\n",
    "        ('cb', CatBoostRegressor(iterations=10000, learning_rate=0.01, depth=7, l2_leaf_reg=5, bagging_temperature=0.8, random_seed=Config.random_state, verbose=0)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=500, max_depth=12, min_samples_leaf=3, random_state=Config.random_state, n_jobs=-1, criterion='absolute_error'))\n",
    "    ]\n",
    "\n",
    "    # Candidate final estimators\n",
    "    final_estimators = {\n",
    "        \"ElasticNet\": ElasticNetCV(l1_ratio=[0.1, 0.5, 0.9], n_alphas=100, max_iter=50000, tol=1e-3, cv=5, n_jobs=-1),\n",
    "        \"CatBoost\": CatBoostRegressor(iterations=3000, learning_rate=0.03, depth=6, l2_leaf_reg=3, random_seed=Config.random_state, verbose=0),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=3000, learning_rate=0.03, num_leaves=64, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=5, reg_lambda=1.0, max_depth=-1, objective=\"mae\", random_state=Config.random_state, n_jobs=-1, verbose=-1),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=3000, learning_rate=0.03, max_depth=6, subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, gamma=0.1, objective=\"reg:absoluteerror\", random_state=Config.random_state, n_jobs=-1)\n",
    "    }\n",
    "\n",
    "    final_estimator = final_estimators['ElasticNet']\n",
    "    \n",
    "    stacker = StackingRegressor(estimators=estimators, final_estimator=final_estimator, passthrough=True, cv=5, n_jobs=-1)\n",
    "\n",
    "    stacker.fit(X_train, y_train)\n",
    "    return stacker\n",
    "\n",
    "def train_with_xgb(label, X_train, y_train, X_val, y_val):\n",
    "    print(f\"Training XGB model for label: {label}\")\n",
    "    if label==\"Tg\": # Medium Data (~1.2k samples)\n",
    "        Model = XGBRegressor(\n",
    "            n_estimators=10000, learning_rate=0.01, max_depth=5, # <-- Reduced from 6\n",
    "            colsample_bytree=1.0, reg_lambda=7.0, gamma=0.1, subsample=0.5, # <-- Increased lambda, reduced subsample\n",
    "            objective=\"reg:absoluteerror\", random_state=Config.random_state, \n",
    "            early_stopping_rounds=50, eval_metric=\"mae\"\n",
    "        )\n",
    "    elif label=='Rg': # Very Low Data (~600 samples), HIGHEST PRIORITY\n",
    "        Model = XGBRegressor(\n",
    "            n_estimators=10000, learning_rate=0.06, max_depth=4, \n",
    "            colsample_bytree=1.0, reg_lambda=10.0, gamma=0.1, subsample=0.6, \n",
    "            objective=\"reg:absoluteerror\", random_state=Config.random_state, \n",
    "            early_stopping_rounds=50, eval_metric=\"mae\"\n",
    "        )\n",
    "    elif label=='FFV': # High Data (~8k samples), LOWEST PRIORITY\n",
    "        Model = XGBRegressor(\n",
    "            n_estimators=10000, learning_rate=0.06, max_depth=7, \n",
    "            colsample_bytree=0.8, reg_lambda=2.0, gamma=0.0, subsample=0.6, \n",
    "            objective=\"reg:absoluteerror\", random_state=Config.random_state, \n",
    "            early_stopping_rounds=50, eval_metric=\"mae\"\n",
    "        )\n",
    "    elif label=='Tc': # Low Data (~900 samples), HIGH PRIORITY\n",
    "        Model = XGBRegressor(\n",
    "            n_estimators=10000, learning_rate=0.01, max_depth=4, \n",
    "            colsample_bytree=0.8, reg_lambda=7.0, gamma=0.0, subsample=0.6, \n",
    "            objective=\"reg:absoluteerror\", random_state=Config.random_state, \n",
    "            early_stopping_rounds=50, eval_metric=\"mae\"\n",
    "        )\n",
    "    elif label=='Density': # Medium Data (~1.2k samples)\n",
    "        Model = XGBRegressor(\n",
    "            n_estimators=10000, learning_rate=0.06, max_depth=5, \n",
    "            colsample_bytree=1.0, reg_lambda=3.0, gamma=0.0, subsample=0.8, \n",
    "            objective=\"reg:absoluteerror\", random_state=Config.random_state, \n",
    "            early_stopping_rounds=50, eval_metric=\"mae\"\n",
    "        )\n",
    "        \n",
    "    print(f\"Model {label} trained with shape: {X_train.shape}, {y_train.shape}\")\n",
    "\n",
    "    Model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    return Model\n",
    "\n",
    "def preprocess_numerical_features(X, label=None):\n",
    "    # Ensure numeric types\n",
    "    X_num = X.select_dtypes(include=[np.number]).copy()\n",
    "    \n",
    "    # Replace inf/-inf with NaN\n",
    "    X_num.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    valid_cols = X_num.columns\n",
    "    # # Drop columns with any NaN\n",
    "    # valid_cols = [col for col in X_num.columns if not X_num[col].isnull().any()]\n",
    "    \n",
    "    # dropped_cols = set(X_num.columns) - set(valid_cols)\n",
    "    # if dropped_cols:\n",
    "    #     print(f\"Dropped columns with NaN/Inf for {label}: {list(dropped_cols)}\")\n",
    "\n",
    "    # # Keep only valid columns\n",
    "    # X_num = X_num[valid_cols]\n",
    "    \n",
    "    # Calculate median for each column (for use in test set)\n",
    "    median_values = X_num.median()\n",
    "    # Scale features if enabled\n",
    "    if getattr(Config, 'use_standard_scaler', False):\n",
    "        scaler = StandardScaler()\n",
    "        X_num_scaled = scaler.fit_transform(X_num)\n",
    "        X_num = pd.DataFrame(X_num_scaled, columns=valid_cols, index=X.index)\n",
    "    else:\n",
    "        scaler = None\n",
    "        X_num = X_num.copy()\n",
    "    \n",
    "    # Display categorical features\n",
    "    cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if cat_cols:\n",
    "        print(f\"Categorical (non-numeric) features for {label}: {cat_cols}\")\n",
    "    else:\n",
    "        print(f\"No categorical (non-numeric) features for {label}.\")\n",
    "    return X_num, valid_cols, scaler, median_values\n",
    "\n",
    "def select_features_with_lasso(X, y, label):\n",
    "    \"\"\"\n",
    "    Performs feature selection using Lasso (L1) regularization.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): The input feature matrix.\n",
    "        y (pd.Series or np.array): The target values.\n",
    "        label (str): The name of the target property (e.g., 'Rg', 'Tc').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing only the selected features.\n",
    "    \"\"\"\n",
    "    # Lasso is sensitive to feature scaling, so we scale the data first.\n",
    "    # We also need to handle any potential NaN values before scaling.\n",
    "    X_filled = X.fillna(X.median())\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_filled)\n",
    "\n",
    "    # Use LassoCV to automatically find the best alpha (regularization strength)\n",
    "    # through cross-validation. This is more robust than picking a single alpha.\n",
    "    lasso_cv = LassoCV(cv=5, random_state=42, n_jobs=-1, max_iter=2000)\n",
    "\n",
    "    # Use SelectFromModel to wrap the LassoCV regressor.\n",
    "    # This will select features where the Lasso coefficient is non-zero.\n",
    "    # The 'threshold=\"median\"' can be a good strategy to select the top 50% of features\n",
    "    # if LassoCV is too lenient and keeps too many. Start with the default (None).\n",
    "    feature_selector = SelectFromModel(lasso_cv, prefit=False, threshold=None)\n",
    "\n",
    "    print(f\"[{label}] Fitting LassoCV to find optimal features...\")\n",
    "    feature_selector.fit(X_scaled, y)\n",
    "\n",
    "    # Get the names of the features that were kept\n",
    "    selected_feature_names = X.columns[feature_selector.get_support()]\n",
    "\n",
    "    print(f\"[{label}] Original number of features: {X.shape[1]}\")\n",
    "    print(f\"[{label}] Features selected by Lasso: {len(selected_feature_names)}\")\n",
    "\n",
    "    # Return the original DataFrame with only the selected columns\n",
    "    return selected_feature_names\n",
    "\n",
    "\n",
    "def check_inf_nan(X, y, label=None):\n",
    "    \"\"\"\n",
    "    Checks for inf, -inf, and NaN values in X (DataFrame) and y (array/Series).\n",
    "    Prints summary and returns True if any such values are found.\n",
    "    \"\"\"\n",
    "    X_inf = np.isinf(X.values).any()\n",
    "    X_nan = np.isnan(X.values).any()\n",
    "    y_inf = np.isinf(y).any()\n",
    "    y_nan = np.isnan(y).any()\n",
    "    if label is None:\n",
    "        label = \"\"\n",
    "    else:\n",
    "        label = f\" [{label}]\"\n",
    "    if X_inf or X_nan or y_inf or y_nan:\n",
    "        print(f\"‚ö†Ô∏è Detected inf/nan in X or y{label}: X_inf={X_inf}, X_nan={X_nan}, y_inf={y_inf}, y_nan={y_nan}\")\n",
    "        if X_inf:\n",
    "            print(f\"  X columns with inf: {X.columns[np.isinf(X.values).any(axis=0)].tolist()}\")\n",
    "        if X_nan:\n",
    "            print(f\"  X columns with nan: {X.columns[np.isnan(X.values).any(axis=0)].tolist()}\")\n",
    "        if y_inf:\n",
    "            print(\"  y contains inf values.\")\n",
    "        if y_nan:\n",
    "            print(\"  y contains nan values.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"No inf/nan in X or y{label}.\")\n",
    "        return False\n",
    "\n",
    "# Utility: Display model summary if torchinfo is available\n",
    "def show_model_summary(model, input_dim, batch_size=32):\n",
    "    try:\n",
    "        from torchinfo import summary\n",
    "        print(summary(model, input_size=(batch_size, input_dim)))\n",
    "    except ImportError:\n",
    "        print(\"torchinfo is not installed. Install it with 'pip install torchinfo' to see model summaries.\")\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    epochs=3000, batch_size=32, lr=1e-3, weight_decay=1e-4, patience=30, verbose=True\n",
    "):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    X_val = np.asarray(X_val)\n",
    "    y_val = np.asarray(y_val)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=verbose)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "    use_early_stopping = X_val is not None and y_val is not None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if verbose and (epoch+1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        if use_early_stopping:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_preds = model(X_val_tensor)\n",
    "                val_loss = criterion(val_preds, y_val_tensor).item()\n",
    "            scheduler.step(val_loss)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}, best val loss: {best_val_loss:.4f}\")\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n",
    "class FeedforwardNet(nn.Module):\n",
    "    def __init__(self, input_dim, neurons, dropouts):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i, n in enumerate(neurons):\n",
    "            layers.append(nn.Linear(input_dim, n))\n",
    "            # layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            if i < len(dropouts) and dropouts[i] > 0:\n",
    "                layers.append(nn.Dropout(dropouts[i]))\n",
    "            input_dim = n\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_with_nn(label, X_train, X_val, y_train, y_val):\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    if getattr(Config, \"search_nn\", False):\n",
    "        print(f\"--- Starting targeted NN architecture search for label: {label} ---\")\n",
    "        \n",
    "        search_configs_by_label = {\n",
    "            'Low': [ # For Rg, Tc. Simple models with high regularization.\n",
    "                # --- Single Layer Focus ---\n",
    "                {\"neurons\": [32], \"dropouts\": [0.3]},\n",
    "                {\"neurons\": [64], \"dropouts\": [0.4]},\n",
    "                {\"neurons\": [128], \"dropouts\": [0.5]},\n",
    "                {\"neurons\": [256], \"dropouts\": [0.5]},\n",
    "\n",
    "                # --- Two Layer Rectangular Focus (based on Rg's winner) ---\n",
    "                {\"neurons\": [64, 64], \"dropouts\": [0.4, 0.4]},\n",
    "                {\"neurons\": [96, 96], \"dropouts\": [0.5, 0.5]},\n",
    "                {\"neurons\": [128, 128], \"dropouts\": [0.5, 0.5]}, # Previous winner\n",
    "                {\"neurons\": [192, 192], \"dropouts\": [0.5, 0.5]},\n",
    "                {\"neurons\": [256, 256], \"dropouts\": [0.5, 0.5]},\n",
    "\n",
    "                # --- Two Layer Tapering Focus ---\n",
    "                {\"neurons\": [128, 32], \"dropouts\": [0.5, 0.3]},\n",
    "                {\"neurons\": [128, 64], \"dropouts\": [0.5, 0.4]},\n",
    "                {\"neurons\": [256, 64], \"dropouts\": [0.5, 0.4]},\n",
    "                {\"neurons\": [256, 128], \"dropouts\": [0.5, 0.4]},\n",
    "                {\"neurons\": [512, 128], \"dropouts\": [0.5, 0.4]},\n",
    "\n",
    "                # --- Three Layer Focus ---\n",
    "                {\"neurons\": [64, 64, 64], \"dropouts\": [0.4, 0.4, 0.4]},\n",
    "                {\"neurons\": [128, 128, 128], \"dropouts\": [0.5, 0.5, 0.5]},\n",
    "                {\"neurons\": [128, 64, 32], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                {\"neurons\": [256, 128, 64], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                {\"neurons\": [256, 64, 32], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                {\"neurons\": [512, 128, 32], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "            ],\n",
    "\n",
    "            'Medium': [ # For Tg, Density. Balanced complexity.\n",
    "                # --- Two Layer Focus ---\n",
    "                {\"neurons\": [256, 64], \"dropouts\": [0.4, 0.3]},\n",
    "                {\"neurons\": [256, 128], \"dropouts\": [0.4, 0.3]},\n",
    "                {\"neurons\": [512, 64], \"dropouts\": [0.5, 0.3]},\n",
    "                {\"neurons\": [512, 128], \"dropouts\": [0.5, 0.4]}, # Previous winner\n",
    "                {\"neurons\": [512, 256], \"dropouts\": [0.5, 0.4]},\n",
    "                {\"neurons\": [1024, 128], \"dropouts\": [0.5, 0.4]},\n",
    "                {\"neurons\": [1024, 256], \"dropouts\": [0.5, 0.4]},\n",
    "                {\"neurons\": [256, 256], \"dropouts\": [0.4, 0.4]},\n",
    "                {\"neurons\": [512, 512], \"dropouts\": [0.5, 0.5]},\n",
    "\n",
    "                # --- Three Layer Focus ---\n",
    "                {\"neurons\": [256, 128, 64], \"dropouts\": [0.4, 0.3, 0.2]},\n",
    "                {\"neurons\": [512, 128, 64], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                {\"neurons\": [512, 256, 64], \"dropouts\": [0.5, 0.4, 0.2]},\n",
    "                {\"neurons\": [512, 256, 128], \"dropouts\": [0.5, 0.4, 0.3]}, # Previous winner\n",
    "                {\"neurons\": [1024, 256, 64], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                {\"neurons\": [1024, 512, 128], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                {\"neurons\": [1024, 512, 256], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                {\"neurons\": [256, 256, 256], \"dropouts\": [0.4, 0.4, 0.4]},\n",
    "                {\"neurons\": [512, 512, 512], \"dropouts\": [0.5, 0.5, 0.5]},\n",
    "\n",
    "                # --- Four Layer Focus ---\n",
    "                {\"neurons\": [512, 256, 128, 64], \"dropouts\": [0.5, 0.4, 0.3, 0.2]},\n",
    "                {\"neurons\": [1024, 512, 256, 128], \"dropouts\": [0.5, 0.4, 0.3, 0.2]},\n",
    "            ],\n",
    "\n",
    "            'High': [ # For FFV. Exploring width and depth.\n",
    "                # --- Refining Around Winner ([512, 256]) ---\n",
    "                {\"neurons\": [512, 128], \"dropouts\": [0.3, 0.2]},\n",
    "                {\"neurons\": [512, 256], \"dropouts\": [0.3, 0.2]}, # Previous winner\n",
    "                {\"neurons\": [512, 512], \"dropouts\": [0.3, 0.3]},\n",
    "                {\"neurons\": [1024, 256], \"dropouts\": [0.4, 0.3]},\n",
    "                {\"neurons\": [1024, 512], \"dropouts\": [0.4, 0.3]},\n",
    "                {\"neurons\": [1024, 1024], \"dropouts\": [0.4, 0.4]},\n",
    "                {\"neurons\": [2048, 512], \"dropouts\": [0.5, 0.4]},\n",
    "                {\"neurons\": [2048, 1024], \"dropouts\": [0.5, 0.4]},\n",
    "\n",
    "                # --- Three Layer Focus ---\n",
    "                {\"neurons\": [512, 256, 128], \"dropouts\": [0.3, 0.2, 0.2]},\n",
    "                {\"neurons\": [1024, 256, 64], \"dropouts\": [0.4, 0.3, 0.2]},\n",
    "                {\"neurons\": [1024, 512, 128], \"dropouts\": [0.4, 0.3, 0.2]},\n",
    "                {\"neurons\": [1024, 512, 256], \"dropouts\": [0.4, 0.3, 0.2]},\n",
    "                {\"neurons\": [2048, 512, 128], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                {\"neurons\": [2048, 1024, 512], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "                {\"neurons\": [512, 512, 512], \"dropouts\": [0.3, 0.3, 0.3]},\n",
    "                {\"neurons\": [1024, 1024, 1024], \"dropouts\": [0.4, 0.4, 0.4]},\n",
    "\n",
    "                # --- Four+ Layer Focus ---\n",
    "                {\"neurons\": [512, 256, 256, 128], \"dropouts\": [0.3, 0.2, 0.2, 0.1]},\n",
    "                {\"neurons\": [1024, 512, 256, 128], \"dropouts\": [0.4, 0.3, 0.2, 0.2]},\n",
    "                {\"neurons\": [1024, 512, 512, 256], \"dropouts\": [0.4, 0.3, 0.3, 0.2]},\n",
    "                {\"neurons\": [512, 512, 512, 512], \"dropouts\": [0.3, 0.3, 0.3, 0.3]},\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Determine which set of configs to use\n",
    "        if label in ['Rg', 'Tc']:\n",
    "            configs_to_search = search_configs_by_label['Low']\n",
    "            print(\"Using search space for LOW data targets.\")\n",
    "        elif label == 'FFV':\n",
    "            configs_to_search = search_configs_by_label['High']\n",
    "            print(\"Using search space for HIGH data targets.\")\n",
    "        else: # Tg, Density\n",
    "            configs_to_search = search_configs_by_label['Medium']\n",
    "            print(\"Using search space for MEDIUM data targets.\")\n",
    "\n",
    "        results = []\n",
    "        for i, cfg in enumerate(configs_to_search):\n",
    "            print(f\"\\n---> Searching config {i+1}/{len(configs_to_search)}: Neurons={cfg['neurons']}, Dropouts={cfg['dropouts']}\")\n",
    "            model = FeedforwardNet(input_dim, cfg[\"neurons\"], cfg[\"dropouts\"])\n",
    "            model = train_model(model, X_train, X_val, y_train, y_val, verbose=False) # Turn off verbose for cleaner search logs\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_val_np = np.asarray(X_val) if isinstance(X_val, pd.DataFrame) else X_val\n",
    "                device = next(model.parameters()).device\n",
    "                X_val_tensor = torch.tensor(X_val_np, dtype=torch.float32).to(device)\n",
    "                y_pred = model(X_val_tensor).cpu().numpy().flatten()\n",
    "            \n",
    "            val_mae = mean_absolute_error(y_val, y_pred)\n",
    "            print(f\"     Resulting Val MAE: {val_mae:.6f}\")\n",
    "            results.append({\"neurons\": cfg[\"neurons\"], \"dropouts\": cfg[\"dropouts\"], \"val_mae\": val_mae})\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        print(\"\\n--- Neural Network Search Results ---\")\n",
    "        print(df.sort_values(by='val_mae').to_string(index=False))\n",
    "        \n",
    "        df.to_csv(f\"nn_config_validation_mae_{label}.csv\", index=False)\n",
    "        print(f\"\\nSaved results to nn_config_validation_mae_{label}.csv\")\n",
    "        \n",
    "        best_row = df.loc[df['val_mae'].idxmin()]\n",
    "        print(f\"\\nBest config: neurons={best_row['neurons']}, dropouts={best_row['dropouts']}, Validation MAE: {best_row['val_mae']:.6f}\")\n",
    "        \n",
    "        config = best_row.to_dict()\n",
    "        print(\"Re-training best model on the full training data...\")\n",
    "    \n",
    "    else: # If not searching, use a single pre-defined configuration\n",
    "        best_configs = {\n",
    "            \"Tg\":      {\"neurons\": [256, 128, 64], \"dropouts\": [0.4, 0.3, 0.2]},\n",
    "            \"Density\": {\"neurons\": [256, 128, 64], \"dropouts\": [0.4, 0.3, 0.2]},\n",
    "            \"FFV\":     {\"neurons\": [512, 256, 128], \"dropouts\": [0.3, 0.2, 0.2]},\n",
    "            \"Tc\":      {\"neurons\": [128, 64], \"dropouts\": [0.4, 0.3]},\n",
    "            \"Rg\":      {\"neurons\": [128, 64], \"dropouts\": [0.4, 0.3]},\n",
    "        }\n",
    "        config = best_configs.get(label)\n",
    "        print(f\"Using pre-defined best config for {label}: Neurons={config['neurons']}, Dropouts={config['dropouts']}\")\n",
    "\n",
    "    # Final model training\n",
    "    best_model = FeedforwardNet(input_dim, config[\"neurons\"], config[\"dropouts\"])\n",
    "    show_model_summary(best_model, input_dim)\n",
    "    best_model = train_model(best_model, X_train, X_val, y_train, y_val, verbose=True)\n",
    "    return best_model\n",
    "\n",
    "# Utility to set random state everywhere\n",
    "def set_global_random_seed(seed, config=None):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if config is not None:\n",
    "        config.random_state = seed\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def stable_hash(obj, max_value=1_000_000):\n",
    "    \"\"\"\n",
    "    Deterministic hash for objects (e.g. labels).\n",
    "    Always returns the same value across runs/machines.\n",
    "    \"\"\"\n",
    "    # Convert to string and encode\n",
    "    s = str(obj).encode(\"utf-8\")\n",
    "    # Use MD5 (fast & deterministic)\n",
    "    h = hashlib.md5(s).hexdigest()\n",
    "    # Convert hex digest to int and limit range\n",
    "    return int(h, 16) % max_value\n",
    "\n",
    "def train_and_evaluate_models(label, X_main, y_main, splits, nfold, Config):\n",
    "    \"\"\"\n",
    "    Trains models for the given label using the specified configuration.\n",
    "    Returns: models, fold_maes, mean_fold_mae, std_fold_mae\n",
    "    \"\"\"\n",
    "    # Use a prime multiplier for folds\n",
    "    FOLD_PRIME = 9973   # a large prime\n",
    "\n",
    "    models = []\n",
    "    fold_maes = []\n",
    "    mean_fold_mae = None\n",
    "    std_fold_mae = None\n",
    "\n",
    "    # Use stacking only if enabled in config\n",
    "    if getattr(Config, 'use_stacking', False):\n",
    "        Model = train_with_stacking(label, X_main, y_main)\n",
    "        models.append(Model)\n",
    "        save_model(Model, label, 1, Config.model_name)\n",
    "    elif Config.model_name in ['autogluon']:\n",
    "        Model = train_with_autogluon(label, X_main, y_main)\n",
    "        models.append(Model)\n",
    "        # save_model(Model, label, 1, Config.model_name)\n",
    "    else:\n",
    "        for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "            print(f\"\\n--- Fold {fold+1}/{nfold} ---\")\n",
    "            # Set a different random seed for each fold for best possible result\n",
    "            # Use a deterministic but varied seed: base + fold + hash(label)\n",
    "            base_seed = getattr(Config, 'random_state', 42)\n",
    "            label_hash = stable_hash(label)   # replaces abs(hash(label)) % 10000\n",
    "            fold_seed = base_seed + fold * FOLD_PRIME  + label_hash\n",
    "\n",
    "            set_global_random_seed(fold_seed, config=Config)\n",
    "\n",
    "            # Robustly handle both DataFrame and ndarray\n",
    "            if isinstance(X_main, np.ndarray):\n",
    "                X_train, X_val = X_main[train_idx], X_main[val_idx]\n",
    "            else:\n",
    "                X_train, X_val = X_main.iloc[train_idx], X_main.iloc[val_idx]\n",
    "            if isinstance(y_main, np.ndarray):\n",
    "                y_train, y_val = y_main[train_idx], y_main[val_idx]\n",
    "            else:\n",
    "                y_train, y_val = y_main.iloc[train_idx], y_main.iloc[val_idx]\n",
    "\n",
    "            if Config.model_name == 'xgb':\n",
    "                Model = train_with_xgb(label, X_train, y_train, X_val, y_val)\n",
    "            elif Config.model_name in ['catboost', 'lgbm', 'extratrees', 'randomforest', 'balancedrf', 'tabnet', 'hgbm', 'autogluon', 'nn']:\n",
    "                Model = train_with_other_models(Config.model_name, label, X_train, y_train, X_val, y_val)\n",
    "            else:\n",
    "                assert False, \"No model present. Set Config.use_train_with_xgb = True to train a model.\"\n",
    "\n",
    "            # Save model for later holdout prediction\n",
    "            models.append(Model)\n",
    "            save_model(Model, label, fold, Config.model_name)\n",
    "\n",
    "            # Predict on validation set for this fold\n",
    "            if hasattr(Model, 'forward') and not hasattr(Model, 'predict'):\n",
    "                Model.eval()\n",
    "                X_val_np = np.asarray(X_val) if isinstance(X_val, pd.DataFrame) else X_val\n",
    "                device = next(Model.parameters()).device\n",
    "                with torch.no_grad():\n",
    "                    X_val_tensor = torch.tensor(X_val_np, dtype=torch.float32).to(device)\n",
    "                    y_val_pred = Model(X_val_tensor).cpu().numpy().flatten()\n",
    "            else:\n",
    "                y_val_pred = Model.predict(X_val)\n",
    "\n",
    "            fold_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "            print(f\"Fold {fold+1} MAE (on validation set): {fold_mae}\")\n",
    "            fold_maes.append(fold_mae)\n",
    "            # Save y_val, y_val_pred, and residuals in sorted order for each fold\n",
    "            residuals = y_val - y_val_pred\n",
    "            results_df = pd.DataFrame({\n",
    "                'y_val': y_val,\n",
    "                'y_val_pred': y_val_pred,\n",
    "                'residual': residuals\n",
    "            })\n",
    "            results_df = results_df.sort_values(by='residual', ascending=False).reset_index(drop=True)\n",
    "            os.makedirs(f'NeurIPS/fold_residuals/{label}', exist_ok=True)\n",
    "            results_df.to_csv(f'NeurIPS/fold_residuals/{label}/fold_{fold+1}_val_pred_residuals.csv', index=False)\n",
    "        mean_fold_mae = np.mean(fold_maes)\n",
    "        std_fold_mae = np.std(fold_maes)\n",
    "        print(f\"{label} 5-Fold CV mean_absolute_error (on validation sets): {mean_fold_mae} ¬± {std_fold_mae}\")\n",
    "\n",
    "    return models, fold_maes, mean_fold_mae, std_fold_mae\n",
    "\n",
    "def save_feature_selection_info(label, kept_columns, least_important_features, correlated_features_dropped, scaler, X_holdout, y_holdout, median_values):\n",
    "    holdout_dir = f\"NeurIPS/feature_selection/{label}\"\n",
    "    os.makedirs(holdout_dir, exist_ok=True)\n",
    "    feature_info = {\n",
    "        \"kept_columns\": list(kept_columns),\n",
    "        \"least_important_features\": list(least_important_features),\n",
    "        \"correlated_features_dropped\": list(correlated_features_dropped),\n",
    "    }\n",
    "    # Save median_values if provided\n",
    "    if median_values is not None:\n",
    "        if hasattr(median_values, 'to_dict'):\n",
    "            feature_info[\"median_values\"] = median_values.to_dict()\n",
    "        else:\n",
    "            feature_info[\"median_values\"] = median_values\n",
    "\n",
    "    # Save X_holdout and y_holdout for this label\n",
    "    X_holdout_path = os.path.join(holdout_dir, \"X_holdout.csv\")\n",
    "    y_holdout_path = os.path.join(holdout_dir, \"y_holdout.csv\")\n",
    "    pd.DataFrame(X_holdout).to_csv(X_holdout_path, index=False)\n",
    "    pd.DataFrame({\"y_holdout\": y_holdout}).to_csv(y_holdout_path, index=False)\n",
    "\n",
    "    feature_info_path = os.path.join(holdout_dir, f\"{label}_feature_info.json\")\n",
    "    with open(feature_info_path, \"w\") as f:\n",
    "        json.dump(feature_info, f, indent=2)\n",
    "\n",
    "    # Save scaler object\n",
    "    scaler_path = os.path.join(holdout_dir, \"scaler.joblib\")\n",
    "    if scaler is not None:\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "\n",
    "def load_feature_selection_info(label, base_dir):\n",
    "    \"\"\"\n",
    "    Loads feature selection info saved by save_feature_selection_info for a given label.\n",
    "    Returns a dict with keys: kept_columns, least_important_features, correlated_features_dropped, scaler, X_holdout, y_holdout.\n",
    "    \"\"\"\n",
    "\n",
    "    holdout_dir = os.path.join(base_dir, f\"NeurIPS/feature_selection/{label}\")\n",
    "    feature_info_path = os.path.join(holdout_dir, f\"{label}_feature_info.json\")\n",
    "    X_holdout_path = os.path.join(holdout_dir, \"X_holdout.csv\")\n",
    "    y_holdout_path = os.path.join(holdout_dir, \"y_holdout.csv\")\n",
    "\n",
    "    if not os.path.exists(feature_info_path):\n",
    "        raise FileNotFoundError(f\"Feature info file not found: {feature_info_path}\")\n",
    "\n",
    "    with open(feature_info_path, \"r\") as f:\n",
    "        feature_info = json.load(f)\n",
    "\n",
    "    X_holdout = pd.read_csv(X_holdout_path)\n",
    "    y_holdout = pd.read_csv(y_holdout_path)[\"y_holdout\"].values\n",
    "\n",
    "    # Note: scaler is not restored as an object (only its params or type string is saved)\n",
    "    # If you need the actual scaler object, you must save it with joblib or pickle\n",
    "\n",
    "    # Load scaler object\n",
    "    scaler_path = os.path.join(holdout_dir, \"scaler.joblib\")\n",
    "    if os.path.exists(scaler_path):\n",
    "        scaler = joblib.load(scaler_path)\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    # Try to load median_values if present in feature_info, else set to empty Series\n",
    "    if \"median_values\" in feature_info:\n",
    "        median_values = pd.Series(feature_info[\"median_values\"])\n",
    "    else:\n",
    "        median_values = pd.Series(dtype=float)\n",
    "    return {\n",
    "        \"kept_columns\": feature_info.get(\"kept_columns\", []),\n",
    "        \"least_important_features\": feature_info.get(\"least_important_features\", []),\n",
    "        \"correlated_features_dropped\": feature_info.get(\"correlated_features_dropped\", []),\n",
    "        \"X_holdout\": X_holdout,\n",
    "        \"y_holdout\": y_holdout,\n",
    "        \"scaler\": scaler,\n",
    "        \"median_values\": median_values\n",
    "    }\n",
    "\n",
    "def load_models_for_label(label, models_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Loads all models for a given label from the specified directory.\n",
    "    Model filenames must start with the label (e.g., 'Tg_fold1_xgb.joblib').\n",
    "    Returns a list of loaded models.\n",
    "    \"\"\"\n",
    "\n",
    "    models = []\n",
    "    if not os.path.exists(models_dir):\n",
    "        print(f\"Models directory '{models_dir}' does not exist.\")\n",
    "        return models\n",
    "\n",
    "    # Match both .joblib and .pt (for torch) files\n",
    "    pattern_joblib = os.path.join(models_dir, f\"{label}_*.joblib\")\n",
    "    pattern_pt = os.path.join(models_dir, f\"{label}_*.pt\")\n",
    "    model_files = glob.glob(pattern_joblib) + glob.glob(pattern_pt)\n",
    "    if not model_files:\n",
    "        print(f\"No models found for label '{label}' in '{models_dir}'.\")\n",
    "        return models\n",
    "\n",
    "    for model_file in sorted(model_files):\n",
    "        if model_file.endswith(\".joblib\"):\n",
    "            try:\n",
    "                model = joblib.load(model_file)\n",
    "                models.append(model)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load model {model_file}: {e}\")\n",
    "        elif model_file.endswith(\".pt\"):\n",
    "            # Torch model loading requires model class and architecture\n",
    "            print(f\"Skipping torch model {model_file} (requires model class definition).\")\n",
    "            # You can implement torch loading here if needed\n",
    "    print(f\"Loaded {len(models)} models for label '{label}'.\")\n",
    "    return models\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_ids\n",
    "})\n",
    "\n",
    "# --- Store and display mean_absolute_error for each label ---\n",
    "mae_results = []\n",
    "\n",
    "def prepare_label_data(label, subtables, Config):\n",
    "    print(f\"Processing label: {label}\")\n",
    "    print(subtables[label].head())\n",
    "    print(subtables[label].shape)\n",
    "    original_smiles = subtables[label]['SMILES'].tolist()\n",
    "    original_labels = subtables[label][label].values\n",
    "\n",
    "    # Canonicalize SMILES and deduplicate at molecule level before augmentation\n",
    "    canonical_smiles = [get_canonical_smiles(s) for s in original_smiles]\n",
    "    smiles_label_df = pd.DataFrame({\n",
    "        'SMILES': canonical_smiles,\n",
    "        'label': original_labels\n",
    "    })\n",
    "    before_dedup = len(smiles_label_df)\n",
    "    smiles_label_df = smiles_label_df.drop_duplicates(subset=['SMILES'], keep='first').reset_index(drop=True)\n",
    "    after_dedup = len(smiles_label_df)\n",
    "    num_dropped = before_dedup - after_dedup\n",
    "    print(f\"Dropped {num_dropped} duplicate SMILES rows for {label} before augmentation.\")\n",
    "    original_smiles = smiles_label_df['SMILES'].tolist()\n",
    "    original_labels = smiles_label_df['label'].values\n",
    "\n",
    "    if Config.use_augmentation and not Config.debug:\n",
    "        print(f\"SMILES before augmentation: {len(original_smiles)}\")\n",
    "        smiles_aug, labels_aug = augment_smiles_dataset(original_smiles, original_labels, num_augments=1)\n",
    "        print(f\"SMILES after augmentation: {len(smiles_aug)} (increase: {len(smiles_aug) - len(original_smiles)})\")\n",
    "        original_smiles, original_labels = smiles_aug, labels_aug\n",
    "\n",
    "    # After augmentation, deduplicate again at molecule level (canonical SMILES)\n",
    "    canonical_smiles_aug = [get_canonical_smiles(s) for s in original_smiles]\n",
    "    smiles_label_aug_df = pd.DataFrame({\n",
    "        'SMILES': canonical_smiles_aug,\n",
    "        'label': original_labels\n",
    "    })\n",
    "    smiles_label_aug_df = smiles_label_aug_df.drop_duplicates(subset=['SMILES'], keep='first').reset_index(drop=True)\n",
    "    original_smiles = smiles_label_aug_df['SMILES'].tolist()\n",
    "    original_labels = smiles_label_aug_df['label'].values\n",
    "\n",
    "    fp_df, descriptor_df, valid_smiles, invalid_indices = smiles_to_combined_fingerprints_with_descriptors(original_smiles)\n",
    "\n",
    "    print(f\"Invalid indices for {label}: {invalid_indices}\")\n",
    "    y = np.delete(original_labels, invalid_indices)\n",
    "    print(fp_df.shape)\n",
    "    fp_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if not descriptor_df.empty:\n",
    "        X = pd.DataFrame(descriptor_df)\n",
    "        X, kept_columns, scaler, median_values = preprocess_numerical_features(X, label)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        if not fp_df.empty:\n",
    "            X = pd.concat([X, fp_df], axis=1)\n",
    "    else:\n",
    "        kept_columns = []\n",
    "        scaler = None\n",
    "        X = fp_df\n",
    "\n",
    "    # Remove duplicate rows in X and corresponding values in y (feature-level duplicates)\n",
    "    X_dup = X.duplicated(keep='first')\n",
    "    if X_dup.any():\n",
    "        print(f\"Found {X_dup.sum()} duplicate rows in X for {label}, removing them.\")\n",
    "        X = X[~X_dup]\n",
    "        y = y[~X_dup]\n",
    "    print(f\"After concat: {X.shape}\")\n",
    "    # Fill NaN in train with median from train\n",
    "    # Only fill NaN with median if using neural network\n",
    "    if Config.model_name == 'nn':\n",
    "        X = X.fillna(median_values)\n",
    "    check_inf_nan(X, y, label)\n",
    "\n",
    "    # display_outlier_summary(y, X=X, name=label, z_thresh=3, iqr_factor=1.5, iso_contamination=0.01, lof_contamination=0.01)\n",
    "\n",
    "    # Drop least important features from X and test\n",
    "\n",
    "    least_important_features = []\n",
    "    if getattr(Config, 'use_least_important_features_all_methods', False):\n",
    "        for i in range(4):\n",
    "            print(f\"Iteration {i+1} for least important feature removal on {label}\")\n",
    "            least_important_feature = get_least_important_features_all_methods(X, y, label, model_name=Config.model_name)\n",
    "            least_important_features.extend(least_important_feature)\n",
    "            if len(least_important_feature) > 0:\n",
    "                print(f\"label: {label} Dropping least important features: {least_important_feature}\")\n",
    "                X = X.drop(columns=least_important_feature)\n",
    "                print(f\"After dropping least important features: {X.shape}\")\n",
    "\n",
    "    check_inf_nan(X, y, label)\n",
    "    # Drop highly correlated features using label-specific correlation threshold from Config\n",
    "    correlation_threshold = Config.correlation_thresholds.get(label, 1.0)\n",
    "    if correlation_threshold < 1.0:\n",
    "        X, correlated_features_dropped = drop_correlated_features(pd.DataFrame(X), threshold=correlation_threshold)\n",
    "    else:\n",
    "        correlated_features_dropped = []\n",
    "\n",
    "    print(f\"After correlation cut (threshold={correlation_threshold}): {X.shape}, dropped columns: {correlated_features_dropped}\")\n",
    "    print(f\"After dropping correlated features: {X.shape}\")\n",
    "    check_inf_nan(X, y, label)\n",
    "\n",
    "    if getattr(Config, 'use_variance_threshold', False):\n",
    "        threshold = 0.01\n",
    "        selector = VarianceThreshold(threshold=threshold)\n",
    "        X_sel = selector.fit_transform(X)\n",
    "        # Get mask of selected features\n",
    "        selected_cols_variance = X.columns[selector.get_support()]\n",
    "        # Convert back to DataFrame with column names\n",
    "        X = pd.DataFrame(X_sel, columns=selected_cols_variance, index=X.index)\n",
    "        print(f\"After variance cut: {X.shape}\")\n",
    "        print(f'Type of X: {type(X)}')\n",
    "\n",
    "    if Config.add_gaussian and not Config.debug:\n",
    "        n_samples = 1000\n",
    "        X, y = augment_dataset(X, y, n_samples=n_samples)\n",
    "        print(f\"After augment cut: {X.shape}\")\n",
    "\n",
    "    # --- Hold out 10% for final MAE calculation ---\n",
    "    # X_main, X_holdout, y_main, y_holdout = train_test_split(X, y, test_size=0.1, random_state=Config.random_state)\n",
    "    # Bin y for stratification\n",
    "    y_bins = pd.qcut(y, q=5, duplicates='drop', labels=False)\n",
    "    X_main, X_holdout, y_main, y_holdout = train_test_split(X, y, test_size=0.10, random_state=Config.random_state, stratify=y_bins)\n",
    "    if Config.useAllDataForTraining == True:\n",
    "        X_main = X\n",
    "        y_main = y\n",
    "    # --- Optionally apply PCA ---\n",
    "    if getattr(Config, 'use_pca', False):\n",
    "        X_main, X_holdout, pca = apply_pca(X_main, X_holdout, verbose=True)\n",
    "    else:\n",
    "        pca = None\n",
    "\n",
    "    # --- Cross-Validation or Single Split (for speed) ---\n",
    "    fold_maes = []\n",
    "    test_preds = []\n",
    "    val_preds = np.zeros(len(y_main))\n",
    "    if getattr(Config, 'use_cross_validation', True):\n",
    "        nfold = 10\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=Config.random_state)\n",
    "        # For regression, bin y_main for stratification\n",
    "        y_bins = pd.qcut(y_main, q=nfold, duplicates='drop', labels=False)\n",
    "        splits = skf.split(X_main, y_bins)\n",
    "    else:\n",
    "        # Use a single split: 80% train, 20% val\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            np.arange(len(X_main)), test_size=0.2, random_state=Config.random_state\n",
    "        )\n",
    "        splits = [(train_idx, val_idx)]\n",
    "        nfold = 1\n",
    "\n",
    "    return {\n",
    "        \"X_main\": X_main,\n",
    "        \"X_holdout\": X_holdout,\n",
    "        \"y_main\": y_main,\n",
    "        \"y_holdout\": y_holdout,\n",
    "        \"kept_columns\": kept_columns,\n",
    "        \"scaler\": scaler,\n",
    "        \"median_values\": median_values,\n",
    "        \"least_important_features\": least_important_features,\n",
    "        \"correlated_features_dropped\": correlated_features_dropped,\n",
    "        \"selector\": selector if getattr(Config, 'use_variance_threshold', False) else None,\n",
    "        \"selected_cols_variance\": selected_cols_variance if getattr(Config, 'use_variance_threshold', False) else None,\n",
    "        \"pca\": pca,\n",
    "        \"splits\": splits,\n",
    "        \"nfold\": nfold,\n",
    "        \"fold_maes\": fold_maes,\n",
    "        \"test_preds\": test_preds,\n",
    "        \"val_preds\": val_preds\n",
    "    }\n",
    "\n",
    "def load_label_data(label, model_dir=None):\n",
    "    if model_dir is not None:\n",
    "        # Load model and data for the specified label\n",
    "        model_path = os.path.join(model_dir, f\"{label}_model.pkl\")\n",
    "        data_path = os.path.join(model_dir, f\"{label}_data.pkl\")\n",
    "        model = joblib.load(model_path)\n",
    "        data = joblib.load(data_path)\n",
    "        return model, data\n",
    "    return None, None\n",
    "\n",
    "def train_or_predict(train_model=True, model_dir=None):\n",
    "    for label in labels:\n",
    "        if train_model:\n",
    "            print(f\"\\n=== Training/Predicting for label: {label} ===\")\n",
    "            label_data = prepare_label_data(label, subtables, config)\n",
    "            X_main = label_data[\"X_main\"]\n",
    "            X_holdout = label_data[\"X_holdout\"]\n",
    "            y_main = label_data[\"y_main\"]\n",
    "            y_holdout = label_data[\"y_holdout\"]\n",
    "            kept_columns = label_data[\"kept_columns\"]\n",
    "            scaler = label_data[\"scaler\"]\n",
    "            median_values = label_data[\"median_values\"]\n",
    "            least_important_features = label_data[\"least_important_features\"]\n",
    "            correlated_features_dropped = label_data[\"correlated_features_dropped\"]\n",
    "            selector = label_data[\"selector\"]\n",
    "            selected_cols_variance = label_data[\"selected_cols_variance\"]\n",
    "            pca = label_data[\"pca\"]\n",
    "            splits = label_data[\"splits\"]\n",
    "            nfold = label_data[\"nfold\"]\n",
    "            fold_maes = label_data[\"fold_maes\"]\n",
    "            test_preds = label_data[\"test_preds\"]\n",
    "            val_preds = label_data[\"val_preds\"]\n",
    "\n",
    "            # --- Save feature selection info for this label ---\n",
    "            save_feature_selection_info(label, kept_columns, least_important_features, correlated_features_dropped, scaler, X_holdout, y_holdout, median_values)  \n",
    "            os.makedirs('models', exist_ok=True)\n",
    "            models = []\n",
    "\n",
    "            # labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "            # --- Hyperparameter tuning for this label ---\n",
    "            if Config.enable_param_tuning:\n",
    "                if label == 'Tg':\n",
    "                    db_path = f\"xgb_tuning_{label}.db\"\n",
    "                    init_xgb_tuning_db(db_path)\n",
    "                    # Example param_grid (customize as needed)\n",
    "                    param_grid = {\n",
    "                        'n_estimators': [3000],\n",
    "                        'max_depth': [4, 5, 6, 7],\n",
    "                        'learning_rate': [0.001, 0.01, 0.06, 0.1],\n",
    "                        'subsample': [0.6, 0.8, 1.0],\n",
    "                        'colsample_bytree': [0.8, 1.0],\n",
    "                        'gamma': [0, 0.1],\n",
    "                        'reg_lambda': [1.0, 5.0, 10.0],\n",
    "                        'early_stopping_rounds': [50],\n",
    "                        'objective': [\"reg:squarederror\"],\n",
    "                        'eval_metric': [\"rmse\"]\n",
    "                    }\n",
    "                    # You must define X_train and y_train for tuning here\n",
    "                    xgb_grid_search_with_db(X_main, y_main, param_grid, db_path=db_path)\n",
    "                else:\n",
    "                    continue\n",
    "            # FIXME save features_keep\n",
    "            # Ensure only features present in X_main are kept\n",
    "\n",
    "            # if label in ['Tg', 'Tc', 'Density', 'Rg']:\n",
    "            #     features_keep = select_features_with_lasso(X_main, y_main, label)\n",
    "            #     X_main = X_main[features_keep]\n",
    "            models, fold_maes, mean_fold_mae, std_fold_mae = train_and_evaluate_models(label, X_main, y_main, splits, nfold, Config)\n",
    "        else:\n",
    "            print(f\"\\n=== Loading models and data for label: {label} ===\")\n",
    "            mean_fold_mae, std_fold_mae = None, None\n",
    "            feature_info = load_feature_selection_info(label, model_dir)\n",
    "            kept_columns = feature_info[\"kept_columns\"]\n",
    "            least_important_features = feature_info[\"least_important_features\"]\n",
    "            correlated_features_dropped = feature_info[\"correlated_features_dropped\"]\n",
    "            scaler = feature_info.get(\"scaler\", None)\n",
    "            X_holdout = feature_info[\"X_holdout\"]\n",
    "            y_holdout = feature_info[\"y_holdout\"]\n",
    "            median_values = feature_info[\"median_values\"]\n",
    "            selector = None\n",
    "            selected_cols_variance = None\n",
    "            pca = None\n",
    "\n",
    "            models = load_models_for_label(label, os.path.join(model_dir, 'models'))\n",
    "            test_preds = []\n",
    "\n",
    "        # Prepare test set once\n",
    "        fp_df, descriptor_df, valid_smiles, invalid_indices = smiles_to_combined_fingerprints_with_descriptors(test_smiles)\n",
    "        # median_values = label_data[\"median_values\"]\n",
    "        if not descriptor_df.empty:\n",
    "            # Safely align test columns to training columns. \n",
    "            # This adds any missing columns and fills them with NaN.\n",
    "            descriptor_df = descriptor_df.reindex(columns=kept_columns)\n",
    "            if Config.model_name == 'nn':\n",
    "                # Fill NaN in test with median from train\n",
    "                descriptor_df = descriptor_df.fillna(median_values)\n",
    "            # Scale test set using the same scaler and kept_columns, then convert to DataFrame\n",
    "            if getattr(Config, 'use_standard_scaler', False) and scaler is not None:\n",
    "                descriptor_df = pd.DataFrame(scaler.transform(descriptor_df), columns=kept_columns, index=descriptor_df.index)\n",
    "            descriptor_df.reset_index(drop=True, inplace=True)\n",
    "            if not fp_df.empty:\n",
    "                fp_df = fp_df.reset_index(drop=True)\n",
    "                test = pd.concat([descriptor_df, fp_df], axis=1)\n",
    "            else:\n",
    "                test = descriptor_df\n",
    "        else:\n",
    "            test = fp_df\n",
    "\n",
    "        if len(least_important_features) > 0:\n",
    "            test = test.drop(columns=least_important_features)\n",
    "        if len(correlated_features_dropped) > 0:\n",
    "            print(f\"Dropping correlated columns from test: {correlated_features_dropped}\")\n",
    "            test = test.drop(correlated_features_dropped, axis=1, errors='ignore')\n",
    "        if getattr(Config, 'use_variance_threshold', False):\n",
    "            test_sel = selector.transform(test)\n",
    "            # Convert back to DataFrame\n",
    "            test = pd.DataFrame(test_sel, columns=selected_cols_variance, index=test.index)\n",
    "        # Optionally apply PCA to test set if enabled\n",
    "        if getattr(Config, 'use_pca', False):\n",
    "            test = pca.transform(test)\n",
    "        # if label in ['Tg', 'Tc', 'Density', 'Rg']:\n",
    "        #     X_holdout = X_holdout[features_keep]\n",
    "        #     test = test[features_keep]\n",
    "        # --- Holdout set evaluation with all trained models ---\n",
    "        holdout_maes = []\n",
    "        for i, Model in enumerate(models):\n",
    "            is_torch_model = hasattr(Model, 'forward') and not hasattr(Model, 'predict')\n",
    "\n",
    "            if is_torch_model:\n",
    "                Model.eval()\n",
    "                X_holdout_np = np.asarray(X_holdout) if isinstance(X_holdout, pd.DataFrame) else X_holdout\n",
    "                test_np = np.asarray(test) if isinstance(test, pd.DataFrame) else test\n",
    "                device = next(Model.parameters()).device\n",
    "                with torch.no_grad():\n",
    "                    X_holdout_tensor = torch.tensor(X_holdout_np, dtype=torch.float32).to(device)\n",
    "                    test_tensor = torch.tensor(test_np, dtype=torch.float32).to(device)\n",
    "                    y_holdout_pred = Model(X_holdout_tensor).detach().cpu().numpy().flatten()\n",
    "                    y_test_pred = Model(test_tensor).detach().cpu().numpy().flatten()\n",
    "            else:\n",
    "                y_holdout_pred = Model.predict(X_holdout)\n",
    "                y_test_pred = Model.predict(test)\n",
    "\n",
    "            holdout_mae = mean_absolute_error(y_holdout, y_holdout_pred)\n",
    "            print(f\"Model {i+1} holdout MAE: {holdout_mae}\")\n",
    "            holdout_maes.append(holdout_mae)\n",
    "\n",
    "            if isinstance(y_test_pred, pd.Series):\n",
    "                y_test_pred = y_test_pred.values.flatten()\n",
    "            else:\n",
    "                y_test_pred = y_test_pred.flatten()        \n",
    "            test_preds.append(y_test_pred)\n",
    "\n",
    "        mean_holdout_mae = np.mean(holdout_maes)\n",
    "        std_holdout_mae = np.std(holdout_maes)\n",
    "        print(f\"{label} Holdout MAE (mean ¬± std over all models): {mean_holdout_mae:.5f} ¬± {std_holdout_mae:.5f}\")\n",
    "\n",
    "        mae_results.append({\n",
    "            'label': label,\n",
    "            'fold_mae_mean': mean_fold_mae,\n",
    "            'fold_mae_std': std_fold_mae,\n",
    "            'holdout_mae_mean': mean_holdout_mae,\n",
    "            'holdout_mae_std': std_holdout_mae\n",
    "        })\n",
    "\n",
    "        # Average test predictions across folds\n",
    "        test_preds = np.array(test_preds)\n",
    "        y_pred = np.mean(test_preds, axis=0)\n",
    "        print(y_pred)\n",
    "        new_column_name = label\n",
    "        output_df[new_column_name] = y_pred\n",
    "\n",
    "    # Save MAE results to CSV and display\n",
    "    mae_df = pd.DataFrame(mae_results)\n",
    "    mae_df.to_csv('NeurIPS/mae_results.csv', index=False)\n",
    "    print(\"\\nMean Absolute Error for each label:\")\n",
    "    print(mae_df)\n",
    "\n",
    "# train_or_predict()\n",
    "\n",
    "# output_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "# output_df = pd.DataFrame({\n",
    "#     'id': test_ids\n",
    "# })\n",
    "# MODEL_DIR1 = '/kaggle/input/neurips-2025/nn_v4'\n",
    "# train_or_predict(train_model=False, model_dir=MODEL_DIR1)\n",
    "# # output_dfs.append(train_or_predict(output_df, train_model=False, model_dir=MODEL_DIR1))\n",
    "# print(output_df)\n",
    "# output_dfs.append(output_df.copy())\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_ids\n",
    "})\n",
    "MODEL_DIR1 = '/kaggle/input/neurips-2025/xgb_v3'\n",
    "train_or_predict(train_model=False, model_dir=MODEL_DIR1)\n",
    "print(output_df)\n",
    "output_dfs.append(output_df.copy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b0c6d",
   "metadata": {
    "papermill": {
     "duration": 0.009283,
     "end_time": "2025-09-14T21:53:54.719059",
     "exception": false,
     "start_time": "2025-09-14T21:53:54.709776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7ea6e84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T21:53:54.738710Z",
     "iopub.status.busy": "2025-09-14T21:53:54.738476Z",
     "iopub.status.idle": "2025-09-14T21:53:58.782784Z",
     "shell.execute_reply": "2025-09-14T21:53:58.781984Z"
    },
    "papermill": {
     "duration": 4.055554,
     "end_time": "2025-09-14T21:53:58.784114",
     "exception": false,
     "start_time": "2025-09-14T21:53:54.728560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/torch-geometric-2-6-1/torch_geometric-2.6.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.12.13)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (2025.5.1)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.1.6)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (1.26.4)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (7.0.0)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (3.0.9)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (2.32.4)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.6.1) (4.67.1)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (6.6.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.6.1) (1.20.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.6.1) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric==2.6.1) (2.4.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.6.1) (2025.6.15)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric==2.6.1) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric==2.6.1) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric==2.6.1) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric==2.6.1) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric==2.6.1) (2024.2.0)\r\n",
      "Installing collected packages: torch-geometric\r\n",
      "Successfully installed torch-geometric-2.6.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/torch-geometric-2-6-1/torch_geometric-2.6.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "253ee672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T21:53:58.806951Z",
     "iopub.status.busy": "2025-09-14T21:53:58.806522Z",
     "iopub.status.idle": "2025-09-14T21:54:32.879822Z",
     "shell.execute_reply": "2025-09-14T21:54:32.878961Z"
    },
    "papermill": {
     "duration": 34.086712,
     "end_time": "2025-09-14T21:54:32.880892",
     "exception": false,
     "start_time": "2025-09-14T21:53:58.794180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading competition data...\n",
      "Training data shape: (7973, 7), Test data shape: (3, 2)\n",
      "Cleaning and validating SMILES...\n",
      "   Removed 0 invalid SMILES from training data\n",
      "   Removed 0 invalid SMILES from test data\n",
      "   Final training samples: 7973\n",
      "   Final test samples: 3\n",
      "\n",
      "üìÇ Loading external datasets...\n",
      "   ‚úÖ Tc data: 874 samples\n",
      "   ‚ö†Ô∏è TgSS enriched data failed: [Errno 2] No such file or directory: '/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleane\n",
      "   ‚úÖ JCIM Tg data: 662 samples\n",
      "   ‚úÖ Xlsx Tg data: 501 samples\n",
      "   ‚úÖ Density data: 786 samples\n",
      "   ‚úÖ dataset 4: 862 samples\n",
      "\n",
      "üîÑ Integrating external data...\n",
      "   Processing Tc data...\n",
      "      Processing 874 Tc samples...\n",
      "      Kept 874/874 valid samples\n",
      "      Tc: +129 samples, +129 unique SMILES\n",
      "      Filled 0 missing entries in train for Tc\n",
      "      Added 129 new entries for Tc\n",
      "   Processing Tg data...\n",
      "      Processing 662 Tg samples...\n",
      "      Kept 662/662 valid samples\n",
      "      Tg: +151 samples, +136 unique SMILES\n",
      "      Filled 15 missing entries in train for Tg\n",
      "      Added 136 new entries for Tg\n",
      "   Processing Tg data...\n",
      "      Processing 501 Tg samples...\n",
      "      Kept 501/501 valid samples\n",
      "      Tg: +499 samples, +499 unique SMILES\n",
      "      Filled 0 missing entries in train for Tg\n",
      "      Added 499 new entries for Tg\n",
      "   Processing Density data...\n",
      "      Processing 786 Density samples...\n",
      "      Kept 780/786 valid samples\n",
      "      Density: +634 samples, +524 unique SMILES\n",
      "      Filled 110 missing entries in train for Density\n",
      "      Added 524 new entries for Density\n",
      "   Processing FFV data...\n",
      "      Processing 862 FFV samples...\n",
      "      Kept 862/862 valid samples\n",
      "      FFV: +862 samples, +819 unique SMILES\n",
      "      Filled 43 missing entries in train for FFV\n",
      "      Added 819 new entries for FFV\n",
      "\n",
      "üìä Final training data:\n",
      "   Original samples: 7973\n",
      "   Extended samples: 10080\n",
      "   Gain: +2107 samples\n",
      "   Tg: 1,161 samples (+650)\n",
      "   FFV: 7,892 samples (+862)\n",
      "   Tc: 866 samples (+129)\n",
      "   Density: 1,247 samples (+634)\n",
      "   Rg: 614 samples (+0)\n",
      "\n",
      "‚úÖ Data integration complete with clean SMILES!\n",
      "\n",
      "==================== Processing GNN for label: Tg ====================\n",
      "Using MLP Config: Neurons=[512, 256, 128], Dropouts=[0.5, 0.4, 0.2]\n",
      "Loading 10 models and ALL 3 RobustScalers for Tg ensemble...\n",
      "Loaded Y, U, and X RobustScalers.\n",
      "Successfully loaded saved model for Tg_fold0 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold0.pth\n",
      "Successfully loaded saved model for Tg_fold1 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold1.pth\n",
      "Successfully loaded saved model for Tg_fold2 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold2.pth\n",
      "Successfully loaded saved model for Tg_fold3 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold3.pth\n",
      "Successfully loaded saved model for Tg_fold4 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold4.pth\n",
      "Successfully loaded saved model for Tg_fold5 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold5.pth\n",
      "Successfully loaded saved model for Tg_fold6 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold6.pth\n",
      "Successfully loaded saved model for Tg_fold7 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold7.pth\n",
      "Successfully loaded saved model for Tg_fold8 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold8.pth\n",
      "Successfully loaded saved model for Tg_fold9 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tg_fold9.pth\n",
      "Successfully loaded 10 models for ensemble.\n",
      "Making ensemble (scaled) predictions for Tg using 10 models...\n",
      "\n",
      "==================== Processing GNN for label: FFV ====================\n",
      "Using MLP Config: Neurons=[1024, 512, 64], Dropouts=[0.6, 0.5, 0.4]\n",
      "Loading 10 models and ALL 3 RobustScalers for FFV ensemble...\n",
      "Loaded Y, U, and X RobustScalers.\n",
      "Successfully loaded saved model for FFV_fold0 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold0.pth\n",
      "Successfully loaded saved model for FFV_fold1 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold1.pth\n",
      "Successfully loaded saved model for FFV_fold2 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold2.pth\n",
      "Successfully loaded saved model for FFV_fold3 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold3.pth\n",
      "Successfully loaded saved model for FFV_fold4 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold4.pth\n",
      "Successfully loaded saved model for FFV_fold5 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold5.pth\n",
      "Successfully loaded saved model for FFV_fold6 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold6.pth\n",
      "Successfully loaded saved model for FFV_fold7 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold7.pth\n",
      "Successfully loaded saved model for FFV_fold8 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold8.pth\n",
      "Successfully loaded saved model for FFV_fold9 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_FFV_fold9.pth\n",
      "Successfully loaded 10 models for ensemble.\n",
      "Making ensemble (scaled) predictions for FFV using 10 models...\n",
      "\n",
      "==================== Processing GNN for label: Tc ====================\n",
      "Using MLP Config: Neurons=[128, 64], Dropouts=[0.4, 0.3]\n",
      "Loading 10 models and ALL 3 RobustScalers for Tc ensemble...\n",
      "Loaded Y, U, and X RobustScalers.\n",
      "Successfully loaded saved model for Tc_fold0 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold0.pth\n",
      "Successfully loaded saved model for Tc_fold1 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold1.pth\n",
      "Successfully loaded saved model for Tc_fold2 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold2.pth\n",
      "Successfully loaded saved model for Tc_fold3 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold3.pth\n",
      "Successfully loaded saved model for Tc_fold4 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold4.pth\n",
      "Successfully loaded saved model for Tc_fold5 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold5.pth\n",
      "Successfully loaded saved model for Tc_fold6 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold6.pth\n",
      "Successfully loaded saved model for Tc_fold7 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold7.pth\n",
      "Successfully loaded saved model for Tc_fold8 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold8.pth\n",
      "Successfully loaded saved model for Tc_fold9 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Tc_fold9.pth\n",
      "Successfully loaded 10 models for ensemble.\n",
      "Making ensemble (scaled) predictions for Tc using 10 models...\n",
      "\n",
      "==================== Processing GNN for label: Density ====================\n",
      "Using MLP Config: Neurons=[1024, 256, 64], Dropouts=[0.5, 0.4, 0.3]\n",
      "Loading 10 models and ALL 3 RobustScalers for Density ensemble...\n",
      "Loaded Y, U, and X RobustScalers.\n",
      "Successfully loaded saved model for Density_fold0 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold0.pth\n",
      "Successfully loaded saved model for Density_fold1 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold1.pth\n",
      "Successfully loaded saved model for Density_fold2 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold2.pth\n",
      "Successfully loaded saved model for Density_fold3 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold3.pth\n",
      "Successfully loaded saved model for Density_fold4 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold4.pth\n",
      "Successfully loaded saved model for Density_fold5 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold5.pth\n",
      "Successfully loaded saved model for Density_fold6 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold6.pth\n",
      "Successfully loaded saved model for Density_fold7 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold7.pth\n",
      "Successfully loaded saved model for Density_fold8 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold8.pth\n",
      "Successfully loaded saved model for Density_fold9 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Density_fold9.pth\n",
      "Successfully loaded 10 models for ensemble.\n",
      "Making ensemble (scaled) predictions for Density using 10 models...\n",
      "\n",
      "==================== Processing GNN for label: Rg ====================\n",
      "Using MLP Config: Neurons=[128, 64, 64], Dropouts=[0.4, 0.3, 0.3]\n",
      "Loading 10 models and ALL 3 RobustScalers for Rg ensemble...\n",
      "Loaded Y, U, and X RobustScalers.\n",
      "Successfully loaded saved model for Rg_fold0 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold0.pth\n",
      "Successfully loaded saved model for Rg_fold1 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold1.pth\n",
      "Successfully loaded saved model for Rg_fold2 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold2.pth\n",
      "Successfully loaded saved model for Rg_fold3 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold3.pth\n",
      "Successfully loaded saved model for Rg_fold4 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold4.pth\n",
      "Successfully loaded saved model for Rg_fold5 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold5.pth\n",
      "Successfully loaded saved model for Rg_fold6 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold6.pth\n",
      "Successfully loaded saved model for Rg_fold7 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold7.pth\n",
      "Successfully loaded saved model for Rg_fold8 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold8.pth\n",
      "Successfully loaded saved model for Rg_fold9 from /kaggle/input/neurips-2025/GATConv_v29/models/gnn/gnn_model_Rg_fold9.pth\n",
      "Successfully loaded 10 models for ensemble.\n",
      "Making ensemble (scaled) predictions for Rg using 10 models...\n",
      "\n",
      "‚úÖ GNN Ensemble predictions (Original Scale) saved to submission_hybrid_gnn_final.csv\n",
      "\n",
      "GNN Submission Preview:\n",
      "           id          Tg       FFV        Tc   Density         Rg\n",
      "0  1109053969  189.751849  0.373679  0.187229  1.168659  21.798934\n",
      "1  1422188626  174.595003  0.375173  0.266134  1.103980  23.134463\n",
      "2  2032016830  111.029739  0.350787  0.250217  1.113623  19.598631\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import GCNConv, GINEConv, global_mean_pool, global_max_pool\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import json\n",
    "import torch\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "\n",
    "RDKIT_AVAILABLE = True\n",
    "TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "os.makedirs(\"NeurIPS\", exist_ok=True)\n",
    "class Config:\n",
    "    debug = False\n",
    "    use_cross_validation = True  # Set to False to use a single split for speed\n",
    "    use_external_data = True  # Set to True to use external datasets\n",
    "    random_state = 42\n",
    "\n",
    "# Create a single config instance to use everywhere\n",
    "config = Config()\n",
    "\n",
    "\"\"\"\n",
    "Load competition data with complete filtering of problematic polymer notation\n",
    "\"\"\"\n",
    "print(\"Loading competition data...\")\n",
    "train = pd.read_csv(BASE_PATH + 'train.csv')\n",
    "test = pd.read_csv(BASE_PATH + 'test.csv')\n",
    "\n",
    "if config.debug:\n",
    "    print(\"   Debug mode: sampling 1000 training examples\")\n",
    "    train = train.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Training data shape: {train.shape}, Test data shape: {test.shape}\")\n",
    "\n",
    "def clean_and_validate_smiles(smiles):\n",
    "    \"\"\"Completely clean and validate SMILES, removing all problematic patterns\"\"\"\n",
    "    if not isinstance(smiles, str) or len(smiles) == 0:\n",
    "        return None\n",
    "    \n",
    "    # List of all problematic patterns we've seen\n",
    "    bad_patterns = [\n",
    "        '[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]', \n",
    "        \"[R']\", '[R\"]', 'R1', 'R2', 'R3', 'R4', 'R5',\n",
    "        # Additional patterns that cause issues\n",
    "        '([R])', '([R1])', '([R2])', \n",
    "    ]\n",
    "    \n",
    "    for pattern in bad_patterns:\n",
    "        if pattern in smiles:\n",
    "            return None\n",
    "    \n",
    "    # Additional check: if it contains ] followed by [ without valid atoms, likely polymer notation\n",
    "    if '][' in smiles and any(x in smiles for x in ['[R', 'R]']):\n",
    "        return None\n",
    "    \n",
    "    # Try to parse with RDKit if available\n",
    "    if RDKIT_AVAILABLE:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # If RDKit not available, return cleaned SMILES\n",
    "    return smiles\n",
    "\n",
    "# Clean and validate all SMILES\n",
    "print(\"Cleaning and validating SMILES...\")\n",
    "train['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)\n",
    "test['SMILES'] = test['SMILES'].apply(clean_and_validate_smiles)\n",
    "\n",
    "# Remove invalid SMILES\n",
    "invalid_train = train['SMILES'].isnull().sum()\n",
    "invalid_test = test['SMILES'].isnull().sum()\n",
    "\n",
    "print(f\"   Removed {invalid_train} invalid SMILES from training data\")\n",
    "print(f\"   Removed {invalid_test} invalid SMILES from test data\")\n",
    "\n",
    "train = train[train['SMILES'].notnull()].reset_index(drop=True)\n",
    "test = test[test['SMILES'].notnull()].reset_index(drop=True)\n",
    "\n",
    "print(f\"   Final training samples: {len(train)}\")\n",
    "print(f\"   Final test samples: {len(test)}\")\n",
    "\n",
    "def add_extra_data_clean(df_train, df_extra, target):\n",
    "    \"\"\"Add external data with thorough SMILES cleaning\"\"\"\n",
    "    n_samples_before = len(df_train[df_train[target].notnull()])\n",
    "    \n",
    "    print(f\"      Processing {len(df_extra)} {target} samples...\")\n",
    "    \n",
    "    # Clean external SMILES\n",
    "    df_extra['SMILES'] = df_extra['SMILES'].apply(clean_and_validate_smiles)\n",
    "    \n",
    "    # Remove invalid SMILES and missing targets\n",
    "    before_filter = len(df_extra)\n",
    "    df_extra = df_extra[df_extra['SMILES'].notnull()]\n",
    "    df_extra = df_extra.dropna(subset=[target])\n",
    "    after_filter = len(df_extra)\n",
    "    \n",
    "    print(f\"      Kept {after_filter}/{before_filter} valid samples\")\n",
    "    \n",
    "    if len(df_extra) == 0:\n",
    "        print(f\"      No valid data remaining for {target}\")\n",
    "        return df_train\n",
    "    \n",
    "    # Group by canonical SMILES and average duplicates\n",
    "    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n",
    "    \n",
    "    cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n",
    "    unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n",
    "\n",
    "    # Fill missing values\n",
    "    filled_count = 0\n",
    "    for smile in df_train[df_train[target].isnull()]['SMILES'].tolist():\n",
    "        if smile in cross_smiles:\n",
    "            df_train.loc[df_train['SMILES']==smile, target] = \\\n",
    "                df_extra[df_extra['SMILES']==smile][target].values[0]\n",
    "            filled_count += 1\n",
    "    \n",
    "    # Add unique SMILES\n",
    "    extra_to_add = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)].copy()\n",
    "    if len(extra_to_add) > 0:\n",
    "        for col in TARGETS:\n",
    "            if col not in extra_to_add.columns:\n",
    "                extra_to_add[col] = np.nan\n",
    "        \n",
    "        extra_to_add = extra_to_add[['SMILES'] + TARGETS]\n",
    "        df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)\n",
    "\n",
    "    n_samples_after = len(df_train[df_train[target].notnull()])\n",
    "    print(f'      {target}: +{n_samples_after-n_samples_before} samples, +{len(unique_smiles_extra)} unique SMILES')\n",
    "    print(f\"      Filled {filled_count} missing entries in train for {target}\")\n",
    "    print(f\"      Added {len(extra_to_add)} new entries for {target}\")\n",
    "    return df_train\n",
    "\n",
    "# Load external datasets with robust error handling\n",
    "print(\"\\nüìÇ Loading external datasets...\")\n",
    "\n",
    "external_datasets = []\n",
    "\n",
    "# Function to safely load datasets\n",
    "def safe_load_dataset(path, target, processor_func, description):\n",
    "    try:\n",
    "        if path.endswith('.xlsx'):\n",
    "            data = pd.read_excel(path)\n",
    "        else:\n",
    "            data = pd.read_csv(path)\n",
    "        \n",
    "        data = processor_func(data)\n",
    "        external_datasets.append((target, data))\n",
    "        print(f\"   ‚úÖ {description}: {len(data)} samples\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è {description} failed: {str(e)[:100]}\")\n",
    "        return False\n",
    "\n",
    "# Load each dataset\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/tc-smiles/Tc_SMILES.csv',\n",
    "    'Tc',\n",
    "    lambda df: df.rename(columns={'TC_mean': 'Tc'}),\n",
    "    'Tc data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv',\n",
    "    'Tg', \n",
    "    lambda df: df[['SMILES', 'Tg']] if 'Tg' in df.columns else df,\n",
    "    'TgSS enriched data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv',\n",
    "    'Tg',\n",
    "    lambda df: df[['SMILES', 'Tg (C)']].rename(columns={'Tg (C)': 'Tg'}),\n",
    "    'JCIM Tg data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/smiles-extra-data/data_tg3.xlsx',\n",
    "    'Tg',\n",
    "    lambda df: df.rename(columns={'Tg [K]': 'Tg'}).assign(Tg=lambda x: x['Tg'] - 273.15),\n",
    "    'Xlsx Tg data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/smiles-extra-data/data_dnst1.xlsx',\n",
    "    'Density',\n",
    "    lambda df: df.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\n",
    "                .query('SMILES.notnull() and Density.notnull() and Density != \"nylon\"')\n",
    "                .assign(Density=lambda x: x['Density'].astype(float) - 0.118),\n",
    "    'Density data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    BASE_PATH + 'train_supplement/dataset4.csv',\n",
    "    'FFV', \n",
    "    lambda df: df[['SMILES', 'FFV']] if 'FFV' in df.columns else df,\n",
    "    'dataset 4'\n",
    ")\n",
    "\n",
    "# Integrate external data\n",
    "print(\"\\nüîÑ Integrating external data...\")\n",
    "train_extended = train[['SMILES'] + TARGETS].copy()\n",
    "\n",
    "if getattr(config, \"use_external_data\", True) and  not config.debug:\n",
    "    for target, dataset in external_datasets:\n",
    "        print(f\"   Processing {target} data...\")\n",
    "        train_extended = add_extra_data_clean(train_extended, dataset, target)\n",
    "\n",
    "print(f\"\\nüìä Final training data:\")\n",
    "print(f\"   Original samples: {len(train)}\")\n",
    "print(f\"   Extended samples: {len(train_extended)}\")\n",
    "print(f\"   Gain: +{len(train_extended) - len(train)} samples\")\n",
    "\n",
    "for target in TARGETS:\n",
    "    count = train_extended[target].notna().sum()\n",
    "    original_count = train[target].notna().sum() if target in train.columns else 0\n",
    "    gain = count - original_count\n",
    "    print(f\"   {target}: {count:,} samples (+{gain})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data integration complete with clean SMILES!\")\n",
    "\n",
    "def separate_subtables(train_df):\n",
    "    labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "    subtables = {}\n",
    "    for label in labels:\n",
    "        # Filter out NaNs, select columns, reset index\n",
    "        subtables[label] = train_df[train_df[label].notna()][['SMILES', label]].reset_index(drop=True)\n",
    "\n",
    "    return subtables\n",
    "\n",
    "def augment_smiles_dataset(smiles_list, labels, num_augments=3):\n",
    "    \"\"\"\n",
    "    Augments a list of SMILES strings by generating randomized versions.\n",
    "\n",
    "    Parameters:\n",
    "        smiles_list (list of str): Original SMILES strings.\n",
    "        labels (list or np.array): Corresponding labels.\n",
    "        num_augments (int): Number of augmentations per SMILES.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (augmented_smiles, augmented_labels)\n",
    "    \"\"\"\n",
    "    augmented_smiles = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for smiles, label in zip(smiles_list, labels):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "        # Add original\n",
    "        augmented_smiles.append(smiles)\n",
    "        augmented_labels.append(label)\n",
    "        # Add randomized versions\n",
    "        for _ in range(num_augments):\n",
    "            rand_smiles = Chem.MolToSmiles(mol, doRandom=True)\n",
    "            augmented_smiles.append(rand_smiles)\n",
    "            augmented_labels.append(label)\n",
    "\n",
    "    return augmented_smiles, np.array(augmented_labels)\n",
    "\n",
    "required_descriptors = {'graph_diameter','num_cycles','avg_shortest_path','MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}\n",
    "\n",
    "def augment_dataset(X, y, n_samples=1000, n_components=5, random_state=None):\n",
    "    \"\"\"\n",
    "    Augments a dataset using Gaussian Mixture Models.\n",
    "\n",
    "    Parameters:\n",
    "    - X: pd.DataFrame or np.ndarray ‚Äî feature matrix\n",
    "    - y: pd.Series or np.ndarray ‚Äî target values\n",
    "    - n_samples: int ‚Äî number of synthetic samples to generate\n",
    "    - n_components: int ‚Äî number of GMM components\n",
    "    - random_state: int ‚Äî random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - X_augmented: pd.DataFrame ‚Äî augmented feature matrix\n",
    "    - y_augmented: pd.Series ‚Äî augmented target values\n",
    "    \"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    elif not isinstance(X, pd.DataFrame):\n",
    "        raise ValueError(\"X must be a pandas DataFrame or a NumPy array\")\n",
    "\n",
    "    X.columns = X.columns.astype(str)\n",
    "\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = pd.Series(y)\n",
    "    elif not isinstance(y, pd.Series):\n",
    "        raise ValueError(\"y must be a pandas Series or a NumPy array\")\n",
    "\n",
    "    df = X.copy()\n",
    "    df['Target'] = y.values\n",
    "\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "    gmm.fit(df)\n",
    "\n",
    "    synthetic_data, _ = gmm.sample(n_samples)\n",
    "    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "\n",
    "    augmented_df = pd.concat([df, synthetic_df], ignore_index=True)\n",
    "\n",
    "    X_augmented = augmented_df.drop(columns='Target')\n",
    "    y_augmented = augmented_df['Target']\n",
    "\n",
    "    return X_augmented, y_augmented\n",
    "\n",
    "\n",
    "train_df=train_extended\n",
    "test_df=test\n",
    "subtables = separate_subtables(train_df)\n",
    "\n",
    "test_smiles = test_df['SMILES'].tolist()\n",
    "test_ids = test_df['id'].values\n",
    "labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# --- GNN MODEL AND DATA PREPARATION ---\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# A dictionary to map atom symbols to integer indices for the GNN\n",
    "ATOM_MAP = {\n",
    "    'C': 0, 'N': 1, 'O': 2, 'F': 3, 'P': 4, 'S': 5, 'Cl': 6, 'Br': 7, 'I': 8, 'H': 9,\n",
    "    # --- NEWLY ADDED SYMBOLS ---\n",
    "    'Si': 10, # Silicon\n",
    "    'Na': 11, # Sodium\n",
    "    '*' : 12, # Wildcard atom\n",
    "    # --- NEWLY ADDED SYMBOLS ---\n",
    "    'B': 13,  # Boron\n",
    "    'Ge': 14, # Germanium\n",
    "    'Sn': 15, # Tin\n",
    "    'Se': 16, # Selenium\n",
    "    'Te': 17, # Tellurium\n",
    "    'Ca': 18, # Calcium\n",
    "    'Cd': 19, # Cadmium\n",
    "}\n",
    "\n",
    "def smiles_to_graph(smiles_str: str, y_val=None):\n",
    "    \"\"\"\n",
    "    Converts a SMILES string to a graph, adding selected global\n",
    "    molecular features to each node's feature vector.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles_str)\n",
    "        if mol is None: return None\n",
    "\n",
    "        # 1. Calculate global features once per molecule\n",
    "        global_features = [\n",
    "            Descriptors.MolWt(mol),\n",
    "            Descriptors.TPSA(mol),\n",
    "            Descriptors.NumRotatableBonds(mol),\n",
    "            Descriptors.MolLogP(mol)\n",
    "        ]\n",
    "\n",
    "        node_features = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            # Initialize atom-specific features (one-hot encoding)\n",
    "            atom_features = [0] * len(ATOM_MAP)\n",
    "            symbol = atom.GetSymbol()\n",
    "            if symbol in ATOM_MAP:\n",
    "                atom_features[ATOM_MAP[symbol]] = 1\n",
    "\n",
    "            # Add other standard atom features\n",
    "            atom_features.extend([\n",
    "                atom.GetAtomicNum(),\n",
    "                atom.GetTotalDegree(),\n",
    "                atom.GetFormalCharge(),\n",
    "                atom.GetTotalNumHs(),\n",
    "                int(atom.GetIsAromatic())\n",
    "            ])\n",
    "            \n",
    "            # 2. Append the global features to each atom's feature vector\n",
    "            atom_features.extend(global_features)\n",
    "            \n",
    "            node_features.append(atom_features)\n",
    "        \n",
    "        if not node_features: return None\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "        edge_indices, edge_attrs = [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            edge_indices.extend([(i, j), (j, i)])\n",
    "            bond_type = bond.GetBondTypeAsDouble()\n",
    "            edge_attrs.extend([[bond_type], [bond_type]])\n",
    "\n",
    "        if not edge_indices:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            edge_attr = torch.empty((0, 1), dtype=torch.float)\n",
    "        else:\n",
    "            edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "            edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "\n",
    "        if y_val is not None:\n",
    "            y_tensor = torch.tensor([[y_val]], dtype=torch.float)\n",
    "            return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y_tensor)\n",
    "        else:\n",
    "            return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "# A dictionary mapping labels to their most important global features from XGBoost\n",
    "LABEL_SPECIFIC_FEATURES = {\n",
    "    'Tg': [\n",
    "        \"HallKierAlpha\", # Topological charge index\n",
    "        \"MolLogP\",       # Lipophilicity\n",
    "        \"NumRotatableBonds\", # Flexibility\n",
    "        \"TPSA\",          # Polarity\n",
    "    ],\n",
    "    'FFV': [\n",
    "        \"NHOHCount\",     # Count of NH and OH groups (H-bonding)\n",
    "        \"NumRotatableBonds\",\n",
    "        \"MolWt\",         # Size\n",
    "        \"TPSA\",\n",
    "    ],\n",
    "    'Tc': [\n",
    "        \"MolLogP\",\n",
    "        \"NumValenceElectrons\",\n",
    "        \"SPS\",           # Molecular shape index\n",
    "        \"MolWt\",\n",
    "    ],\n",
    "    'Density': [\n",
    "        \"MolWt\",\n",
    "        \"MolMR\",         # Molar refractivity (related to volume)\n",
    "        \"FractionCSP3\",  # Proportion of sp3 hybridized carbons (related to saturation)\n",
    "        \"NumHeteroatoms\",\n",
    "    ],\n",
    "    'Rg': [\n",
    "        \"HallKierAlpha\",\n",
    "        \"MolWt\",\n",
    "        \"NumValenceElectrons\",\n",
    "        \"qed\",           # Quantitative Estimation of Drug-likeness\n",
    "    ]\n",
    "}\n",
    "\n",
    "# A helper dictionary to easily call RDKit functions from their string names\n",
    "RDKIT_DESC_CALCULATORS = {name: func for name, func in Descriptors.descList}\n",
    "RDKIT_DESC_CALCULATORS['qed'] = Descriptors.qed # Add qed as it's not in the default list\n",
    "\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "\n",
    "# This ATOM_MAP dictionary must be defined globally in your script (it already is)\n",
    "# ATOM_MAP = {'C': 0, 'N': 1, ...}\n",
    "\n",
    "def smiles_to_graph_label_specific(smiles_str: str, label: str, y_val=None):\n",
    "    \"\"\"\n",
    "    (BASELINE VERSION - SIMPLE FEATURES)\n",
    "    - This is the original hybrid GNN featurizer that produced your best score.\n",
    "    - Node Features (x): Atom one-hot (20) + 5 atom features = 25 features.\n",
    "    - Edge Features (edge_attr): Bond type as double = 1 feature.\n",
    "    - Global Features (u): Label-specific descriptors are stored separately in 'data.u'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles_str)\n",
    "        if mol is None: \n",
    "            return None\n",
    "\n",
    "        # --- 1. Calculate and store label-specific GLOBAL features ---\n",
    "        global_features = []\n",
    "        features_to_calculate = LABEL_SPECIFIC_FEATURES.get(label, [])\n",
    "        \n",
    "        for feature_name in features_to_calculate:\n",
    "            calculator_func = RDKIT_DESC_CALCULATORS.get(feature_name)\n",
    "            if calculator_func:\n",
    "                try:\n",
    "                    val = calculator_func(mol)\n",
    "                    # Ensure value is valid, replace inf/nan with 0\n",
    "                    global_features.append(val if np.isfinite(val) else 0.0)\n",
    "                except Exception as e:\n",
    "                    global_features.append(0.0)\n",
    "            else:\n",
    "                global_features.append(0.0)\n",
    "\n",
    "        # --- 2. Create Node Features (SIMPLE) ---\n",
    "        node_features = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            # One-Hot Symbol (len 20, from global ATOM_MAP)\n",
    "            atom_features = [0] * len(ATOM_MAP)\n",
    "            symbol = atom.GetSymbol()\n",
    "            if symbol in ATOM_MAP:\n",
    "                atom_features[ATOM_MAP[symbol]] = 1\n",
    "\n",
    "            # Standard Features (len 5)\n",
    "            atom_features.extend([\n",
    "                atom.GetAtomicNum(),\n",
    "                atom.GetTotalDegree(),\n",
    "                atom.GetFormalCharge(),\n",
    "                atom.GetTotalNumHs(),\n",
    "                int(atom.GetIsAromatic())\n",
    "            ])\n",
    "            # Total features = 25\n",
    "            node_features.append(atom_features)\n",
    "        \n",
    "        if not node_features: return None\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "        # --- 3. Create Edge Features (SIMPLE) ---\n",
    "        edge_indices, edge_attrs = [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            edge_indices.extend([(i, j), (j, i)])\n",
    "            bond_type = bond.GetBondTypeAsDouble()\n",
    "            edge_attrs.extend([[bond_type], [bond_type]]) # 1-dim feature\n",
    "\n",
    "        if not edge_indices:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            edge_attr = torch.empty((0, 1), dtype=torch.float) # Shape (0, 1)\n",
    "        else:\n",
    "            edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "            edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "\n",
    "        # --- 4. Create Data Object ---\n",
    "        data_obj = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        data_obj.u = torch.tensor([global_features], dtype=torch.float) # Store globals in 'u'\n",
    "\n",
    "        if y_val is not None:\n",
    "            data_obj.y = torch.tensor([[y_val]], dtype=torch.float)\n",
    "        \n",
    "        return data_obj\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected molecule-level errors\n",
    "        print(f\"CRITICAL ERROR converting SMILES '{smiles_str}': {e}\")\n",
    "        return None\n",
    "            \n",
    "class GNNModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the Graph Neural Network architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_node_features, hidden_channels=128):\n",
    "        super(GNNModel, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels * 2)\n",
    "        self.conv3 = GCNConv(hidden_channels * 2, hidden_channels * 4)\n",
    "        self.lin = torch.nn.Linear(hidden_channels * 4, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = global_max_pool(x, batch) # Aggregate node features to get a graph-level embedding\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def predict_with_gnn(trained_model, test_smiles):\n",
    "    \"\"\"\n",
    "    Uses a pre-trained GNN model to make predictions on a list of test SMILES.\n",
    "    \"\"\"\n",
    "    if trained_model is None:\n",
    "        print(\"Prediction skipped because the GNN model is invalid.\")\n",
    "        return np.full(len(test_smiles), np.nan)\n",
    "\n",
    "    print(\"--- Making predictions with trained GNN... ---\")\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Convert test SMILES to graph data\n",
    "    test_data_list = [smiles_to_graph(s) for s in test_smiles]\n",
    "    \n",
    "    # We need to keep track of which original indices are valid\n",
    "    valid_indices = [i for i, data in enumerate(test_data_list) if data is not None]\n",
    "    valid_test_data = [data for data in test_data_list if data is not None]\n",
    "\n",
    "    if not valid_test_data:\n",
    "        print(\"Warning: No valid test molecules could be converted to graphs.\")\n",
    "        return np.full(len(test_smiles), np.nan)\n",
    "        \n",
    "    test_loader = PyGDataLoader(valid_test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "    trained_model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(DEVICE)\n",
    "            out = trained_model(data)\n",
    "            all_preds.append(out.cpu())\n",
    "\n",
    "    # Combine predictions from all batches\n",
    "    test_preds_tensor = torch.cat(all_preds, dim=0).numpy().flatten()\n",
    "    \n",
    "    # Create a full-sized prediction array and fill in the values at their original positions\n",
    "    final_predictions = np.full(len(test_smiles), np.nan)\n",
    "    if len(test_preds_tensor) == len(valid_indices):\n",
    "        final_predictions[valid_indices] = test_preds_tensor\n",
    "    else:\n",
    "        print(f\"Warning: Mismatch in GNN prediction count. This can happen with invalid SMILES.\")\n",
    "        fill_count = min(len(valid_indices), len(test_preds_tensor))\n",
    "        final_predictions[valid_indices[:fill_count]] = test_preds_tensor[:fill_count]\n",
    "\n",
    "    return final_predictions\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def save_gnn_model(model, label, model_dir=\"models/gnn\"):\n",
    "    \"\"\"\n",
    "    (MODIFIED) Saves the GNN model state_dict and its full constructor config.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        print(f\"Skipping save for {label}, model is None.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, f\"gnn_model_{label}.pth\")\n",
    "    config_path = os.path.join(model_dir, f\"gnn_config_{label}.json\")\n",
    "\n",
    "    # Save the model parameters (the weights)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save the full configuration dictionary\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(model.config_args, f, indent=4)\n",
    "        \n",
    "    print(f\"Saved final model for {label} to {model_path}\")\n",
    "\n",
    "\n",
    "def load_gnn_model(label, model_dir=\"models/gnn\"):\n",
    "    \"\"\"\n",
    "    (MODIFIED) Loads a saved GNN model using its full config file.\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(model_dir, f\"gnn_model_{label}.pth\")\n",
    "    config_path = os.path.join(model_dir, f\"gnn_config_{label}.json\")\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    if not os.path.exists(model_path) or not os.path.exists(config_path):\n",
    "        print(f\"Warning: Model or config file not found for {label}. Cannot load model.\")\n",
    "        return None\n",
    "\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    try:\n",
    "        # Re-initialize the model using all saved config args via dictionary unpacking\n",
    "        model = TaskSpecificGNN(**config).to(DEVICE)\n",
    "        \n",
    "        # Load the saved model weights\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        print(f\"Successfully loaded saved model for {label} from {model_path}\")\n",
    "        return model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR loading model for {label}: {e}\")\n",
    "        print(\"This may be due to a mismatch between the saved model and the current model class definition.\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def create_dynamic_mlp(input_dim, layer_list, dropout_list):\n",
    "    \"\"\"\n",
    "    Helper function to dynamically build the task-specific MLP.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    current_dim = input_dim\n",
    "    \n",
    "    for neurons, dropout in zip(layer_list, dropout_list):\n",
    "        layers.append(torch.nn.Linear(current_dim, neurons))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Dropout(dropout))\n",
    "        current_dim = neurons\n",
    "        \n",
    "    # Add the final single-output prediction layer\n",
    "    layers.append(torch.nn.Linear(current_dim, 1))\n",
    "    \n",
    "    return torch.nn.Sequential(*layers)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "\n",
    "class TaskSpecificGNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, num_global_features,\n",
    "                 hidden_channels_gnn, mlp_neurons, mlp_dropouts, heads=8):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # --- 1. GNN Backbone (Using GATConv, No BatchNorm) ---\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        # Layer 1\n",
    "        self.convs.append(\n",
    "            GATConv(num_node_features, hidden_channels_gnn, heads=heads,\n",
    "                    edge_dim=num_edge_features)\n",
    "        )\n",
    "\n",
    "        # Layer 2\n",
    "        self.convs.append(\n",
    "            GATConv(hidden_channels_gnn * heads, hidden_channels_gnn * 2, heads=heads,\n",
    "                    edge_dim=num_edge_features)\n",
    "        )\n",
    "\n",
    "        # Layer 3 (Final GNN layer)\n",
    "        self.convs.append(\n",
    "            GATConv(hidden_channels_gnn * 2 * heads, hidden_channels_gnn * 4, heads=heads,\n",
    "                    concat=False, edge_dim=num_edge_features)\n",
    "        )\n",
    "\n",
    "        gnn_output_dim = hidden_channels_gnn * 4\n",
    "\n",
    "        # --- 2. Readout Head ---\n",
    "        combined_feature_size = gnn_output_dim + num_global_features\n",
    "\n",
    "        self.readout_mlp = create_dynamic_mlp(\n",
    "            input_dim=combined_feature_size,\n",
    "            layer_list=mlp_neurons,\n",
    "            dropout_list=mlp_dropouts\n",
    "        )\n",
    "\n",
    "        # --- 3. Store config for saving/loading ---\n",
    "        self.config_args = {\n",
    "            'num_node_features': num_node_features,\n",
    "            'num_edge_features': num_edge_features,\n",
    "            'num_global_features': num_global_features,\n",
    "            'hidden_channels_gnn': hidden_channels_gnn,\n",
    "            'mlp_neurons': mlp_neurons,\n",
    "            'mlp_dropouts': mlp_dropouts,\n",
    "            'heads': heads\n",
    "        }\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch\n",
    "\n",
    "        # GNN Layers with ReLU and Dropout\n",
    "        x = F.relu(self.convs[0](x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = F.relu(self.convs[1](x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = F.relu(self.convs[2](x, edge_index, edge_attr))\n",
    "\n",
    "        # Readout\n",
    "        graph_embedding = global_mean_pool(x, batch)\n",
    "        combined_features = torch.cat([graph_embedding, u], dim=1)\n",
    "        output = self.readout_mlp(combined_features)\n",
    "\n",
    "        return output\n",
    "        \n",
    "# This is a new helper, just to make scaling code cleaner inside the loops\n",
    "def scale_graph_features(data_list, u_scaler, x_scaler, atom_map_len):\n",
    "    \"\"\"Applies fitted scalers in-place to a list of Data objects.\"\"\"\n",
    "    try:\n",
    "        for data in data_list:\n",
    "            # 1. Scale global features (u)\n",
    "            data.u = torch.tensor(u_scaler.transform(data.u.numpy()), dtype=torch.float)\n",
    "            \n",
    "            # 2. Scale continuous part of node features (x)\n",
    "            x_one_hot = data.x[:, :atom_map_len]\n",
    "            x_continuous = data.x[:, atom_map_len:]\n",
    "            \n",
    "            x_continuous_scaled = x_scaler.transform(x_continuous.numpy())\n",
    "            x_continuous_scaled_tensor = torch.tensor(x_continuous_scaled, dtype=torch.float)\n",
    "            \n",
    "            # Recombine scaled features\n",
    "            data.x = torch.cat([x_one_hot, x_continuous_scaled_tensor], dim=1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR applying scalers: {e}. Check feature dimensions. AtomMapLen={atom_map_len}\")\n",
    "        raise e\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def train_gnn_model(label, train_data_list, val_data_list, mlp_neurons, mlp_dropouts, epochs=300): # Increased default epochs\n",
    "    \"\"\"\n",
    "    (REVISED)\n",
    "    - Accepts both train and val data lists.\n",
    "    - Implements ReduceLROnPlateau scheduler based on val_loss.\n",
    "    - Implements Early Stopping based on val_loss patience.\n",
    "    \"\"\"\n",
    "    print(f\"--- Training GNN for label: {label} ---\")\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    if not train_data_list:\n",
    "        print(f\"Warning: Empty train data list passed for {label}.\")\n",
    "        return None\n",
    "    if not val_data_list:\n",
    "        print(f\"Warning: Empty validation data list passed for {label}.\")\n",
    "        return None\n",
    "\n",
    "    # drop_last=True is important for training stability, prevents variance from tiny final batches.\n",
    "    train_loader = PyGDataLoader(train_data_list, batch_size=32, shuffle=True, drop_last=True) \n",
    "    val_loader = PyGDataLoader(val_data_list, batch_size=32, shuffle=False) # No shuffle/drop for val\n",
    "\n",
    "    # Get feature dimensions from the first data object\n",
    "    first_data = train_data_list[0]\n",
    "    num_node_features = first_data.x.shape[1]\n",
    "    num_global_features = first_data.u.shape[1]\n",
    "    num_edge_features = first_data.edge_attr.shape[1]\n",
    "    \n",
    "    print(f\"Model Features (Scaled): Nodes={num_node_features}, Edges={num_edge_features}, Global={num_global_features}\")\n",
    "\n",
    "    model = TaskSpecificGNN(  # This should be your (no-BN) model class\n",
    "        num_node_features=num_node_features,\n",
    "        num_edge_features=num_edge_features,\n",
    "        num_global_features=num_global_features,\n",
    "        hidden_channels_gnn=128, \n",
    "        mlp_neurons=mlp_neurons,\n",
    "        mlp_dropouts=mlp_dropouts\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    criterion = torch.nn.L1Loss() \n",
    "\n",
    "    # --- 1. ADD SCHEDULER ---\n",
    "    # This will cut the LR by half (factor=0.5) if val loss doesn't improve for 10 epochs (patience=10)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "    # --- 2. ADD EARLY STOPPING VARS ---\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    PATIENCE_EPOCHS = 30  # Stop training if val loss doesn't improve for 30 straight epochs\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for data in train_loader:\n",
    "            if data.x.shape[0] <= 1: # Skip batches with one node (can happen)\n",
    "                continue\n",
    "            data = data.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        if len(train_loader.dataset) == 0:\n",
    "            avg_train_loss = 0\n",
    "        else:\n",
    "            avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # --- 3. ADD VALIDATION LOOP (INSIDE EPOCH LOOP) ---\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(DEVICE)\n",
    "                out = model(data)\n",
    "                loss = criterion(out, data.y)\n",
    "                total_val_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        if len(val_loader.dataset) == 0:\n",
    "             avg_val_loss = 0\n",
    "        else:\n",
    "            avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "             print(f\"Epoch: {epoch:03d}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "        # --- 4. SCHEDULER & EARLY STOPPING LOGIC ---\n",
    "        scheduler.step(avg_val_loss) # Feed validation loss to the scheduler\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE_EPOCHS and epoch > 50: # Give it at least 50 epochs to warm up\n",
    "            print(f\"--- Early stopping triggered at epoch {epoch} ---\")\n",
    "            break\n",
    "            \n",
    "    print(f\"--- GNN training for {label} complete. Best Val Loss: {best_val_loss:.6f} ---\")\n",
    "    return model\n",
    "\n",
    "def predict_with_gnn(trained_model, test_smiles, label, u_scaler, x_scaler, atom_map_len):\n",
    "    \"\"\"\n",
    "    (MODIFIED for Full Scaling)\n",
    "    - Requires both u_scaler (global) and x_scaler (node) to transform features.\n",
    "    - Returns SCALED predictions.\n",
    "    \"\"\"\n",
    "    if trained_model is None or u_scaler is None or x_scaler is None:\n",
    "        print(f\"Prediction skipped for {label} due to missing model or scaler.\")\n",
    "        return np.full(len(test_smiles), np.nan)\n",
    "\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # 1. Featurize test data (features are NOT scaled yet)\n",
    "    test_data_list = [smiles_to_graph_label_specific(s, label, y_val=None) for s in test_smiles]\n",
    "    \n",
    "    valid_indices = [i for i, data in enumerate(test_data_list) if data is not None]\n",
    "    valid_test_data = [data for data in test_data_list if data is not None]\n",
    "\n",
    "    if not valid_test_data:\n",
    "        print(f\"Warning: No valid test molecules could be converted for {label}.\")\n",
    "        return np.full(len(test_smiles), np.nan)\n",
    "        \n",
    "    # 2. Apply fitted scalers to all valid test features\n",
    "    try:\n",
    "        valid_test_data = scale_graph_features(valid_test_data, u_scaler, x_scaler, atom_map_len)\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR applying scalers during prediction: {e}.\")\n",
    "        return np.full(len(test_smiles), np.nan)\n",
    "\n",
    "    test_loader = PyGDataLoader(valid_test_data, batch_size=32, shuffle=False) \n",
    "\n",
    "    trained_model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(DEVICE)\n",
    "            out = trained_model(data)\n",
    "            all_preds.append(out.cpu())\n",
    "\n",
    "    test_preds_tensor = torch.cat(all_preds, dim=0).numpy().flatten()\n",
    "    \n",
    "    # Fill predictions array (these are SCALED predictions)\n",
    "    final_predictions = np.full(len(test_smiles), np.nan)\n",
    "    if len(test_preds_tensor) == len(valid_indices):\n",
    "        final_predictions[valid_indices] = test_preds_tensor\n",
    "    else:\n",
    "        print(f\"Warning: Mismatch in GNN prediction count for {label}.\")\n",
    "        fill_count = min(len(valid_indices), len(test_preds_tensor))\n",
    "        final_predictions[valid_indices[:fill_count]] = test_preds_tensor[:fill_count]\n",
    "\n",
    "    return final_predictions # These predictions are on the SCALED range\n",
    "\n",
    "\n",
    "def train_or_predict_gnn(train_model=True, model_dir=\"models/gnn\", n_splits=10):\n",
    "    \"\"\"\n",
    "    (FINAL COMPLETE VERSION)\n",
    "    - All data hardening (coerce, filter) and RobustScaler logic is included.\n",
    "    - CV loop is modified to create a val_data_list.\n",
    "    - Calls the new, optimized train_gnn_model with scheduler/early stopping.\n",
    "    - Correctly passes all arguments (config['neurons'], config['dropouts']) to fix the TypeError.\n",
    "    \"\"\"\n",
    "    \n",
    "    ATOM_MAP_LEN = 20  # Make sure this matches your global ATOM_MAP\n",
    "    \n",
    "    # Plausible physical ranges to filter catastrophic outliers BEFORE scaling\n",
    "    VALID_RANGES = {\n",
    "        'Tg':      (-100, 500),  \n",
    "        'FFV':     (0.01, 0.99), \n",
    "        'Tc':      (0, 1000),    \n",
    "        'Density': (0.1, 3.0),   \n",
    "        'Rg':      (0.1, 200)    \n",
    "    }\n",
    "\n",
    "    # MLP configs for the GNN readout head\n",
    "    best_configs = {\n",
    "        # Classic funnel, slightly lower final dropout\n",
    "        \"Tg\":      {\"neurons\": [512, 256, 128], \"dropouts\": [0.5, 0.4, 0.2]},\n",
    "        # Original wide funnel for this complex feature\n",
    "        \"Density\": {\"neurons\": [1024, 256, 64], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "        # Even wider and deeper, with strong regularization for presumed complexity\n",
    "        \"FFV\":     {\"neurons\": [1024, 512, 64], \"dropouts\": [0.6, 0.5, 0.4]},\n",
    "        # Slightly deeper than the simplest model to capture more features\n",
    "        \"Tc\":      {\"neurons\": [128, 64], \"dropouts\": [0.4, 0.3]},\n",
    "        # A gentle funnel instead of a pure block to encourage feature compression\n",
    "        \"Rg\":      {\"neurons\": [128, 64, 64], \"dropouts\": [0.4, 0.3, 0.3]},\n",
    "    }\n",
    "    default_config = {\"neurons\": [128, 64], \"dropouts\": [0.3, 0.3]}\n",
    "\n",
    "    output_df = pd.DataFrame({'id': test_df['id']})\n",
    "    cv_mae_results = []\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    warnings.filterwarnings(\"ignore\", \"Mean of empty slice\", RuntimeWarning)\n",
    "\n",
    "    for label in labels: \n",
    "        print(f\"\\n{'='*20} Processing GNN for label: {label} {'='*20}\")\n",
    "        \n",
    "        config = best_configs.get(label, default_config)\n",
    "        print(f\"Using MLP Config: Neurons={config['neurons']}, Dropouts={config['dropouts']}\")\n",
    "        \n",
    "        ensemble_models = []\n",
    "        y_scaler_path = os.path.join(model_dir, f\"gnn_yscaler_{label}.joblib\")\n",
    "        u_scaler_path = os.path.join(model_dir, f\"gnn_uscaler_{label}.joblib\")\n",
    "        x_scaler_path = os.path.join(model_dir, f\"gnn_xscaler_{label}.joblib\")\n",
    "        \n",
    "        if train_model:\n",
    "            # --- START DATA HARDENING ---\n",
    "            all_smiles_raw = subtables[label]['SMILES']\n",
    "            all_y_raw = subtables[label][label] \n",
    "            \n",
    "            all_y_numeric = pd.to_numeric(all_y_raw, errors='coerce')\n",
    "            original_count = len(all_y_numeric)\n",
    "\n",
    "            valid_min, valid_max = VALID_RANGES.get(label, (-np.inf, np.inf))\n",
    "            valid_mask = (all_y_numeric >= valid_min) & (all_y_numeric <= valid_max) & (all_y_numeric.notna())\n",
    "            \n",
    "            all_y = all_y_numeric[valid_mask].reset_index(drop=True)\n",
    "            all_smiles = all_smiles_raw[valid_mask].reset_index(drop=True)\n",
    "            \n",
    "            print(f\"FILTERING: Coerced {original_count} rows. Kept {len(all_y)} valid rows within range ({valid_min}, {valid_max}).\")\n",
    "            \n",
    "            if len(all_y) < (2 * n_splits): \n",
    "                print(f\"CRITICAL: Not enough valid data ({len(all_y)}) to train for {label} with {n_splits} splits. Skipping.\")\n",
    "                continue\n",
    "            # --- END DATA HARDENING ---\n",
    "\n",
    "            # --- 1. FIT Y-SCALER (ROBUST) ---\n",
    "            print(\"Using RobustScaler for Y-Scaler.\")\n",
    "            y_scaler = RobustScaler()  \n",
    "            all_y_scaled = y_scaler.fit_transform(all_y.values.reshape(-1, 1)).flatten()\n",
    "            joblib.dump(y_scaler, y_scaler_path)\n",
    "            print(f\"Saved Y-Scaler for {label}\")\n",
    "\n",
    "            # --- 2. FIT INPUT SCALERS (ROBUST) ---\n",
    "            print(\"Pre-computing all graph features to fit input scalers...\")\n",
    "            all_train_graphs_raw = [smiles_to_graph_label_specific(s, label, None) for s in all_smiles]\n",
    "            \n",
    "            # Sync graph list with all data (skipping any SMILES that fail featurization)\n",
    "            all_train_graphs_synced = []\n",
    "            all_y_scaled_synced = [] \n",
    "            all_y_original_synced = [] # Also sync original Y for the CV split\n",
    "            all_smiles_synced = []     # Also sync SMILES for the CV split\n",
    "            \n",
    "            for i, graph in enumerate(all_train_graphs_raw):\n",
    "                if graph is not None:\n",
    "                    all_train_graphs_synced.append(graph)\n",
    "                    all_y_scaled_synced.append(all_y_scaled[i]) \n",
    "                    all_y_original_synced.append(all_y[i]) # Keep the original, unscaled, clean Y\n",
    "                    all_smiles_synced.append(all_smiles[i]) # Keep the matching SMILES\n",
    "            \n",
    "            all_train_graphs = all_train_graphs_synced \n",
    "            all_y_scaled = np.array(all_y_scaled_synced)\n",
    "            all_y_original_df = pd.Series(all_y_original_synced) # Store as Series for .iloc\n",
    "            all_smiles_df = pd.Series(all_smiles_synced)         # Store as Series for .iloc\n",
    "\n",
    "            if not all_train_graphs:\n",
    "                print(f\"CRITICAL: No valid training graphs could be featurized for {label}. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            all_u_data = np.concatenate([d.u.numpy() for d in all_train_graphs], axis=0)\n",
    "            print(\"Using RobustScaler for U-Scaler.\")\n",
    "            u_scaler = RobustScaler().fit(all_u_data)  # Use RobustScaler\n",
    "            joblib.dump(u_scaler, u_scaler_path)\n",
    "            print(f\"Saved U-Scaler for {label}\")\n",
    "\n",
    "            all_x_data = torch.cat([d.x for d in all_train_graphs], dim=0)\n",
    "            all_x_continuous = all_x_data[:, ATOM_MAP_LEN:].numpy()\n",
    "            print(\"Using RobustScaler for X-Scaler.\")\n",
    "            x_scaler = RobustScaler().fit(all_x_continuous)  # Use RobustScaler\n",
    "            joblib.dump(x_scaler, x_scaler_path)\n",
    "            print(f\"Saved X-Scaler for {label}\")\n",
    "\n",
    "            # --- 3. APPLY SCALERS ---\n",
    "            all_data_objects_scaled = scale_graph_features(all_train_graphs, u_scaler, x_scaler, ATOM_MAP_LEN)\n",
    "            for i, data_obj in enumerate(all_data_objects_scaled):\n",
    "                data_obj.y = torch.tensor([[all_y_scaled[i]]], dtype=torch.float)\n",
    "            \n",
    "            # --- 4. K-FOLD CV LOOP (MODIFIED) ---\n",
    "            kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "            fold_val_scores = []\n",
    "            fold_indices_gen = kf.split(all_data_objects_scaled) # Split the synced, valid, scaled data\n",
    "\n",
    "            for fold, (train_idx, val_idx) in enumerate(fold_indices_gen):\n",
    "                print(f\"\\n--- Fold {fold+1}/{n_splits} for {label} ---\")\n",
    "                \n",
    "                train_data_list = [all_data_objects_scaled[i] for i in train_idx]\n",
    "                val_data_list = [all_data_objects_scaled[i] for i in val_idx] # <-- CREATE VAL LIST\n",
    "                \n",
    "                val_smiles_list = all_smiles_df.iloc[val_idx].tolist()\n",
    "                y_val_original = all_y_original_df.iloc[val_idx].values \n",
    "\n",
    "                fold_model = train_gnn_model(\n",
    "                    label,\n",
    "                    train_data_list, # Pass train data\n",
    "                    val_data_list,   # <-- Pass val data\n",
    "                    config['neurons'],    # <-- PASSES mlp_neurons\n",
    "                    config['dropouts'],   # <-- PASSES mlp_dropouts (FIXES ERROR)\n",
    "                    epochs=300       # <-- Train longer (will stop early)\n",
    "                )\n",
    "                \n",
    "                if fold_model:\n",
    "                    print(\"Running final validation prediction on the best model...\")\n",
    "                    val_preds_scaled = predict_with_gnn(fold_model, val_smiles_list, label, u_scaler, x_scaler, ATOM_MAP_LEN)\n",
    "                    \n",
    "                    train_y_scaled_median = 0.0 # RobustScaler median is 0\n",
    "                    val_preds_scaled_filled = pd.Series(val_preds_scaled).fillna(train_y_scaled_median)\n",
    "                    \n",
    "                    val_preds_original = y_scaler.inverse_transform(\n",
    "                        val_preds_scaled_filled.values.reshape(-1, 1)\n",
    "                    ).flatten()\n",
    "\n",
    "                    mae = mean_absolute_error(y_val_original, val_preds_original)\n",
    "                    print(f\"‚úÖ Fold {fold+1} Validation MAE (Original Scale): {mae:.4f}\")\n",
    "                    fold_val_scores.append(mae)\n",
    "                    \n",
    "                    model_save_name = f\"{label}_fold{fold}\"\n",
    "                    save_gnn_model(fold_model, model_save_name, model_dir)\n",
    "                    ensemble_models.append(fold_model)\n",
    "                else:\n",
    "                    print(f\"Warning: Training failed for Fold {fold+1}. Model will be skipped.\")\n",
    "            \n",
    "            if fold_val_scores:\n",
    "                avg_cv_mae = np.mean(fold_val_scores)\n",
    "                print(f\"\\n{'*'*10} Average CV MAE for {label} (Original Scale): {avg_cv_mae:.4f} {'*'*10}\")\n",
    "                cv_mae_results.append({'label': label, 'avg_cv_mae': avg_cv_mae})\n",
    "\n",
    "        else:\n",
    "            # --- PREDICTION-ONLY MODE ---\n",
    "            print(f\"Loading {n_splits} models and ALL 3 RobustScalers for {label} ensemble...\")\n",
    "            model_path = '/kaggle/input/neurips-2025/GATConv_v29/models/gnn/'\n",
    "            try:\n",
    "                y_scaler = joblib.load(f'{model_path}gnn_yscaler_{label}.joblib')\n",
    "                u_scaler = joblib.load(f'{model_path}gnn_uscaler_{label}.joblib')\n",
    "                x_scaler = joblib.load(f'{model_path}gnn_xscaler_{label}.joblib')\n",
    "                print(\"Loaded Y, U, and X RobustScalers.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"CRITICAL: Scaler files not found for {label}. Cannot make predictions.\")\n",
    "                continue\n",
    "\n",
    "            for fold in range(n_splits):\n",
    "                loaded_model = load_gnn_model(f\"{label}_fold{fold}\", model_path.rstrip('/'))\n",
    "                if loaded_model:\n",
    "                    ensemble_models.append(loaded_model)\n",
    "            \n",
    "            if not ensemble_models: print(f\"Warning: No models found for label {label}.\")\n",
    "            else: print(f\"Successfully loaded {len(ensemble_models)} models for ensemble.\")\n",
    "\n",
    "\n",
    "        # --- ENSEMBLE PREDICTION STEP (Test Set) ---\n",
    "        test_smiles = test_df['SMILES'].tolist()\n",
    "        \n",
    "        if ensemble_models and y_scaler and u_scaler and x_scaler:\n",
    "            print(f\"Making ensemble (scaled) predictions for {label} using {len(ensemble_models)} models...\")\n",
    "            all_fold_preds_scaled = []\n",
    "            for model in ensemble_models:\n",
    "                fold_test_preds_scaled = predict_with_gnn(model, test_smiles, label, u_scaler, x_scaler, ATOM_MAP_LEN)\n",
    "                all_fold_preds_scaled.append(fold_test_preds_scaled)\n",
    "            \n",
    "            preds_stack_scaled = np.stack(all_fold_preds_scaled)\n",
    "            final_ensemble_preds_scaled = np.nanmean(preds_stack_scaled, axis=0) \n",
    "            pred_series_scaled = pd.Series(final_ensemble_preds_scaled)\n",
    "            \n",
    "            pred_series_scaled_filled = pred_series_scaled.fillna(0.0) # Impute with scaled median (0.0)\n",
    "\n",
    "            final_preds_original = y_scaler.inverse_transform(\n",
    "                pred_series_scaled_filled.values.reshape(-1, 1)\n",
    "            ).flatten()\n",
    "            \n",
    "            output_df[label] = final_preds_original\n",
    "            \n",
    "        else:\n",
    "            print(f\"No models or scalers available for {label}. Filling with (filtered) training median.\")\n",
    "            # Robust median fallback logic\n",
    "            fallback_median = 0.0\n",
    "            try:\n",
    "                if 'all_y' in locals() and not all_y.empty:\n",
    "                     fallback_median = all_y.median()\n",
    "                else: \n",
    "                     print(\"Loading data to calculate fallback median...\")\n",
    "                     fb_y_raw = subtables[label][label]\n",
    "                     fb_y_num = pd.to_numeric(fb_y_raw, errors='coerce')\n",
    "                     valid_min, valid_max = VALID_RANGES.get(label, (-np.inf, np.inf))\n",
    "                     fb_mask = (fb_y_num >= valid_min) & (fb_y_num <= valid_max) & (fb_y_num.notna())\n",
    "                     fallback_median = fb_y_num[fb_mask].median()\n",
    "                print(f\"Using filtered median fallback: {fallback_median}\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error getting median, falling back to 0: {e}\")\n",
    "                 fallback_median = 0.0 \n",
    "                 \n",
    "            output_df[label] = fallback_median\n",
    "\n",
    "    # --- Display final CV MAE summary ---\n",
    "    if train_model and cv_mae_results:\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"üìä HYBRID GNN 5-Fold CV MAE Summary (Original Scale):\")\n",
    "        print(\"=\"*40)\n",
    "        mae_df = pd.DataFrame(cv_mae_results)\n",
    "        print(mae_df.to_string(index=False))\n",
    "        mae_df.to_csv(\"gnn_hybrid_cv_mae_results.csv\", index=False)\n",
    "        print(\"\\nCV results saved to gnn_hybrid_cv_mae_results.csv\")\n",
    "\n",
    "    submission_path = 'submission_hybrid_gnn_final.csv'\n",
    "    output_df.to_csv(submission_path, index=False)\n",
    "    print(f\"\\n‚úÖ GNN Ensemble predictions (Original Scale) saved to {submission_path}\")\n",
    "    \n",
    "    warnings.filterwarnings(\"default\", \"Mean of empty slice\", RuntimeWarning)\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "# To train the models and then predict:\n",
    "gnn_submission_df = train_or_predict_gnn(train_model=False)\n",
    "\n",
    "output_dfs.append(gnn_submission_df)\n",
    "\n",
    "print(\"\\nGNN Submission Preview:\")\n",
    "print(gnn_submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcf56695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T21:54:32.905899Z",
     "iopub.status.busy": "2025-09-14T21:54:32.905635Z",
     "iopub.status.idle": "2025-09-14T21:54:32.910735Z",
     "shell.execute_reply": "2025-09-14T21:54:32.910050Z"
    },
    "papermill": {
     "duration": 0.018637,
     "end_time": "2025-09-14T21:54:32.911904",
     "exception": false,
     "start_time": "2025-09-14T21:54:32.893267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d98f61ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T21:54:32.936597Z",
     "iopub.status.busy": "2025-09-14T21:54:32.936396Z",
     "iopub.status.idle": "2025-09-14T21:54:32.955267Z",
     "shell.execute_reply": "2025-09-14T21:54:32.954601Z"
    },
    "papermill": {
     "duration": 0.032362,
     "end_time": "2025-09-14T21:54:32.956353",
     "exception": false,
     "start_time": "2025-09-14T21:54:32.923991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1109053969</td>\n",
       "      <td>178.488763</td>\n",
       "      <td>0.374376</td>\n",
       "      <td>0.184476</td>\n",
       "      <td>1.169528</td>\n",
       "      <td>21.164075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1422188626</td>\n",
       "      <td>173.045594</td>\n",
       "      <td>0.375658</td>\n",
       "      <td>0.253156</td>\n",
       "      <td>1.093830</td>\n",
       "      <td>21.549809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2032016830</td>\n",
       "      <td>111.899482</td>\n",
       "      <td>0.350854</td>\n",
       "      <td>0.251462</td>\n",
       "      <td>1.113791</td>\n",
       "      <td>20.611282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id          Tg       FFV        Tc   Density         Rg\n",
       "0  1109053969  178.488763  0.374376  0.184476  1.169528  21.164075\n",
       "1  1422188626  173.045594  0.375658  0.253156  1.093830  21.549809\n",
       "2  2032016830  111.899482  0.350854  0.251462  1.113791  20.611282"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average predictions from all output DataFrames\n",
    "final_df = pd.concat(output_dfs, axis=0).groupby('id').mean().reset_index()\n",
    "final_df.to_csv('submission.csv', index=False)\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12966160,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "datasetId": 7279248,
     "sourceId": 11605551,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7678100,
     "sourceId": 12189904,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7690162,
     "sourceId": 12207625,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7706066,
     "sourceId": 12237259,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7709869,
     "sourceId": 12330396,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8238327,
     "sourceId": 13048231,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 247698673,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 247701857,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 251268093,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 380893,
     "modelInstanceId": 359690,
     "sourceId": 442871,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 136.970595,
   "end_time": "2025-09-14T21:54:36.548919",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-14T21:52:19.578324",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
