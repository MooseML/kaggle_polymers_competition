{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61979795",
   "metadata": {},
   "source": [
    "# Polymer Property Predictions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ace_tools_open as tools\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import pickle\n",
    "import joblib\n",
    "import os \n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Module, Sequential, Dropout\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# OGB dataset \n",
    "from ogb.lsc import PygPCQM4Mv2Dataset, PCQM4Mv2Dataset\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# RDKit\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "\n",
    "# ChemML\n",
    "from chemml.chem import Molecule, RDKitFingerprint, CoulombMatrix, tensorise_molecules\n",
    "from chemml.models import MLP, NeuralGraphHidden, NeuralGraphOutput\n",
    "from chemml.utils import regression_metrics\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589db70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Built with CUDA: True\n",
      "CUDA available: True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device: /physical_device:GPU:0\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: data\n",
      "LMDB directory: data\\processed_chunks\n",
      "Train LMDB: data\\processed_chunks\\polymer_train3d_dist.lmdb\n",
      "Test LMDB: data\\processed_chunks\\polymer_test3d_dist.lmdb\n",
      "LMDBs already exist.\n"
     ]
    }
   ],
   "source": [
    "# Paths - Fixed for Kaggle environment\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  # Writable directory\n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/best_gnn_transformer_hybrid.pt'\n",
    "else:\n",
    "    DATA_ROOT = 'data'\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = 'best_gnn_transformer_hybrid.pt'\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"LMDB directory: {CHUNK_DIR}\")\n",
    "print(f\"Train LMDB: {TRAIN_LMDB}\")\n",
    "print(f\"Test LMDB: {TEST_LMDB}\")\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    # Run the LMDB builders\n",
    "    !python build_polymer_lmdb_fixed.py train\n",
    "    !python build_polymer_lmdb_fixed.py test\n",
    "    print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c34b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Tg → parents train=  408 val=  103 | aug rows train=  4080 val=  1030\n",
      "    FFV → parents train= 5624 val= 1406 | aug rows train= 56240 val= 14060\n",
      "     Tc → parents train=  589 val=  148 | aug rows train=  5890 val=  1480\n",
      "Density → parents train=  490 val=  123 | aug rows train=  4900 val=  1230\n",
      "     Rg → parents train=  491 val=  123 | aug rows train=  4910 val=  1230\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 1: parent-aware wiring (works for both GNN + ET) ====\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "AUG_KEY_MULT = 1000  # must match the LMDB builder\n",
    "\n",
    "# Paths expected: DATA_ROOT, TRAIN_LMDB\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_csv[\"id\"] = train_csv[\"id\"].astype(int)\n",
    "\n",
    "# LMDB ids (augmented key_ids)\n",
    "lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "lmdb_ids = np.loadtxt(lmdb_ids_path, dtype=np.int64)\n",
    "if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "\n",
    "# Parent map (preferred); fallback derives from key structure\n",
    "pmap_path = TRAIN_LMDB + \".parent_map.tsv\"\n",
    "if os.path.exists(pmap_path):\n",
    "    pmap = pd.read_csv(pmap_path, sep=\"\\t\")  # cols: key_id, parent_id, aug_idx, seed\n",
    "    pmap[\"key_id\"] = pmap[\"key_id\"].astype(np.int64)\n",
    "    pmap[\"parent_id\"] = pmap[\"parent_id\"].astype(np.int64)\n",
    "else:\n",
    "    pmap = pd.DataFrame({\n",
    "        \"key_id\": lmdb_ids.astype(np.int64),\n",
    "        \"parent_id\": (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "    })\n",
    "\n",
    "parents_in_lmdb = np.sort(pmap[\"parent_id\"].unique().astype(np.int64))\n",
    "\n",
    "def parents_with_label(task: str) -> np.ndarray:\n",
    "    m = ~train_csv[task].isna()\n",
    "    have = train_csv.loc[m, \"id\"].astype(int).values\n",
    "    return np.intersect1d(have, parents_in_lmdb, assume_unique=False)\n",
    "\n",
    "def task_parent_split(task: str, test_size=0.2, seed=42):\n",
    "    parents_labeled = parents_with_label(task)\n",
    "    if parents_labeled.size == 0:\n",
    "        raise ValueError(f\"No parents with labels for {task}\")\n",
    "    p_tr, p_va = train_test_split(parents_labeled, test_size=test_size, random_state=seed)\n",
    "    tr_keys = pmap.loc[pmap.parent_id.isin(p_tr), \"key_id\"].astype(np.int64).values\n",
    "    va_keys = pmap.loc[pmap.parent_id.isin(p_va), \"key_id\"].astype(np.int64).values\n",
    "    return np.sort(tr_keys), np.sort(va_keys), np.sort(p_tr), np.sort(p_va)\n",
    "\n",
    "# Pools for all tasks (augmented key_ids for GNN)\n",
    "task_pools = {}\n",
    "task_parent_splits = {}\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys, p_tr, p_va = task_parent_split(t, test_size=0.2, seed=42)\n",
    "    task_pools[t] = (tr_keys, va_keys)\n",
    "    task_parent_splits[t] = (p_tr, p_va)\n",
    "\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys = task_pools[t]\n",
    "    p_tr, p_va = task_parent_splits[t]\n",
    "    print(f\"{t:>7} → parents train={len(p_tr):5d} val={len(p_va):5d} | aug rows train={len(tr_keys):6d} val={len(va_keys):6d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd3c3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# --- CONSTANT RDF EDGES: 12 edges -> 11 bins (ALWAYS) ---\n",
    "RDF_EDGES = torch.tensor([0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 6], dtype=torch.float32)\n",
    "RDF_NUM_BINS = len(RDF_EDGES) - 1  # 11\n",
    "\n",
    "def _hist_fixed(x: torch.Tensor, edges: torch.Tensor = RDF_EDGES):\n",
    "    \"\"\"Normalized histogram with a FIXED number of bins (len(edges) - 1).\"\"\"\n",
    "    if x.numel() == 0:\n",
    "        return [0.0] * (len(edges) - 1)\n",
    "    h = torch.histc(x, bins=len(edges) - 1, min=float(edges[0]), max=float(edges[-1]))\n",
    "    h = (h / (h.sum() + 1e-8)).tolist()\n",
    "    return h\n",
    "\n",
    "def _rbf(d: torch.Tensor, K: int = 32, beta: float = 5.0, dmax: float = 6.0, device=None):\n",
    "    c = torch.linspace(0.0, dmax, K, device=device)\n",
    "    return torch.exp(-beta * (d.unsqueeze(-1) - c) ** 2)  # [M,K]\n",
    "\n",
    "def geom_features_from_rec(\n",
    "    rec,\n",
    "    rdkit_dim_expected: int = 15,\n",
    "    rbf_K: int = 32,\n",
    "    max_pairs: int = 20000\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns a FIXED-LENGTH (120) feature vector per LMDB record:\n",
    "      15 RDKit globals\n",
    "      5  sizes/degree/has_xyz     : [n_atoms, n_bonds, deg_mean, deg_max, has_xyz]\n",
    "      3  inertia eigenvalues      : λ1..λ3 (descending)\n",
    "      2  shape                    : [Rg_geom, anisotropy]\n",
    "      3  bbox extents             : [dx, dy, dz]\n",
    "      3  radius-from-centroid     : [mean, std, max]\n",
    "      4  bond distance stats      : [mean, std, min, max]\n",
    "      5  SPD histogram            : [hop0, hop1, hop2, hop3, hop>=4] (normalized)\n",
    "      5  extra atom mean (if 5-D; else zeros)\n",
    "      32 RBF(bond distances) mean\n",
    "      32 RBF(pairwise distances) mean (sampled if too large)\n",
    "      11 RDF histogram over pairwise distances (0..6Å, fixed bins)\n",
    "      Total = 120 dims\n",
    "    \"\"\"\n",
    "    # ---- RDKit globals (expected 15) ----\n",
    "    rd = getattr(rec, \"rdkit_feats\", None)\n",
    "    if rd is not None:\n",
    "        rd = torch.as_tensor(rd).view(-1).float().detach().cpu().numpy()\n",
    "    else:\n",
    "        rd = np.zeros((rdkit_dim_expected,), dtype=np.float32)\n",
    "    if rd.size != rdkit_dim_expected:\n",
    "        rd = np.zeros((rdkit_dim_expected,), dtype=np.float32)\n",
    "\n",
    "    # ---- Graph sizes & degree ----\n",
    "    x  = torch.as_tensor(getattr(rec, \"x\", np.zeros((0, 1), np.float32)))\n",
    "    ei = torch.as_tensor(getattr(rec, \"edge_index\", np.zeros((2, 0), np.int64)))\n",
    "    n  = int(x.shape[0])\n",
    "    e  = int(ei.shape[1]) if ei.ndim == 2 else 0\n",
    "    deg = torch.bincount(ei[0], minlength=n) if e > 0 else torch.zeros(n, dtype=torch.long)\n",
    "    deg_mean = deg.float().mean().item() if n > 0 else 0.0\n",
    "    deg_max  = deg.max().item() if n > 0 else 0.0\n",
    "\n",
    "    # ---- has_xyz ----\n",
    "    has_xyz = 0\n",
    "    if hasattr(rec, \"has_xyz\"):\n",
    "        hz = getattr(rec, \"has_xyz\")\n",
    "        has_xyz = int(bool(hz[0].item() if isinstance(hz, torch.Tensor) else hz))\n",
    "\n",
    "    # ---- Geometry from pos ----\n",
    "    pos = getattr(rec, \"pos\", None)\n",
    "    inertia = np.zeros(3, dtype=np.float32)\n",
    "    rg_geom = 0.0\n",
    "    anisotropy = 0.0\n",
    "    extents = np.zeros(3, dtype=np.float32)\n",
    "    rad_stats = np.zeros(3, dtype=np.float32)\n",
    "    bond_stats = np.zeros(4, dtype=np.float32)  # mean, std, min, max\n",
    "\n",
    "    rbf_pair_mean = np.zeros(rbf_K, dtype=np.float32)\n",
    "    rbf_bond_mean = np.zeros(rbf_K, dtype=np.float32)\n",
    "    rdf_hist = [0.0] * RDF_NUM_BINS  # ALWAYS 11 bins\n",
    "    dists = torch.tensor([])  # keep a handle for later checks\n",
    "\n",
    "    if pos is not None and n > 0 and has_xyz:\n",
    "        P = torch.as_tensor(pos).float()\n",
    "        ctr = P.mean(0, keepdim=True)\n",
    "        C = P - ctr\n",
    "\n",
    "        # inertia tensor (mass = 1 per atom)\n",
    "        I = torch.zeros(3, 3, dtype=P.dtype, device=P.device)\n",
    "        for r in C:\n",
    "            x_, y_, z_ = r\n",
    "            I += torch.tensor([[y_*y_ + z_*z_, -x_*y_,        -x_*z_],\n",
    "                               [ -x_*y_,       x_*x_ + z_*z_, -y_*z_],\n",
    "                               [ -x_*z_,       -y_*z_,        x_*x_ + y_*y_]],\n",
    "                              dtype=P.dtype, device=P.device)\n",
    "        evals, _ = torch.linalg.eigh(I)   # ascending\n",
    "        lam1, lam2, lam3 = evals.flip(0)  # descending\n",
    "        inertia = torch.stack([lam1, lam2, lam3]).detach().cpu().numpy()\n",
    "        rg_geom = float(torch.sqrt(evals.sum() / max(1, n)))\n",
    "        anisotropy = float((lam1 - (lam2 + lam3) / 2.0) / (evals.sum() + 1e-8))\n",
    "\n",
    "        # bbox extents\n",
    "        mn, mx = P.min(0).values, P.max(0).values\n",
    "        extents = (mx - mn).detach().cpu().numpy()\n",
    "\n",
    "        # radii from centroid\n",
    "        r = C.norm(dim=1)\n",
    "        rad_stats = np.array([\n",
    "            r.mean().item(),\n",
    "            r.std(unbiased=False).item(),\n",
    "            r.max().item()\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        # pairwise distances (cap for speed)\n",
    "        if n >= 2:\n",
    "            total_pairs = n * (n - 1) // 2\n",
    "            if total_pairs > max_pairs:\n",
    "                # kNN-style sampling to approximate the distribution\n",
    "                k = int(math.sqrt(max_pairs))\n",
    "                a = min(n, k)\n",
    "                anchors = torch.randperm(n)[:a]\n",
    "                dmat = torch.cdist(P[anchors], P)\n",
    "                _, nn = torch.topk(dmat, k=min(n, k), largest=False)\n",
    "                dists = (P[anchors].unsqueeze(1) - P[nn]).norm(dim=2).reshape(-1)\n",
    "            else:\n",
    "                dists = torch.pdist(P, p=2)\n",
    "\n",
    "            if dists.numel() > 0:\n",
    "                # FIXED-LENGTH RDF\n",
    "                rdf_hist = _hist_fixed(dists, RDF_EDGES)\n",
    "                # RBF over pairs\n",
    "                rbf_pair = _rbf(dists, K=rbf_K, beta=5.0, dmax=float(RDF_EDGES[-1]), device=P.device)\n",
    "                rbf_pair_mean = rbf_pair.mean(0).detach().cpu().numpy()\n",
    "\n",
    "        # bond distances + RBF\n",
    "        if e > 0:\n",
    "            d_bond = (P[ei[0]] - P[ei[1]]).norm(dim=1)\n",
    "            bond_stats = np.array([\n",
    "                d_bond.mean().item(),\n",
    "                d_bond.std(unbiased=False).item(),\n",
    "                d_bond.min().item(),\n",
    "                d_bond.max().item(),\n",
    "            ], dtype=np.float32)\n",
    "            rbf_bond = _rbf(d_bond, K=rbf_K, beta=5.0, dmax=float(RDF_EDGES[-1]), device=P.device)\n",
    "            rbf_bond_mean = rbf_bond.mean(0).detach().cpu().numpy()\n",
    "\n",
    "    # ---- SPD histogram (prefer 'hops', fallback 'dist') ----\n",
    "    spd_hist = np.zeros(5, dtype=np.float32)  # [0,1,2,3,>=4]\n",
    "    H = getattr(rec, \"hops\", None)\n",
    "    if H is None:\n",
    "        H = getattr(rec, \"dist\", None)\n",
    "    if H is not None:\n",
    "        H = torch.as_tensor(H).float()\n",
    "        if H.ndim == 2:\n",
    "            H = H[:n, :n]\n",
    "            finite = H[torch.isfinite(H) & (H >= 0)]\n",
    "            if finite.numel() > 0:\n",
    "                counts = [\n",
    "                    (finite == 0).float().sum(),\n",
    "                    (finite == 1).float().sum(),\n",
    "                    (finite == 2).float().sum(),\n",
    "                    (finite == 3).float().sum(),\n",
    "                    (finite >= 4).float().sum(),\n",
    "                ]\n",
    "                total = sum(counts) + 1e-8\n",
    "                spd_hist = np.array([float(c / total) for c in counts], dtype=np.float32)\n",
    "\n",
    "    # ---- extra atom features mean (expect 5 dims if present) ----\n",
    "    extra_mean = np.zeros(5, dtype=np.float32)\n",
    "    if hasattr(rec, \"extra_atom_feats\") and getattr(rec, \"extra_atom_feats\") is not None:\n",
    "        EA = torch.as_tensor(rec.extra_atom_feats).float()\n",
    "        if EA.ndim == 2 and EA.shape[1] == 5:\n",
    "            extra_mean = EA.mean(0).detach().cpu().numpy()\n",
    "\n",
    "    scalars = np.array([n, e, deg_mean, deg_max, float(has_xyz)], dtype=np.float32)\n",
    "    rdf_flat = np.array(rdf_hist, dtype=np.float32)  # ALWAYS length 11\n",
    "\n",
    "    vec = np.concatenate([\n",
    "        rd,                     # 15\n",
    "        scalars,                # 5  -> 20\n",
    "        inertia,                # 3  -> 23\n",
    "        np.array([rg_geom, anisotropy], dtype=np.float32),  # 2 -> 25\n",
    "        extents,                # 3  -> 28\n",
    "        rad_stats,              # 3  -> 31\n",
    "        bond_stats,             # 4  -> 35\n",
    "        spd_hist,               # 5  -> 40\n",
    "        extra_mean,             # 5  -> 45\n",
    "        rbf_bond_mean,          # 32 -> 77\n",
    "        rbf_pair_mean,          # 32 -> 109\n",
    "        rdf_flat                # 11 -> 120\n",
    "    ], axis=0)\n",
    "\n",
    "    # Safety: enforce fixed size 120 (pad/truncate if anything drifts)\n",
    "    if vec.shape[0] != 120:\n",
    "        if vec.shape[0] < 120:\n",
    "            vec = np.pad(vec, (0, 120 - vec.shape[0]), mode='constant')\n",
    "        else:\n",
    "            vec = vec[:120]\n",
    "    return vec.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a0a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors as rdmd, DataStructs\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def morgan_bits(smiles_list, n_bits=1024, radius=3):\n",
    "    X = np.zeros((len(smiles_list), n_bits), dtype=np.uint8)\n",
    "    for i, s in enumerate(smiles_list):\n",
    "        arr = np.zeros((n_bits,), dtype=np.uint8)\n",
    "        m = Chem.MolFromSmiles(s)\n",
    "        if m is not None:\n",
    "            fp = rdmd.GetMorganFingerprintAsBitVect(m, radius=radius, nBits=n_bits)\n",
    "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        X[i] = arr\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "def build_rf_features_from_lmdb(ids: np.ndarray, lmdb_path: str, smiles_list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns X = [Morgan1024 | LMDB-3D-global(69)] for each id/smiles.\n",
    "    Assumes ids and smiles_list are aligned with the CSV used to build LMDB.\n",
    "    \"\"\"\n",
    "    base = LMDBDataset(ids, lmdb_path)\n",
    "    # 3D/global block\n",
    "    feats3d = []\n",
    "    for i in range(len(base)):\n",
    "        rec = base[i]\n",
    "        feats3d.append(geom_features_from_rec(rec))  # shape (69,)\n",
    "    X3d = np.vstack(feats3d).astype(np.float32) if feats3d else np.zeros((0, 69), dtype=np.float32)\n",
    "\n",
    "    # Morgan FP block (2D)\n",
    "    Xfp = morgan_bits(smiles_list, n_bits=1024, radius=3)   # (N,1024)\n",
    "\n",
    "    # concat\n",
    "    X = np.hstack([Xfp, X3d]).astype(np.float32)            # (N, 1024+69)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0467213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 4: fp3d features aggregated per parent for ET ====\n",
    "AUG_KEY_MULT = 1000  # must match builder\n",
    "\n",
    "def build_fp3d_features_from_lmdb_parents(parent_ids, lmdb_path, smiles_list, *, agg=\"mean\"):\n",
    "    \"\"\"\n",
    "    Expands each parent -> its augmented key_ids, calls your existing\n",
    "    build_rf_features_from_lmdb(key_ids, lmdb_path, smiles_for_each_key),\n",
    "    then aggregates per parent (mean/median/max) -> one row per parent.\n",
    "    Returns X_parent, keep_idx (indices into parent_ids/smiles_list).\n",
    "    \"\"\"\n",
    "    # parent_map\n",
    "    pmap_path = lmdb_path + \".parent_map.tsv\"\n",
    "    if os.path.exists(pmap_path):\n",
    "        pmap = pd.read_csv(pmap_path, sep=\"\\t\")\n",
    "        pmap['key_id'] = pmap['key_id'].astype(np.int64)\n",
    "        pmap['parent_id'] = pmap['parent_id'].astype(np.int64)\n",
    "        group = pmap.groupby('parent_id')['key_id'].apply(list).to_dict()\n",
    "    else:\n",
    "        lmdb_ids = np.loadtxt(lmdb_path + \".ids.txt\", dtype=np.int64)\n",
    "        if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "        dfmap = pd.DataFrame({\n",
    "            'parent_id': (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "            'key_id': lmdb_ids.astype(np.int64),\n",
    "        })\n",
    "        group = dfmap.groupby('parent_id')['key_id'].apply(list).to_dict()\n",
    "\n",
    "    # expand\n",
    "    flat_keys, flat_smiles, seg_sizes = [], [], []\n",
    "    for pid, smi in zip(parent_ids, smiles_list):\n",
    "        keys = group.get(int(pid), [])\n",
    "        seg_sizes.append(len(keys))\n",
    "        if len(keys):\n",
    "            flat_keys.extend(keys)\n",
    "            flat_smiles.extend([smi] * len(keys))\n",
    "\n",
    "    if len(flat_keys) == 0:\n",
    "        raise ValueError(\"No augmented key_ids found for provided parent ids.\")\n",
    "\n",
    "    # IMPORTANT: this uses your existing function\n",
    "    X_all = build_rf_features_from_lmdb(np.array(flat_keys, dtype=np.int64),\n",
    "                                        lmdb_path,\n",
    "                                        flat_smiles)  # -> (sum_augs, D)\n",
    "\n",
    "    # fold back per parent\n",
    "    rows, keep_idx = [], []\n",
    "    i0 = 0\n",
    "    for i, k in enumerate(seg_sizes):\n",
    "        if k == 0: continue\n",
    "        Xi = X_all[i0:i0+k]\n",
    "        i0 += k\n",
    "        if   agg == \"mean\":   rows.append(Xi.mean(axis=0))\n",
    "        elif agg == \"median\": rows.append(np.median(Xi, axis=0))\n",
    "        elif agg == \"max\":    rows.append(Xi.max(axis=0))\n",
    "        else: raise ValueError(f\"agg={agg} not supported\")\n",
    "        keep_idx.append(i)\n",
    "\n",
    "    X_parent = np.vstack(rows).astype(np.float32)\n",
    "    keep_idx = np.asarray(keep_idx, dtype=int)\n",
    "    return X_parent, keep_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e663914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Optional, Tuple, List\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import rdMolDescriptors as rdmd, DataStructs\n",
    "\n",
    "# def smiles_to_morgan_fp(smi: str, n_bits=1024, radius=3) -> Optional[np.ndarray]:\n",
    "#     m = Chem.MolFromSmiles(smi)\n",
    "#     if m is None: return None\n",
    "#     bv = rdmd.GetMorganFingerprintAsBitVect(m, radius, nBits=n_bits)\n",
    "#     arr = np.zeros((n_bits,), dtype=np.int8)\n",
    "#     DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "#     return arr.astype(np.float32)\n",
    "\n",
    "# def build_features_for_rows(\n",
    "#     ids: np.ndarray,\n",
    "#     smiles: List[str],\n",
    "#     *,\n",
    "#     feature_backend: str,           # \"fp\" or \"fp3d\"\n",
    "#     lmdb_path: Optional[str] = None,\n",
    "#     rbf_K: int = 32,\n",
    "#     cache_npz: Optional[str] = None\n",
    "# ) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Return X for rows in the given order.\n",
    "#     If feature_backend==\"fp3d\", requires lmdb_path and uses LMDBDataset.\n",
    "#     Optionally caches to an .npz file keyed by a hash of ids+backend.\n",
    "#     \"\"\"\n",
    "#     assert feature_backend in {\"fp\", \"fp3d\"}\n",
    "#     N = len(smiles)\n",
    "\n",
    "#     # Optional cache\n",
    "#     if cache_npz and os.path.exists(cache_npz):\n",
    "#         try:\n",
    "#             z = np.load(cache_npz, allow_pickle=False)\n",
    "#             return z[\"X\"]\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "#     # FP block\n",
    "#     Xfp = np.zeros((N, 1024), dtype=np.float32)\n",
    "#     keep = np.ones(N, dtype=bool)\n",
    "#     for i, s in enumerate(smiles):\n",
    "#         arr = smiles_to_morgan_fp(s)\n",
    "#         if arr is None:\n",
    "#             keep[i] = False\n",
    "#         else:\n",
    "#             Xfp[i] = arr\n",
    "\n",
    "#     if feature_backend == \"fp\":\n",
    "#         X = Xfp[keep]\n",
    "#     else:\n",
    "#         assert lmdb_path is not None, \"lmdb_path required for feature_backend='fp3d'\"\n",
    "#         from dataset_polymer_fixed import LMDBDataset\n",
    "#         ds = LMDBDataset(ids.astype(int), lmdb_path)\n",
    "#         feats3d = []\n",
    "#         for i in range(len(ds)):\n",
    "#             rec = ds[i]\n",
    "#             feats3d.append(geom_features_from_rec(rec, rbf_K=rbf_K))\n",
    "#         X3d = np.vstack(feats3d).astype(np.float32) if feats3d else np.zeros((0, 1), dtype=np.float32)\n",
    "#         X = np.hstack([Xfp, X3d])[keep]\n",
    "\n",
    "#     if cache_npz:\n",
    "#         np.savez_compressed(cache_npz, X=X)\n",
    "#     return X\n",
    "\n",
    "\n",
    "# ==== Cell 5: override prepare_features_for_target for fp3d backend ====\n",
    "def prepare_features_for_target(\n",
    "    df: pd.DataFrame, target_col: str, *,\n",
    "    lmdb_path: str, feature_backend: str, cache_dir: str = None, agg: str = \"mean\"\n",
    "):\n",
    "    # filter to labeled parents present in LMDB\n",
    "    mask = ~df[target_col].isna()\n",
    "    parent_ids = df.loc[mask, 'id'].astype(int).values\n",
    "    smiles     = df.loc[mask, 'SMILES'].astype(str).tolist()\n",
    "    y          = df.loc[mask, target_col].astype(float).values\n",
    "\n",
    "    if feature_backend == \"fp3d\":\n",
    "        # aggregate augmented features -> one row per parent\n",
    "        X, keep_idx = build_fp3d_features_from_lmdb_parents(parent_ids, lmdb_path, smiles, agg=agg)\n",
    "        y = y[keep_idx]\n",
    "        df_clean = df.loc[mask].iloc[keep_idx].reset_index(drop=True)\n",
    "        return df_clean, y, X\n",
    "\n",
    "    # else: add your other backends here as you had before\n",
    "    raise ValueError(f\"Unknown feature_backend={feature_backend}\")\n",
    "\n",
    "    return work[[\"SMILES\", target_col, \"id\"]], y, X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe69f3",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff620911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "@dataclass\n",
    "class TabularSplits:\n",
    "    # unscaled (for RF)\n",
    "    X_train: np.ndarray\n",
    "    X_test:  np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    y_test:  np.ndarray\n",
    "    # scaled (for KRR/MLP)\n",
    "    X_train_scaled: Optional[np.ndarray] = None\n",
    "    X_test_scaled:  Optional[np.ndarray] = None\n",
    "    y_train_scaled: Optional[np.ndarray] = None  # shape (N,1)\n",
    "    y_test_scaled:  Optional[np.ndarray] = None\n",
    "    x_scaler: Optional[StandardScaler] = None\n",
    "    y_scaler: Optional[StandardScaler] = None\n",
    "\n",
    "def _make_regression_stratify_bins(y: np.ndarray, n_bins: int = 10) -> np.ndarray:\n",
    "    \"\"\"Return integer bins for approximate stratification in regression.\"\"\"\n",
    "    y = y.ravel()\n",
    "    # handle degenerate case\n",
    "    if np.unique(y).size < n_bins:\n",
    "        n_bins = max(2, np.unique(y).size)\n",
    "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "    bins = np.unique(np.quantile(y, quantiles))\n",
    "    # ensure strictly increasing\n",
    "    bins = np.unique(bins)\n",
    "    # np.digitize expects right-open intervals by default\n",
    "    strat = np.digitize(y, bins[1:-1], right=False)\n",
    "    return strat\n",
    "\n",
    "def make_tabular_splits(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    scale_X: bool = True,\n",
    "    scale_y: bool = True,\n",
    "    stratify_regression: bool = False,\n",
    "    n_strat_bins: int = 10,\n",
    "    # if you already decided splits (e.g., scaffold split), pass indices:\n",
    "    train_idx: Optional[np.ndarray] = None,\n",
    "    test_idx: Optional[np.ndarray] = None,\n",
    ") -> TabularSplits:\n",
    "    \"\"\"\n",
    "    Split and (optionally) scale tabular features/targets for a single target.\n",
    "    Returns both scaled and unscaled arrays, plus fitted scalers.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    if train_idx is not None and test_idx is not None:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "    else:\n",
    "        strat = None\n",
    "        if stratify_regression:\n",
    "            strat = _make_regression_stratify_bins(y, n_bins=n_strat_bins)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "        )\n",
    "\n",
    "    # Unscaled outputs (for RF, tree models)\n",
    "    splits = TabularSplits(\n",
    "        X_train=X_train, X_test=X_test,\n",
    "        y_train=y_train, y_test=y_test\n",
    "    )\n",
    "\n",
    "    # Scaled versions (for KRR/MLP)\n",
    "    if scale_X:\n",
    "        xscaler = StandardScaler()\n",
    "        splits.X_train_scaled = xscaler.fit_transform(X_train)\n",
    "        splits.X_test_scaled  = xscaler.transform(X_test)\n",
    "        splits.x_scaler = xscaler\n",
    "    if scale_y:\n",
    "        yscaler = StandardScaler()\n",
    "        splits.y_train_scaled = yscaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        splits.y_test_scaled  = yscaler.transform(y_test.reshape(-1, 1))\n",
    "        splits.y_scaler = yscaler\n",
    "\n",
    "    # Shapes summary\n",
    "    print(\"Splits:\")\n",
    "    print(\"X_train:\", splits.X_train.shape, \"| X_test:\", splits.X_test.shape)\n",
    "    if splits.X_train_scaled is not None:\n",
    "        print(\"X_train_scaled:\", splits.X_train_scaled.shape, \"| X_test_scaled:\", splits.X_test_scaled.shape)\n",
    "    print(\"y_train:\", splits.y_train.shape, \"| y_test:\", splits.y_test.shape)\n",
    "    if splits.y_train_scaled is not None:\n",
    "        print(\"y_train_scaled:\", splits.y_train_scaled.shape, \"| y_test_scaled:\", splits.y_test_scaled.shape)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c284cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import ExtraTreesRegressor as ETR\n",
    "def train_eval_et(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    et_params: Dict[str, Any],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    save_dir: str = \"saved_models/et\",\n",
    "    tag: str = \"model\",\n",
    ") -> Tuple[ExtraTreesRegressor, Dict[str, float], TabularSplits, str]:\n",
    "    \"\"\"\n",
    "    Trains a RandomForest on unscaled features; returns (model, metrics, splits, path).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # Pick a safe number of bins based on dataset size\n",
    "    if stratify_regression:\n",
    "        adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "    else:\n",
    "        adaptive_bins = n_strat_bins\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False,                 # RF doesn't need scaling\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=adaptive_bins\n",
    "    )\n",
    "\n",
    "    et = ETR(random_state=random_state, n_jobs=-1, **et_params)\n",
    "    et.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "    pred_tr = et.predict(splits.X_train)\n",
    "    pred_te = et.predict(splits.X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_MAE\": mean_absolute_error(splits.y_train, pred_tr),\n",
    "        \"train_RMSE\": mean_squared_error(splits.y_train, pred_tr),\n",
    "        \"train_R2\": r2_score(splits.y_train, pred_tr),\n",
    "        \"val_MAE\": mean_absolute_error(splits.y_test, pred_te),\n",
    "        \"val_RMSE\": mean_squared_error(splits.y_test, pred_te),\n",
    "        \"val_R2\": r2_score(splits.y_test, pred_te),\n",
    "    }\n",
    "    print(f\"[ET/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"et_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": et, \"metrics\": metrics, \"et_params\": et_params}, path)\n",
    "    return et, metrics, splits, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0438e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Dict, Any, Tuple\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# def train_eval_rf(\n",
    "#     X: np.ndarray,\n",
    "#     y: np.ndarray,\n",
    "#     *,\n",
    "#     rf_params: Dict[str, Any],\n",
    "#     test_size: float = 0.2,\n",
    "#     random_state: int = 42,\n",
    "#     stratify_regression: bool = True,\n",
    "#     n_strat_bins: int = 10,\n",
    "#     save_dir: str = \"saved_models/rf\",\n",
    "#     tag: str = \"model\",\n",
    "# ) -> Tuple[RandomForestRegressor, Dict[str, float], TabularSplits, str]:\n",
    "#     \"\"\"\n",
    "#     Trains a RandomForest on unscaled features; returns (model, metrics, splits, path).\n",
    "#     \"\"\"\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     # Pick a safe number of bins based on dataset size\n",
    "#     if stratify_regression:\n",
    "#         adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "#     else:\n",
    "#         adaptive_bins = n_strat_bins\n",
    "#     splits = make_tabular_splits(\n",
    "#         X, y,\n",
    "#         test_size=test_size,\n",
    "#         random_state=random_state,\n",
    "#         scale_X=False, scale_y=False,                 # RF doesn't need scaling\n",
    "#         stratify_regression=stratify_regression,\n",
    "#         n_strat_bins=adaptive_bins\n",
    "#     )\n",
    "\n",
    "#     rf = RandomForestRegressor(random_state=random_state, n_jobs=-1, **rf_params)\n",
    "#     rf.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "#     pred_tr = rf.predict(splits.X_train)\n",
    "#     pred_te = rf.predict(splits.X_test)\n",
    "\n",
    "#     metrics = {\n",
    "#         \"train_MAE\": mean_absolute_error(splits.y_train, pred_tr),\n",
    "#         \"train_RMSE\": mean_squared_error(splits.y_train, pred_tr, squared=False),\n",
    "#         \"train_R2\": r2_score(splits.y_train, pred_tr),\n",
    "#         \"val_MAE\": mean_absolute_error(splits.y_test, pred_te),\n",
    "#         \"val_RMSE\": mean_squared_error(splits.y_test, pred_te, squared=False),\n",
    "#         \"val_R2\": r2_score(splits.y_test, pred_te),\n",
    "#     }\n",
    "#     print(f\"[RF/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "#     path = os.path.join(save_dir, f\"rf_{tag}.joblib\")\n",
    "#     joblib.dump({\"model\": rf, \"metrics\": metrics, \"rf_params\": rf_params}, path)\n",
    "#     return rf, metrics, splits, path\n",
    "\n",
    "\n",
    "# rf_cfg = {\n",
    "#     \"FFV\": {\"n_estimators\": 100, \"max_depth\": 60},\n",
    "#     \"Tc\":  {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "#     \"Rg\":  {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "# }\n",
    "\n",
    "# rf_ffv, m_ffv, splits_ffv, p_ffv = train_eval_rf(X_ffv, y_ffv, rf_params=rf_cfg[\"FFV\"], tag=\"FFV\")\n",
    "# rf_tc,  m_tc,  splits_tc,  p_tc  = train_eval_rf(X_tc,  y_tc,  rf_params=rf_cfg[\"Tc\"],  tag=\"Tc\")\n",
    "# rf_rg,  m_rg,  splits_rg,  p_rg  = train_eval_rf(X_rg,  y_rg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Rg\")\n",
    "# rf_tg,  m_tg,  splits_tg,  p_tg  = train_eval_rf(X_tg,  y_tg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Tg\")\n",
    "# rf_density,  m_density,  splits_density,  p_density  = train_eval_rf(X_density,  y_density,  rf_params=rf_cfg[\"Rg\"],  tag=\"Density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac98b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_et_for_target(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    et_params: dict,\n",
    "    *,\n",
    "    lmdb_path: Optional[str],\n",
    "    feature_backend: str = \"fp3d\",   # default to augmented\n",
    "    save_dir: str = \"saved_models/et\",\n",
    "    tag_prefix: str = \"et\",\n",
    "    **split_kwargs\n",
    "):\n",
    "    df_clean, y, X = prepare_features_for_target(\n",
    "        df, target_col,\n",
    "        lmdb_path=lmdb_path,\n",
    "        feature_backend=feature_backend,\n",
    "        cache_dir=os.path.join(save_dir, \"cache\")\n",
    "    )\n",
    "    model, metrics, splits, path = train_eval_et(\n",
    "        X, y,\n",
    "        et_params=et_params,\n",
    "        save_dir=save_dir,\n",
    "        tag=f\"{tag_prefix}_{feature_backend}_{target_col}\",\n",
    "        **split_kwargs\n",
    "    )\n",
    "    return model, metrics, splits, path\n",
    "\n",
    "# rf_cfg_aug = {\n",
    "#     \"FFV\":     {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.2, \"bootstrap\": True},\n",
    "#     \"Tc\":      {\"n_estimators\": 800, \"max_depth\": 20, \"min_samples_split\": 6, \"min_samples_leaf\": 2, \"max_features\": \"sqrt\", \"bootstrap\": False},\n",
    "#     \"Rg\":      {\"n_estimators\": 400, \"max_depth\": 260, \"min_samples_split\": 6, \"min_samples_leaf\": 4, \"max_features\": 1.0, \"bootstrap\": True},\n",
    "#     \"Tg\":      {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.2, \"bootstrap\": True},\n",
    "#     \"Density\": {\"n_estimators\": 600, \"max_depth\": 40, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "# }\n",
    "\n",
    "etr_cfg_full = {\n",
    "  \"FFV\":     {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.2, \"bootstrap\": False},\n",
    "  \"Tc\":      {\"n_estimators\": 1500, \"max_depth\": None, \"min_samples_leaf\": 3, \"max_features\": 0.15, \"bootstrap\": False},\n",
    "  \"Rg\":      {\"n_estimators\": 400, \"max_depth\": 260, \"min_samples_split\": 6, \"min_samples_leaf\": 4, \"max_features\": 1.0, \"bootstrap\": True},\n",
    "  \"Tg\":      {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.2, \"bootstrap\": False},\n",
    "  \"Density\": {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.25, \"bootstrap\": False},\n",
    "}\n",
    "\n",
    "\n",
    "# TRAIN_CSV = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "# df_all = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# et_models, et_metrics = {}, {}\n",
    "# for t in [\"FFV\", \"Tg\", \"Tc\", \"Rg\", \"Density\"]:\n",
    "#     print(f\"\\n>>> ET ({t}) with backend=fp3d\")\n",
    "#     m, met, sp, p = train_et_for_target(\n",
    "#         df_all, t, etr_cfg_full[t],\n",
    "#         lmdb_path=TRAIN_LMDB,\n",
    "#         feature_backend=\"fp3d\",\n",
    "#         save_dir=\"saved_models/et_aug3d\",\n",
    "#         tag_prefix=\"aug3D\",\n",
    "#         test_size=0.2, random_state=42, stratify_regression=True, n_strat_bins=10,\n",
    "#     )\n",
    "#     et_models[t], et_metrics[t] = m, met\n",
    "#     print(f\"[ET+3D/{t}] val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc3120",
   "metadata": {},
   "source": [
    ">>> ET (FFV) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (5624, 1144) | X_test: (1406, 1144)\n",
    "y_train: (5624,) | y_test: (1406,)\n",
    "[ET/aug3D_fp3d_FFV] val_MAE=0.006635  val_RMSE=0.016826  val_R2=0.6880\n",
    "[ET+3D/FFV] val_MAE=0.006635  val_RMSE=0.016826  val_R2=0.6880\n",
    "\n",
    ">>> ET (Tg) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (408, 1144) | X_test: (103, 1144)\n",
    "y_train: (408,) | y_test: (103,)\n",
    "[ET/aug3D_fp3d_Tg] val_MAE=58.521052  val_RMSE=74.475532  val_R2=0.5826\n",
    "[ET+3D/Tg] val_MAE=58.521052  val_RMSE=74.475532  val_R2=0.5826\n",
    "\n",
    ">>> ET (Tc) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (589, 1144) | X_test: (148, 1144)\n",
    "y_train: (589,) | y_test: (148,)\n",
    "[ET/aug3D_fp3d_Tc] val_MAE=0.027990  val_RMSE=0.042644  val_R2=0.7591\n",
    "[ET+3D/Tc] val_MAE=0.027990  val_RMSE=0.042644  val_R2=0.7591\n",
    "\n",
    ">>> ET (Rg) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (491, 1144) | X_test: (123, 1144)\n",
    "y_train: (491,) | y_test: (123,)\n",
    "[ET/aug3D_fp3d_Rg] val_MAE=1.609396  val_RMSE=2.526705  val_R2=0.7227\n",
    "[ET+3D/Rg] val_MAE=1.609396  val_RMSE=2.526705  val_R2=0.7227\n",
    "\n",
    ">>> ET (Density) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (490, 1144) | X_test: (123, 1144)\n",
    "y_train: (490,) | y_test: (123,)\n",
    "[ET/aug3D_fp3d_Density] val_MAE=0.028135  val_RMSE=0.051842  val_R2=0.8850\n",
    "[ET+3D/Density] val_MAE=0.028135  val_RMSE=0.051842  val_R2=0.8850"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ac398",
   "metadata": {},
   "source": [
    "\n",
    "[ET/aug3D_fp3d_FFV] val_MAE=0.006635  val_RMSE=0.016826  val_R2=0.6880\n",
    "\n",
    "[ET/aug3D_fp3d_Tg] val_MAE=58.521052  val_RMSE=74.475532  val_R2=0.5826\n",
    "\n",
    "[ET/aug3D_fp3d_Tc] val_MAE=0.027990  val_RMSE=0.042644  val_R2=0.7591\n",
    "\n",
    "[ET/aug3D_fp3d_Rg] val_MAE=1.609396  val_RMSE=2.526705  val_R2=0.7227\n",
    "\n",
    "[ET/aug3D_fp3d_Density] val_MAE=0.028135  val_RMSE=0.051842  val_R2=0.8850\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda5af1",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9beafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add these imports once ---\n",
    "import os, joblib, numpy as np, pandas as pd\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# LightGBM / XGBoost\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# ========= Common metric helper =========\n",
    "def _reg_metrics(y_tr, p_tr, y_va, p_va):\n",
    "    return {\n",
    "        \"train_MAE\": mean_absolute_error(y_tr, p_tr),\n",
    "        \"train_RMSE\": mean_squared_error(y_tr, p_tr),\n",
    "        \"train_R2\": r2_score(y_tr, p_tr),\n",
    "        \"val_MAE\": mean_absolute_error(y_va, p_va),\n",
    "        \"val_RMSE\": mean_squared_error(y_va, p_va),\n",
    "        \"val_R2\": r2_score(y_va, p_va),\n",
    "    }\n",
    "\n",
    "# ========= LightGBM =========\n",
    "import lightgbm as lgb\n",
    "\n",
    "def train_eval_lgbm(\n",
    "    X, y, *,\n",
    "    lgbm_params,\n",
    "    test_size=0.2, random_state=42,\n",
    "    stratify_regression=True, n_strat_bins=10,\n",
    "    save_dir=\"saved_models/lgbm\", tag=\"model\",\n",
    "    early_stopping_rounds=400,\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y))))) if stratify_regression else n_strat_bins\n",
    "    splits = make_tabular_splits(\n",
    "        X, y, test_size=test_size, random_state=random_state,\n",
    "        scale_X=False, scale_y=False,\n",
    "        stratify_regression=stratify_regression, n_strat_bins=adaptive_bins\n",
    "    )\n",
    "\n",
    "    Xtr = np.asarray(splits.X_train, dtype=np.float32)\n",
    "    Ytr = np.asarray(splits.y_train, dtype=np.float32)\n",
    "    Xva = np.asarray(splits.X_test,  dtype=np.float32)\n",
    "    Yva = np.asarray(splits.y_test,  dtype=np.float32)\n",
    "\n",
    "    base = dict(\n",
    "        n_estimators=4000,\n",
    "        learning_rate=0.03,\n",
    "        objective=\"l1\",            # optimize MAE\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbosity=-1,              # quiet model logs\n",
    "    )\n",
    "    # scrub xgb-style aliases if they sneak in\n",
    "    lgb_params = {k: v for k, v in lgbm_params.items() if k not in (\"colsample_bytree\", \"subsample\", \"subsample_freq\")}\n",
    "    # if no bagging, drop bagging_freq to avoid warning\n",
    "    if lgb_params.get(\"bagging_fraction\", 1.0) >= 1.0:\n",
    "        lgb_params.pop(\"bagging_freq\", None)\n",
    "    base.update(lgb_params)\n",
    "\n",
    "    # optional: fully silence LightGBM's logger (including alias warnings)\n",
    "    try:\n",
    "        lgb.register_logger(lambda msg: None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    model = lgb.LGBMRegressor(**base)\n",
    "    model.fit(\n",
    "        Xtr, Ytr,\n",
    "        eval_set=[(Xva, Yva)],\n",
    "        eval_metric=\"l1\",\n",
    "        callbacks=[lgb.early_stopping(early_stopping_rounds, verbose=False),\n",
    "                   lgb.log_evaluation(period=0)]\n",
    "    )\n",
    "\n",
    "    p_tr = model.predict(Xtr, num_iteration=model.best_iteration_)\n",
    "    p_va = model.predict(Xva, num_iteration=model.best_iteration_)\n",
    "    metrics = _reg_metrics(Ytr, p_tr, Yva, p_va)\n",
    "    print(f\"[LGBM/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"lgbm_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": model, \"metrics\": metrics, \"lgbm_params\": base}, path)\n",
    "    return model, metrics, splits, path\n",
    "\n",
    "\n",
    "# ========= XGBoost =========\n",
    "def _xgb_tree_method():\n",
    "    # Use GPU if available (optional)\n",
    "    try:\n",
    "        import torch\n",
    "        return \"gpu_hist\" if torch.cuda.is_available() else \"hist\"\n",
    "    except Exception:\n",
    "        return \"hist\"\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import os, joblib, numpy as np, inspect\n",
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_eval_xgb(\n",
    "    X, y,\n",
    "    *,\n",
    "    xgb_params: Dict[str, Any],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    save_dir: str = \"saved_models/xgb\",\n",
    "    tag: str = \"model\",\n",
    "    early_stopping_rounds: int = 100,\n",
    ") -> Tuple[xgb.XGBRegressor, Dict[str, float], \"TabularSplits\", str]:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # ---- split (your helper)\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False,\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "    )\n",
    "    Xtr, Ytr, Xva, Yva = splits.X_train, splits.y_train, splits.X_test, splits.y_test\n",
    "    \n",
    "    base = dict(\n",
    "        device=\"cuda\",\n",
    "        n_estimators=6000,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        colsample_bylevel=0.8,\n",
    "        colsample_bynode=0.8,\n",
    "        reg_lambda=2.0,          # L2\n",
    "        reg_alpha=0.0,           # try 0.1–0.5 if overfitting\n",
    "        min_child_weight=2.0,    # ↑ to regularize more (3–6)\n",
    "        gamma=0.0,               # try 0.05–0.3 if splits look too eager\n",
    "        tree_method=\"hist\",      # use \"gpu_hist\" if you have a GPU\n",
    "        max_bin=512,             # denser histograms may help\n",
    "        objective=\"reg:squarederror\",  # fallback objective\n",
    "        eval_metric=\"mae\",\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    base.update(xgb_params)\n",
    "    model = xgb.XGBRegressor(**base)\n",
    "\n",
    "    # ---- Robust fit across versions\n",
    "    fit_sig = inspect.signature(xgb.XGBRegressor.fit)\n",
    "    supports_callbacks = \"callbacks\" in fit_sig.parameters\n",
    "    supports_esr = \"early_stopping_rounds\" in fit_sig.parameters\n",
    "\n",
    "    used_es = False\n",
    "    if supports_callbacks:\n",
    "        try:\n",
    "            from xgboost.callback import EarlyStopping\n",
    "            es_cb = EarlyStopping(rounds=early_stopping_rounds, save_best=True, maximize=False)\n",
    "            model.fit(Xtr, Ytr, eval_set=[(Xva, Yva)], verbose=False, callbacks=[es_cb])\n",
    "            used_es = True\n",
    "        except Exception:\n",
    "            pass\n",
    "    if (not used_es) and supports_esr:\n",
    "        try:\n",
    "            model.fit(Xtr, Ytr, eval_set=[(Xva, Yva)], verbose=False,\n",
    "                      early_stopping_rounds=early_stopping_rounds)\n",
    "            used_es = True\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not used_es:\n",
    "        # Fallback: train w/o early stopping\n",
    "        # Tip: keep n_estimators reasonable and rely on reg_*\n",
    "        print(\"[XGB] Early stopping not supported by this xgboost build — training without it.\")\n",
    "        model.fit(Xtr, Ytr, eval_set=[(Xva, Yva)], verbose=False)\n",
    "\n",
    "    # ---- Predict with best-iteration awareness where available\n",
    "    def _predict_best(mdl, Xdata):\n",
    "        # XGB >= 1.6 often exposes iteration_range; older exposes ntree_limit; older still – neither.\n",
    "        try:\n",
    "            booster = mdl.get_booster()\n",
    "        except Exception:\n",
    "            booster = None\n",
    "\n",
    "        # best_iteration on wrapper:\n",
    "        best_iter = getattr(mdl, \"best_iteration\", None)\n",
    "        if best_iter is not None:\n",
    "            try:\n",
    "                return mdl.predict(Xdata, iteration_range=(0, best_iter + 1))\n",
    "            except TypeError:\n",
    "                pass\n",
    "\n",
    "        # ntree_limit on booster:\n",
    "        if booster is not None and hasattr(booster, \"best_ntree_limit\"):\n",
    "            ntl = getattr(booster, \"best_ntree_limit\", None)\n",
    "            if ntl is not None and ntl > 0:\n",
    "                try:\n",
    "                    return mdl.predict(Xdata, ntree_limit=ntl)\n",
    "                except TypeError:\n",
    "                    pass\n",
    "\n",
    "        # Fallback:\n",
    "        return mdl.predict(Xdata)\n",
    "    \n",
    "    def _predict_best(mdl, Xdata):\n",
    "        # *** THE FIX: Explicitly move data to the GPU before prediction ***\n",
    "        # This prevents the warning and can improve performance.\n",
    "        Xdata_gpu = torch.from_numpy(Xdata).to(mdl.device)\n",
    "\n",
    "        try:\n",
    "            booster = mdl.get_booster()\n",
    "        except Exception:\n",
    "            booster = None\n",
    "\n",
    "        best_iter = getattr(mdl, \"best_iteration\", None)\n",
    "        if best_iter is not None:\n",
    "            try:\n",
    "                # Use the GPU tensor for prediction\n",
    "                return mdl.predict(Xdata_gpu, iteration_range=(0, best_iter + 1))\n",
    "            except TypeError:\n",
    "                pass\n",
    "\n",
    "        if booster is not None and hasattr(booster, \"best_ntree_limit\"):\n",
    "            ntl = getattr(booster, \"best_ntree_limit\", None)\n",
    "            if ntl is not None and ntl > 0:\n",
    "                try:\n",
    "                    # Use the GPU tensor for prediction\n",
    "                    return mdl.predict(Xdata_gpu, ntree_limit=ntl)\n",
    "                except TypeError:\n",
    "                    pass\n",
    "\n",
    "        # Fallback to CPU data if GPU prediction fails for some reason\n",
    "        return mdl.predict(Xdata)\n",
    "\n",
    "    pred_tr = _predict_best(model, Xtr)\n",
    "    pred_te = _predict_best(model, Xva)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_MAE\": mean_absolute_error(Ytr, pred_tr),\n",
    "        \"train_RMSE\": mean_squared_error(Ytr, pred_tr),\n",
    "        \"train_R2\": r2_score(Ytr, pred_tr),\n",
    "        \"val_MAE\": mean_absolute_error(Yva, pred_te),\n",
    "        \"val_RMSE\": mean_squared_error(Yva, pred_te),\n",
    "        \"val_R2\": r2_score(Yva, pred_te),\n",
    "    }\n",
    "    print(f\"[XGB/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"xgb_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": model, \"metrics\": metrics, \"xgb_params\": base, \"used_es\": used_es}, path)\n",
    "    return model, metrics, splits, path\n",
    "\n",
    "# ========= Dispatcher so your calling code stays tidy =========\n",
    "def train_tabular_for_target(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    model_name: str,                # 'etr' | 'lgbm' | 'xgb'\n",
    "    model_params: Dict[str, Any],\n",
    "    *,\n",
    "    lmdb_path: Optional[str],\n",
    "    feature_backend: str = \"fp3d\",\n",
    "    save_dir: str = \"saved_models/tabular\",\n",
    "    tag_prefix: str = \"tab\",\n",
    "    **split_kwargs\n",
    "):\n",
    "    df_clean, y, X = prepare_features_for_target(\n",
    "        df, target_col,\n",
    "        lmdb_path=lmdb_path,\n",
    "        feature_backend=feature_backend,\n",
    "        cache_dir=os.path.join(save_dir, \"cache\")\n",
    "    )\n",
    "    tag = f\"{tag_prefix}_{feature_backend}_{target_col}\"\n",
    "\n",
    "    if model_name.lower() == \"etr\":\n",
    "        from sklearn.ensemble import ExtraTreesRegressor as ETR\n",
    "        model, metrics, splits, path = train_eval_et(\n",
    "            X, y, et_params=model_params, save_dir=save_dir, tag=tag, **split_kwargs\n",
    "        )\n",
    "    elif model_name.lower() == \"lgbm\":\n",
    "        model, metrics, splits, path = train_eval_lgbm(\n",
    "            X, y, lgbm_params=model_params, save_dir=save_dir, tag=tag, **split_kwargs\n",
    "        )\n",
    "    elif model_name.lower() == \"xgb\":\n",
    "        model, metrics, splits, path = train_eval_xgb(\n",
    "            X, y, xgb_params=model_params, save_dir=save_dir, tag=tag, **split_kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"model_name must be one of: 'etr', 'lgbm', 'xgb'\")\n",
    "\n",
    "    return model, metrics, splits, path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4334e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_cfg = {\n",
    "  \"FFV\":     {\"num_leaves\": 127, \"min_child_samples\": 20, \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 1},\n",
    "  \"Tc\":      {'objective': 'regression_l1', 'learning_rate': 0.11826496463933994, 'num_leaves': 452, 'max_depth': -1, 'min_data_in_leaf': 13, 'min_split_gain': 0.07077032474764056, 'feature_fraction': 0.9220353641373867, 'bagging_fraction': 0.7178475806562494, 'lambda_l1': 5.870126202873261e-07, 'lambda_l2': 5.218320773596195e-05, 'bagging_freq': 3},\n",
    "  \"Rg\":      {'objective': 'regression_l1', 'learning_rate': 0.012498104173072, 'num_leaves': 77, 'max_depth': 6, 'min_data_in_leaf': 5, 'min_split_gain': 0.10421642537134, 'feature_fraction': 0.7064591956409744, 'bagging_fraction': 0.8068199036103922, 'lambda_l1': 1.6040584907223563e-08, 'lambda_l2': 4.615422442889681e-07, 'bagging_freq': 4},\n",
    "  \"Tg\":      {'objective': 'regression', 'learning_rate': 0.03623100041838883, 'num_leaves': 41, 'max_depth': -1, 'min_data_in_leaf': 60, 'min_split_gain': 0.19800773424146345, 'feature_fraction': 0.9585660159911279, 'bagging_fraction': 0.6080651761351819, 'lambda_l1': 0.00015459491585016372, 'lambda_l2': 6.600923276281373e-07, 'bagging_freq': 6},\n",
    "  \"Density\": {'objective': 'regression_l1', 'learning_rate': 0.014386060636303035, 'num_leaves': 102, 'max_depth': 4, 'min_data_in_leaf': 5, 'min_split_gain': 0.16942680482974726, 'feature_fraction': 0.5924797518298991, 'bagging_fraction': 0.9346086621083698, 'lambda_l1': 6.564856472007785e-08, 'lambda_l2': 0.009468122760559656, 'bagging_freq': 5},\n",
    "}\n",
    "\n",
    "# lgbm_cfg = {\n",
    "#   # smooth + strong signal\n",
    "#   \"FFV\":     {\"num_leaves\": 127, \"min_child_samples\": 20, \"feature_fraction\": 0.85, \"bagging_fraction\": 0.8, \"bagging_freq\": 1, \"lambda_l2\": 2.0},\n",
    "#   # moderate\n",
    "#   \"Tc\":      {\"num_leaves\": 63,  \"min_child_samples\": 20, \"feature_fraction\": 0.75, \"bagging_fraction\": 0.8, \"bagging_freq\": 1, \"lambda_l2\": 3.0},\n",
    "#   # more complex\n",
    "#   \"Rg\":      {\"num_leaves\": 255, \"min_child_samples\": 15, \"feature_fraction\": 0.9,  \"bagging_fraction\": 0.8, \"bagging_freq\": 1, \"lambda_l2\": 2.0, \"min_split_gain\": 0.01},\n",
    "#   # more complex\n",
    "#   \"Tg\":      {\"num_leaves\": 255, \"min_child_samples\": 15, \"feature_fraction\": 0.85, \"bagging_fraction\": 0.8, \"bagging_freq\": 1, \"lambda_l2\": 3.0, \"min_split_gain\": 0.01},\n",
    "#   # simpler\n",
    "#   \"Density\": {\"num_leaves\": 63,  \"min_child_samples\": 20, \"feature_fraction\": 0.8,  \"bagging_fraction\": 0.8, \"bagging_freq\": 1, \"lambda_l2\": 2.0},\n",
    "# }\n",
    "\n",
    "# xgb_cfg = {\n",
    "#   \"FFV\":     {\"max_depth\": 7, \"subsample\": 0.9, \"colsample_bytree\": 0.8, \"reg_lambda\": 1.0},\n",
    "#   \"Tc\":      {\"max_depth\": 6, \"subsample\": 0.9, \"colsample_bytree\": 0.8, \"reg_lambda\": 1.0},\n",
    "#   \"Rg\":      {\"max_depth\": 8, \"subsample\": 0.8, \"colsample_bytree\": 0.9, \"reg_lambda\": 1.0},\n",
    "#   \"Tg\":      {\"max_depth\": 8, \"subsample\": 0.8, \"colsample_bytree\": 0.8, \"reg_lambda\": 1.0},\n",
    "#   \"Density\": {\"max_depth\": 6, \"subsample\": 0.9, \"colsample_bytree\": 0.8, \"reg_lambda\": 1.0},\n",
    "# }\n",
    "xgb_cfg = {\n",
    "  \"FFV\":     {'objective': 'reg:absoluteerror', 'eta': 0.0114287249603117, 'max_depth': 11, 'min_child_weight': 8.74657524930709, 'subsample': 0.5034760652655954, 'colsample_bytree': 0.7553736512887829, 'colsample_bylevel': 0.7087055015743895, 'colsample_bynode': 0.6110539052353652, 'lambda': 0.003974905761171867, 'alpha': 1.0927895733904103e-05, 'gamma': 0.4714548519562596, 'max_bin': 1024, 'grow_policy': 'lossguide', 'max_leaves': 449},\n",
    "  \"Tc\":      {'objective': 'reg:absoluteerror', 'eta': 0.025090663566956314, 'max_depth': 12, 'min_child_weight': 6.1968781131090696, 'subsample': 0.6165892971655643, 'colsample_bytree': 0.7319696635455195, 'colsample_bylevel': 0.6241975729552441, 'colsample_bynode': 0.9936183664523051, 'lambda': 96.20132244931914, 'alpha': 3.147759100873883e-08, 'gamma': 0.34460453202719615, 'max_bin': 512, 'grow_policy': 'depthwise'},\n",
    "  \"Rg\":      {'objective': 'reg:absoluteerror', 'eta': 0.01435111533570771, 'max_depth': 5, 'min_child_weight': 4.018997069936428, 'subsample': 0.8611079146606072, 'colsample_bytree': 0.7761740838682192, 'colsample_bylevel': 0.9479225089613308, 'colsample_bynode': 0.9656509026704986, 'lambda': 28.605920863320357, 'alpha': 6.891536837408214e-07, 'gamma': 0.21921172256812527, 'max_bin': 1024, 'grow_policy': 'depthwise'},\n",
    "  \"Tg\":      {\"max_depth\": 10, \"min_child_weight\": 4.0, \"gamma\": 0.2, \"reg_lambda\": 3.0, \"reg_alpha\": 0.1, \"colsample_bytree\": 0.85},\n",
    "  \"Density\": {'objective': 'reg:absoluteerror', 'eta': 0.0030867498488133575, 'max_depth': 9, 'min_child_weight': 2.303294371061212, 'subsample': 0.9519675087287788, 'colsample_bytree': 0.7766998909434009, 'colsample_bylevel': 0.6187311242041665, 'colsample_bynode': 0.7959321722371097, 'lambda': 0.038520030462907764, 'alpha': 0.010852150664597634, 'gamma': 0.0014564429240612486, 'max_bin': 1024, 'grow_policy': 'lossguide', 'max_leaves': 142},\n",
    "}\n",
    "\n",
    "# xgb_cfg = {\n",
    "#   \"FFV\":     {\"grow_policy\": \"lossguide\", \"max_depth\": 0, \"max_leaves\": 256},\n",
    "#   \"Tc\":      {\"grow_policy\": \"lossguide\", \"max_depth\": 0, \"max_leaves\": 256},\n",
    "#   \"Rg\":      {\"grow_policy\": \"lossguide\", \"max_depth\": 0, \"max_leaves\": 256},\n",
    "#   \"Tg\":      {\"grow_policy\": \"lossguide\", \"max_depth\": 0, \"max_leaves\": 256},\n",
    "#   \"Density\": {\"grow_policy\": \"lossguide\", \"max_depth\": 0, \"max_leaves\": 256},\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8d70f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d4323e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> LGBM (FFV) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (5624, 1144) | X_test: (1406, 1144)\n",
      "y_train: (5624,) | y_test: (1406,)\n",
      "[LGBM/aug3D_fp3d_FFV] val_MAE=0.006486  val_RMSE=0.000317  val_R2=0.6509\n",
      "[LGBM+3D/FFV] val_MAE=0.006486  val_RMSE=0.000317  val_R2=0.6509\n",
      "\n",
      ">>> LGBM (Tg) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (408, 1144) | X_test: (103, 1144)\n",
      "y_train: (408,) | y_test: (103,)\n",
      "[LGBM/aug3D_fp3d_Tg] val_MAE=48.795881  val_RMSE=4017.619337  val_R2=0.6977\n",
      "[LGBM+3D/Tg] val_MAE=48.795881  val_RMSE=4017.619337  val_R2=0.6977\n",
      "\n",
      ">>> LGBM (Tc) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (589, 1144) | X_test: (148, 1144)\n",
      "y_train: (589,) | y_test: (148,)\n",
      "[LGBM/aug3D_fp3d_Tc] val_MAE=0.028928  val_RMSE=0.002150  val_R2=0.7152\n",
      "[LGBM+3D/Tc] val_MAE=0.028928  val_RMSE=0.002150  val_R2=0.7152\n",
      "\n",
      ">>> LGBM (Rg) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (491, 1144) | X_test: (123, 1144)\n",
      "y_train: (491,) | y_test: (123,)\n",
      "[LGBM/aug3D_fp3d_Rg] val_MAE=1.545092  val_RMSE=5.766040  val_R2=0.7496\n",
      "[LGBM+3D/Rg] val_MAE=1.545092  val_RMSE=5.766040  val_R2=0.7496\n",
      "\n",
      ">>> LGBM (Density) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (490, 1144) | X_test: (123, 1144)\n",
      "y_train: (490,) | y_test: (123,)\n",
      "[LGBM/aug3D_fp3d_Density] val_MAE=0.029514  val_RMSE=0.002655  val_R2=0.8864\n",
      "[LGBM+3D/Density] val_MAE=0.029514  val_RMSE=0.002655  val_R2=0.8864\n",
      "\n",
      ">>> XGB (FFV) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (5624, 1144) | X_test: (1406, 1144)\n",
      "y_train: (5624,) | y_test: (1406,)\n",
      "[XGB] Early stopping not supported by this xgboost build — training without it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [14:47:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGB/aug3D_fp3d_FFV] val_MAE=0.006368  val_RMSE=0.000318  val_R2=0.6497\n",
      "[XGB+3D/FFV] val_MAE=0.006368  val_RMSE=0.000318  val_R2=0.6497\n",
      "\n",
      ">>> XGB (Tg) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (408, 1144) | X_test: (103, 1144)\n",
      "y_train: (408,) | y_test: (103,)\n",
      "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
      "[XGB/aug3D_fp3d_Tg] val_MAE=55.900224  val_RMSE=5046.809502  val_R2=0.6202\n",
      "[XGB+3D/Tg] val_MAE=55.900224  val_RMSE=5046.809502  val_R2=0.6202\n",
      "\n",
      ">>> XGB (Tc) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (589, 1144) | X_test: (148, 1144)\n",
      "y_train: (589,) | y_test: (148,)\n",
      "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
      "[XGB/aug3D_fp3d_Tc] val_MAE=0.034650  val_RMSE=0.002416  val_R2=0.6799\n",
      "[XGB+3D/Tc] val_MAE=0.034650  val_RMSE=0.002416  val_R2=0.6799\n",
      "\n",
      ">>> XGB (Rg) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (491, 1144) | X_test: (123, 1144)\n",
      "y_train: (491,) | y_test: (123,)\n",
      "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
      "[XGB/aug3D_fp3d_Rg] val_MAE=1.562010  val_RMSE=5.667108  val_R2=0.7539\n",
      "[XGB+3D/Rg] val_MAE=1.562010  val_RMSE=5.667108  val_R2=0.7539\n",
      "\n",
      ">>> XGB (Density) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (490, 1144) | X_test: (123, 1144)\n",
      "y_train: (490,) | y_test: (123,)\n",
      "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
      "[XGB/aug3D_fp3d_Density] val_MAE=0.027283  val_RMSE=0.002206  val_R2=0.9056\n",
      "[XGB+3D/Density] val_MAE=0.027283  val_RMSE=0.002206  val_R2=0.9056\n"
     ]
    }
   ],
   "source": [
    "TRAIN_CSV = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "df_all = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "lgbm_models, lgbm_metrics = {}, {}\n",
    "for t in [\"FFV\", \"Tg\", \"Tc\", \"Rg\", \"Density\"]:\n",
    "    print(f\"\\n>>> LGBM ({t}) with backend=fp3d\")\n",
    "    m, met, sp, p = train_tabular_for_target(\n",
    "        df_all, t, \"lgbm\", lgbm_cfg[t],\n",
    "        lmdb_path=TRAIN_LMDB,\n",
    "        feature_backend=\"fp3d\",\n",
    "        save_dir=\"saved_models/lgbm_aug3d\",\n",
    "        tag_prefix=\"aug3D\",\n",
    "        test_size=0.2, random_state=42, stratify_regression=True, n_strat_bins=10,\n",
    "    )\n",
    "    lgbm_models[t], lgbm_metrics[t] = m, met\n",
    "    print(f\"[LGBM+3D/{t}] val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")\n",
    "\n",
    "xgb_models, xgb_metrics = {}, {}\n",
    "for t in [\"FFV\", \"Tg\", \"Tc\", \"Rg\", \"Density\"]:\n",
    "    print(f\"\\n>>> XGB ({t}) with backend=fp3d\")\n",
    "    m, met, sp, p = train_tabular_for_target(\n",
    "        df_all, t, \"xgb\", xgb_cfg[t],\n",
    "        lmdb_path=TRAIN_LMDB,\n",
    "        feature_backend=\"fp3d\",\n",
    "        save_dir=\"saved_models/xgb_aug3d\",\n",
    "        tag_prefix=\"aug3D\",\n",
    "        test_size=0.2, random_state=42, stratify_regression=True, n_strat_bins=10,\n",
    "    )\n",
    "    xgb_models[t], xgb_metrics[t] = m, met\n",
    "    print(f\"[XGB+3D/{t}] val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327147f",
   "metadata": {},
   "source": [
    ">>> LGBM (FFV) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (5624, 1144) | X_test: (1406, 1144)\n",
    "y_train: (5624,) | y_test: (1406,)\n",
    "[LGBM/aug3D_fp3d_FFV] val_MAE=0.006486  val_RMSE=0.017799  val_R2=0.6509\n",
    "[LGBM+3D/FFV] val_MAE=0.006486  val_RMSE=0.017799  val_R2=0.6509\n",
    "\n",
    ">>> LGBM (Tg) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (408, 1144) | X_test: (103, 1144)\n",
    "y_train: (408,) | y_test: (103,)\n",
    "[LGBM/aug3D_fp3d_Tg] val_MAE=55.623544  val_RMSE=69.218274  val_R2=0.6395\n",
    "[LGBM+3D/Tg] val_MAE=55.623544  val_RMSE=69.218274  val_R2=0.6395\n",
    "\n",
    ">>> LGBM (Tc) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (589, 1144) | X_test: (148, 1144)\n",
    "y_train: (589,) | y_test: (148,)\n",
    "[LGBM/aug3D_fp3d_Tc] val_MAE=0.028928  val_RMSE=0.046366  val_R2=0.7152\n",
    "[LGBM+3D/Tc] val_MAE=0.028928  val_RMSE=0.046366  val_R2=0.7152\n",
    "\n",
    ">>> LGBM (Rg) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (491, 1144) | X_test: (123, 1144)\n",
    "y_train: (491,) | y_test: (123,)\n",
    "[LGBM/aug3D_fp3d_Rg] val_MAE=1.545092  val_RMSE=2.401258  val_R2=0.7496\n",
    "[LGBM+3D/Rg] val_MAE=1.545092  val_RMSE=2.401258  val_R2=0.7496\n",
    "\n",
    ">>> LGBM (Density) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (490, 1144) | X_test: (123, 1144)\n",
    "y_train: (490,) | y_test: (123,)\n",
    "[LGBM/aug3D_fp3d_Density] val_MAE=0.029514  val_RMSE=0.051530  val_R2=0.8864\n",
    "[LGBM+3D/Density] val_MAE=0.029514  val_RMSE=0.051530  val_R2=0.8864\n",
    "\n",
    ">>> XGB (FFV) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (5624, 1144) | X_test: (1406, 1144)\n",
    "y_train: (5624,) | y_test: (1406,)\n",
    "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
    "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [11:35:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
    "Potential solutions:\n",
    "- Use a data structure that matches the device ordinal in the booster.\n",
    "- Set the device for booster before call to inplace_predict.\n",
    "\n",
    "This warning will only be shown once.\n",
    "\n",
    "  warnings.warn(smsg, UserWarning)\n",
    "[XGB/aug3D_fp3d_FFV] val_MAE=0.006273  val_RMSE=0.015642  val_R2=0.7304\n",
    "[XGB+3D/FFV] val_MAE=0.006273  val_RMSE=0.015642  val_R2=0.7304\n",
    "\n",
    ">>> XGB (Tg) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (408, 1144) | X_test: (103, 1144)\n",
    "y_train: (408,) | y_test: (103,)\n",
    "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
    "[XGB/aug3D_fp3d_Tg] val_MAE=55.704242  val_RMSE=70.547759  val_R2=0.6255\n",
    "[XGB+3D/Tg] val_MAE=55.704242  val_RMSE=70.547759  val_R2=0.6255\n",
    "\n",
    ">>> XGB (Tc) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (589, 1144) | X_test: (148, 1144)\n",
    "y_train: (589,) | y_test: (148,)\n",
    "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
    "[XGB/aug3D_fp3d_Tc] val_MAE=0.028906  val_RMSE=0.045927  val_R2=0.7205\n",
    "[XGB+3D/Tc] val_MAE=0.028906  val_RMSE=0.045927  val_R2=0.7205\n",
    "\n",
    ">>> XGB (Rg) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (491, 1144) | X_test: (123, 1144)\n",
    "y_train: (491,) | y_test: (123,)\n",
    "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
    "[XGB/aug3D_fp3d_Rg] val_MAE=1.555057  val_RMSE=2.360328  val_R2=0.7580\n",
    "[XGB+3D/Rg] val_MAE=1.555057  val_RMSE=2.360328  val_R2=0.7580\n",
    "\n",
    ">>> XGB (Density) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (490, 1144) | X_test: (123, 1144)\n",
    "y_train: (490,) | y_test: (123,)\n",
    "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
    "[XGB/aug3D_fp3d_Density] val_MAE=0.027153  val_RMSE=0.047139  val_R2=0.9049\n",
    "[XGB+3D/Density] val_MAE=0.027153  val_RMSE=0.047139  val_R2=0.9049"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29508319",
   "metadata": {},
   "source": [
    "\n",
    "[LGBM+3D/FFV] val_MAE=0.006486  val_RMSE=0.017799  val_R2=0.6509\n",
    "[LGBM+3D/Tg] val_MAE=55.623544  val_RMSE=69.218274  val_R2=0.6395\n",
    "[LGBM+3D/Tc] val_MAE=0.028928  val_RMSE=0.046366  val_R2=0.7152\n",
    "[LGBM+3D/Rg] val_MAE=1.545092  val_RMSE=2.401258  val_R2=0.7496\n",
    "[LGBM+3D/Density] val_MAE=0.029731  val_RMSE=0.052601  val_R2=0.8816\n",
    "\n",
    "[XGB+3D/FFV] val_MAE=0.006233  val_RMSE=0.015591  val_R2=0.7322\n",
    "[XGB+3D/Tg] val_MAE=55.403413  val_RMSE=71.757702  val_R2=0.6125\n",
    "[XGB+3D/Tc] val_MAE=0.028574  val_RMSE=0.046616  val_R2=0.7121\n",
    "[XGB+3D/Rg] val_MAE=1.645115  val_RMSE=2.440275  val_R2=0.7414\n",
    "[XGB+3D/Density] val_MAE=0.025209  val_RMSE=0.044170  val_R2=0.9165\n",
    "\n",
    "\n",
    "Trial 2: new configs\n",
    "[LGBM+3D/FFV] val_MAE=0.006478  val_RMSE=0.017853  val_R2=0.6488\n",
    "[LGBM+3D/Tg] val_MAE=56.155952  val_RMSE=71.321801  val_R2=0.6172\n",
    "[LGBM+3D/Tc] val_MAE=0.029255  val_RMSE=0.047131  val_R2=0.7057\n",
    "[LGBM+3D/Rg] val_MAE=1.678481  val_RMSE=2.521272  val_R2=0.7239\n",
    "[LGBM+3D/Density] val_MAE=0.032221  val_RMSE=0.059702  val_R2=0.8475\n",
    "\n",
    "[XGB+3D/FFV] val_MAE=0.006120  val_RMSE=0.015041  val_R2=0.7507\n",
    "[XGB+3D/Tg] val_MAE=57.169018  val_RMSE=72.417251  val_R2=0.6054\n",
    "[XGB+3D/Tc] val_MAE=0.034371  val_RMSE=0.049126  val_R2=0.6803\n",
    "[XGB+3D/Rg] val_MAE=1.646285  val_RMSE=2.460963  val_R2=0.7370\n",
    "[XGB+3D/Density] val_MAE=0.026259  val_RMSE=0.044726  val_R2=0.9144\n",
    "\n",
    "Trial 3: huber loss for lgb and new dict for xgb with trial 2 configs \n",
    "[LGBM+3D/FFV] val_MAE=0.006653  val_RMSE=0.017677  val_R2=0.6557\n",
    "[LGBM+3D/Tg] val_MAE=62.675503  val_RMSE=83.564070  val_R2=0.4745\n",
    "[LGBM+3D/Tc] val_MAE=0.027693  val_RMSE=0.044296  val_R2=0.7400\n",
    "[LGBM+3D/Rg] val_MAE=1.618186  val_RMSE=2.423270  val_R2=0.7450\n",
    "[LGBM+3D/Density] val_MAE=0.032516  val_RMSE=0.054482  val_R2=0.8730\n",
    "\n",
    "[XGB+3D/FFV] val_MAE=0.005926  val_RMSE=0.014915  val_R2=0.7549\n",
    "[XGB+3D/Tg] val_MAE=55.900224  val_RMSE=71.040900  val_R2=0.6202\n",
    "[XGB+3D/Tc] val_MAE=0.034650  val_RMSE=0.049152  val_R2=0.6799\n",
    "[XGB+3D/Rg] val_MAE=1.560507  val_RMSE=2.326133  val_R2=0.7650\n",
    "[XGB+3D/Density] val_MAE=0.026017  val_RMSE=0.047280  val_R2=0.9044\n",
    "\n",
    "Trial 4: huber loss for lgb with original config, XGB: objective=\"reg:absoluteerror\" + new config and dict\n",
    "\n",
    "[LGBM+3D/FFV] val_MAE=0.006533  val_RMSE=0.017517  val_R2=0.6619\n",
    "[LGBM+3D/Tg] val_MAE=59.495712  val_RMSE=78.644181  val_R2=0.5346\n",
    "[LGBM+3D/Tc] val_MAE=0.027822  val_RMSE=0.044622  val_R2=0.7362\n",
    "[LGBM+3D/Rg] val_MAE=1.554533  val_RMSE=2.455474  val_R2=0.7382\n",
    "[LGBM+3D/Density] val_MAE=0.030911  val_RMSE=0.052446  val_R2=0.8823\n",
    "\n",
    "[XGB+3D/FFV] val_MAE=0.006236  val_RMSE=0.017128  val_R2=0.6768\n",
    "[XGB+3D/Tg] val_MAE=56.481308  val_RMSE=71.475180  val_R2=0.6156\n",
    "[XGB+3D/Tc] val_MAE=0.029000  val_RMSE=0.045747  val_R2=0.7227\n",
    "[XGB+3D/Rg] val_MAE=1.582031  val_RMSE=2.488855  val_R2=0.7310\n",
    "[XGB+3D/Density] val_MAE=0.025539  val_RMSE=0.045819  val_R2=0.9102\n",
    "\n",
    "Trial 5: back to l1 loss with original config, XGB: seems like FFV, Tg did worse while Tc improved, and Rg/Density very similar, will try reg:pseudohubererror next. Then will eval results, pick which model is best for each target and run a submission. Then, will perform tuning on each distinct target.\n",
    "\n",
    "[LGBM+3D/FFV] val_MAE=0.006486  val_RMSE=0.017799  val_R2=0.6509\n",
    "[LGBM+3D/Tg] val_MAE=55.623544  val_RMSE=69.218274  val_R2=0.6395\n",
    "[LGBM+3D/Tc] val_MAE=0.028928  val_RMSE=0.046366  val_R2=0.7152\n",
    "[LGBM+3D/Rg] val_MAE=1.545092  val_RMSE=2.401258  val_R2=0.7496\n",
    "[LGBM+3D/Density] val_MAE=0.029514  val_RMSE=0.051530  val_R2=0.8864\n",
    "\n",
    "[XGB+3D/FFV] val_MAE=0.005960  val_RMSE=0.014577  val_R2=0.7659\n",
    "[XGB+3D/Tg] val_MAE=90.490287  val_RMSE=123.772864  val_R2=-0.1528\n",
    "[XGB+3D/Tc] val_MAE=0.034317  val_RMSE=0.049155  val_R2=0.6799\n",
    "[XGB+3D/Rg] val_MAE=2968.611342  val_RMSE=2968.615220  val_R2=-382726.1776\n",
    "[XGB+3D/Density] val_MAE=0.024336  val_RMSE=0.040957  val_R2=0.9282\n",
    "\n",
    "Trial 6: same as 5 beside leaf wise growth for XGB\n",
    "\n",
    "[LGBM+3D/FFV] val_MAE=0.006486  val_RMSE=0.017799  val_R2=0.6509\n",
    "[LGBM+3D/Tg] val_MAE=55.623544  val_RMSE=69.218274  val_R2=0.6395\n",
    "[LGBM+3D/Tc] val_MAE=0.028928  val_RMSE=0.046366  val_R2=0.7152\n",
    "[LGBM+3D/Rg] val_MAE=1.545092  val_RMSE=2.401258  val_R2=0.7496\n",
    "[LGBM+3D/Density] val_MAE=0.029514  val_RMSE=0.051530  val_R2=0.8864\n",
    "\n",
    "[XGB+3D/FFV] val_MAE=0.006273  val_RMSE=0.015642  val_R2=0.7304\n",
    "[XGB+3D/Tg] val_MAE=55.704242  val_RMSE=70.547759  val_R2=0.6255\n",
    "[XGB+3D/Tc] val_MAE=0.028906  val_RMSE=0.045927  val_R2=0.7205\n",
    "[XGB+3D/Rg] val_MAE=1.555057  val_RMSE=2.360328  val_R2=0.7580\n",
    "[XGB+3D/Density] val_MAE=0.027153  val_RMSE=0.047139  val_R2=0.9049\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c964ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =========================\n",
    "# # Optuna tuning (10 studies)\n",
    "# # =========================\n",
    "# # Prereqs: prepare_features_for_target, make_tabular_splits exist in your notebook.\n",
    "\n",
    "# # 0) Imports / setup\n",
    "# import optuna \n",
    "# import os, json, joblib, numpy as np, pandas as pd, time\n",
    "# from typing import Dict, Any, Optional, Tuple\n",
    "# from functools import partial\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from optuna.samplers import TPESampler\n",
    "# from optuna.pruners import MedianPruner\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "\n",
    "# # ---- Paths / constants\n",
    "# DATA_ROOT   = DATA_ROOT            # already defined earlier in your notebook\n",
    "# TRAIN_CSV   = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "# LMDB_PATH   = TRAIN_LMDB           # <- use your augmented train LMDB\n",
    "# FEATURE_BACKEND = \"fp3d\"           # <- you’re using fp3d augmented features\n",
    "\n",
    "# SAVE_ROOT_LGB = \"saved_models/lgbm_optuna_fp3d\"\n",
    "# SAVE_ROOT_XGB = \"saved_models/xgb_optuna_fp3d\"\n",
    "# os.makedirs(SAVE_ROOT_LGB, exist_ok=True)\n",
    "# os.makedirs(SAVE_ROOT_XGB, exist_ok=True)\n",
    "\n",
    "# RANDOM_STATE = 42\n",
    "# VAL_TEST_SIZE = 0.2\n",
    "# VAL_STRATIFY = True\n",
    "# VAL_BINS = 10                      # regression strat bins (your pipeline)\n",
    "# NUM_BOOST_ROUND = 10000            # both LGBM/XGB upper bound\n",
    "# EARLY_STOP_ROUNDS = 400\n",
    "# TIMEOUT_PER_STUDY = 60 * 60        # 1 hour each\n",
    "\n",
    "# TARGETS = [\"FFV\", \"Tg\", \"Tc\", \"Rg\", \"Density\"]\n",
    "\n",
    "# # ---- Optional: order studies by weight (approx wMAE importance)\n",
    "# def get_target_weights(csv_path: str, target_names):\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     scale_norm = []\n",
    "#     count_norm = []\n",
    "#     for t in target_names:\n",
    "#         vals = df[t].values\n",
    "#         vals = vals[~np.isnan(vals)]\n",
    "#         scale_norm.append(1.0 / (np.max(vals) - np.min(vals)))\n",
    "#         count_norm.append((1.0/len(vals))**0.5)\n",
    "#     scale_norm = np.array(scale_norm)\n",
    "#     count_norm = np.array(count_norm)\n",
    "#     w = scale_norm * len(target_names) * count_norm / np.sum(count_norm)\n",
    "#     return dict(zip(target_names, w))\n",
    "\n",
    "# WEIGHTS = get_target_weights(TRAIN_CSV, TARGETS)\n",
    "# TARGETS_BY_WEIGHT = sorted(TARGETS, key=lambda t: -WEIGHTS[t])\n",
    "# print(\"Run order by weight (heaviest → lightest):\", TARGETS_BY_WEIGHT, \"\\nweights:\", WEIGHTS)\n",
    "\n",
    "# # ---- Feature builder per target (uses your helpers)\n",
    "# def build_Xy_for_target(df: pd.DataFrame, target: str):\n",
    "#     # Uses your prepare_features_for_target (fp3d features w/ LMDB)\n",
    "#     df_clean, y, X = prepare_features_for_target(\n",
    "#         df, target,\n",
    "#         lmdb_path=LMDB_PATH,\n",
    "#         feature_backend=FEATURE_BACKEND,\n",
    "#         cache_dir=os.path.join(\"saved_models\", \"cache_optuna_fp3d\"),\n",
    "#     )\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     y = np.asarray(y, dtype=np.float32)\n",
    "#     return X, y\n",
    "\n",
    "# # ---- Split helper (same behavior as your training code)\n",
    "# def split_Xy(X, y):\n",
    "#     splits = make_tabular_splits(\n",
    "#         X, y,\n",
    "#         test_size=VAL_TEST_SIZE,\n",
    "#         random_state=RANDOM_STATE,\n",
    "#         scale_X=False, scale_y=False,\n",
    "#         stratify_regression=VAL_STRATIFY,\n",
    "#         n_strat_bins=min(VAL_BINS, max(3, int(np.sqrt(len(y)))))\n",
    "#     )\n",
    "#     return splits.X_train, splits.y_train, splits.X_test, splits.y_test\n",
    "\n",
    "# def _xgb_tree_method():\n",
    "#     try:\n",
    "#         import torch\n",
    "#         return \"gpu_hist\" if torch.cuda.is_available() else \"hist\"\n",
    "#     except Exception:\n",
    "#         return \"hist\"\n",
    "\n",
    "# # ==================\n",
    "# # LGBM Objective\n",
    "# # ==================\n",
    "# def lgbm_objective(target: str, df_all: pd.DataFrame, trial: optuna.trial.Trial) -> float:\n",
    "#     X, y = build_Xy_for_target(df_all, target)\n",
    "#     Xtr, Ytr, Xva, Yva = split_Xy(X, y)\n",
    "\n",
    "#     dtrain = lgb.Dataset(Xtr, label=Ytr)\n",
    "#     dvalid = lgb.Dataset(Xva, label=Yva)\n",
    "\n",
    "#     # Search space (wide, but safe)\n",
    "#     params = {\n",
    "#         \"objective\": trial.suggest_categorical(\"objective\", [\"regression_l1\", \"regression\"]),\n",
    "#         \"metric\": \"l1\",\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.2, log=True),\n",
    "#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 512, log=True),\n",
    "#         \"max_depth\": trial.suggest_categorical(\"max_depth\", [-1, 4, 6, 8, 10, 12, 14, 16]),\n",
    "#         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 200),\n",
    "#         \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "#         \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "#         \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "#         \"bagging_freq\": 0,  # set >0 only if bagging_fraction < 1.0\n",
    "#         \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "#         \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "#         \"verbosity\": -1,\n",
    "#         \"seed\": RANDOM_STATE,\n",
    "#         \"num_threads\": os.cpu_count() or 8,\n",
    "#         # LightGBM GPU is optional; avoid to be safe across envs\n",
    "#         # \"device_type\": \"gpu\",\n",
    "#     }\n",
    "#     if params[\"bagging_fraction\"] < 0.999:\n",
    "#         params[\"bagging_freq\"] = trial.suggest_int(\"bagging_freq\", 1, 7)\n",
    "\n",
    "#     # Pruning callback\n",
    "#     try:\n",
    "#         from optuna.integration import LightGBMPruningCallback\n",
    "#         pruning_cb = LightGBMPruningCallback(trial, \"l1\")\n",
    "#         callbacks = [lgb.early_stopping(EARLY_STOP_ROUNDS, verbose=False),\n",
    "#                      lgb.log_evaluation(period=0),\n",
    "#                      pruning_cb]\n",
    "#     except Exception:\n",
    "#         callbacks = [lgb.early_stopping(EARLY_STOP_ROUNDS, verbose=False),\n",
    "#                      lgb.log_evaluation(period=0)]\n",
    "\n",
    "#     booster = lgb.train(\n",
    "#         params,\n",
    "#         dtrain,\n",
    "#         num_boost_round=NUM_BOOST_ROUND,\n",
    "#         valid_sets=[dvalid],\n",
    "#         valid_names=[\"valid\"],\n",
    "#         callbacks=callbacks\n",
    "#     )\n",
    "#     best_iter = booster.best_iteration or NUM_BOOST_ROUND\n",
    "#     pred_va = booster.predict(Xva, num_iteration=best_iter)\n",
    "#     mae = mean_absolute_error(Yva, pred_va)\n",
    "\n",
    "#     # save best_iter to trial for later refit\n",
    "#     trial.set_user_attr(\"best_iteration\", int(best_iter))\n",
    "#     return float(mae)\n",
    "\n",
    "# def run_lgbm_study_for_target(target: str, df_all: pd.DataFrame, timeout_s: int = TIMEOUT_PER_STUDY):\n",
    "#     study = optuna.create_study(\n",
    "#         direction=\"minimize\",\n",
    "#         sampler=TPESampler(seed=RANDOM_STATE),\n",
    "#         pruner=MedianPruner(n_warmup_steps=10),\n",
    "#         study_name=f\"LGBM_{FEATURE_BACKEND}_{target}\"\n",
    "#     )\n",
    "#     study.optimize(partial(lgbm_objective, target, df_all), timeout=timeout_s, gc_after_trial=True)\n",
    "#     print(f\"[LGBM/{target}] best MAE={study.best_value:.6f}\")\n",
    "#     # Refit full model on all data with best params & best_iter\n",
    "#     X, y = build_Xy_for_target(df_all, target)\n",
    "#     best_params = study.best_params.copy()\n",
    "#     best_params.update({\"metric\": \"l1\", \"verbosity\": -1, \"seed\": RANDOM_STATE, \"num_threads\": os.cpu_count() or 8})\n",
    "#     best_iter = study.best_trial.user_attrs.get(\"best_iteration\", 1000)\n",
    "\n",
    "#     dtrain_full = lgb.Dataset(X, label=y)\n",
    "#     booster_full = lgb.train(\n",
    "#         best_params, dtrain_full,\n",
    "#         num_boost_round=int(best_iter),\n",
    "#         valid_sets=[dtrain_full],\n",
    "#         valid_names=[\"train\"],\n",
    "#         callbacks=[lgb.log_evaluation(period=0)]\n",
    "#     )\n",
    "\n",
    "#     out_txt = os.path.join(SAVE_ROOT_LGB, f\"lgbm_{FEATURE_BACKEND}_{target}.txt\")\n",
    "#     booster_full.save_model(out_txt)\n",
    "#     meta = {\n",
    "#         \"best_value\": study.best_value,\n",
    "#         \"best_params\": best_params,\n",
    "#         \"best_iteration\": int(best_iter),\n",
    "#         \"feature_backend\": FEATURE_BACKEND,\n",
    "#         \"target\": target,\n",
    "#     }\n",
    "#     joblib.dump(meta, os.path.join(SAVE_ROOT_LGB, f\"lgbm_{FEATURE_BACKEND}_{target}.meta.joblib\"))\n",
    "#     print(f\"[LGBM/{target}] saved model -> {out_txt}\")\n",
    "#     return study\n",
    "\n",
    "# # ==================\n",
    "# # XGBoost Objective\n",
    "# # ==================\n",
    "# # --- Put these near your XGB objective ---\n",
    "# from typing import Optional\n",
    "\n",
    "# def _xgb_best_iteration(bst) -> Optional[int]:\n",
    "#     \"\"\"Return best iteration across xgboost versions.\"\"\"\n",
    "#     # 1) Preferred (xgb >= 1.6 / 2.x)\n",
    "#     bi = getattr(bst, \"best_iteration\", None)\n",
    "#     if isinstance(bi, (int, np.integer)):\n",
    "#         return int(bi)\n",
    "#     # 2) Older versions\n",
    "#     bi2 = getattr(bst, \"best_ntree_limit\", None)\n",
    "#     if isinstance(bi2, (int, np.integer)):\n",
    "#         return int(bi2)\n",
    "#     # 3) Last resort: number of boosted rounds (no early stopping info)\n",
    "#     try:\n",
    "#         return int(bst.num_boosted_rounds())\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "# def _xgb_predict_at_best(bst, dmat):\n",
    "#     \"\"\"Predict using best iteration when available, with version fallbacks.\"\"\"\n",
    "#     bi = _xgb_best_iteration(bst)\n",
    "#     # iteration_range is new-ish; ntree_limit is old; default otherwise\n",
    "#     if bi is not None:\n",
    "#         try:\n",
    "#             # newer API\n",
    "#             return bst.predict(dmat, iteration_range=(0, bi + 1))\n",
    "#         except TypeError:\n",
    "#             try:\n",
    "#                 # older API\n",
    "#                 return bst.predict(dmat, ntree_limit=bi)\n",
    "#             except TypeError:\n",
    "#                 pass\n",
    "#     return bst.predict(dmat)\n",
    "\n",
    "\n",
    "# def xgb_objective(target: str, df_all: pd.DataFrame, trial: optuna.trial.Trial) -> float:\n",
    "#     X, y = build_Xy_for_target(df_all, target)\n",
    "#     Xtr, Ytr, Xva, Yva = split_Xy(X, y)\n",
    "\n",
    "#     dtrain = xgb.DMatrix(Xtr, label=Ytr)\n",
    "#     dvalid = xgb.DMatrix(Xva, label=Yva)\n",
    "\n",
    "#     tm = _xgb_tree_method()\n",
    "#     params = {\n",
    "#         \"tree_method\": tm,\n",
    "#         \"eval_metric\": \"mae\",\n",
    "#         \"objective\": trial.suggest_categorical(\"objective\", [\"reg:squarederror\", \"reg:absoluteerror\"]),\n",
    "#         \"eta\": trial.suggest_float(\"eta\", 1e-3, 0.3, log=True),\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "#         \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 10.0),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "#         \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "#         \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 100.0, log=True),\n",
    "#         \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 10.0, log=True),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 0.0, 0.5),\n",
    "#         \"max_bin\": trial.suggest_categorical(\"max_bin\", [256, 512, 1024]),\n",
    "#         \"verbosity\": 0,\n",
    "#         \"seed\": RANDOM_STATE,\n",
    "#     }\n",
    "#     grow = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "#     params[\"grow_policy\"] = grow\n",
    "#     if grow == \"lossguide\":\n",
    "#         params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 16, 512, log=True)\n",
    "\n",
    "#     bst = xgb.train(\n",
    "#         params,\n",
    "#         dtrain,\n",
    "#         num_boost_round=NUM_BOOST_ROUND,\n",
    "#         evals=[(dvalid, \"valid\")],\n",
    "#         early_stopping_rounds=EARLY_STOP_ROUNDS,\n",
    "#         verbose_eval=False\n",
    "#     )\n",
    "\n",
    "#     # Safe best-iteration + prediction\n",
    "#     best_iter = _xgb_best_iteration(bst)\n",
    "#     if best_iter is not None:\n",
    "#         trial.set_user_attr(\"best_iteration\", int(best_iter))\n",
    "\n",
    "#     pred_va = _xgb_predict_at_best(bst, dvalid)\n",
    "#     mae = mean_absolute_error(Yva, pred_va)\n",
    "#     return float(mae)\n",
    "\n",
    "\n",
    "# def run_xgb_study_for_target(target: str, df_all: pd.DataFrame, timeout_s: int = TIMEOUT_PER_STUDY):\n",
    "#     study = optuna.create_study(\n",
    "#         direction=\"minimize\",\n",
    "#         sampler=TPESampler(seed=RANDOM_STATE),\n",
    "#         pruner=MedianPruner(n_warmup_steps=10),\n",
    "#         study_name=f\"XGB_{FEATURE_BACKEND}_{target}\"\n",
    "#     )\n",
    "#     study.optimize(partial(xgb_objective, target, df_all), timeout=timeout_s, gc_after_trial=True)\n",
    "#     print(f\"[XGB/{target}] best MAE={study.best_value:.6f}\")\n",
    "\n",
    "#     # Refit full model with best params & best_iter\n",
    "#     X, y = build_Xy_for_target(df_all, target)\n",
    "#     dfull = xgb.DMatrix(X, label=y)\n",
    "#     best_params = study.best_params.copy()\n",
    "#     best_params.update({\n",
    "#         \"tree_method\": _xgb_tree_method(),\n",
    "#         \"eval_metric\": \"mae\",\n",
    "#         \"verbosity\": 0,\n",
    "#         \"seed\": RANDOM_STATE,\n",
    "#     })\n",
    "#     best_iter = int(study.best_trial.user_attrs.get(\"best_iteration\", 1000))\n",
    "#     bst_full = xgb.train(\n",
    "#         best_params,\n",
    "#         dfull,\n",
    "#         num_boost_round=best_iter,\n",
    "#         evals=[(dfull, \"train\")],\n",
    "#         verbose_eval=False\n",
    "#     )\n",
    "#     out_json = os.path.join(SAVE_ROOT_XGB, f\"xgb_{FEATURE_BACKEND}_{target}.json\")\n",
    "#     bst_full.save_model(out_json)\n",
    "#     meta = {\n",
    "#         \"best_value\": study.best_value,\n",
    "#         \"best_params\": best_params,\n",
    "#         \"best_iteration\": best_iter,\n",
    "#         \"feature_backend\": FEATURE_BACKEND,\n",
    "#         \"target\": target,\n",
    "#     }\n",
    "#     joblib.dump(meta, os.path.join(SAVE_ROOT_XGB, f\"xgb_{FEATURE_BACKEND}_{target}.meta.joblib\"))\n",
    "#     print(f\"[XGB/{target}] saved model -> {out_json}\")\n",
    "#     return study\n",
    "\n",
    "# def _lgbm_artifact_path(t: str) -> str:\n",
    "#     return os.path.join(SAVE_ROOT_LGB, f\"lgbm_{FEATURE_BACKEND}_{t}.txt\")\n",
    "\n",
    "# def _xgb_artifact_path(t: str) -> str:\n",
    "#     return os.path.join(SAVE_ROOT_XGB, f\"xgb_{FEATURE_BACKEND}_{t}.json\")\n",
    "\n",
    "# def _skip_lgbm(t: str) -> bool:\n",
    "#     return os.path.exists(_lgbm_artifact_path(t))\n",
    "\n",
    "# def _skip_xgb(t: str) -> bool:\n",
    "#     return os.path.exists(_xgb_artifact_path(t))\n",
    "\n",
    "# # ==================\n",
    "# # Orchestrate all 10\n",
    "# # ==================\n",
    "# df_all = pd.read_csv(TRAIN_CSV)\n",
    "# ORDER = TARGETS_BY_WEIGHT  # or TARGETS\n",
    "\n",
    "# studies = {\"lgbm\": {}, \"xgb\": {}}\n",
    "\n",
    "# # LGBM studies (skip if model already saved)\n",
    "# for t in ORDER:\n",
    "#     if _skip_lgbm(t):\n",
    "#         print(f\"Skipping LGBM tuning for {t} (found {_lgbm_artifact_path(t)})\")\n",
    "#         continue\n",
    "#     print(f\"\\n==== LGBM tuning for {t} (<= {TIMEOUT_PER_STUDY//60} min) ====\")\n",
    "#     studies[\"lgbm\"][t] = run_lgbm_study_for_target(t, df_all, TIMEOUT_PER_STUDY)\n",
    "\n",
    "# # XGB studies (skip if model already saved)\n",
    "# for t in ORDER:\n",
    "#     if _skip_xgb(t):\n",
    "#         print(f\"Skipping XGB tuning for {t} (found {_xgb_artifact_path(t)})\")\n",
    "#         continue\n",
    "#     print(f\"\\n==== XGB tuning for {t} (<= {TIMEOUT_PER_STUDY//60} min) ====\")\n",
    "#     studies[\"xgb\"][t] = run_xgb_study_for_target(t, df_all, TIMEOUT_PER_STUDY)\n",
    "\n",
    "# # Optional: print only studies that actually ran\n",
    "# print(\"\\nFinished. Best MAE for completed studies:\")\n",
    "# for kind in [\"lgbm\", \"xgb\"]:\n",
    "#     for t, st in studies[kind].items():\n",
    "#         print(f\"{kind.upper()} {t}: {st.best_value:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a0004",
   "metadata": {},
   "source": [
    "# Graph Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4881ec38",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d599b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Tg → parents train=  408 val=  103 | aug rows train=  4080 val=  1030\n",
      "    FFV → parents train= 5624 val= 1406 | aug rows train= 56240 val= 14060\n",
      "     Tc → parents train=  589 val=  148 | aug rows train=  5890 val=  1480\n",
      "Density → parents train=  490 val=  123 | aug rows train=  4900 val=  1230\n",
      "     Rg → parents train=  491 val=  123 | aug rows train=  4910 val=  1230\n"
     ]
    }
   ],
   "source": [
    "# ==== Parent-aware wiring (CSV parents -> augmented LMDB key_ids) ====\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "AUG_KEY_MULT = 1000  # must match your LMDB builder\n",
    "\n",
    "# CSV (parents)\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_csv[\"id\"] = train_csv[\"id\"].astype(int)\n",
    "\n",
    "# LMDB ids (augmented key_ids)\n",
    "lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "lmdb_ids = np.loadtxt(lmdb_ids_path, dtype=np.int64)\n",
    "if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "\n",
    "# Parent map (preferred) → key_id list per parent\n",
    "pmap_path = TRAIN_LMDB + \".parent_map.tsv\"\n",
    "if os.path.exists(pmap_path):\n",
    "    pmap = pd.read_csv(pmap_path, sep=\"\\t\")  # cols: key_id, parent_id, aug_idx, seed\n",
    "    pmap[\"key_id\"] = pmap[\"key_id\"].astype(np.int64)\n",
    "    pmap[\"parent_id\"] = pmap[\"parent_id\"].astype(np.int64)\n",
    "else:\n",
    "    # fallback if parent_map missing: derive parent by integer division\n",
    "    pmap = pd.DataFrame({\n",
    "        \"key_id\": lmdb_ids.astype(np.int64),\n",
    "        \"parent_id\": (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "    })\n",
    "parents_in_lmdb = np.sort(pmap[\"parent_id\"].unique().astype(np.int64))\n",
    "\n",
    "def parents_with_label(task: str) -> np.ndarray:\n",
    "    m = ~train_csv[task].isna()\n",
    "    have = train_csv.loc[m, \"id\"].astype(int).values  # parents that have this label\n",
    "    return np.intersect1d(have, parents_in_lmdb, assume_unique=False)\n",
    "\n",
    "# Split BY PARENT (no leakage), then expand to augmented key_ids\n",
    "def task_parent_split_keys(task: str, test_size=0.2, seed=42):\n",
    "    parents_labeled = parents_with_label(task)\n",
    "    if parents_labeled.size == 0:\n",
    "        raise ValueError(f\"No parents with labels for {task}\")\n",
    "    p_tr, p_va = train_test_split(parents_labeled, test_size=test_size, random_state=seed)\n",
    "    tr_keys = pmap.loc[pmap.parent_id.isin(p_tr), \"key_id\"].astype(np.int64).values\n",
    "    va_keys = pmap.loc[pmap.parent_id.isin(p_va), \"key_id\"].astype(np.int64).values\n",
    "    return np.sort(tr_keys), np.sort(va_keys), np.sort(p_tr), np.sort(p_va)\n",
    "\n",
    "# Build pools (augmented key_ids) per task\n",
    "task_pools = {}\n",
    "task_parent_splits = {}\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys, p_tr, p_va = task_parent_split_keys(t, test_size=0.2, seed=42)\n",
    "    task_pools[t] = (tr_keys, va_keys)\n",
    "    task_parent_splits[t] = (p_tr, p_va)\n",
    "\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys = task_pools[t]\n",
    "    p_tr, p_va = task_parent_splits[t]\n",
    "    print(f\"{t:>7} → parents train={len(p_tr):5d} val={len(p_va):5d} | aug rows train={len(tr_keys):6d} val={len(va_keys):6d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3efce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import torch, numpy as np\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def _get_rdkit_feats_from_record(rec):\n",
    "    arr = getattr(rec, \"rdkit_feats\", None)\n",
    "    if arr is None:\n",
    "        return torch.zeros(1, 15, dtype=torch.float32)  # keep (1, D)\n",
    "    v = torch.as_tensor(np.asarray(arr, np.float32).reshape(1, -1), dtype=torch.float32)\n",
    "    return v  # (1, D)\n",
    "\n",
    "class LMDBtoPyGSingleTask(Dataset):\n",
    "    def __init__(self,\n",
    "                 ids,                # <<< must be augmented key_ids\n",
    "                 lmdb_path,\n",
    "                 target_index=None,\n",
    "                 *,\n",
    "                 use_mixed_edges: bool = True,\n",
    "                 include_extra_atom_feats: bool = True):\n",
    "        self.ids = np.asarray(ids, dtype=np.int64)\n",
    "        self.base = LMDBDataset(self.ids, lmdb_path)\n",
    "        self.t = target_index\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        self.include_extra_atom_feats = include_extra_atom_feats\n",
    "\n",
    "    def __len__(self): return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.base[idx]\n",
    "        x  = torch.as_tensor(rec.x, dtype=torch.long)\n",
    "        ei = torch.as_tensor(rec.edge_index, dtype=torch.long)\n",
    "        ea = torch.as_tensor(rec.edge_attr)\n",
    "\n",
    "        # Mixed edges: 3 categorical + 32 RBF; categorical-only if disabled\n",
    "        edge_attr = ea.to(torch.float32) if self.use_mixed_edges else ea[:, :3].to(torch.long)\n",
    "\n",
    "        d = Data(x=x, edge_index=ei, edge_attr=edge_attr,\n",
    "                 rdkit_feats=_get_rdkit_feats_from_record(rec))  # (1, D)\n",
    "\n",
    "        if hasattr(rec, \"pos\"):                d.pos  = torch.as_tensor(rec.pos, dtype=torch.float32)\n",
    "        if self.include_extra_atom_feats and hasattr(rec, \"extra_atom_feats\"):\n",
    "                                               d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32)\n",
    "        if hasattr(rec, \"has_xyz\"):            d.has_xyz = torch.as_tensor(rec.has_xyz, dtype=torch.float32)  # (1,)\n",
    "        if hasattr(rec, \"dist\"):               d.hops = torch.as_tensor(rec.dist, dtype=torch.long).unsqueeze(0)  # (1,L,L)\n",
    "\n",
    "        if (self.t is not None) and hasattr(rec, \"y\"):\n",
    "            yv = torch.as_tensor(rec.y, dtype=torch.float32).view(-1)\n",
    "            if self.t < yv.numel(): d.y = yv[self.t:self.t+1]  # (1,)\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694612d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "def make_loaders_for_task_from_pools(task, task_pools, *,\n",
    "                                     batch_size=64,\n",
    "                                     use_mixed_edges=True,\n",
    "                                     include_extra_atom_feats=True):\n",
    "    t = task2idx[task]\n",
    "    tr_keys, va_keys = task_pools[task]\n",
    "    if len(tr_keys) == 0 or len(va_keys) == 0:\n",
    "        raise ValueError(f\"Empty pools for {task}. Check splits.\")\n",
    "    tr_ds = LMDBtoPyGSingleTask(tr_keys, TRAIN_LMDB, target_index=t,\n",
    "                                use_mixed_edges=use_mixed_edges,\n",
    "                                include_extra_atom_feats=include_extra_atom_feats)\n",
    "    va_ds = LMDBtoPyGSingleTask(va_keys, TRAIN_LMDB, target_index=t,\n",
    "                                use_mixed_edges=use_mixed_edges,\n",
    "                                include_extra_atom_feats=include_extra_atom_feats)\n",
    "    tr = GeoDataLoader(tr_ds, batch_size=batch_size, shuffle=True,  num_workers=8, pin_memory=True)\n",
    "    va = GeoDataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    return tr, va\n",
    "\n",
    "# Build loaders (no leakage; parent-split → aug-expanded)\n",
    "# train_loader_tg,  val_loader_tg  = make_loaders_for_task_from_pools(\"Tg\",      task_pools, batch_size=512)\n",
    "# train_loader_den, val_loader_den = make_loaders_for_task_from_pools(\"Density\", task_pools, batch_size=512)\n",
    "# train_loader_rg,  val_loader_rg  = make_loaders_for_task_from_pools(\"Rg\",      task_pools, batch_size=512)\n",
    "# train_loader_ffv, val_loader_ffv = make_loaders_for_task_from_pools(\"FFV\",     task_pools, batch_size=512)\n",
    "# train_loader_tc,  val_loader_tc  = make_loaders_for_task_from_pools(\"Tc\",      task_pools, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983db98",
   "metadata": {},
   "source": [
    "## Step 5: Define the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc992041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numpy as np, torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, RMSprop\n",
    "from torch.amp import GradScaler, autocast\n",
    "from copy import deepcopy\n",
    "\n",
    "def train_hybrid_gnn_sota(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    *,\n",
    "    lr: float = 5e-4,\n",
    "    optimizer: str = \"AdamW\",\n",
    "    weight_decay: float = 1e-5,\n",
    "    epochs: int = 120,\n",
    "    warmup_epochs: int = 5,\n",
    "    patience: int = 15,\n",
    "    clip_norm: float = 1.0,\n",
    "    amp: bool = True,\n",
    "    loss_name: str = \"mse\",   # \"mse\" or \"huber\"\n",
    "    save_dir: str = \"saved_models/gnn\",\n",
    "    tag: str = \"model_sota\",\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    opt_name = optimizer.lower()\n",
    "    if opt_name == \"rmsprop\":\n",
    "        opt = RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.0)\n",
    "    else:\n",
    "        opt = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # cosine schedule w/ warmup\n",
    "    def lr_factor(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / max(1, warmup_epochs)\n",
    "        t = (epoch - warmup_epochs) / max(1, (epochs - warmup_epochs))\n",
    "        return 0.5 * (1 + math.cos(math.pi * t))\n",
    "    scaler = GradScaler(\"cuda\", enabled=amp)\n",
    "\n",
    "    def loss_fn(pred, target):\n",
    "        if loss_name.lower() == \"huber\":\n",
    "            return F.huber_loss(pred, target, delta=1.0)\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_once(loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        for b in loader:\n",
    "            b = b.to(device)\n",
    "            p = model(b)\n",
    "            preds.append(p.detach().cpu())\n",
    "            trues.append(b.y.view(-1,1).cpu())\n",
    "        preds = torch.cat(preds).numpy(); trues = torch.cat(trues).numpy()\n",
    "        mae = np.mean(np.abs(preds - trues))\n",
    "        rmse = float(np.sqrt(np.mean((preds - trues)**2)))\n",
    "        r2 = float(1 - np.sum((preds - trues)**2) / np.sum((trues - trues.mean())**2))\n",
    "        return mae, rmse, r2\n",
    "\n",
    "    best_mae = float(\"inf\")\n",
    "    best = None\n",
    "    best_path = os.path.join(save_dir, f\"{tag}.pt\")\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # schedule\n",
    "        for g in opt.param_groups:\n",
    "            g[\"lr\"] = lr * lr_factor(ep-1)\n",
    "\n",
    "        model.train()\n",
    "        total, count = 0.0, 0\n",
    "        for b in train_loader:\n",
    "            b = b.to(device)\n",
    "            with autocast(\"cuda\", enabled=amp):\n",
    "                pred = model(b)\n",
    "                loss = loss_fn(pred, b.y.view(-1,1))\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            if clip_norm is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "            scaler.step(opt); scaler.update()\n",
    "\n",
    "            total += loss.item() * b.num_graphs\n",
    "            count += b.num_graphs\n",
    "\n",
    "        tr_mse = total / max(1, count)\n",
    "        mae, rmse, r2 = eval_once(val_loader)\n",
    "        print(f\"Epoch {ep:03d} | tr_MSE {tr_mse:.5f} | val_MAE {mae:.5f} | val_RMSE {rmse:.5f} | R2 {r2:.4f}\")\n",
    "\n",
    "        if mae < best_mae - 1e-6:\n",
    "            best_mae = mae\n",
    "            best = deepcopy(model.state_dict())\n",
    "            torch.save(best, best_path)\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best is not None:\n",
    "        model.load_state_dict(best)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "    final_mae, final_rmse, final_r2 = eval_once(val_loader)\n",
    "    print(f\"[{tag}] Best Val — MAE {final_mae:.6f} | RMSE {final_rmse:.6f} | R2 {final_r2:.4f}\")\n",
    "    return model, best_path, {\"MAE\": final_mae, \"RMSE\": final_rmse, \"R2\": final_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9449bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "def _act(name: str):\n",
    "    name = (name or \"relu\").lower()\n",
    "    if name == \"gelu\": return nn.GELU()\n",
    "    if name in (\"silu\", \"swish\"): return nn.SiLU()\n",
    "    return nn.ReLU()\n",
    "\n",
    "\n",
    "class AttnBiasFull(nn.Module):\n",
    "    \"\"\"\n",
    "    Produces additive per-head attention bias of shape (B, H, L0, L0)\n",
    "    from geometry (xyz), adjacency, SPD buckets, and categorical edge types.\n",
    "\n",
    "    Accepts both old arg names (use_geo/use_adj_const/spd_max/rbf_K) and\n",
    "    new ones (use_geo_bias/use_adj_bias/spd_buckets/rbf_k/edge_cats).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_heads: int,\n",
    "        *,\n",
    "        # old names\n",
    "        use_geo: bool = None, use_adj_const: bool = None, use_spd: bool = True,\n",
    "        spd_max: int = None, rbf_K: int = None,\n",
    "        # new alias names\n",
    "        use_geo_bias: bool = None, use_adj_bias: bool = None,\n",
    "        spd_buckets: int = None, rbf_k: int = None,\n",
    "        edge_cats: tuple = (5, 6, 2),\n",
    "        use_edge_bias: bool = True,\n",
    "        # shared\n",
    "        rbf_beta: float = 5.0, activation: str = \"relu\",\n",
    "        edge_cont_dim: int = 32,  # (kept for compatibility; not used here)\n",
    "        use_headnorm: bool = True,\n",
    "        bound_scale: float = 0.1,   # tanh scale for gentle bounding\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_heads = int(n_heads)\n",
    "        self.bound_scale = float(bound_scale)\n",
    "        self.use_headnorm = bool(use_headnorm)\n",
    "\n",
    "        # ---- resolve aliases / defaults ----\n",
    "        def pick(*vals, default):\n",
    "            for v in vals:\n",
    "                if v is not None:\n",
    "                    return v\n",
    "            return default\n",
    "\n",
    "        self.use_geo = bool(pick(use_geo, use_geo_bias, default=True))\n",
    "        self.use_adj_const = bool(pick(use_adj_const, use_adj_bias, default=True))\n",
    "\n",
    "        # SPD: if spd_buckets given, use exactly that; else spd_max + 2 (0..spd_max + catch-all)\n",
    "        if spd_buckets is not None:\n",
    "            self.spd_buckets = int(spd_buckets)\n",
    "        else:\n",
    "            smax = 5 if spd_max is None else int(spd_max)\n",
    "            self.spd_buckets = smax + 2  # 0..smax + 1(>=)\n",
    "\n",
    "        K = int(pick(rbf_K, rbf_k, default=16))\n",
    "        self.rbf_beta = float(rbf_beta)\n",
    "\n",
    "        # ---- geometry → per-head bias ----\n",
    "        if self.use_geo:\n",
    "            centers = torch.linspace(0.0, 10.0, K)\n",
    "            self.register_buffer(\"centers\", centers, persistent=False)\n",
    "            self.geo_mlp = nn.Sequential(\n",
    "                nn.Linear(K, self.n_heads),  # simple per-head projection\n",
    "            )\n",
    "\n",
    "        # ---- adjacency constant per head ----\n",
    "        if self.use_adj_const:\n",
    "            self.adj_bias = nn.Parameter(torch.zeros(self.n_heads))\n",
    "\n",
    "        # ---- SPD buckets → per-head bias ----\n",
    "        self.use_spd = bool(use_spd)\n",
    "        if self.use_spd:\n",
    "            self.spd_emb = nn.Embedding(self.spd_buckets, self.n_heads)\n",
    "\n",
    "        # ---- edge categorical bias (configurable widths) ----\n",
    "        t, s, c = edge_cats\n",
    "        self.use_edge_bias = bool(use_edge_bias)\n",
    "        if self.use_edge_bias:\n",
    "            self.edge_emb0 = nn.Embedding(int(t), self.n_heads)\n",
    "            self.edge_emb1 = nn.Embedding(int(s), self.n_heads)\n",
    "            self.edge_emb2 = nn.Embedding(int(c), self.n_heads)\n",
    "        else:\n",
    "            self.edge_emb0 = self.edge_emb1 = self.edge_emb2 = None\n",
    "\n",
    "        # ---- per-component learnable scalers ----\n",
    "        self.alpha_geo  = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_spd  = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_adj  = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_edge = nn.Parameter(torch.tensor(0.2))\n",
    "\n",
    "        # ---- simple head-wise LayerNorms (normalize across H) ----\n",
    "        if self.use_headnorm:\n",
    "            self.ln_geo  = nn.LayerNorm(self.n_heads)\n",
    "            self.ln_spd  = nn.LayerNorm(self.n_heads)\n",
    "            self.ln_edge = nn.LayerNorm(self.n_heads)\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def _apply_ln_heads(self, t: torch.Tensor, ln: nn.LayerNorm) -> torch.Tensor:\n",
    "        \"\"\"Apply LayerNorm across heads for a (B,H,L,L) tensor.\"\"\"\n",
    "        # (B,H,L,L) -> (B,L,L,H) -> LN(H) -> (B,H,L,L)\n",
    "        t = t.permute(0, 2, 3, 1)\n",
    "        t = ln(t)\n",
    "        t = t.permute(0, 3, 1, 2).contiguous()\n",
    "        return t\n",
    "\n",
    "    def _bound(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Bound magnitudes to avoid dominating softmax; keeps gradients smooth.\"\"\"\n",
    "        return self.bound_scale * torch.tanh(t)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _spd_bias(self, hops: torch.Tensor, valid_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        hops: (B, MAX_NODES, MAX_NODES) or (B, L0, L0) shortest-path distances (uint8/long)\n",
    "        valid_mask: (B, L0, L0) bool, True where both tokens are real (not PAD)\n",
    "        returns: (B, H, L0, L0) additive per-head bias\n",
    "        \"\"\"\n",
    "        if hops.dim() == 2:  # (L,L) -> (1,L,L)\n",
    "            hops = hops.unsqueeze(0)\n",
    "\n",
    "        B, L0, _ = valid_mask.shape\n",
    "\n",
    "        # align SPD to current L0 (top-left block)\n",
    "        if hops.size(1) != L0 or hops.size(2) != L0:\n",
    "            hops = hops[:, :L0, :L0]\n",
    "\n",
    "        # bucketize SPD: last bucket = catch-all (>= last)\n",
    "        last = self.spd_buckets - 1\n",
    "        raw = hops.to(valid_mask.device).long().clamp_min_(0)\n",
    "        catch_all = raw >= last\n",
    "        raw = raw.clamp_max(last - 1)\n",
    "        bucket = torch.where(catch_all, raw.new_full(raw.shape, last), raw)\n",
    "\n",
    "        # wipe invalid pairs\n",
    "        bucket = torch.where(valid_mask, bucket, torch.zeros_like(bucket))\n",
    "\n",
    "        emb = self.spd_emb(bucket)              # (B, L0, L0, H)\n",
    "        return emb.permute(0, 3, 1, 2).contiguous()  # (B, H, L0, L0)\n",
    "\n",
    "    def _edge_bias(self, edge_index, edge_attr, batch, L0, ptr=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-head additive bias from categorical bond attributes.\n",
    "        Returns: (B, H, L0, L0)\n",
    "        \"\"\"\n",
    "        u, v = edge_index\n",
    "        be   = batch[u]  # graph id per edge\n",
    "\n",
    "        if ptr is None:\n",
    "            B = int(batch.max().item()) + 1\n",
    "            counts = torch.bincount(batch, minlength=B)\n",
    "            ptr = torch.zeros(B + 1, dtype=torch.long, device=batch.device)\n",
    "            ptr[1:] = torch.cumsum(counts, dim=0)\n",
    "        B = int(ptr.numel() - 1)\n",
    "\n",
    "        start = ptr[be]\n",
    "        u_loc = (u - start).long()\n",
    "        v_loc = (v - start).long()\n",
    "\n",
    "        cat = edge_attr[:, :3].long()\n",
    "        eh  = ( self.edge_emb0(cat[:, 0])\n",
    "              + self.edge_emb1(cat[:, 1])\n",
    "              + self.edge_emb2(cat[:, 2]) )  # (E,H)\n",
    "\n",
    "        H = self.n_heads\n",
    "        eb = torch.zeros((B, H, L0, L0), device=edge_attr.device, dtype=torch.float32)\n",
    "        for b in range(B):\n",
    "            m = (be == b)\n",
    "            if not torch.any(m):\n",
    "                continue\n",
    "            eb[b, :, u_loc[m], v_loc[m]] += eh[m].T\n",
    "        return eb\n",
    "\n",
    "    # ---------- forward ----------\n",
    "    def forward(self, pos, edge_index, edge_attr, batch, key_padding_mask, hops=None, ptr=None):\n",
    "        \"\"\"\n",
    "        Returns (B, H, L0, L0) additive bias. PAD rows/cols are filled with large negative.\n",
    "        \"\"\"\n",
    "        A = to_dense_adj(edge_index, batch=batch).squeeze(1)  # (B,L0,L0)\n",
    "        B, L0, _ = A.shape\n",
    "        H = self.n_heads\n",
    "        device = A.device\n",
    "\n",
    "        valid = ~key_padding_mask                             # (B,L0)\n",
    "        valid2d = valid.unsqueeze(2) & valid.unsqueeze(1)     # (B,L0,L0)\n",
    "\n",
    "        # geometry\n",
    "        if self.use_geo and (pos is not None):\n",
    "            pad_pos, _ = to_dense_batch(pos, batch)           # (B,L0,3)\n",
    "            diff = pad_pos.unsqueeze(2) - pad_pos.unsqueeze(1)      # (B,L0,L0,3)\n",
    "            dist = torch.sqrt(torch.clamp((diff**2).sum(-1), min=0.0))  # (B,L0,L0)\n",
    "            centers = self.centers.to(dist.device)\n",
    "            rbf = torch.exp(-self.rbf_beta * (dist.unsqueeze(-1) - centers)**2)\n",
    "            geo = self.geo_mlp(rbf).permute(0, 3, 1, 2).contiguous()    # (B,H,L0,L0)\n",
    "        else:\n",
    "            geo = torch.zeros((B, H, L0, L0), device=device)\n",
    "\n",
    "        # adjacency constant per head\n",
    "        if self.use_adj_const:\n",
    "            adj = A.unsqueeze(1) * self.adj_bias.view(1, H, 1, 1)       # (B,H,L0,L0)\n",
    "        else:\n",
    "            adj = torch.zeros_like(geo)\n",
    "\n",
    "        # SPD\n",
    "        if self.use_spd and (hops is not None):\n",
    "            spd = self._spd_bias(hops, valid2d)                          # (B,H,L0,L0)\n",
    "        else:\n",
    "            spd = torch.zeros_like(geo)\n",
    "\n",
    "        # edge categorical\n",
    "        if self.use_edge_bias and (edge_attr is not None):\n",
    "            edg = self._edge_bias(edge_index, edge_attr, batch, L0, ptr) # (B,H,L0,L0)\n",
    "        else:\n",
    "            edg = torch.zeros_like(geo)\n",
    "\n",
    "        # ---- normalize & bound each component, then scale ----\n",
    "        if self.use_headnorm:\n",
    "            if self.use_geo:  geo = self._apply_ln_heads(geo,  self.ln_geo)\n",
    "            if self.use_spd:  spd = self._apply_ln_heads(spd,  self.ln_spd)\n",
    "            if self.use_edge_bias: edg = self._apply_ln_heads(edg, self.ln_edge)\n",
    "\n",
    "        # gently bound to keep attention stable\n",
    "        if self.use_geo:       geo = self._bound(geo)\n",
    "        if self.use_spd:       spd = self._bound(spd)\n",
    "        if self.use_edge_bias: edg = self._bound(edg)\n",
    "        # typically don't bound adj; it’s already a small learned scalar per head\n",
    "\n",
    "        bias = (self.alpha_geo  * geo\n",
    "              + self.alpha_spd  * spd\n",
    "              + self.alpha_adj  * adj\n",
    "              + self.alpha_edge * edg)\n",
    "\n",
    "        # mask PAD rows/cols; keep diagonal 0 for valid tokens\n",
    "        pad = key_padding_mask\n",
    "        big_neg = torch.tensor(-1e4, device=bias.device, dtype=bias.dtype)\n",
    "        bias = bias.masked_fill(pad.view(B, 1, L0, 1), big_neg)\n",
    "        bias = bias.masked_fill(pad.view(B, 1, 1, L0), big_neg)\n",
    "        I = torch.eye(L0, device=device, dtype=torch.bool).view(1, 1, L0, L0)\n",
    "        bias = torch.where(I, bias.new_zeros(()), bias)\n",
    "\n",
    "        return bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026345f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINEConv\n",
    "\n",
    "class GINEBlock(nn.Module):\n",
    "    def __init__(self, dim, activation=\"silu\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.conv = GINEConv(nn.Sequential(\n",
    "            nn.Linear(dim, dim), act, nn.Linear(dim, dim)\n",
    "        ))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(nn.Linear(dim, 2*dim), act, nn.Dropout(dropout), nn.Linear(2*dim, dim))\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_emb):\n",
    "        h = self.conv(self.norm1(x), edge_index, edge_emb)\n",
    "        x = x + self.drop1(h)\n",
    "        x = x + self.drop2(self.ffn(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class EdgeEncoderMixed(nn.Module):\n",
    "    def __init__(self, emb_dim: int, cont_dim: int = 32, activation=\"silu\"):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.emb0 = nn.Embedding(5, emb_dim)\n",
    "        self.emb1 = nn.Embedding(6, emb_dim)\n",
    "        self.emb2 = nn.Embedding(2, emb_dim)\n",
    "        self.mlp_cont = nn.Sequential(\n",
    "            nn.Linear(cont_dim, emb_dim),\n",
    "            act,\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.LayerNorm(emb_dim),       # <<< add\n",
    "        )\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        cat  = edge_attr[:, :3].long()\n",
    "        cont = edge_attr[:, 3:].float()\n",
    "        e_cat  = self.emb0(cat[:,0]) + self.emb1(cat[:,1]) + self.emb2(cat[:,2])\n",
    "        e_cont = self.mlp_cont(cont)\n",
    "        return e_cat + 0.5 * e_cont     # <<< gentle scale on cont branch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class GraphTransformerGPS(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 8,\n",
    "        nlayers: int = 6,\n",
    "        dropout: float = 0.2,\n",
    "        drop_path: float = 0.0,   # (kept for extensibility)\n",
    "        activation: str = \"silu\",\n",
    "        rdkit_dim: int = 15,\n",
    "        use_extra_atom_feats: bool = True,\n",
    "        extra_atom_dim: int = 5,\n",
    "        # local GNN (GPS) settings\n",
    "        local_layers: int = 2,\n",
    "        use_mixed_edges: bool = True,\n",
    "        cont_dim: int = 32,\n",
    "        # bias knobs\n",
    "        use_geo_bias: bool = True,\n",
    "        use_spd_bias: bool = True,\n",
    "        spd_max: int = 5,\n",
    "        use_adj_const: bool = True,\n",
    "        use_edge_bias: bool = True,\n",
    "        # readout\n",
    "        use_cls: bool = True,\n",
    "        use_has_xyz: bool = True,\n",
    "        head_hidden: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead   = nhead\n",
    "        self.use_cls = use_cls\n",
    "        self.use_has_xyz = use_has_xyz\n",
    "        self.use_extra_atom_feats = use_extra_atom_feats\n",
    "        self.bias_builder = AttnBiasFull(\n",
    "            n_heads=nhead,\n",
    "            rbf_k=32,\n",
    "            rbf_beta=5.0,\n",
    "            use_geo_bias=use_geo_bias,          # was use_geo\n",
    "            use_adj_bias=use_adj_const,         # was use_adj_const (name matches here)\n",
    "            use_spd=use_spd_bias,               # was use_spd\n",
    "            spd_buckets=(spd_max + 1),          # was spd_max; +1 gives the \">= spd_max\" bucket\n",
    "            use_edge_bias=use_edge_bias,\n",
    "            edge_cats=(5, 6, 2),\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "\n",
    "        act = _act(activation)\n",
    "\n",
    "        # encoders\n",
    "        self.atom_enc = AtomEncoder(emb_dim=d_model)\n",
    "        if use_extra_atom_feats:\n",
    "            self.extra_proj = nn.Sequential(nn.Linear(extra_atom_dim, d_model), act, nn.Linear(d_model, d_model))\n",
    "            self.extra_gate = nn.Sequential(nn.Linear(2*d_model, d_model), act)\n",
    "\n",
    "        # local GNN stack\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        if use_mixed_edges:\n",
    "            self.edge_enc = EdgeEncoderMixed(d_model, cont_dim=cont_dim, activation=activation)\n",
    "        else:\n",
    "            from ogb.graphproppred.mol_encoder import BondEncoder\n",
    "            self.edge_enc = BondEncoder(emb_dim=d_model)\n",
    "        self.local_blocks = nn.ModuleList([GINEBlock(d_model, activation=activation, dropout=dropout) \n",
    "                                           for _ in range(local_layers)])\n",
    "\n",
    "        # transformer stack (PyTorch encoder)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=4*d_model,\n",
    "                                               dropout=dropout, activation=activation, batch_first=True, \n",
    "                                               norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers, enable_nested_tensor=False)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        # readout: concat mean + max + (optional) CLS + attention pool\n",
    "        self.gate_pool = nn.Sequential(nn.Linear(d_model, d_model//2), act, nn.Linear(d_model//2, 1))\n",
    "        # features: mean(d), max(d), attn(d) = 3d, (+cls d) optional, + rdkit, + has_xyz\n",
    "        pooled_dim = 3*d_model + (d_model if use_cls else 0)\n",
    "        head_in = pooled_dim + rdkit_dim + (1 if use_has_xyz else 0)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(head_in),\n",
    "            nn.Linear(head_in, head_hidden), act, nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden, head_hidden//2), act, nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden//2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # 1) atom encoding + optional per-atom extras\n",
    "        x = self.atom_enc(data.x)  # (N,D)\n",
    "        if getattr(self, \"use_extra_atom_feats\", False) and hasattr(data, \"extra_atom_feats\"):\n",
    "            xa = self.extra_proj(data.extra_atom_feats.float())          # (N,D)\n",
    "            x  = self.extra_gate(torch.cat([x, xa], dim=1))              # (N,D)\n",
    "\n",
    "        # 2) local GNN over sparse graph\n",
    "        e = self.edge_enc(data.edge_attr)\n",
    "        for blk in self.local_blocks:\n",
    "            x = blk(x, data.edge_index, e)                               # (N,D)\n",
    "\n",
    "        # 3) pack to dense (no CLS yet)\n",
    "        x_pad, valid = to_dense_batch(x, data.batch)                     # (B,L0,D)\n",
    "        B, L0, D = x_pad.shape\n",
    "        key_padding = ~valid                                             # (B,L0) True == PAD\n",
    "\n",
    "        # 4) head-wise attention bias on L0 tokens (B,H,L0,L0), pre-CLS\n",
    "        #    Your AttnBiasFull typically supports SPD, geometry, adjacency, edges\n",
    "        hops = getattr(data, \"hops\", None)                               # (B,MAX_NODES,MAX_NODES) or None\n",
    "        ptr = getattr(data, \"ptr\", None)\n",
    "        attn_bias = self.bias_builder(\n",
    "            pos=(data.pos if hasattr(data, \"pos\") else None),\n",
    "            edge_index=data.edge_index,\n",
    "            edge_attr=(data.edge_attr if hasattr(data, \"edge_attr\") else None),\n",
    "            batch=data.batch,\n",
    "            key_padding_mask=key_padding,   # (B,L0), True=PAD\n",
    "            hops=getattr(data, \"hops\", None),\n",
    "            ptr=ptr\n",
    "        )  # (B,H,L0,L0)                                                # (B,H,L0,L0)\n",
    "\n",
    "        # 5) finalize bias (mask PAD rows/cols, keep diagonal 0), then optionally append CLS\n",
    "        B, H, L = attn_bias.shape[0], attn_bias.shape[1], attn_bias.shape[-1]\n",
    "        pad = key_padding                                                 # (B,L)\n",
    "        huge = attn_bias.new_tensor(-1e4)\n",
    "\n",
    "        # rows FROM PAD, cols TO PAD\n",
    "        attn_bias = attn_bias.masked_fill(pad.view(B, 1, L, 1), huge)\n",
    "        attn_bias = attn_bias.masked_fill(pad.view(B, 1, 1, L), huge)\n",
    "\n",
    "        # keep diagonal = 0 on valid tokens\n",
    "        I = torch.eye(L, device=attn_bias.device, dtype=torch.bool).view(1, 1, L, L)\n",
    "        attn_bias = torch.where(I, attn_bias.new_zeros(()), attn_bias)\n",
    "\n",
    "        # (optional) append CLS token at the end\n",
    "        if getattr(self, \"use_cls\", False):\n",
    "            # append CLS embedding\n",
    "            cls = self.cls_token.expand(B, 1, D)                         # (B,1,D)\n",
    "            x_pad = torch.cat([x_pad, cls], dim=1)                       # (B,L+1,D)\n",
    "\n",
    "            # extend key_padding: CLS is always valid (False)\n",
    "            key_padding = torch.cat(\n",
    "                [key_padding, torch.zeros(B, 1, dtype=torch.bool, device=x_pad.device)],\n",
    "                dim=1\n",
    "            )                                                             # (B,L+1)\n",
    "\n",
    "            # pad bias by one row/col with zeros for CLS -> (B,H,L+1,L+1)\n",
    "            attn_bias = F.pad(attn_bias, (0, 1, 0, 1), value=0.0)\n",
    "            L = L + 1\n",
    "\n",
    "        # 6) transformer encoder with 3D additive mask (B*H,L,L)\n",
    "        attn_mask_3d = attn_bias.reshape(B * H, L, L).to(x_pad.dtype)\n",
    "        h = self.encoder(                                                # returns (B,L,D) when batch_first=True\n",
    "            x_pad,\n",
    "            mask=attn_mask_3d, # additive float mask \n",
    "        )\n",
    "\n",
    "        # 7) pooling (mean + max + gated attention), plus optional CLS; then RDKit/has_xyz and head\n",
    "        # exclude CLS from token pools\n",
    "        h_tok = h[:, :L0, :]                                             # (B,L0,D)\n",
    "        mask_f = valid.float()                                           # (B,L0)\n",
    "\n",
    "        mean = (h_tok * mask_f.unsqueeze(-1)).sum(1) / (mask_f.sum(1, keepdim=True) + 1e-8)  # (B,D)\n",
    "        mmax, _ = (h_tok + (1.0 - mask_f.unsqueeze(-1)) * (-1e4)).max(dim=1)                 # (B,D)\n",
    "\n",
    "        gate_logits = self.gate_pool(h_tok).squeeze(-1)                  # (B,L0)\n",
    "        gate = torch.softmax(gate_logits.masked_fill(~valid, -1e4), dim=1)\n",
    "        attn_pool = (h_tok * gate.unsqueeze(-1)).sum(1)                  # (B,D)\n",
    "\n",
    "        parts = [mean, mmax, attn_pool]\n",
    "\n",
    "        if getattr(self, \"use_cls\", False):\n",
    "            parts.append(h[:, L-1, :])                                   # CLS vector (B,D)\n",
    "\n",
    "        # RDKit globals\n",
    "        rd = data.rdkit_feats.view(B, -1).float()                        # (B, rdkit_dim)\n",
    "        parts.append(rd)\n",
    "\n",
    "        # optional has_xyz scalar if present\n",
    "        if getattr(self, \"use_has_xyz\", False) and hasattr(data, \"has_xyz\"):\n",
    "            parts.append(data.has_xyz.view(B, 1).float())\n",
    "\n",
    "        out = torch.cat(parts, dim=1)\n",
    "        return self.head(out)                                            # (B,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9de38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# introspect dims\n",
    "b = next(iter(train_loader_tg))\n",
    "rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "model_tg = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=6, dropout=0.1,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=True, use_edge_bias=True,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_tg, ckpt_tg, met_tg = train_hybrid_gnn_sota(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=100, warmup_epochs=5, patience=10,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "    save_dir=\"saved_models/gt_tg_spd\", tag=\"graphtransformer_tg_spd\"\n",
    ")\n",
    "\n",
    "\n",
    "model_den = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=6, dropout=0.2,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=True, use_edge_bias=True,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_den, ckpt_den, met_den = train_hybrid_gnn_sota(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=10, warmup_epochs=3, patience=10,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "    save_dir=\"saved_models/gt_den_spd\", tag=\"graphtransformer_den_spd\"\n",
    ")\n",
    "\n",
    "# Rg\n",
    "model_rg = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=6, dropout=0.1,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=True, use_edge_bias=True,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_rg, ckpt_rg, met_rg = train_hybrid_gnn_sota(\n",
    "    model_rg, train_loader_rg, val_loader_rg,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=100, warmup_epochs=10, patience=10,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "    save_dir=\"saved_models/gt_rg_spd\", tag=\"graphtransformer_rg_spd\"\n",
    ")\n",
    "\n",
    "# Tc\n",
    "model_tc = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=6, dropout=0.1,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=True, use_edge_bias=True,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_tc, ckpt_tc, met_tc = train_hybrid_gnn_sota(\n",
    "    model_tc, train_loader_tc, val_loader_tc,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=100, warmup_epochs=10, patience=10,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "    save_dir=\"saved_models/gt_tc_spd\", tag=\"graphtransformer_tc_spd\"\n",
    ")\n",
    "\n",
    "model_ffv = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=6, dropout=0.1,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=True, use_edge_bias=True,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_ffv, ckpt_ffv, met_ffv = train_hybrid_gnn_sota(\n",
    "    model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=100, warmup_epochs=10, patience=10,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "    save_dir=\"saved_models/gt_ffv_spd\", tag=\"graphtransformer_ffv_spd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb05b8",
   "metadata": {},
   "source": [
    "\n",
    "[graphtransformer_tg_spd] Best Val — MAE 52.418751 | RMSE 68.783745 | R2 0.5044\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.080994 | RMSE 0.105409 | R2 0.3673\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.259735 | RMSE 3.240687 | R2 0.5228\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.027223 | RMSE 0.041304 | R2 0.8002\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428bf75",
   "metadata": {},
   "source": [
    "\n",
    "[graphtransformer_tg_spd] Best Val — MAE 62.587055 | RMSE 78.081322 | R2 0.3613\n",
    "\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.097926 | RMSE 0.126593 | R2 0.0874\n",
    "\n",
    "[graphtransformer_rg_spd] Best Val — MAE 1.952458 | RMSE 2.967103 | R2 0.6000\n",
    "\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.028464 | RMSE 0.043981 | R2 0.7735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895acd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0  loss 0.0193\n"
     ]
    }
   ],
   "source": [
    "import torch, random\n",
    "from torch.utils.data import Subset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "train_ds = train_loader_den.dataset\n",
    "idx = random.sample(range(len(train_ds)), 64)\n",
    "tiny = Subset(train_ds, idx)\n",
    "tiny_loader = type(train_loader_den)(tiny, batch_size=32, shuffle=True)\n",
    "\n",
    "model = model_den  # reuse\n",
    "model.train()\n",
    "opt = AdamW(model.parameters(), lr=2e-4, weight_decay=0.05)\n",
    "crit = torch.nn.SmoothL1Loss(beta=1.0)  # Huber\n",
    "\n",
    "dev = next(model.parameters()).device\n",
    "for step, b in enumerate(tiny_loader):\n",
    "    b = b.to(dev)\n",
    "    if b.x.dtype != torch.long: b.x = b.x.long()\n",
    "    p = model(b)\n",
    "    loss = crit(p.view_as(b.y), b.y)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    opt.step()\n",
    "    if step % 5 == 0:\n",
    "        print(f\"step {step}  loss {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673460",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "| Model Type | Feature | MAE | RMSE | R2 |\n",
    "|---|---|---|---|---|\n",
    "| RF3D | FFV | 0.007621 | 0.017553 | 0.6605 |\n",
    "| RF3D_Aug | FFV | 0.007578 | 0.017404 | 0.6662 |\n",
    "| GNN2 | FFV | 0.013817 | 0.023902 | 0.4473 |\n",
    "| GNN2_Aug | FFV | 0.013092 | 0.022793 | 0.4974 |\n",
    "| ET | FFV | 0.006651 | 0.016818 | 0.6883 |\n",
    "| **ET_Aug** | **FFV** | **0.006635** | **0.016826** | **0.6880** |\n",
    "| GT | FFV | 0.XXXXXX | 0.XXXXXX | 0.XXXX |\n",
    "| GT_Aug | FFV | 0.XXXXXX | 0.XXXXXX | 0.XXXX |\n",
    "| RF3D | Tg | 58.315801 | 74.296699 | 0.5846 |\n",
    "| RF3D_Aug | Tg | 58.143107 | 74.521032 | 0.5821 |\n",
    "| **GNN2** | **Tg** | **47.105114** | **61.480179** | **0.6040** |\n",
    "| GNN2_Aug | Tg | 51.539692 | 70.575638 | 0.4782 |\n",
    "| ET | Tg | 58.973811 | 74.658978 | 0.5806 |\n",
    "| ET_Aug | Tg | 58.521052 | 74.475532 | 0.5826 |\n",
    "| GT | Tg | 78.903389 | 98.401192 |-0.0143 |\n",
    "| GT_Aug | Tg | 52.365578 | 67.529610 | 0.5223 |\n",
    "| RF3D | Tc | 0.029937 | 0.045036 | 0.7313 |\n",
    "| RF3D_Aug | Tc | 0.029675 | 0.044853 | 0.7335 |\n",
    "| **GNN2** | **Tc** | **0.025115** | **0.041331** | **0.8000** |\n",
    "| **GNN2_Aug** | **Tc** | **0.025252** | **0.039670** | **0.8157** |\n",
    "| ET | Tc | 0.028888 | 0.043469 | 0.7497 |\n",
    "| ET_Aug | Tc | 0.027990 | 0.042644 | 0.7591 |\n",
    "| GT | Tc | 0.032644 | 0.046613 | 0.7456 |\n",
    "| GT_Aug | Tc | 0.028590 | 0.043121 | 0.7822 |\n",
    "| RF3D | Rg | 1.648818 | 2.493712 | 0.7299 |\n",
    "| RF3D_Aug | Rg | 1.668425 | 2.517235 | 0.7248 |\n",
    "| GNN2 | Rg | 2.115880 | 2.801481 | 0.6434 |\n",
    "| GNN2_Aug | Rg | 1.532573 | 2.405382 | 0.7371 |\n",
    "| ET | Rg | 1.619464 | 2.522478 | 0.7237 |\n",
    "| **ET_Aug** | **Rg** | **1.609396** | **2.526705** | **0.7227** |\n",
    "| GT | Rg | 2.579300 | 3.521387 | 0.4366 |\n",
    "| GT_Aug | Rg | 2.134301 | 3.066199 | 0.5728 |\n",
    "| RF3D | Density | 0.037793 | 0.070932 | 0.7847 |\n",
    "| RF3D_Aug | Density | 0.037123 | 0.070212 | 0.7891 |\n",
    "| GNN2 | Density | 0.031735 | 0.067845 | 0.7379 |\n",
    "| GNN2_Aug | Density | 0.030458 | 0.070372 | 0.7180 |\n",
    "| ET | Density | 0.028492 | 0.052839 | 0.8805 |\n",
    "| **ET_Aug** | **Density** | **0.028135** | **0.051842** | **0.8850** |\n",
    "| GT | Density | 0.104749 | 0.134771 | -0.0343 |\n",
    "| GT_Aug | Density | 0.087159 | 0.126079 | 0.0948 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
