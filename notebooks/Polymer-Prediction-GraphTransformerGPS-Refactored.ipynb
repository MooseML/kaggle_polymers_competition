{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61979795",
   "metadata": {},
   "source": [
    "# Polymer Property Predictions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a7a2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd, numpy as np\n",
    "\n",
    "# LMDB_OUT = \"C:/Users/mattg/Downloads/kaggle_polymers_competition/data/processed_chunks_new/polymer_train3d_dist.lmdb\"  # same base as in builder\n",
    "# meta = pd.read_csv(LMDB_OUT + \".parent_meta.tsv\", sep=\"\\t\")\n",
    "# print(\"parents:\", len(meta))\n",
    "\n",
    "# q = meta['n_atoms_2d'].quantile([0.5, 0.9, 0.95, 0.99, 1.00]).round().astype(int)\n",
    "# print(\"n_atoms quantiles:\", q.to_dict())\n",
    "# print(\"star_count mean:\", meta['star_count'].mean(), \"max:\", meta['star_count'].max())\n",
    "\n",
    "# # starter budgets (24GB-ish)\n",
    "# p95 = int(q.loc[0.95])\n",
    "# max_graphs    = 64\n",
    "# max_tokens    = max(4000, 8 * p95)     # sum(L_i)\n",
    "# max_quadratic = 1_200_000              # sum(L_i^2)\n",
    "# print(dict(max_graphs=max_graphs, max_tokens=max_tokens, max_quadratic=max_quadratic))\n",
    "\n",
    "# # whale cutoff (optional)\n",
    "# p99 = int(q.loc[0.99])\n",
    "# print(\"whale cutoff >\", p99, \"atoms; share:\", float((meta['n_atoms_2d']>p99).mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ace_tools_open as tools\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import pickle\n",
    "import joblib\n",
    "import os \n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Module, Sequential, Dropout\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# OGB dataset \n",
    "from ogb.lsc import PygPCQM4Mv2Dataset, PCQM4Mv2Dataset\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# RDKit\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "\n",
    "# ChemML\n",
    "from chemml.chem import Molecule, RDKitFingerprint, CoulombMatrix, tensorise_molecules\n",
    "from chemml.models import MLP, NeuralGraphHidden, NeuralGraphOutput\n",
    "from chemml.utils import regression_metrics\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589db70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Built with CUDA: True\n",
      "CUDA available: True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device: /physical_device:GPU:0\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: data\n",
      "LMDB directory: data\\processed_chunks\n",
      "Train LMDB: data\\processed_chunks\\polymer_train3d_dist.lmdb\n",
      "Test LMDB: data\\processed_chunks\\polymer_test3d_dist.lmdb\n",
      "LMDBs already exist.\n"
     ]
    }
   ],
   "source": [
    "# Paths - Fixed for Kaggle environment\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  # Writable directory\n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/best_gnn_transformer_hybrid.pt'\n",
    "else:\n",
    "    DATA_ROOT = 'data'\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = 'best_gnn_transformer_hybrid.pt'\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"LMDB directory: {CHUNK_DIR}\")\n",
    "print(f\"Train LMDB: {TRAIN_LMDB}\")\n",
    "print(f\"Test LMDB: {TEST_LMDB}\")\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    # Run the LMDB builders\n",
    "    !python build_polymer_lmdb_fixed.py train\n",
    "    !python build_polymer_lmdb_fixed.py test\n",
    "    print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c34b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Tg → parents train=  408 val=  103 | aug rows train=  4080 val=  1030\n",
      "    FFV → parents train= 5624 val= 1406 | aug rows train= 56240 val= 14060\n",
      "     Tc → parents train=  589 val=  148 | aug rows train=  5890 val=  1480\n",
      "Density → parents train=  490 val=  123 | aug rows train=  4900 val=  1230\n",
      "     Rg → parents train=  491 val=  123 | aug rows train=  4910 val=  1230\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 1: parent-aware wiring (works for both GNN + ET) ====\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "AUG_KEY_MULT = 1000  # must match the LMDB builder\n",
    "\n",
    "# Paths expected: DATA_ROOT, TRAIN_LMDB\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_csv[\"id\"] = train_csv[\"id\"].astype(int)\n",
    "\n",
    "# LMDB ids (augmented key_ids)\n",
    "lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "lmdb_ids = np.loadtxt(lmdb_ids_path, dtype=np.int64)\n",
    "if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "\n",
    "# Parent map (preferred); fallback derives from key structure\n",
    "pmap_path = TRAIN_LMDB + \".parent_map.tsv\"\n",
    "if os.path.exists(pmap_path):\n",
    "    pmap = pd.read_csv(pmap_path, sep=\"\\t\")  # cols: key_id, parent_id, aug_idx, seed\n",
    "    pmap[\"key_id\"] = pmap[\"key_id\"].astype(np.int64)\n",
    "    pmap[\"parent_id\"] = pmap[\"parent_id\"].astype(np.int64)\n",
    "else:\n",
    "    pmap = pd.DataFrame({\n",
    "        \"key_id\": lmdb_ids.astype(np.int64),\n",
    "        \"parent_id\": (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "    })\n",
    "\n",
    "parents_in_lmdb = np.sort(pmap[\"parent_id\"].unique().astype(np.int64))\n",
    "\n",
    "def parents_with_label(task: str) -> np.ndarray:\n",
    "    m = ~train_csv[task].isna()\n",
    "    have = train_csv.loc[m, \"id\"].astype(int).values\n",
    "    return np.intersect1d(have, parents_in_lmdb, assume_unique=False)\n",
    "\n",
    "def task_parent_split(task: str, test_size=0.2, seed=42):\n",
    "    parents_labeled = parents_with_label(task)\n",
    "    if parents_labeled.size == 0:\n",
    "        raise ValueError(f\"No parents with labels for {task}\")\n",
    "    p_tr, p_va = train_test_split(parents_labeled, test_size=test_size, random_state=seed)\n",
    "    tr_keys = pmap.loc[pmap.parent_id.isin(p_tr), \"key_id\"].astype(np.int64).values\n",
    "    va_keys = pmap.loc[pmap.parent_id.isin(p_va), \"key_id\"].astype(np.int64).values\n",
    "    return np.sort(tr_keys), np.sort(va_keys), np.sort(p_tr), np.sort(p_va)\n",
    "\n",
    "# Pools for all tasks (augmented key_ids for GNN)\n",
    "task_pools = {}\n",
    "task_parent_splits = {}\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys, p_tr, p_va = task_parent_split(t, test_size=0.2, seed=42)\n",
    "    task_pools[t] = (tr_keys, va_keys)\n",
    "    task_parent_splits[t] = (p_tr, p_va)\n",
    "\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys = task_pools[t]\n",
    "    p_tr, p_va = task_parent_splits[t]\n",
    "    print(f\"{t:>7} → parents train={len(p_tr):5d} val={len(p_va):5d} | aug rows train={len(tr_keys):6d} val={len(va_keys):6d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3c3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# --- CONSTANT RDF EDGES: 12 edges -> 11 bins (ALWAYS) ---\n",
    "RDF_EDGES = torch.tensor([0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 6], dtype=torch.float32)\n",
    "RDF_NUM_BINS = len(RDF_EDGES) - 1  # 11\n",
    "\n",
    "def _hist_fixed(x: torch.Tensor, edges: torch.Tensor = RDF_EDGES):\n",
    "    \"\"\"Normalized histogram with a FIXED number of bins (len(edges) - 1).\"\"\"\n",
    "    if x.numel() == 0:\n",
    "        return [0.0] * (len(edges) - 1)\n",
    "    h = torch.histc(x, bins=len(edges) - 1, min=float(edges[0]), max=float(edges[-1]))\n",
    "    h = (h / (h.sum() + 1e-8)).tolist()\n",
    "    return h\n",
    "\n",
    "def _rbf(d: torch.Tensor, K: int = 32, beta: float = 5.0, dmax: float = 6.0, device=None):\n",
    "    c = torch.linspace(0.0, dmax, K, device=device)\n",
    "    return torch.exp(-beta * (d.unsqueeze(-1) - c) ** 2)  # [M,K]\n",
    "\n",
    "def geom_features_from_rec(\n",
    "    rec,\n",
    "    rdkit_dim_expected: int = 15,\n",
    "    rbf_K: int = 32,\n",
    "    max_pairs: int = 20000\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns a FIXED-LENGTH (120) feature vector per LMDB record:\n",
    "      15 RDKit globals\n",
    "      5  sizes/degree/has_xyz     : [n_atoms, n_bonds, deg_mean, deg_max, has_xyz]\n",
    "      3  inertia eigenvalues      : λ1..λ3 (descending)\n",
    "      2  shape                    : [Rg_geom, anisotropy]\n",
    "      3  bbox extents             : [dx, dy, dz]\n",
    "      3  radius-from-centroid     : [mean, std, max]\n",
    "      4  bond distance stats      : [mean, std, min, max]\n",
    "      5  SPD histogram            : [hop0, hop1, hop2, hop3, hop>=4] (normalized)\n",
    "      5  extra atom mean (if 5-D; else zeros)\n",
    "      32 RBF(bond distances) mean\n",
    "      32 RBF(pairwise distances) mean (sampled if too large)\n",
    "      11 RDF histogram over pairwise distances (0..6Å, fixed bins)\n",
    "      Total = 120 dims\n",
    "    \"\"\"\n",
    "    # ---- RDKit globals (expected 15) ----\n",
    "    rd = getattr(rec, \"rdkit_feats\", None)\n",
    "    if rd is not None:\n",
    "        rd = torch.as_tensor(rd).view(-1).float().detach().cpu().numpy()\n",
    "    else:\n",
    "        rd = np.zeros((rdkit_dim_expected,), dtype=np.float32)\n",
    "    if rd.size != rdkit_dim_expected:\n",
    "        rd = np.zeros((rdkit_dim_expected,), dtype=np.float32)\n",
    "\n",
    "    # ---- Graph sizes & degree ----\n",
    "    x  = torch.as_tensor(getattr(rec, \"x\", np.zeros((0, 1), np.float32)))\n",
    "    ei = torch.as_tensor(getattr(rec, \"edge_index\", np.zeros((2, 0), np.int64)))\n",
    "    n  = int(x.shape[0])\n",
    "    e  = int(ei.shape[1]) if ei.ndim == 2 else 0\n",
    "    deg = torch.bincount(ei[0], minlength=n) if e > 0 else torch.zeros(n, dtype=torch.long)\n",
    "    deg_mean = deg.float().mean().item() if n > 0 else 0.0\n",
    "    deg_max  = deg.max().item() if n > 0 else 0.0\n",
    "\n",
    "    # ---- has_xyz ----\n",
    "    has_xyz = 0\n",
    "    if hasattr(rec, \"has_xyz\"):\n",
    "        hz = getattr(rec, \"has_xyz\")\n",
    "        has_xyz = int(bool(hz[0].item() if isinstance(hz, torch.Tensor) else hz))\n",
    "\n",
    "    # ---- Geometry from pos ----\n",
    "    pos = getattr(rec, \"pos\", None)\n",
    "    inertia = np.zeros(3, dtype=np.float32)\n",
    "    rg_geom = 0.0\n",
    "    anisotropy = 0.0\n",
    "    extents = np.zeros(3, dtype=np.float32)\n",
    "    rad_stats = np.zeros(3, dtype=np.float32)\n",
    "    bond_stats = np.zeros(4, dtype=np.float32)  # mean, std, min, max\n",
    "\n",
    "    rbf_pair_mean = np.zeros(rbf_K, dtype=np.float32)\n",
    "    rbf_bond_mean = np.zeros(rbf_K, dtype=np.float32)\n",
    "    rdf_hist = [0.0] * RDF_NUM_BINS  # ALWAYS 11 bins\n",
    "    dists = torch.tensor([])  # keep a handle for later checks\n",
    "\n",
    "    if pos is not None and n > 0 and has_xyz:\n",
    "        P = torch.as_tensor(pos).float()\n",
    "        ctr = P.mean(0, keepdim=True)\n",
    "        C = P - ctr\n",
    "\n",
    "        # inertia tensor (mass = 1 per atom)\n",
    "        I = torch.zeros(3, 3, dtype=P.dtype, device=P.device)\n",
    "        for r in C:\n",
    "            x_, y_, z_ = r\n",
    "            I += torch.tensor([[y_*y_ + z_*z_, -x_*y_,        -x_*z_],\n",
    "                               [ -x_*y_,       x_*x_ + z_*z_, -y_*z_],\n",
    "                               [ -x_*z_,       -y_*z_,        x_*x_ + y_*y_]],\n",
    "                              dtype=P.dtype, device=P.device)\n",
    "        evals, _ = torch.linalg.eigh(I)   # ascending\n",
    "        lam1, lam2, lam3 = evals.flip(0)  # descending\n",
    "        inertia = torch.stack([lam1, lam2, lam3]).detach().cpu().numpy()\n",
    "        rg_geom = float(torch.sqrt(evals.sum() / max(1, n)))\n",
    "        anisotropy = float((lam1 - (lam2 + lam3) / 2.0) / (evals.sum() + 1e-8))\n",
    "\n",
    "        # bbox extents\n",
    "        mn, mx = P.min(0).values, P.max(0).values\n",
    "        extents = (mx - mn).detach().cpu().numpy()\n",
    "\n",
    "        # radii from centroid\n",
    "        r = C.norm(dim=1)\n",
    "        rad_stats = np.array([\n",
    "            r.mean().item(),\n",
    "            r.std(unbiased=False).item(),\n",
    "            r.max().item()\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        # pairwise distances (cap for speed)\n",
    "        if n >= 2:\n",
    "            total_pairs = n * (n - 1) // 2\n",
    "            if total_pairs > max_pairs:\n",
    "                # kNN-style sampling to approximate the distribution\n",
    "                k = int(math.sqrt(max_pairs))\n",
    "                a = min(n, k)\n",
    "                anchors = torch.randperm(n)[:a]\n",
    "                dmat = torch.cdist(P[anchors], P)\n",
    "                _, nn = torch.topk(dmat, k=min(n, k), largest=False)\n",
    "                dists = (P[anchors].unsqueeze(1) - P[nn]).norm(dim=2).reshape(-1)\n",
    "            else:\n",
    "                dists = torch.pdist(P, p=2)\n",
    "\n",
    "            if dists.numel() > 0:\n",
    "                # FIXED-LENGTH RDF\n",
    "                rdf_hist = _hist_fixed(dists, RDF_EDGES)\n",
    "                # RBF over pairs\n",
    "                rbf_pair = _rbf(dists, K=rbf_K, beta=5.0, dmax=float(RDF_EDGES[-1]), device=P.device)\n",
    "                rbf_pair_mean = rbf_pair.mean(0).detach().cpu().numpy()\n",
    "\n",
    "        # bond distances + RBF\n",
    "        if e > 0:\n",
    "            d_bond = (P[ei[0]] - P[ei[1]]).norm(dim=1)\n",
    "            bond_stats = np.array([\n",
    "                d_bond.mean().item(),\n",
    "                d_bond.std(unbiased=False).item(),\n",
    "                d_bond.min().item(),\n",
    "                d_bond.max().item(),\n",
    "            ], dtype=np.float32)\n",
    "            rbf_bond = _rbf(d_bond, K=rbf_K, beta=5.0, dmax=float(RDF_EDGES[-1]), device=P.device)\n",
    "            rbf_bond_mean = rbf_bond.mean(0).detach().cpu().numpy()\n",
    "\n",
    "    # ---- SPD histogram (prefer 'hops', fallback 'dist') ----\n",
    "    spd_hist = np.zeros(5, dtype=np.float32)  # [0,1,2,3,>=4]\n",
    "    H = getattr(rec, \"hops\", None)\n",
    "    if H is None:\n",
    "        H = getattr(rec, \"dist\", None)\n",
    "    if H is not None:\n",
    "        H = torch.as_tensor(H).float()\n",
    "        if H.ndim == 2:\n",
    "            H = H[:n, :n]\n",
    "            finite = H[torch.isfinite(H) & (H >= 0)]\n",
    "            if finite.numel() > 0:\n",
    "                counts = [\n",
    "                    (finite == 0).float().sum(),\n",
    "                    (finite == 1).float().sum(),\n",
    "                    (finite == 2).float().sum(),\n",
    "                    (finite == 3).float().sum(),\n",
    "                    (finite >= 4).float().sum(),\n",
    "                ]\n",
    "                total = sum(counts) + 1e-8\n",
    "                spd_hist = np.array([float(c / total) for c in counts], dtype=np.float32)\n",
    "\n",
    "    # ---- extra atom features mean (expect 5 dims if present) ----\n",
    "    extra_mean = np.zeros(5, dtype=np.float32)\n",
    "    if hasattr(rec, \"extra_atom_feats\") and getattr(rec, \"extra_atom_feats\") is not None:\n",
    "        EA = torch.as_tensor(rec.extra_atom_feats).float()\n",
    "        if EA.ndim == 2 and EA.shape[1] == 5:\n",
    "            extra_mean = EA.mean(0).detach().cpu().numpy()\n",
    "\n",
    "    scalars = np.array([n, e, deg_mean, deg_max, float(has_xyz)], dtype=np.float32)\n",
    "    rdf_flat = np.array(rdf_hist, dtype=np.float32)  # ALWAYS length 11\n",
    "\n",
    "    vec = np.concatenate([\n",
    "        rd,                     # 15\n",
    "        scalars,                # 5  -> 20\n",
    "        inertia,                # 3  -> 23\n",
    "        np.array([rg_geom, anisotropy], dtype=np.float32),  # 2 -> 25\n",
    "        extents,                # 3  -> 28\n",
    "        rad_stats,              # 3  -> 31\n",
    "        bond_stats,             # 4  -> 35\n",
    "        spd_hist,               # 5  -> 40\n",
    "        extra_mean,             # 5  -> 45\n",
    "        rbf_bond_mean,          # 32 -> 77\n",
    "        rbf_pair_mean,          # 32 -> 109\n",
    "        rdf_flat                # 11 -> 120\n",
    "    ], axis=0)\n",
    "\n",
    "    # Safety: enforce fixed size 120 (pad/truncate if anything drifts)\n",
    "    if vec.shape[0] != 120:\n",
    "        if vec.shape[0] < 120:\n",
    "            vec = np.pad(vec, (0, 120 - vec.shape[0]), mode='constant')\n",
    "        else:\n",
    "            vec = vec[:120]\n",
    "    return vec.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4a0a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors as rdmd, DataStructs\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def morgan_bits(smiles_list, n_bits=1024, radius=3):\n",
    "    X = np.zeros((len(smiles_list), n_bits), dtype=np.uint8)\n",
    "    for i, s in enumerate(smiles_list):\n",
    "        arr = np.zeros((n_bits,), dtype=np.uint8)\n",
    "        m = Chem.MolFromSmiles(s)\n",
    "        if m is not None:\n",
    "            fp = rdmd.GetMorganFingerprintAsBitVect(m, radius=radius, nBits=n_bits)\n",
    "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        X[i] = arr\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "def build_rf_features_from_lmdb(ids: np.ndarray, lmdb_path: str, smiles_list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns X = [Morgan1024 | LMDB-3D-global(69)] for each id/smiles.\n",
    "    Assumes ids and smiles_list are aligned with the CSV used to build LMDB.\n",
    "    \"\"\"\n",
    "    base = LMDBDataset(ids, lmdb_path)\n",
    "    # 3D/global block\n",
    "    feats3d = []\n",
    "    for i in range(len(base)):\n",
    "        rec = base[i]\n",
    "        feats3d.append(geom_features_from_rec(rec))  # shape (69,)\n",
    "    X3d = np.vstack(feats3d).astype(np.float32) if feats3d else np.zeros((0, 69), dtype=np.float32)\n",
    "\n",
    "    # Morgan FP block (2D)\n",
    "    Xfp = morgan_bits(smiles_list, n_bits=1024, radius=3)   # (N,1024)\n",
    "\n",
    "    # concat\n",
    "    X = np.hstack([Xfp, X3d]).astype(np.float32)            # (N, 1024+69)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0467213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 4: fp3d features aggregated per parent for ET ====\n",
    "AUG_KEY_MULT = 1000  # must match builder\n",
    "\n",
    "def build_fp3d_features_from_lmdb_parents(parent_ids, lmdb_path, smiles_list, *, agg=\"mean\"):\n",
    "    \"\"\"\n",
    "    Expands each parent -> its augmented key_ids, calls your existing\n",
    "    build_rf_features_from_lmdb(key_ids, lmdb_path, smiles_for_each_key),\n",
    "    then aggregates per parent (mean/median/max) -> one row per parent.\n",
    "    Returns X_parent, keep_idx (indices into parent_ids/smiles_list).\n",
    "    \"\"\"\n",
    "    # parent_map\n",
    "    pmap_path = lmdb_path + \".parent_map.tsv\"\n",
    "    if os.path.exists(pmap_path):\n",
    "        pmap = pd.read_csv(pmap_path, sep=\"\\t\")\n",
    "        pmap['key_id'] = pmap['key_id'].astype(np.int64)\n",
    "        pmap['parent_id'] = pmap['parent_id'].astype(np.int64)\n",
    "        group = pmap.groupby('parent_id')['key_id'].apply(list).to_dict()\n",
    "    else:\n",
    "        lmdb_ids = np.loadtxt(lmdb_path + \".ids.txt\", dtype=np.int64)\n",
    "        if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "        dfmap = pd.DataFrame({\n",
    "            'parent_id': (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "            'key_id': lmdb_ids.astype(np.int64),\n",
    "        })\n",
    "        group = dfmap.groupby('parent_id')['key_id'].apply(list).to_dict()\n",
    "\n",
    "    # expand\n",
    "    flat_keys, flat_smiles, seg_sizes = [], [], []\n",
    "    for pid, smi in zip(parent_ids, smiles_list):\n",
    "        keys = group.get(int(pid), [])\n",
    "        seg_sizes.append(len(keys))\n",
    "        if len(keys):\n",
    "            flat_keys.extend(keys)\n",
    "            flat_smiles.extend([smi] * len(keys))\n",
    "\n",
    "    if len(flat_keys) == 0:\n",
    "        raise ValueError(\"No augmented key_ids found for provided parent ids.\")\n",
    "\n",
    "    # IMPORTANT: this uses your existing function\n",
    "    X_all = build_rf_features_from_lmdb(np.array(flat_keys, dtype=np.int64),\n",
    "                                        lmdb_path,\n",
    "                                        flat_smiles)  # -> (sum_augs, D)\n",
    "\n",
    "    # fold back per parent\n",
    "    rows, keep_idx = [], []\n",
    "    i0 = 0\n",
    "    for i, k in enumerate(seg_sizes):\n",
    "        if k == 0: continue\n",
    "        Xi = X_all[i0:i0+k]\n",
    "        i0 += k\n",
    "        if   agg == \"mean\":   rows.append(Xi.mean(axis=0))\n",
    "        elif agg == \"median\": rows.append(np.median(Xi, axis=0))\n",
    "        elif agg == \"max\":    rows.append(Xi.max(axis=0))\n",
    "        else: raise ValueError(f\"agg={agg} not supported\")\n",
    "        keep_idx.append(i)\n",
    "\n",
    "    X_parent = np.vstack(rows).astype(np.float32)\n",
    "    keep_idx = np.asarray(keep_idx, dtype=int)\n",
    "    return X_parent, keep_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e663914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Optional, Tuple, List\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import rdMolDescriptors as rdmd, DataStructs\n",
    "\n",
    "# def smiles_to_morgan_fp(smi: str, n_bits=1024, radius=3) -> Optional[np.ndarray]:\n",
    "#     m = Chem.MolFromSmiles(smi)\n",
    "#     if m is None: return None\n",
    "#     bv = rdmd.GetMorganFingerprintAsBitVect(m, radius, nBits=n_bits)\n",
    "#     arr = np.zeros((n_bits,), dtype=np.int8)\n",
    "#     DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "#     return arr.astype(np.float32)\n",
    "\n",
    "# def build_features_for_rows(\n",
    "#     ids: np.ndarray,\n",
    "#     smiles: List[str],\n",
    "#     *,\n",
    "#     feature_backend: str,           # \"fp\" or \"fp3d\"\n",
    "#     lmdb_path: Optional[str] = None,\n",
    "#     rbf_K: int = 32,\n",
    "#     cache_npz: Optional[str] = None\n",
    "# ) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Return X for rows in the given order.\n",
    "#     If feature_backend==\"fp3d\", requires lmdb_path and uses LMDBDataset.\n",
    "#     Optionally caches to an .npz file keyed by a hash of ids+backend.\n",
    "#     \"\"\"\n",
    "#     assert feature_backend in {\"fp\", \"fp3d\"}\n",
    "#     N = len(smiles)\n",
    "\n",
    "#     # Optional cache\n",
    "#     if cache_npz and os.path.exists(cache_npz):\n",
    "#         try:\n",
    "#             z = np.load(cache_npz, allow_pickle=False)\n",
    "#             return z[\"X\"]\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "#     # FP block\n",
    "#     Xfp = np.zeros((N, 1024), dtype=np.float32)\n",
    "#     keep = np.ones(N, dtype=bool)\n",
    "#     for i, s in enumerate(smiles):\n",
    "#         arr = smiles_to_morgan_fp(s)\n",
    "#         if arr is None:\n",
    "#             keep[i] = False\n",
    "#         else:\n",
    "#             Xfp[i] = arr\n",
    "\n",
    "#     if feature_backend == \"fp\":\n",
    "#         X = Xfp[keep]\n",
    "#     else:\n",
    "#         assert lmdb_path is not None, \"lmdb_path required for feature_backend='fp3d'\"\n",
    "#         from dataset_polymer_fixed import LMDBDataset\n",
    "#         ds = LMDBDataset(ids.astype(int), lmdb_path)\n",
    "#         feats3d = []\n",
    "#         for i in range(len(ds)):\n",
    "#             rec = ds[i]\n",
    "#             feats3d.append(geom_features_from_rec(rec, rbf_K=rbf_K))\n",
    "#         X3d = np.vstack(feats3d).astype(np.float32) if feats3d else np.zeros((0, 1), dtype=np.float32)\n",
    "#         X = np.hstack([Xfp, X3d])[keep]\n",
    "\n",
    "#     if cache_npz:\n",
    "#         np.savez_compressed(cache_npz, X=X)\n",
    "#     return X\n",
    "\n",
    "\n",
    "# ==== Cell 5: override prepare_features_for_target for fp3d backend ====\n",
    "def prepare_features_for_target(\n",
    "    df: pd.DataFrame, target_col: str, *,\n",
    "    lmdb_path: str, feature_backend: str, cache_dir: str = None, agg: str = \"mean\"\n",
    "):\n",
    "    # filter to labeled parents present in LMDB\n",
    "    mask = ~df[target_col].isna()\n",
    "    parent_ids = df.loc[mask, 'id'].astype(int).values\n",
    "    smiles     = df.loc[mask, 'SMILES'].astype(str).tolist()\n",
    "    y          = df.loc[mask, target_col].astype(float).values\n",
    "\n",
    "    if feature_backend == \"fp3d\":\n",
    "        # aggregate augmented features -> one row per parent\n",
    "        X, keep_idx = build_fp3d_features_from_lmdb_parents(parent_ids, lmdb_path, smiles, agg=agg)\n",
    "        y = y[keep_idx]\n",
    "        df_clean = df.loc[mask].iloc[keep_idx].reset_index(drop=True)\n",
    "        return df_clean, y, X\n",
    "\n",
    "    # else: add your other backends here as you had before\n",
    "    raise ValueError(f\"Unknown feature_backend={feature_backend}\")\n",
    "\n",
    "    return work[[\"SMILES\", target_col, \"id\"]], y, X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe69f3",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff620911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "@dataclass\n",
    "class TabularSplits:\n",
    "    # unscaled (for RF)\n",
    "    X_train: np.ndarray\n",
    "    X_test:  np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    y_test:  np.ndarray\n",
    "    # scaled (for KRR/MLP)\n",
    "    X_train_scaled: Optional[np.ndarray] = None\n",
    "    X_test_scaled:  Optional[np.ndarray] = None\n",
    "    y_train_scaled: Optional[np.ndarray] = None  # shape (N,1)\n",
    "    y_test_scaled:  Optional[np.ndarray] = None\n",
    "    x_scaler: Optional[StandardScaler] = None\n",
    "    y_scaler: Optional[StandardScaler] = None\n",
    "\n",
    "def _make_regression_stratify_bins(y: np.ndarray, n_bins: int = 10) -> np.ndarray:\n",
    "    \"\"\"Return integer bins for approximate stratification in regression.\"\"\"\n",
    "    y = y.ravel()\n",
    "    # handle degenerate case\n",
    "    if np.unique(y).size < n_bins:\n",
    "        n_bins = max(2, np.unique(y).size)\n",
    "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "    bins = np.unique(np.quantile(y, quantiles))\n",
    "    # ensure strictly increasing\n",
    "    bins = np.unique(bins)\n",
    "    # np.digitize expects right-open intervals by default\n",
    "    strat = np.digitize(y, bins[1:-1], right=False)\n",
    "    return strat\n",
    "\n",
    "def make_tabular_splits(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    scale_X: bool = True,\n",
    "    scale_y: bool = True,\n",
    "    stratify_regression: bool = False,\n",
    "    n_strat_bins: int = 10,\n",
    "    # if you already decided splits (e.g., scaffold split), pass indices:\n",
    "    train_idx: Optional[np.ndarray] = None,\n",
    "    test_idx: Optional[np.ndarray] = None,\n",
    ") -> TabularSplits:\n",
    "    \"\"\"\n",
    "    Split and (optionally) scale tabular features/targets for a single target.\n",
    "    Returns both scaled and unscaled arrays, plus fitted scalers.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    if train_idx is not None and test_idx is not None:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "    else:\n",
    "        strat = None\n",
    "        if stratify_regression:\n",
    "            strat = _make_regression_stratify_bins(y, n_bins=n_strat_bins)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "        )\n",
    "\n",
    "    # Unscaled outputs (for RF, tree models)\n",
    "    splits = TabularSplits(\n",
    "        X_train=X_train, X_test=X_test,\n",
    "        y_train=y_train, y_test=y_test\n",
    "    )\n",
    "\n",
    "    # Scaled versions (for KRR/MLP)\n",
    "    if scale_X:\n",
    "        xscaler = StandardScaler()\n",
    "        splits.X_train_scaled = xscaler.fit_transform(X_train)\n",
    "        splits.X_test_scaled  = xscaler.transform(X_test)\n",
    "        splits.x_scaler = xscaler\n",
    "    if scale_y:\n",
    "        yscaler = StandardScaler()\n",
    "        splits.y_train_scaled = yscaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        splits.y_test_scaled  = yscaler.transform(y_test.reshape(-1, 1))\n",
    "        splits.y_scaler = yscaler\n",
    "\n",
    "    # Shapes summary\n",
    "    print(\"Splits:\")\n",
    "    print(\"X_train:\", splits.X_train.shape, \"| X_test:\", splits.X_test.shape)\n",
    "    if splits.X_train_scaled is not None:\n",
    "        print(\"X_train_scaled:\", splits.X_train_scaled.shape, \"| X_test_scaled:\", splits.X_test_scaled.shape)\n",
    "    print(\"y_train:\", splits.y_train.shape, \"| y_test:\", splits.y_test.shape)\n",
    "    if splits.y_train_scaled is not None:\n",
    "        print(\"y_train_scaled:\", splits.y_train_scaled.shape, \"| y_test_scaled:\", splits.y_test_scaled.shape)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c284cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Dict, Any, Tuple\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from sklearn.ensemble import ExtraTreesRegressor as ETR\n",
    "# def train_eval_et(\n",
    "#     X: np.ndarray,\n",
    "#     y: np.ndarray,\n",
    "#     *,\n",
    "#     et_params: Dict[str, Any],\n",
    "#     test_size: float = 0.2,\n",
    "#     random_state: int = 42,\n",
    "#     stratify_regression: bool = True,\n",
    "#     n_strat_bins: int = 10,\n",
    "#     save_dir: str = \"saved_models/et\",\n",
    "#     tag: str = \"model\",\n",
    "# ) -> Tuple[ExtraTreesRegressor, Dict[str, float], TabularSplits, str]:\n",
    "#     \"\"\"\n",
    "#     Trains a RandomForest on unscaled features; returns (model, metrics, splits, path).\n",
    "#     \"\"\"\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     # Pick a safe number of bins based on dataset size\n",
    "#     if stratify_regression:\n",
    "#         adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "#     else:\n",
    "#         adaptive_bins = n_strat_bins\n",
    "#     splits = make_tabular_splits(\n",
    "#         X, y,\n",
    "#         test_size=test_size,\n",
    "#         random_state=random_state,\n",
    "#         scale_X=False, scale_y=False,                 # RF doesn't need scaling\n",
    "#         stratify_regression=stratify_regression,\n",
    "#         n_strat_bins=adaptive_bins\n",
    "#     )\n",
    "\n",
    "#     et = ETR(random_state=random_state, n_jobs=-1, **et_params)\n",
    "#     et.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "#     pred_tr = et.predict(splits.X_train)\n",
    "#     pred_te = et.predict(splits.X_test)\n",
    "\n",
    "#     metrics = {\n",
    "#         \"train_MAE\": mean_absolute_error(splits.y_train, pred_tr),\n",
    "#         \"train_RMSE\": mean_squared_error(splits.y_train, pred_tr),\n",
    "#         \"train_R2\": r2_score(splits.y_train, pred_tr),\n",
    "#         \"val_MAE\": mean_absolute_error(splits.y_test, pred_te),\n",
    "#         \"val_RMSE\": mean_squared_error(splits.y_test, pred_te),\n",
    "#         \"val_R2\": r2_score(splits.y_test, pred_te),\n",
    "#     }\n",
    "#     print(f\"[ET/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "#     path = os.path.join(save_dir, f\"et_{tag}.joblib\")\n",
    "#     joblib.dump({\"model\": et, \"metrics\": metrics, \"et_params\": et_params}, path)\n",
    "#     return et, metrics, splits, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0438e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Dict, Any, Tuple\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# def train_eval_rf(\n",
    "#     X: np.ndarray,\n",
    "#     y: np.ndarray,\n",
    "#     *,\n",
    "#     rf_params: Dict[str, Any],\n",
    "#     test_size: float = 0.2,\n",
    "#     random_state: int = 42,\n",
    "#     stratify_regression: bool = True,\n",
    "#     n_strat_bins: int = 10,\n",
    "#     save_dir: str = \"saved_models/rf\",\n",
    "#     tag: str = \"model\",\n",
    "# ) -> Tuple[RandomForestRegressor, Dict[str, float], TabularSplits, str]:\n",
    "#     \"\"\"\n",
    "#     Trains a RandomForest on unscaled features; returns (model, metrics, splits, path).\n",
    "#     \"\"\"\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     # Pick a safe number of bins based on dataset size\n",
    "#     if stratify_regression:\n",
    "#         adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "#     else:\n",
    "#         adaptive_bins = n_strat_bins\n",
    "#     splits = make_tabular_splits(\n",
    "#         X, y,\n",
    "#         test_size=test_size,\n",
    "#         random_state=random_state,\n",
    "#         scale_X=False, scale_y=False,                 # RF doesn't need scaling\n",
    "#         stratify_regression=stratify_regression,\n",
    "#         n_strat_bins=adaptive_bins\n",
    "#     )\n",
    "\n",
    "#     rf = RandomForestRegressor(random_state=random_state, n_jobs=-1, **rf_params)\n",
    "#     rf.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "#     pred_tr = rf.predict(splits.X_train)\n",
    "#     pred_te = rf.predict(splits.X_test)\n",
    "\n",
    "#     metrics = {\n",
    "#         \"train_MAE\": mean_absolute_error(splits.y_train, pred_tr),\n",
    "#         \"train_RMSE\": mean_squared_error(splits.y_train, pred_tr, squared=False),\n",
    "#         \"train_R2\": r2_score(splits.y_train, pred_tr),\n",
    "#         \"val_MAE\": mean_absolute_error(splits.y_test, pred_te),\n",
    "#         \"val_RMSE\": mean_squared_error(splits.y_test, pred_te, squared=False),\n",
    "#         \"val_R2\": r2_score(splits.y_test, pred_te),\n",
    "#     }\n",
    "#     print(f\"[RF/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "#     path = os.path.join(save_dir, f\"rf_{tag}.joblib\")\n",
    "#     joblib.dump({\"model\": rf, \"metrics\": metrics, \"rf_params\": rf_params}, path)\n",
    "#     return rf, metrics, splits, path\n",
    "\n",
    "\n",
    "# rf_cfg = {\n",
    "#     \"FFV\": {\"n_estimators\": 100, \"max_depth\": 60},\n",
    "#     \"Tc\":  {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "#     \"Rg\":  {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "# }\n",
    "\n",
    "# rf_ffv, m_ffv, splits_ffv, p_ffv = train_eval_rf(X_ffv, y_ffv, rf_params=rf_cfg[\"FFV\"], tag=\"FFV\")\n",
    "# rf_tc,  m_tc,  splits_tc,  p_tc  = train_eval_rf(X_tc,  y_tc,  rf_params=rf_cfg[\"Tc\"],  tag=\"Tc\")\n",
    "# rf_rg,  m_rg,  splits_rg,  p_rg  = train_eval_rf(X_rg,  y_rg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Rg\")\n",
    "# rf_tg,  m_tg,  splits_tg,  p_tg  = train_eval_rf(X_tg,  y_tg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Tg\")\n",
    "# rf_density,  m_density,  splits_density,  p_density  = train_eval_rf(X_density,  y_density,  rf_params=rf_cfg[\"Rg\"],  tag=\"Density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ac98b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_et_for_target(\n",
    "#     df: pd.DataFrame,\n",
    "#     target_col: str,\n",
    "#     et_params: dict,\n",
    "#     *,\n",
    "#     lmdb_path: Optional[str],\n",
    "#     feature_backend: str = \"fp3d\",   # default to augmented\n",
    "#     save_dir: str = \"saved_models/et\",\n",
    "#     tag_prefix: str = \"et\",\n",
    "#     **split_kwargs\n",
    "# ):\n",
    "#     df_clean, y, X = prepare_features_for_target(\n",
    "#         df, target_col,\n",
    "#         lmdb_path=lmdb_path,\n",
    "#         feature_backend=feature_backend,\n",
    "#         cache_dir=os.path.join(save_dir, \"cache\")\n",
    "#     )\n",
    "#     model, metrics, splits, path = train_eval_et(\n",
    "#         X, y,\n",
    "#         et_params=et_params,\n",
    "#         save_dir=save_dir,\n",
    "#         tag=f\"{tag_prefix}_{feature_backend}_{target_col}\",\n",
    "#         **split_kwargs\n",
    "#     )\n",
    "#     return model, metrics, splits, path\n",
    "\n",
    "# # rf_cfg_aug = {\n",
    "# #     \"FFV\":     {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.2, \"bootstrap\": True},\n",
    "# #     \"Tc\":      {\"n_estimators\": 800, \"max_depth\": 20, \"min_samples_split\": 6, \"min_samples_leaf\": 2, \"max_features\": \"sqrt\", \"bootstrap\": False},\n",
    "# #     \"Rg\":      {\"n_estimators\": 400, \"max_depth\": 260, \"min_samples_split\": 6, \"min_samples_leaf\": 4, \"max_features\": 1.0, \"bootstrap\": True},\n",
    "# #     \"Tg\":      {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.2, \"bootstrap\": True},\n",
    "# #     \"Density\": {\"n_estimators\": 600, \"max_depth\": 40, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "# # }\n",
    "\n",
    "# etr_cfg_full = {\n",
    "#   \"FFV\":     {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.2, \"bootstrap\": False},\n",
    "#   \"Tc\":      {\"n_estimators\": 1500, \"max_depth\": None, \"min_samples_leaf\": 3, \"max_features\": 0.15, \"bootstrap\": False},\n",
    "#   \"Rg\":      {\"n_estimators\": 400, \"max_depth\": 260, \"min_samples_split\": 6, \"min_samples_leaf\": 4, \"max_features\": 1.0, \"bootstrap\": True},\n",
    "#   \"Tg\":      {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.2, \"bootstrap\": False},\n",
    "#   \"Density\": {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.25, \"bootstrap\": False},\n",
    "# }\n",
    "\n",
    "\n",
    "# TRAIN_CSV = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "# df_all = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# et_models, et_metrics = {}, {}\n",
    "# for t in [\"FFV\", \"Tg\", \"Tc\", \"Rg\", \"Density\"]:\n",
    "#     print(f\"\\n>>> ET ({t}) with backend=fp3d\")\n",
    "#     m, met, sp, p = train_et_for_target(\n",
    "#         df_all, t, etr_cfg_full[t],\n",
    "#         lmdb_path=TRAIN_LMDB,\n",
    "#         feature_backend=\"fp3d\",\n",
    "#         save_dir=\"saved_models/et_aug3d\",\n",
    "#         tag_prefix=\"aug3D\",\n",
    "#         test_size=0.2, random_state=42, stratify_regression=True, n_strat_bins=10,\n",
    "#     )\n",
    "#     et_models[t], et_metrics[t] = m, met\n",
    "#     print(f\"[ET+3D/{t}] val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc3120",
   "metadata": {},
   "source": [
    ">>> ET (FFV) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (5624, 1144) | X_test: (1406, 1144)\n",
    "y_train: (5624,) | y_test: (1406,)\n",
    "[ET/aug3D_fp3d_FFV] val_MAE=0.006635  val_RMSE=0.016826  val_R2=0.6880\n",
    "[ET+3D/FFV] val_MAE=0.006635  val_RMSE=0.016826  val_R2=0.6880\n",
    "\n",
    ">>> ET (Tg) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (408, 1144) | X_test: (103, 1144)\n",
    "y_train: (408,) | y_test: (103,)\n",
    "[ET/aug3D_fp3d_Tg] val_MAE=58.521052  val_RMSE=74.475532  val_R2=0.5826\n",
    "[ET+3D/Tg] val_MAE=58.521052  val_RMSE=74.475532  val_R2=0.5826\n",
    "\n",
    ">>> ET (Tc) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (589, 1144) | X_test: (148, 1144)\n",
    "y_train: (589,) | y_test: (148,)\n",
    "[ET/aug3D_fp3d_Tc] val_MAE=0.027990  val_RMSE=0.042644  val_R2=0.7591\n",
    "[ET+3D/Tc] val_MAE=0.027990  val_RMSE=0.042644  val_R2=0.7591\n",
    "\n",
    ">>> ET (Rg) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (491, 1144) | X_test: (123, 1144)\n",
    "y_train: (491,) | y_test: (123,)\n",
    "[ET/aug3D_fp3d_Rg] val_MAE=1.609396  val_RMSE=2.526705  val_R2=0.7227\n",
    "[ET+3D/Rg] val_MAE=1.609396  val_RMSE=2.526705  val_R2=0.7227\n",
    "\n",
    ">>> ET (Density) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (490, 1144) | X_test: (123, 1144)\n",
    "y_train: (490,) | y_test: (123,)\n",
    "[ET/aug3D_fp3d_Density] val_MAE=0.028135  val_RMSE=0.051842  val_R2=0.8850\n",
    "[ET+3D/Density] val_MAE=0.028135  val_RMSE=0.051842  val_R2=0.8850"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ac398",
   "metadata": {},
   "source": [
    "\n",
    "[ET/aug3D_fp3d_FFV] val_MAE=0.006635  val_RMSE=0.016826  val_R2=0.6880\n",
    "\n",
    "[ET/aug3D_fp3d_Tg] val_MAE=58.521052  val_RMSE=74.475532  val_R2=0.5826\n",
    "\n",
    "[ET/aug3D_fp3d_Tc] val_MAE=0.027990  val_RMSE=0.042644  val_R2=0.7591\n",
    "\n",
    "[ET/aug3D_fp3d_Rg] val_MAE=1.609396  val_RMSE=2.526705  val_R2=0.7227\n",
    "\n",
    "[ET/aug3D_fp3d_Density] val_MAE=0.028135  val_RMSE=0.051842  val_R2=0.8850\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda5af1",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9beafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add these imports once ---\n",
    "import os, joblib, numpy as np, pandas as pd\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# LightGBM / XGBoost\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# ========= Common metric helper =========\n",
    "def _reg_metrics(y_tr, p_tr, y_va, p_va):\n",
    "    return {\n",
    "        \"train_MAE\": mean_absolute_error(y_tr, p_tr),\n",
    "        \"train_RMSE\": mean_squared_error(y_tr, p_tr),\n",
    "        \"train_R2\": r2_score(y_tr, p_tr),\n",
    "        \"val_MAE\": mean_absolute_error(y_va, p_va),\n",
    "        \"val_RMSE\": mean_squared_error(y_va, p_va),\n",
    "        \"val_R2\": r2_score(y_va, p_va),\n",
    "    }\n",
    "\n",
    "# ========= LightGBM =========\n",
    "import lightgbm as lgb\n",
    "\n",
    "def train_eval_lgbm(\n",
    "    X, y, *,\n",
    "    lgbm_params,\n",
    "    test_size=0.2, random_state=42,\n",
    "    stratify_regression=True, n_strat_bins=10,\n",
    "    save_dir=\"saved_models/lgbm\", tag=\"model\",\n",
    "    early_stopping_rounds=400,\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y))))) if stratify_regression else n_strat_bins\n",
    "    splits = make_tabular_splits(\n",
    "        X, y, test_size=test_size, random_state=random_state,\n",
    "        scale_X=False, scale_y=False,\n",
    "        stratify_regression=stratify_regression, n_strat_bins=adaptive_bins\n",
    "    )\n",
    "\n",
    "    Xtr = np.asarray(splits.X_train, dtype=np.float32)\n",
    "    Ytr = np.asarray(splits.y_train, dtype=np.float32)\n",
    "    Xva = np.asarray(splits.X_test,  dtype=np.float32)\n",
    "    Yva = np.asarray(splits.y_test,  dtype=np.float32)\n",
    "\n",
    "    base = dict(\n",
    "        n_estimators=4000,\n",
    "        learning_rate=0.03,\n",
    "        objective=\"l1\",            # optimize MAE\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbosity=-1,              # quiet model logs\n",
    "    )\n",
    "    # scrub xgb-style aliases if they sneak in\n",
    "    lgb_params = {k: v for k, v in lgbm_params.items() if k not in (\"colsample_bytree\", \"subsample\", \"subsample_freq\")}\n",
    "    # if no bagging, drop bagging_freq to avoid warning\n",
    "    if lgb_params.get(\"bagging_fraction\", 1.0) >= 1.0:\n",
    "        lgb_params.pop(\"bagging_freq\", None)\n",
    "    base.update(lgb_params)\n",
    "\n",
    "    # optional: fully silence LightGBM's logger (including alias warnings)\n",
    "    try:\n",
    "        lgb.register_logger(lambda msg: None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    model = lgb.LGBMRegressor(**base)\n",
    "    model.fit(\n",
    "        Xtr, Ytr,\n",
    "        eval_set=[(Xva, Yva)],\n",
    "        eval_metric=\"l1\",\n",
    "        callbacks=[lgb.early_stopping(early_stopping_rounds, verbose=False),\n",
    "                   lgb.log_evaluation(period=0)]\n",
    "    )\n",
    "\n",
    "    p_tr = model.predict(Xtr, num_iteration=model.best_iteration_)\n",
    "    p_va = model.predict(Xva, num_iteration=model.best_iteration_)\n",
    "    metrics = _reg_metrics(Ytr, p_tr, Yva, p_va)\n",
    "    print(f\"[LGBM/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"lgbm_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": model, \"metrics\": metrics, \"lgbm_params\": base}, path)\n",
    "    return model, metrics, splits, path\n",
    "\n",
    "\n",
    "# ========= XGBoost =========\n",
    "def _xgb_tree_method():\n",
    "    # Use GPU if available (optional)\n",
    "    try:\n",
    "        import torch\n",
    "        return \"gpu_hist\" if torch.cuda.is_available() else \"hist\"\n",
    "    except Exception:\n",
    "        return \"hist\"\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import os, joblib, numpy as np, inspect\n",
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_eval_xgb(\n",
    "    X, y,\n",
    "    *,\n",
    "    xgb_params: Dict[str, Any],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    save_dir: str = \"saved_models/xgb\",\n",
    "    tag: str = \"model\",\n",
    "    early_stopping_rounds: int = 100,\n",
    ") -> Tuple[xgb.XGBRegressor, Dict[str, float], \"TabularSplits\", str]:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # ---- split (your helper)\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False,\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "    )\n",
    "    Xtr, Ytr, Xva, Yva = splits.X_train, splits.y_train, splits.X_test, splits.y_test\n",
    "    \n",
    "    base = dict(\n",
    "        device=\"cuda\",\n",
    "        n_estimators=6000,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        colsample_bylevel=0.8,\n",
    "        colsample_bynode=0.8,\n",
    "        reg_lambda=2.0,          # L2\n",
    "        reg_alpha=0.0,           # try 0.1–0.5 if overfitting\n",
    "        min_child_weight=2.0,    # ↑ to regularize more (3–6)\n",
    "        gamma=0.0,               # try 0.05–0.3 if splits look too eager\n",
    "        tree_method=\"hist\",      # use \"gpu_hist\" if you have a GPU\n",
    "        max_bin=512,             # denser histograms may help\n",
    "        objective=\"reg:squarederror\",  # fallback objective\n",
    "        eval_metric=\"mae\",\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    base.update(xgb_params)\n",
    "    model = xgb.XGBRegressor(**base)\n",
    "\n",
    "    # ---- Robust fit across versions\n",
    "    fit_sig = inspect.signature(xgb.XGBRegressor.fit)\n",
    "    supports_callbacks = \"callbacks\" in fit_sig.parameters\n",
    "    supports_esr = \"early_stopping_rounds\" in fit_sig.parameters\n",
    "\n",
    "    used_es = False\n",
    "    if supports_callbacks:\n",
    "        try:\n",
    "            from xgboost.callback import EarlyStopping\n",
    "            es_cb = EarlyStopping(rounds=early_stopping_rounds, save_best=True, maximize=False)\n",
    "            model.fit(Xtr, Ytr, eval_set=[(Xva, Yva)], verbose=False, callbacks=[es_cb])\n",
    "            used_es = True\n",
    "        except Exception:\n",
    "            pass\n",
    "    if (not used_es) and supports_esr:\n",
    "        try:\n",
    "            model.fit(Xtr, Ytr, eval_set=[(Xva, Yva)], verbose=False,\n",
    "                      early_stopping_rounds=early_stopping_rounds)\n",
    "            used_es = True\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not used_es:\n",
    "        # Fallback: train w/o early stopping\n",
    "        # Tip: keep n_estimators reasonable and rely on reg_*\n",
    "        print(\"[XGB] Early stopping not supported by this xgboost build — training without it.\")\n",
    "        model.fit(Xtr, Ytr, eval_set=[(Xva, Yva)], verbose=False)\n",
    "\n",
    "    # ---- Predict with best-iteration awareness where available\n",
    "    def _predict_best(mdl, Xdata):\n",
    "        # XGB >= 1.6 often exposes iteration_range; older exposes ntree_limit; older still – neither.\n",
    "        try:\n",
    "            booster = mdl.get_booster()\n",
    "        except Exception:\n",
    "            booster = None\n",
    "\n",
    "        # best_iteration on wrapper:\n",
    "        best_iter = getattr(mdl, \"best_iteration\", None)\n",
    "        if best_iter is not None:\n",
    "            try:\n",
    "                return mdl.predict(Xdata, iteration_range=(0, best_iter + 1))\n",
    "            except TypeError:\n",
    "                pass\n",
    "\n",
    "        # ntree_limit on booster:\n",
    "        if booster is not None and hasattr(booster, \"best_ntree_limit\"):\n",
    "            ntl = getattr(booster, \"best_ntree_limit\", None)\n",
    "            if ntl is not None and ntl > 0:\n",
    "                try:\n",
    "                    return mdl.predict(Xdata, ntree_limit=ntl)\n",
    "                except TypeError:\n",
    "                    pass\n",
    "\n",
    "        # Fallback:\n",
    "        return mdl.predict(Xdata)\n",
    "    \n",
    "    def _predict_best(mdl, Xdata):\n",
    "        # *** THE FIX: Explicitly move data to the GPU before prediction ***\n",
    "        # This prevents the warning and can improve performance.\n",
    "        Xdata_gpu = torch.from_numpy(Xdata).to(mdl.device)\n",
    "\n",
    "        try:\n",
    "            booster = mdl.get_booster()\n",
    "        except Exception:\n",
    "            booster = None\n",
    "\n",
    "        best_iter = getattr(mdl, \"best_iteration\", None)\n",
    "        if best_iter is not None:\n",
    "            try:\n",
    "                # Use the GPU tensor for prediction\n",
    "                return mdl.predict(Xdata_gpu, iteration_range=(0, best_iter + 1))\n",
    "            except TypeError:\n",
    "                pass\n",
    "\n",
    "        if booster is not None and hasattr(booster, \"best_ntree_limit\"):\n",
    "            ntl = getattr(booster, \"best_ntree_limit\", None)\n",
    "            if ntl is not None and ntl > 0:\n",
    "                try:\n",
    "                    # Use the GPU tensor for prediction\n",
    "                    return mdl.predict(Xdata_gpu, ntree_limit=ntl)\n",
    "                except TypeError:\n",
    "                    pass\n",
    "\n",
    "        # Fallback to CPU data if GPU prediction fails for some reason\n",
    "        return mdl.predict(Xdata)\n",
    "\n",
    "    pred_tr = _predict_best(model, Xtr)\n",
    "    pred_te = _predict_best(model, Xva)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_MAE\": mean_absolute_error(Ytr, pred_tr),\n",
    "        \"train_RMSE\": mean_squared_error(Ytr, pred_tr),\n",
    "        \"train_R2\": r2_score(Ytr, pred_tr),\n",
    "        \"val_MAE\": mean_absolute_error(Yva, pred_te),\n",
    "        \"val_RMSE\": mean_squared_error(Yva, pred_te),\n",
    "        \"val_R2\": r2_score(Yva, pred_te),\n",
    "    }\n",
    "    print(f\"[XGB/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"xgb_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": model, \"metrics\": metrics, \"xgb_params\": base, \"used_es\": used_es}, path)\n",
    "    return model, metrics, splits, path\n",
    "\n",
    "# ========= Dispatcher so your calling code stays tidy =========\n",
    "def train_tabular_for_target(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    model_name: str,                # 'etr' | 'lgbm' | 'xgb'\n",
    "    model_params: Dict[str, Any],\n",
    "    *,\n",
    "    lmdb_path: Optional[str],\n",
    "    feature_backend: str = \"fp3d\",\n",
    "    save_dir: str = \"saved_models/tabular\",\n",
    "    tag_prefix: str = \"tab\",\n",
    "    **split_kwargs\n",
    "):\n",
    "    df_clean, y, X = prepare_features_for_target(\n",
    "        df, target_col,\n",
    "        lmdb_path=lmdb_path,\n",
    "        feature_backend=feature_backend,\n",
    "        cache_dir=os.path.join(save_dir, \"cache\")\n",
    "    )\n",
    "    tag = f\"{tag_prefix}_{feature_backend}_{target_col}\"\n",
    "\n",
    "    if model_name.lower() == \"etr\":\n",
    "        from sklearn.ensemble import ExtraTreesRegressor as ETR\n",
    "        model, metrics, splits, path = train_eval_et(\n",
    "            X, y, et_params=model_params, save_dir=save_dir, tag=tag, **split_kwargs\n",
    "        )\n",
    "    elif model_name.lower() == \"lgbm\":\n",
    "        model, metrics, splits, path = train_eval_lgbm(\n",
    "            X, y, lgbm_params=model_params, save_dir=save_dir, tag=tag, **split_kwargs\n",
    "        )\n",
    "    elif model_name.lower() == \"xgb\":\n",
    "        model, metrics, splits, path = train_eval_xgb(\n",
    "            X, y, xgb_params=model_params, save_dir=save_dir, tag=tag, **split_kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"model_name must be one of: 'etr', 'lgbm', 'xgb'\")\n",
    "\n",
    "    return model, metrics, splits, path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4334e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_cfg = {\n",
    "  \"FFV\":     {\"num_leaves\": 127, \"min_child_samples\": 20, \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 1},\n",
    "  \"Tc\":      {'objective': 'regression_l1', 'learning_rate': 0.11826496463933994, 'num_leaves': 452, 'max_depth': -1, 'min_data_in_leaf': 13, 'min_split_gain': 0.07077032474764056, 'feature_fraction': 0.9220353641373867, 'bagging_fraction': 0.7178475806562494, 'lambda_l1': 5.870126202873261e-07, 'lambda_l2': 5.218320773596195e-05, 'bagging_freq': 3},\n",
    "  \"Rg\":      {'objective': 'regression_l1', 'learning_rate': 0.012498104173072, 'num_leaves': 77, 'max_depth': 6, 'min_data_in_leaf': 5, 'min_split_gain': 0.10421642537134, 'feature_fraction': 0.7064591956409744, 'bagging_fraction': 0.8068199036103922, 'lambda_l1': 1.6040584907223563e-08, 'lambda_l2': 4.615422442889681e-07, 'bagging_freq': 4},\n",
    "  \"Tg\":      {'objective': 'regression', 'learning_rate': 0.03623100041838883, 'num_leaves': 41, 'max_depth': -1, 'min_data_in_leaf': 60, 'min_split_gain': 0.19800773424146345, 'feature_fraction': 0.9585660159911279, 'bagging_fraction': 0.6080651761351819, 'lambda_l1': 0.00015459491585016372, 'lambda_l2': 6.600923276281373e-07, 'bagging_freq': 6},\n",
    "  \"Density\": {'objective': 'regression_l1', 'learning_rate': 0.014386060636303035, 'num_leaves': 102, 'max_depth': 4, 'min_data_in_leaf': 5, 'min_split_gain': 0.16942680482974726, 'feature_fraction': 0.5924797518298991, 'bagging_fraction': 0.9346086621083698, 'lambda_l1': 6.564856472007785e-08, 'lambda_l2': 0.009468122760559656, 'bagging_freq': 5},\n",
    "}\n",
    "\n",
    "# lgbm_cfg = {\n",
    "#   # smooth + strong signal\n",
    "#   \"FFV\":     {\"num_leaves\": 127, \"min_child_samples\": 20, \"feature_fraction\": 0.85, \"bagging_fraction\": 0.8, \"bagging_freq\": 1, \"lambda_l2\": 2.0},\n",
    "#   # moderate\n",
    "#   \"Tc\":      {\"num_leaves\": 63,  \"min_child_samples\": 20, \"feature_fraction\": 0.75, \"bagging_fraction\": 0.8, \"bagging_freq\": 1, \"lambda_l2\": 3.0},\n",
    "#   # more complex\n",
    "#   \"Rg\":      {\"num_leaves\": 255, \"min_child_samples\": 15, \"feature_fraction\": 0.9,  \"bagging_fraction\": 0.8, \"bagging_freq\": 1, \"lambda_l2\": 2.0, \"min_split_gain\": 0.01},\n",
    "#   # more complex\n",
    "#   \"Tg\":      {\"num_leaves\": 255, \"min_child_samples\": 15, \"feature_fraction\": 0.85, \"bagging_fraction\": 0.8, \"bagging_freq\": 1, \"lambda_l2\": 3.0, \"min_split_gain\": 0.01},\n",
    "#   # simpler\n",
    "#   \"Density\": {\"num_leaves\": 63,  \"min_child_samples\": 20, \"feature_fraction\": 0.8,  \"bagging_fraction\": 0.8, \"bagging_freq\": 1, \"lambda_l2\": 2.0},\n",
    "# }\n",
    "\n",
    "# xgb_cfg = {\n",
    "#   \"FFV\":     {\"max_depth\": 7, \"subsample\": 0.9, \"colsample_bytree\": 0.8, \"reg_lambda\": 1.0},\n",
    "#   \"Tc\":      {\"max_depth\": 6, \"subsample\": 0.9, \"colsample_bytree\": 0.8, \"reg_lambda\": 1.0},\n",
    "#   \"Rg\":      {\"max_depth\": 8, \"subsample\": 0.8, \"colsample_bytree\": 0.9, \"reg_lambda\": 1.0},\n",
    "#   \"Tg\":      {\"max_depth\": 8, \"subsample\": 0.8, \"colsample_bytree\": 0.8, \"reg_lambda\": 1.0},\n",
    "#   \"Density\": {\"max_depth\": 6, \"subsample\": 0.9, \"colsample_bytree\": 0.8, \"reg_lambda\": 1.0},\n",
    "# }\n",
    "xgb_cfg = {\n",
    "  \"FFV\":     {'objective': 'reg:absoluteerror', 'eta': 0.0114287249603117, 'max_depth': 11, 'min_child_weight': 8.74657524930709, 'subsample': 0.5034760652655954, 'colsample_bytree': 0.7553736512887829, 'colsample_bylevel': 0.7087055015743895, 'colsample_bynode': 0.6110539052353652, 'lambda': 0.003974905761171867, 'alpha': 1.0927895733904103e-05, 'gamma': 0.4714548519562596, 'max_bin': 1024, 'grow_policy': 'lossguide', 'max_leaves': 449},\n",
    "  \"Tc\":      {'objective': 'reg:absoluteerror', 'eta': 0.025090663566956314, 'max_depth': 12, 'min_child_weight': 6.1968781131090696, 'subsample': 0.6165892971655643, 'colsample_bytree': 0.7319696635455195, 'colsample_bylevel': 0.6241975729552441, 'colsample_bynode': 0.9936183664523051, 'lambda': 96.20132244931914, 'alpha': 3.147759100873883e-08, 'gamma': 0.34460453202719615, 'max_bin': 512, 'grow_policy': 'depthwise'},\n",
    "  \"Rg\":      {'objective': 'reg:absoluteerror', 'eta': 0.01435111533570771, 'max_depth': 5, 'min_child_weight': 4.018997069936428, 'subsample': 0.8611079146606072, 'colsample_bytree': 0.7761740838682192, 'colsample_bylevel': 0.9479225089613308, 'colsample_bynode': 0.9656509026704986, 'lambda': 28.605920863320357, 'alpha': 6.891536837408214e-07, 'gamma': 0.21921172256812527, 'max_bin': 1024, 'grow_policy': 'depthwise'},\n",
    "  \"Tg\":      {\"max_depth\": 10, \"min_child_weight\": 4.0, \"gamma\": 0.2, \"reg_lambda\": 3.0, \"reg_alpha\": 0.1, \"colsample_bytree\": 0.85},\n",
    "  \"Density\": {'objective': 'reg:absoluteerror', 'eta': 0.0030867498488133575, 'max_depth': 9, 'min_child_weight': 2.303294371061212, 'subsample': 0.9519675087287788, 'colsample_bytree': 0.7766998909434009, 'colsample_bylevel': 0.6187311242041665, 'colsample_bynode': 0.7959321722371097, 'lambda': 0.038520030462907764, 'alpha': 0.010852150664597634, 'gamma': 0.0014564429240612486, 'max_bin': 1024, 'grow_policy': 'lossguide', 'max_leaves': 142},\n",
    "}\n",
    "\n",
    "# xgb_cfg = {\n",
    "#   \"FFV\":     {\"grow_policy\": \"lossguide\", \"max_depth\": 0, \"max_leaves\": 256},\n",
    "#   \"Tc\":      {\"grow_policy\": \"lossguide\", \"max_depth\": 0, \"max_leaves\": 256},\n",
    "#   \"Rg\":      {\"grow_policy\": \"lossguide\", \"max_depth\": 0, \"max_leaves\": 256},\n",
    "#   \"Tg\":      {\"grow_policy\": \"lossguide\", \"max_depth\": 0, \"max_leaves\": 256},\n",
    "#   \"Density\": {\"grow_policy\": \"lossguide\", \"max_depth\": 0, \"max_leaves\": 256},\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8d70f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d4323e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_CSV = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "# df_all = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# lgbm_models, lgbm_metrics = {}, {}\n",
    "# for t in [\"FFV\", \"Tg\", \"Tc\", \"Rg\", \"Density\"]:\n",
    "#     print(f\"\\n>>> LGBM ({t}) with backend=fp3d\")\n",
    "#     m, met, sp, p = train_tabular_for_target(\n",
    "#         df_all, t, \"lgbm\", lgbm_cfg[t],\n",
    "#         lmdb_path=TRAIN_LMDB,\n",
    "#         feature_backend=\"fp3d\",\n",
    "#         save_dir=\"saved_models/lgbm_aug3d\",\n",
    "#         tag_prefix=\"aug3D\",\n",
    "#         test_size=0.2, random_state=42, stratify_regression=True, n_strat_bins=10,\n",
    "#     )\n",
    "#     lgbm_models[t], lgbm_metrics[t] = m, met\n",
    "#     print(f\"[LGBM+3D/{t}] val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")\n",
    "\n",
    "# xgb_models, xgb_metrics = {}, {}\n",
    "# for t in [\"FFV\", \"Tg\", \"Tc\", \"Rg\", \"Density\"]:\n",
    "#     print(f\"\\n>>> XGB ({t}) with backend=fp3d\")\n",
    "#     m, met, sp, p = train_tabular_for_target(\n",
    "#         df_all, t, \"xgb\", xgb_cfg[t],\n",
    "#         lmdb_path=TRAIN_LMDB,\n",
    "#         feature_backend=\"fp3d\",\n",
    "#         save_dir=\"saved_models/xgb_aug3d\",\n",
    "#         tag_prefix=\"aug3D\",\n",
    "#         test_size=0.2, random_state=42, stratify_regression=True, n_strat_bins=10,\n",
    "#     )\n",
    "#     xgb_models[t], xgb_metrics[t] = m, met\n",
    "#     print(f\"[XGB+3D/{t}] val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327147f",
   "metadata": {},
   "source": [
    ">>> LGBM (FFV) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (5624, 1144) | X_test: (1406, 1144)\n",
    "y_train: (5624,) | y_test: (1406,)\n",
    "[LGBM/aug3D_fp3d_FFV] val_MAE=0.006486  val_RMSE=0.017799  val_R2=0.6509\n",
    "[LGBM+3D/FFV] val_MAE=0.006486  val_RMSE=0.017799  val_R2=0.6509\n",
    "\n",
    ">>> LGBM (Tg) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (408, 1144) | X_test: (103, 1144)\n",
    "y_train: (408,) | y_test: (103,)\n",
    "[LGBM/aug3D_fp3d_Tg] val_MAE=55.623544  val_RMSE=69.218274  val_R2=0.6395\n",
    "[LGBM+3D/Tg] val_MAE=55.623544  val_RMSE=69.218274  val_R2=0.6395\n",
    "\n",
    ">>> LGBM (Tc) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (589, 1144) | X_test: (148, 1144)\n",
    "y_train: (589,) | y_test: (148,)\n",
    "[LGBM/aug3D_fp3d_Tc] val_MAE=0.028928  val_RMSE=0.046366  val_R2=0.7152\n",
    "[LGBM+3D/Tc] val_MAE=0.028928  val_RMSE=0.046366  val_R2=0.7152\n",
    "\n",
    ">>> LGBM (Rg) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (491, 1144) | X_test: (123, 1144)\n",
    "y_train: (491,) | y_test: (123,)\n",
    "[LGBM/aug3D_fp3d_Rg] val_MAE=1.545092  val_RMSE=2.401258  val_R2=0.7496\n",
    "[LGBM+3D/Rg] val_MAE=1.545092  val_RMSE=2.401258  val_R2=0.7496\n",
    "\n",
    ">>> LGBM (Density) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (490, 1144) | X_test: (123, 1144)\n",
    "y_train: (490,) | y_test: (123,)\n",
    "[LGBM/aug3D_fp3d_Density] val_MAE=0.029514  val_RMSE=0.051530  val_R2=0.8864\n",
    "[LGBM+3D/Density] val_MAE=0.029514  val_RMSE=0.051530  val_R2=0.8864\n",
    "\n",
    ">>> XGB (FFV) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (5624, 1144) | X_test: (1406, 1144)\n",
    "y_train: (5624,) | y_test: (1406,)\n",
    "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
    "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [11:35:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
    "Potential solutions:\n",
    "- Use a data structure that matches the device ordinal in the booster.\n",
    "- Set the device for booster before call to inplace_predict.\n",
    "\n",
    "This warning will only be shown once.\n",
    "\n",
    "  warnings.warn(smsg, UserWarning)\n",
    "[XGB/aug3D_fp3d_FFV] val_MAE=0.006273  val_RMSE=0.015642  val_R2=0.7304\n",
    "[XGB+3D/FFV] val_MAE=0.006273  val_RMSE=0.015642  val_R2=0.7304\n",
    "\n",
    ">>> XGB (Tg) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (408, 1144) | X_test: (103, 1144)\n",
    "y_train: (408,) | y_test: (103,)\n",
    "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
    "[XGB/aug3D_fp3d_Tg] val_MAE=55.704242  val_RMSE=70.547759  val_R2=0.6255\n",
    "[XGB+3D/Tg] val_MAE=55.704242  val_RMSE=70.547759  val_R2=0.6255\n",
    "\n",
    ">>> XGB (Tc) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (589, 1144) | X_test: (148, 1144)\n",
    "y_train: (589,) | y_test: (148,)\n",
    "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
    "[XGB/aug3D_fp3d_Tc] val_MAE=0.028906  val_RMSE=0.045927  val_R2=0.7205\n",
    "[XGB+3D/Tc] val_MAE=0.028906  val_RMSE=0.045927  val_R2=0.7205\n",
    "\n",
    ">>> XGB (Rg) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (491, 1144) | X_test: (123, 1144)\n",
    "y_train: (491,) | y_test: (123,)\n",
    "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
    "[XGB/aug3D_fp3d_Rg] val_MAE=1.555057  val_RMSE=2.360328  val_R2=0.7580\n",
    "[XGB+3D/Rg] val_MAE=1.555057  val_RMSE=2.360328  val_R2=0.7580\n",
    "\n",
    ">>> XGB (Density) with backend=fp3d\n",
    "Splits:\n",
    "X_train: (490, 1144) | X_test: (123, 1144)\n",
    "y_train: (490,) | y_test: (123,)\n",
    "[XGB] Early stopping not supported by this xgboost build — training without it.\n",
    "[XGB/aug3D_fp3d_Density] val_MAE=0.027153  val_RMSE=0.047139  val_R2=0.9049\n",
    "[XGB+3D/Density] val_MAE=0.027153  val_RMSE=0.047139  val_R2=0.9049"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29508319",
   "metadata": {},
   "source": [
    "\n",
    "[LGBM+3D/FFV] val_MAE=0.006486  val_RMSE=0.017799  val_R2=0.6509\n",
    "[LGBM+3D/Tg] val_MAE=55.623544  val_RMSE=69.218274  val_R2=0.6395\n",
    "[LGBM+3D/Tc] val_MAE=0.028928  val_RMSE=0.046366  val_R2=0.7152\n",
    "[LGBM+3D/Rg] val_MAE=1.545092  val_RMSE=2.401258  val_R2=0.7496\n",
    "[LGBM+3D/Density] val_MAE=0.029731  val_RMSE=0.052601  val_R2=0.8816\n",
    "\n",
    "[XGB+3D/FFV] val_MAE=0.006233  val_RMSE=0.015591  val_R2=0.7322\n",
    "[XGB+3D/Tg] val_MAE=55.403413  val_RMSE=71.757702  val_R2=0.6125\n",
    "[XGB+3D/Tc] val_MAE=0.028574  val_RMSE=0.046616  val_R2=0.7121\n",
    "[XGB+3D/Rg] val_MAE=1.645115  val_RMSE=2.440275  val_R2=0.7414\n",
    "[XGB+3D/Density] val_MAE=0.025209  val_RMSE=0.044170  val_R2=0.9165\n",
    "\n",
    "\n",
    "Trial 2: new configs\n",
    "[LGBM+3D/FFV] val_MAE=0.006478  val_RMSE=0.017853  val_R2=0.6488\n",
    "[LGBM+3D/Tg] val_MAE=56.155952  val_RMSE=71.321801  val_R2=0.6172\n",
    "[LGBM+3D/Tc] val_MAE=0.029255  val_RMSE=0.047131  val_R2=0.7057\n",
    "[LGBM+3D/Rg] val_MAE=1.678481  val_RMSE=2.521272  val_R2=0.7239\n",
    "[LGBM+3D/Density] val_MAE=0.032221  val_RMSE=0.059702  val_R2=0.8475\n",
    "\n",
    "[XGB+3D/FFV] val_MAE=0.006120  val_RMSE=0.015041  val_R2=0.7507\n",
    "[XGB+3D/Tg] val_MAE=57.169018  val_RMSE=72.417251  val_R2=0.6054\n",
    "[XGB+3D/Tc] val_MAE=0.034371  val_RMSE=0.049126  val_R2=0.6803\n",
    "[XGB+3D/Rg] val_MAE=1.646285  val_RMSE=2.460963  val_R2=0.7370\n",
    "[XGB+3D/Density] val_MAE=0.026259  val_RMSE=0.044726  val_R2=0.9144\n",
    "\n",
    "Trial 3: huber loss for lgb and new dict for xgb with trial 2 configs \n",
    "[LGBM+3D/FFV] val_MAE=0.006653  val_RMSE=0.017677  val_R2=0.6557\n",
    "[LGBM+3D/Tg] val_MAE=62.675503  val_RMSE=83.564070  val_R2=0.4745\n",
    "[LGBM+3D/Tc] val_MAE=0.027693  val_RMSE=0.044296  val_R2=0.7400\n",
    "[LGBM+3D/Rg] val_MAE=1.618186  val_RMSE=2.423270  val_R2=0.7450\n",
    "[LGBM+3D/Density] val_MAE=0.032516  val_RMSE=0.054482  val_R2=0.8730\n",
    "\n",
    "[XGB+3D/FFV] val_MAE=0.005926  val_RMSE=0.014915  val_R2=0.7549\n",
    "[XGB+3D/Tg] val_MAE=55.900224  val_RMSE=71.040900  val_R2=0.6202\n",
    "[XGB+3D/Tc] val_MAE=0.034650  val_RMSE=0.049152  val_R2=0.6799\n",
    "[XGB+3D/Rg] val_MAE=1.560507  val_RMSE=2.326133  val_R2=0.7650\n",
    "[XGB+3D/Density] val_MAE=0.026017  val_RMSE=0.047280  val_R2=0.9044\n",
    "\n",
    "Trial 4: huber loss for lgb with original config, XGB: objective=\"reg:absoluteerror\" + new config and dict\n",
    "\n",
    "[LGBM+3D/FFV] val_MAE=0.006533  val_RMSE=0.017517  val_R2=0.6619\n",
    "[LGBM+3D/Tg] val_MAE=59.495712  val_RMSE=78.644181  val_R2=0.5346\n",
    "[LGBM+3D/Tc] val_MAE=0.027822  val_RMSE=0.044622  val_R2=0.7362\n",
    "[LGBM+3D/Rg] val_MAE=1.554533  val_RMSE=2.455474  val_R2=0.7382\n",
    "[LGBM+3D/Density] val_MAE=0.030911  val_RMSE=0.052446  val_R2=0.8823\n",
    "\n",
    "[XGB+3D/FFV] val_MAE=0.006236  val_RMSE=0.017128  val_R2=0.6768\n",
    "[XGB+3D/Tg] val_MAE=56.481308  val_RMSE=71.475180  val_R2=0.6156\n",
    "[XGB+3D/Tc] val_MAE=0.029000  val_RMSE=0.045747  val_R2=0.7227\n",
    "[XGB+3D/Rg] val_MAE=1.582031  val_RMSE=2.488855  val_R2=0.7310\n",
    "[XGB+3D/Density] val_MAE=0.025539  val_RMSE=0.045819  val_R2=0.9102\n",
    "\n",
    "Trial 5: back to l1 loss with original config, XGB: seems like FFV, Tg did worse while Tc improved, and Rg/Density very similar, will try reg:pseudohubererror next. Then will eval results, pick which model is best for each target and run a submission. Then, will perform tuning on each distinct target.\n",
    "\n",
    "[LGBM+3D/FFV] val_MAE=0.006486  val_RMSE=0.017799  val_R2=0.6509\n",
    "[LGBM+3D/Tg] val_MAE=55.623544  val_RMSE=69.218274  val_R2=0.6395\n",
    "[LGBM+3D/Tc] val_MAE=0.028928  val_RMSE=0.046366  val_R2=0.7152\n",
    "[LGBM+3D/Rg] val_MAE=1.545092  val_RMSE=2.401258  val_R2=0.7496\n",
    "[LGBM+3D/Density] val_MAE=0.029514  val_RMSE=0.051530  val_R2=0.8864\n",
    "\n",
    "[XGB+3D/FFV] val_MAE=0.005960  val_RMSE=0.014577  val_R2=0.7659\n",
    "[XGB+3D/Tg] val_MAE=90.490287  val_RMSE=123.772864  val_R2=-0.1528\n",
    "[XGB+3D/Tc] val_MAE=0.034317  val_RMSE=0.049155  val_R2=0.6799\n",
    "[XGB+3D/Rg] val_MAE=2968.611342  val_RMSE=2968.615220  val_R2=-382726.1776\n",
    "[XGB+3D/Density] val_MAE=0.024336  val_RMSE=0.040957  val_R2=0.9282\n",
    "\n",
    "Trial 6: same as 5 beside leaf wise growth for XGB\n",
    "\n",
    "[LGBM+3D/FFV] val_MAE=0.006486  val_RMSE=0.017799  val_R2=0.6509\n",
    "[LGBM+3D/Tg] val_MAE=55.623544  val_RMSE=69.218274  val_R2=0.6395\n",
    "[LGBM+3D/Tc] val_MAE=0.028928  val_RMSE=0.046366  val_R2=0.7152\n",
    "[LGBM+3D/Rg] val_MAE=1.545092  val_RMSE=2.401258  val_R2=0.7496\n",
    "[LGBM+3D/Density] val_MAE=0.029514  val_RMSE=0.051530  val_R2=0.8864\n",
    "\n",
    "[XGB+3D/FFV] val_MAE=0.006273  val_RMSE=0.015642  val_R2=0.7304\n",
    "[XGB+3D/Tg] val_MAE=55.704242  val_RMSE=70.547759  val_R2=0.6255\n",
    "[XGB+3D/Tc] val_MAE=0.028906  val_RMSE=0.045927  val_R2=0.7205\n",
    "[XGB+3D/Rg] val_MAE=1.555057  val_RMSE=2.360328  val_R2=0.7580\n",
    "[XGB+3D/Density] val_MAE=0.027153  val_RMSE=0.047139  val_R2=0.9049\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c964ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =========================\n",
    "# # Optuna tuning (10 studies)\n",
    "# # =========================\n",
    "# # Prereqs: prepare_features_for_target, make_tabular_splits exist in your notebook.\n",
    "\n",
    "# # 0) Imports / setup\n",
    "# import optuna \n",
    "# import os, json, joblib, numpy as np, pandas as pd, time\n",
    "# from typing import Dict, Any, Optional, Tuple\n",
    "# from functools import partial\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from optuna.samplers import TPESampler\n",
    "# from optuna.pruners import MedianPruner\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "\n",
    "# # ---- Paths / constants\n",
    "# DATA_ROOT   = DATA_ROOT            # already defined earlier in your notebook\n",
    "# TRAIN_CSV   = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "# LMDB_PATH   = TRAIN_LMDB           # <- use your augmented train LMDB\n",
    "# FEATURE_BACKEND = \"fp3d\"           # <- you’re using fp3d augmented features\n",
    "\n",
    "# SAVE_ROOT_LGB = \"saved_models/lgbm_optuna_fp3d\"\n",
    "# SAVE_ROOT_XGB = \"saved_models/xgb_optuna_fp3d\"\n",
    "# os.makedirs(SAVE_ROOT_LGB, exist_ok=True)\n",
    "# os.makedirs(SAVE_ROOT_XGB, exist_ok=True)\n",
    "\n",
    "# RANDOM_STATE = 42\n",
    "# VAL_TEST_SIZE = 0.2\n",
    "# VAL_STRATIFY = True\n",
    "# VAL_BINS = 10                      # regression strat bins (your pipeline)\n",
    "# NUM_BOOST_ROUND = 10000            # both LGBM/XGB upper bound\n",
    "# EARLY_STOP_ROUNDS = 400\n",
    "# TIMEOUT_PER_STUDY = 60 * 60        # 1 hour each\n",
    "\n",
    "# TARGETS = [\"FFV\", \"Tg\", \"Tc\", \"Rg\", \"Density\"]\n",
    "\n",
    "# # ---- Optional: order studies by weight (approx wMAE importance)\n",
    "# def get_target_weights(csv_path: str, target_names):\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     scale_norm = []\n",
    "#     count_norm = []\n",
    "#     for t in target_names:\n",
    "#         vals = df[t].values\n",
    "#         vals = vals[~np.isnan(vals)]\n",
    "#         scale_norm.append(1.0 / (np.max(vals) - np.min(vals)))\n",
    "#         count_norm.append((1.0/len(vals))**0.5)\n",
    "#     scale_norm = np.array(scale_norm)\n",
    "#     count_norm = np.array(count_norm)\n",
    "#     w = scale_norm * len(target_names) * count_norm / np.sum(count_norm)\n",
    "#     return dict(zip(target_names, w))\n",
    "\n",
    "# WEIGHTS = get_target_weights(TRAIN_CSV, TARGETS)\n",
    "# TARGETS_BY_WEIGHT = sorted(TARGETS, key=lambda t: -WEIGHTS[t])\n",
    "# print(\"Run order by weight (heaviest → lightest):\", TARGETS_BY_WEIGHT, \"\\nweights:\", WEIGHTS)\n",
    "\n",
    "# # ---- Feature builder per target (uses your helpers)\n",
    "# def build_Xy_for_target(df: pd.DataFrame, target: str):\n",
    "#     # Uses your prepare_features_for_target (fp3d features w/ LMDB)\n",
    "#     df_clean, y, X = prepare_features_for_target(\n",
    "#         df, target,\n",
    "#         lmdb_path=LMDB_PATH,\n",
    "#         feature_backend=FEATURE_BACKEND,\n",
    "#         cache_dir=os.path.join(\"saved_models\", \"cache_optuna_fp3d\"),\n",
    "#     )\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     y = np.asarray(y, dtype=np.float32)\n",
    "#     return X, y\n",
    "\n",
    "# # ---- Split helper (same behavior as your training code)\n",
    "# def split_Xy(X, y):\n",
    "#     splits = make_tabular_splits(\n",
    "#         X, y,\n",
    "#         test_size=VAL_TEST_SIZE,\n",
    "#         random_state=RANDOM_STATE,\n",
    "#         scale_X=False, scale_y=False,\n",
    "#         stratify_regression=VAL_STRATIFY,\n",
    "#         n_strat_bins=min(VAL_BINS, max(3, int(np.sqrt(len(y)))))\n",
    "#     )\n",
    "#     return splits.X_train, splits.y_train, splits.X_test, splits.y_test\n",
    "\n",
    "# def _xgb_tree_method():\n",
    "#     try:\n",
    "#         import torch\n",
    "#         return \"gpu_hist\" if torch.cuda.is_available() else \"hist\"\n",
    "#     except Exception:\n",
    "#         return \"hist\"\n",
    "\n",
    "# # ==================\n",
    "# # LGBM Objective\n",
    "# # ==================\n",
    "# def lgbm_objective(target: str, df_all: pd.DataFrame, trial: optuna.trial.Trial) -> float:\n",
    "#     X, y = build_Xy_for_target(df_all, target)\n",
    "#     Xtr, Ytr, Xva, Yva = split_Xy(X, y)\n",
    "\n",
    "#     dtrain = lgb.Dataset(Xtr, label=Ytr)\n",
    "#     dvalid = lgb.Dataset(Xva, label=Yva)\n",
    "\n",
    "#     # Search space (wide, but safe)\n",
    "#     params = {\n",
    "#         \"objective\": trial.suggest_categorical(\"objective\", [\"regression_l1\", \"regression\"]),\n",
    "#         \"metric\": \"l1\",\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.2, log=True),\n",
    "#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 512, log=True),\n",
    "#         \"max_depth\": trial.suggest_categorical(\"max_depth\", [-1, 4, 6, 8, 10, 12, 14, 16]),\n",
    "#         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 200),\n",
    "#         \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "#         \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "#         \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "#         \"bagging_freq\": 0,  # set >0 only if bagging_fraction < 1.0\n",
    "#         \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "#         \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "#         \"verbosity\": -1,\n",
    "#         \"seed\": RANDOM_STATE,\n",
    "#         \"num_threads\": os.cpu_count() or 8,\n",
    "#         # LightGBM GPU is optional; avoid to be safe across envs\n",
    "#         # \"device_type\": \"gpu\",\n",
    "#     }\n",
    "#     if params[\"bagging_fraction\"] < 0.999:\n",
    "#         params[\"bagging_freq\"] = trial.suggest_int(\"bagging_freq\", 1, 7)\n",
    "\n",
    "#     # Pruning callback\n",
    "#     try:\n",
    "#         from optuna.integration import LightGBMPruningCallback\n",
    "#         pruning_cb = LightGBMPruningCallback(trial, \"l1\")\n",
    "#         callbacks = [lgb.early_stopping(EARLY_STOP_ROUNDS, verbose=False),\n",
    "#                      lgb.log_evaluation(period=0),\n",
    "#                      pruning_cb]\n",
    "#     except Exception:\n",
    "#         callbacks = [lgb.early_stopping(EARLY_STOP_ROUNDS, verbose=False),\n",
    "#                      lgb.log_evaluation(period=0)]\n",
    "\n",
    "#     booster = lgb.train(\n",
    "#         params,\n",
    "#         dtrain,\n",
    "#         num_boost_round=NUM_BOOST_ROUND,\n",
    "#         valid_sets=[dvalid],\n",
    "#         valid_names=[\"valid\"],\n",
    "#         callbacks=callbacks\n",
    "#     )\n",
    "#     best_iter = booster.best_iteration or NUM_BOOST_ROUND\n",
    "#     pred_va = booster.predict(Xva, num_iteration=best_iter)\n",
    "#     mae = mean_absolute_error(Yva, pred_va)\n",
    "\n",
    "#     # save best_iter to trial for later refit\n",
    "#     trial.set_user_attr(\"best_iteration\", int(best_iter))\n",
    "#     return float(mae)\n",
    "\n",
    "# def run_lgbm_study_for_target(target: str, df_all: pd.DataFrame, timeout_s: int = TIMEOUT_PER_STUDY):\n",
    "#     study = optuna.create_study(\n",
    "#         direction=\"minimize\",\n",
    "#         sampler=TPESampler(seed=RANDOM_STATE),\n",
    "#         pruner=MedianPruner(n_warmup_steps=10),\n",
    "#         study_name=f\"LGBM_{FEATURE_BACKEND}_{target}\"\n",
    "#     )\n",
    "#     study.optimize(partial(lgbm_objective, target, df_all), timeout=timeout_s, gc_after_trial=True)\n",
    "#     print(f\"[LGBM/{target}] best MAE={study.best_value:.6f}\")\n",
    "#     # Refit full model on all data with best params & best_iter\n",
    "#     X, y = build_Xy_for_target(df_all, target)\n",
    "#     best_params = study.best_params.copy()\n",
    "#     best_params.update({\"metric\": \"l1\", \"verbosity\": -1, \"seed\": RANDOM_STATE, \"num_threads\": os.cpu_count() or 8})\n",
    "#     best_iter = study.best_trial.user_attrs.get(\"best_iteration\", 1000)\n",
    "\n",
    "#     dtrain_full = lgb.Dataset(X, label=y)\n",
    "#     booster_full = lgb.train(\n",
    "#         best_params, dtrain_full,\n",
    "#         num_boost_round=int(best_iter),\n",
    "#         valid_sets=[dtrain_full],\n",
    "#         valid_names=[\"train\"],\n",
    "#         callbacks=[lgb.log_evaluation(period=0)]\n",
    "#     )\n",
    "\n",
    "#     out_txt = os.path.join(SAVE_ROOT_LGB, f\"lgbm_{FEATURE_BACKEND}_{target}.txt\")\n",
    "#     booster_full.save_model(out_txt)\n",
    "#     meta = {\n",
    "#         \"best_value\": study.best_value,\n",
    "#         \"best_params\": best_params,\n",
    "#         \"best_iteration\": int(best_iter),\n",
    "#         \"feature_backend\": FEATURE_BACKEND,\n",
    "#         \"target\": target,\n",
    "#     }\n",
    "#     joblib.dump(meta, os.path.join(SAVE_ROOT_LGB, f\"lgbm_{FEATURE_BACKEND}_{target}.meta.joblib\"))\n",
    "#     print(f\"[LGBM/{target}] saved model -> {out_txt}\")\n",
    "#     return study\n",
    "\n",
    "# # ==================\n",
    "# # XGBoost Objective\n",
    "# # ==================\n",
    "# # --- Put these near your XGB objective ---\n",
    "# from typing import Optional\n",
    "\n",
    "# def _xgb_best_iteration(bst) -> Optional[int]:\n",
    "#     \"\"\"Return best iteration across xgboost versions.\"\"\"\n",
    "#     # 1) Preferred (xgb >= 1.6 / 2.x)\n",
    "#     bi = getattr(bst, \"best_iteration\", None)\n",
    "#     if isinstance(bi, (int, np.integer)):\n",
    "#         return int(bi)\n",
    "#     # 2) Older versions\n",
    "#     bi2 = getattr(bst, \"best_ntree_limit\", None)\n",
    "#     if isinstance(bi2, (int, np.integer)):\n",
    "#         return int(bi2)\n",
    "#     # 3) Last resort: number of boosted rounds (no early stopping info)\n",
    "#     try:\n",
    "#         return int(bst.num_boosted_rounds())\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "# def _xgb_predict_at_best(bst, dmat):\n",
    "#     \"\"\"Predict using best iteration when available, with version fallbacks.\"\"\"\n",
    "#     bi = _xgb_best_iteration(bst)\n",
    "#     # iteration_range is new-ish; ntree_limit is old; default otherwise\n",
    "#     if bi is not None:\n",
    "#         try:\n",
    "#             # newer API\n",
    "#             return bst.predict(dmat, iteration_range=(0, bi + 1))\n",
    "#         except TypeError:\n",
    "#             try:\n",
    "#                 # older API\n",
    "#                 return bst.predict(dmat, ntree_limit=bi)\n",
    "#             except TypeError:\n",
    "#                 pass\n",
    "#     return bst.predict(dmat)\n",
    "\n",
    "\n",
    "# def xgb_objective(target: str, df_all: pd.DataFrame, trial: optuna.trial.Trial) -> float:\n",
    "#     X, y = build_Xy_for_target(df_all, target)\n",
    "#     Xtr, Ytr, Xva, Yva = split_Xy(X, y)\n",
    "\n",
    "#     dtrain = xgb.DMatrix(Xtr, label=Ytr)\n",
    "#     dvalid = xgb.DMatrix(Xva, label=Yva)\n",
    "\n",
    "#     tm = _xgb_tree_method()\n",
    "#     params = {\n",
    "#         \"tree_method\": tm,\n",
    "#         \"eval_metric\": \"mae\",\n",
    "#         \"objective\": trial.suggest_categorical(\"objective\", [\"reg:squarederror\", \"reg:absoluteerror\"]),\n",
    "#         \"eta\": trial.suggest_float(\"eta\", 1e-3, 0.3, log=True),\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "#         \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 10.0),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "#         \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "#         \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 100.0, log=True),\n",
    "#         \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 10.0, log=True),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 0.0, 0.5),\n",
    "#         \"max_bin\": trial.suggest_categorical(\"max_bin\", [256, 512, 1024]),\n",
    "#         \"verbosity\": 0,\n",
    "#         \"seed\": RANDOM_STATE,\n",
    "#     }\n",
    "#     grow = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "#     params[\"grow_policy\"] = grow\n",
    "#     if grow == \"lossguide\":\n",
    "#         params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 16, 512, log=True)\n",
    "\n",
    "#     bst = xgb.train(\n",
    "#         params,\n",
    "#         dtrain,\n",
    "#         num_boost_round=NUM_BOOST_ROUND,\n",
    "#         evals=[(dvalid, \"valid\")],\n",
    "#         early_stopping_rounds=EARLY_STOP_ROUNDS,\n",
    "#         verbose_eval=False\n",
    "#     )\n",
    "\n",
    "#     # Safe best-iteration + prediction\n",
    "#     best_iter = _xgb_best_iteration(bst)\n",
    "#     if best_iter is not None:\n",
    "#         trial.set_user_attr(\"best_iteration\", int(best_iter))\n",
    "\n",
    "#     pred_va = _xgb_predict_at_best(bst, dvalid)\n",
    "#     mae = mean_absolute_error(Yva, pred_va)\n",
    "#     return float(mae)\n",
    "\n",
    "\n",
    "# def run_xgb_study_for_target(target: str, df_all: pd.DataFrame, timeout_s: int = TIMEOUT_PER_STUDY):\n",
    "#     study = optuna.create_study(\n",
    "#         direction=\"minimize\",\n",
    "#         sampler=TPESampler(seed=RANDOM_STATE),\n",
    "#         pruner=MedianPruner(n_warmup_steps=10),\n",
    "#         study_name=f\"XGB_{FEATURE_BACKEND}_{target}\"\n",
    "#     )\n",
    "#     study.optimize(partial(xgb_objective, target, df_all), timeout=timeout_s, gc_after_trial=True)\n",
    "#     print(f\"[XGB/{target}] best MAE={study.best_value:.6f}\")\n",
    "\n",
    "#     # Refit full model with best params & best_iter\n",
    "#     X, y = build_Xy_for_target(df_all, target)\n",
    "#     dfull = xgb.DMatrix(X, label=y)\n",
    "#     best_params = study.best_params.copy()\n",
    "#     best_params.update({\n",
    "#         \"tree_method\": _xgb_tree_method(),\n",
    "#         \"eval_metric\": \"mae\",\n",
    "#         \"verbosity\": 0,\n",
    "#         \"seed\": RANDOM_STATE,\n",
    "#     })\n",
    "#     best_iter = int(study.best_trial.user_attrs.get(\"best_iteration\", 1000))\n",
    "#     bst_full = xgb.train(\n",
    "#         best_params,\n",
    "#         dfull,\n",
    "#         num_boost_round=best_iter,\n",
    "#         evals=[(dfull, \"train\")],\n",
    "#         verbose_eval=False\n",
    "#     )\n",
    "#     out_json = os.path.join(SAVE_ROOT_XGB, f\"xgb_{FEATURE_BACKEND}_{target}.json\")\n",
    "#     bst_full.save_model(out_json)\n",
    "#     meta = {\n",
    "#         \"best_value\": study.best_value,\n",
    "#         \"best_params\": best_params,\n",
    "#         \"best_iteration\": best_iter,\n",
    "#         \"feature_backend\": FEATURE_BACKEND,\n",
    "#         \"target\": target,\n",
    "#     }\n",
    "#     joblib.dump(meta, os.path.join(SAVE_ROOT_XGB, f\"xgb_{FEATURE_BACKEND}_{target}.meta.joblib\"))\n",
    "#     print(f\"[XGB/{target}] saved model -> {out_json}\")\n",
    "#     return study\n",
    "\n",
    "# def _lgbm_artifact_path(t: str) -> str:\n",
    "#     return os.path.join(SAVE_ROOT_LGB, f\"lgbm_{FEATURE_BACKEND}_{t}.txt\")\n",
    "\n",
    "# def _xgb_artifact_path(t: str) -> str:\n",
    "#     return os.path.join(SAVE_ROOT_XGB, f\"xgb_{FEATURE_BACKEND}_{t}.json\")\n",
    "\n",
    "# def _skip_lgbm(t: str) -> bool:\n",
    "#     return os.path.exists(_lgbm_artifact_path(t))\n",
    "\n",
    "# def _skip_xgb(t: str) -> bool:\n",
    "#     return os.path.exists(_xgb_artifact_path(t))\n",
    "\n",
    "# # ==================\n",
    "# # Orchestrate all 10\n",
    "# # ==================\n",
    "# df_all = pd.read_csv(TRAIN_CSV)\n",
    "# ORDER = TARGETS_BY_WEIGHT  # or TARGETS\n",
    "\n",
    "# studies = {\"lgbm\": {}, \"xgb\": {}}\n",
    "\n",
    "# # LGBM studies (skip if model already saved)\n",
    "# for t in ORDER:\n",
    "#     if _skip_lgbm(t):\n",
    "#         print(f\"Skipping LGBM tuning for {t} (found {_lgbm_artifact_path(t)})\")\n",
    "#         continue\n",
    "#     print(f\"\\n==== LGBM tuning for {t} (<= {TIMEOUT_PER_STUDY//60} min) ====\")\n",
    "#     studies[\"lgbm\"][t] = run_lgbm_study_for_target(t, df_all, TIMEOUT_PER_STUDY)\n",
    "\n",
    "# # XGB studies (skip if model already saved)\n",
    "# for t in ORDER:\n",
    "#     if _skip_xgb(t):\n",
    "#         print(f\"Skipping XGB tuning for {t} (found {_xgb_artifact_path(t)})\")\n",
    "#         continue\n",
    "#     print(f\"\\n==== XGB tuning for {t} (<= {TIMEOUT_PER_STUDY//60} min) ====\")\n",
    "#     studies[\"xgb\"][t] = run_xgb_study_for_target(t, df_all, TIMEOUT_PER_STUDY)\n",
    "\n",
    "# # Optional: print only studies that actually ran\n",
    "# print(\"\\nFinished. Best MAE for completed studies:\")\n",
    "# for kind in [\"lgbm\", \"xgb\"]:\n",
    "#     for t, st in studies[kind].items():\n",
    "#         print(f\"{kind.upper()} {t}: {st.best_value:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a0004",
   "metadata": {},
   "source": [
    "# Graph Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d599b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Tg → parents train=  408 val=  103 | aug rows train=  4080 val=  1030\n",
      "    FFV → parents train= 5624 val= 1406 | aug rows train= 56240 val= 14060\n",
      "     Tc → parents train=  589 val=  148 | aug rows train=  5890 val=  1480\n",
      "Density → parents train=  490 val=  123 | aug rows train=  4900 val=  1230\n",
      "     Rg → parents train=  491 val=  123 | aug rows train=  4910 val=  1230\n"
     ]
    }
   ],
   "source": [
    "# ==== Parent-aware wiring (CSV parents -> augmented LMDB key_ids) ====\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "AUG_KEY_MULT = 1000  # must match your LMDB builder\n",
    "\n",
    "# CSV (parents)\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_csv[\"id\"] = train_csv[\"id\"].astype(int)\n",
    "\n",
    "# LMDB ids (augmented key_ids)\n",
    "lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "lmdb_ids = np.loadtxt(lmdb_ids_path, dtype=np.int64)\n",
    "if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "\n",
    "# Parent map (preferred) → key_id list per parent\n",
    "pmap_path = TRAIN_LMDB + \".parent_map.tsv\"\n",
    "if os.path.exists(pmap_path):\n",
    "    pmap = pd.read_csv(pmap_path, sep=\"\\t\")  # cols: key_id, parent_id, aug_idx, seed\n",
    "    pmap[\"key_id\"] = pmap[\"key_id\"].astype(np.int64)\n",
    "    pmap[\"parent_id\"] = pmap[\"parent_id\"].astype(np.int64)\n",
    "else:\n",
    "    # fallback if parent_map missing: derive parent by integer division\n",
    "    pmap = pd.DataFrame({\n",
    "        \"key_id\": lmdb_ids.astype(np.int64),\n",
    "        \"parent_id\": (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "    })\n",
    "parents_in_lmdb = np.sort(pmap[\"parent_id\"].unique().astype(np.int64))\n",
    "\n",
    "def parents_with_label(task: str) -> np.ndarray:\n",
    "    m = ~train_csv[task].isna()\n",
    "    have = train_csv.loc[m, \"id\"].astype(int).values  # parents that have this label\n",
    "    return np.intersect1d(have, parents_in_lmdb, assume_unique=False)\n",
    "\n",
    "# Split BY PARENT (no leakage), then expand to augmented key_ids\n",
    "def task_parent_split_keys(task: str, test_size=0.2, seed=42):\n",
    "    parents_labeled = parents_with_label(task)\n",
    "    if parents_labeled.size == 0:\n",
    "        raise ValueError(f\"No parents with labels for {task}\")\n",
    "    p_tr, p_va = train_test_split(parents_labeled, test_size=test_size, random_state=seed)\n",
    "    tr_keys = pmap.loc[pmap.parent_id.isin(p_tr), \"key_id\"].astype(np.int64).values\n",
    "    va_keys = pmap.loc[pmap.parent_id.isin(p_va), \"key_id\"].astype(np.int64).values\n",
    "    return np.sort(tr_keys), np.sort(va_keys), np.sort(p_tr), np.sort(p_va)\n",
    "\n",
    "# Build pools (augmented key_ids) per task\n",
    "task_pools = {}\n",
    "task_parent_splits = {}\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys, p_tr, p_va = task_parent_split_keys(t, test_size=0.2, seed=42)\n",
    "    task_pools[t] = (tr_keys, va_keys)\n",
    "    task_parent_splits[t] = (p_tr, p_va)\n",
    "\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys = task_pools[t]\n",
    "    p_tr, p_va = task_parent_splits[t]\n",
    "    print(f\"{t:>7} → parents train={len(p_tr):5d} val={len(p_va):5d} | aug rows train={len(tr_keys):6d} val={len(va_keys):6d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9647b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell A — KEY_SIZES + simple finite samplers ====\n",
    "import os, numpy as np, pandas as pd, random\n",
    "from typing import Dict, Optional\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "# --- Build KEY_SIZES: key_id -> parent n_atoms_2d ---\n",
    "parent_meta_path = TRAIN_LMDB + \".parent_meta.tsv\"\n",
    "if not os.path.exists(parent_meta_path):\n",
    "    raise FileNotFoundError(f\"Missing {parent_meta_path}. Rebuild LMDB with parent_meta.tsv enabled.\")\n",
    "\n",
    "parent_meta = pd.read_csv(parent_meta_path, sep=\"\\t\")  # cols: parent_id, n_atoms_2d, star_count, replacement_Z\n",
    "parent_meta[\"parent_id\"] = parent_meta[\"parent_id\"].astype(np.int64)\n",
    "parent_meta = parent_meta.drop_duplicates(subset=[\"parent_id\"])\n",
    "\n",
    "# pmap is already built in your Cell 1 (key_id,parent_id,aug_idx,seed)\n",
    "key_sizes_df = pmap.merge(parent_meta[[\"parent_id\",\"n_atoms_2d\"]], on=\"parent_id\", how=\"left\")\n",
    "if key_sizes_df[\"n_atoms_2d\"].isna().any():\n",
    "    med = int(key_sizes_df[\"n_atoms_2d\"].median())\n",
    "    key_sizes_df[\"n_atoms_2d\"] = key_sizes_df[\"n_atoms_2d\"].fillna(med)\n",
    "\n",
    "KEY_SIZES: Dict[int,int] = dict(\n",
    "    zip(key_sizes_df[\"key_id\"].astype(np.int64).tolist(),\n",
    "        key_sizes_df[\"n_atoms_2d\"].astype(int).tolist())\n",
    ")\n",
    "\n",
    "# --- Samplers (finite, precomputed) ---\n",
    "\n",
    "class EmptyBatchSampler(Sampler):\n",
    "    def __iter__(self):\n",
    "        return iter(())               # empty iterator\n",
    "    def __len__(self):\n",
    "        return 0\n",
    "\n",
    "class TokenBucketBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Precompute a finite list of batches from (keys, sizes) under token/quadratic/max_batch constraints.\n",
    "    __iter__ just yields those batches. __len__ returns exact count.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys, sizes_dict: Dict[int,int], *,\n",
    "                 max_tokens: int, max_quadratic: int,\n",
    "                 max_batch_size: int, shuffle: bool, seed: int, bins: int = 8):\n",
    "        self.keys = np.asarray(keys, dtype=np.int64)\n",
    "        self.sizes_dict = sizes_dict\n",
    "        self.max_tokens = int(max_tokens)\n",
    "        self.max_quadratic = int(max_quadratic)\n",
    "        self.max_batch_size = int(max_batch_size)\n",
    "        self.shuffle = bool(shuffle)\n",
    "        self.seed = int(seed)\n",
    "        self.bins = int(max(1, bins))\n",
    "        self._batches = self._pack_once()   # precompute finite list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batches)\n",
    "\n",
    "    def _pack_once(self):\n",
    "        if self.keys.size == 0:\n",
    "            return []\n",
    "\n",
    "        # materialize (key,size), guard size>=1\n",
    "        pairs = [(int(k), max(1, int(self.sizes_dict.get(int(k), 1)))) for k in self.keys]\n",
    "\n",
    "        # shuffle or deterministic order\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        if self.shuffle:\n",
    "            rng.shuffle(pairs)\n",
    "        else:\n",
    "            pairs.sort(key=lambda t: (t[1], t[0]))\n",
    "\n",
    "        sizes = np.array([s for _, s in pairs], dtype=np.int32)\n",
    "        # Bins by size to reduce padding; keep it safe for small arrays\n",
    "        B = int(min(self.bins, max(1, sizes.size)))\n",
    "        try:\n",
    "            qs = np.quantile(sizes, np.linspace(0, 1, B + 1)) if sizes.size > 1 else np.array([sizes[0], sizes[0]])\n",
    "        except Exception:\n",
    "            qs = np.array([sizes.min(), sizes.max()])\n",
    "\n",
    "        bins = [[] for _ in range(B)]\n",
    "        for i, (k, s) in enumerate(pairs):\n",
    "            b = int(np.searchsorted(qs, s, side=\"right\")) - 1\n",
    "            b = max(0, min(B - 1, b))\n",
    "            bins[b].append((k, s))\n",
    "\n",
    "        if self.shuffle:\n",
    "            for b in range(B):\n",
    "                rng.shuffle(bins[b])\n",
    "\n",
    "        batches = []\n",
    "        # Round-robin draw from bins; each item is used once → finite\n",
    "        progress = True\n",
    "        while progress:\n",
    "            progress = False\n",
    "            for b in range(B):\n",
    "                if not bins[b]:\n",
    "                    continue\n",
    "                progress = True\n",
    "                cur, cur_tokens, cur_quad = [], 0, 0\n",
    "                while bins[b]:\n",
    "                    k, s = bins[b][0]\n",
    "                    next_len = len(cur) + 1\n",
    "                    next_tokens = cur_tokens + s\n",
    "                    next_quad = cur_quad + s * s\n",
    "                    if (next_len <= self.max_batch_size\n",
    "                        and next_tokens <= self.max_tokens\n",
    "                        and next_quad  <= self.max_quadratic):\n",
    "                        cur.append(k)\n",
    "                        cur_tokens = next_tokens\n",
    "                        cur_quad = next_quad\n",
    "                        bins[b].pop(0)\n",
    "                    else:\n",
    "                        break\n",
    "                if cur:\n",
    "                    batches.append(cur)\n",
    "\n",
    "        # Safety fallback (shouldn't trigger, but just in case):\n",
    "        if not batches:\n",
    "            batches = [[int(k)] for (k, _) in pairs]\n",
    "        return batches\n",
    "\n",
    "class ChainBatchSamplers(Sampler):\n",
    "    \"\"\"Concatenate multiple precomputed batch samplers into one finite sequence.\"\"\"\n",
    "    def __init__(self, samplers, *, shuffle_order=False, seed=0):\n",
    "        self.samplers = list(samplers)\n",
    "        self.shuffle_order = bool(shuffle_order)\n",
    "        self.seed = int(seed)\n",
    "        # precompute concatenation so __len__ is exact\n",
    "        order = list(range(len(self.samplers)))\n",
    "        if self.shuffle_order:\n",
    "            rnd = random.Random(self.seed)\n",
    "            rnd.shuffle(order)\n",
    "        self._batches = []\n",
    "        for k in order:\n",
    "            s = self.samplers[k]\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            self._batches.extend(list(s))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batches)\n",
    "\n",
    "def build_bucket_sampler_for_keys(\n",
    "    keys: np.ndarray, *,\n",
    "    max_tokens: int,\n",
    "    max_quadratic: Optional[int],\n",
    "    max_batch_size: int,\n",
    "    shuffle: bool,\n",
    "    seed: int,\n",
    ") -> Sampler:\n",
    "    keys = np.asarray(keys, dtype=np.int64)\n",
    "    if keys.size == 0:\n",
    "        return EmptyBatchSampler()\n",
    "    return TokenBucketBatchSampler(\n",
    "        keys, KEY_SIZES,\n",
    "        max_tokens=max_tokens,\n",
    "        max_quadratic=max_quadratic if max_quadratic is not None else 1_200_000,\n",
    "        max_batch_size=max_batch_size,\n",
    "        shuffle=shuffle, seed=seed, bins=8\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3efce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import torch, numpy as np\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def _get_rdkit_feats_from_record(rec):\n",
    "    arr = getattr(rec, \"rdkit_feats\", None)\n",
    "    if arr is None:\n",
    "        return torch.zeros(1, 15, dtype=torch.float32)  # keep (1, D)\n",
    "    v = torch.as_tensor(np.asarray(arr, np.float32).reshape(1, -1), dtype=torch.float32)\n",
    "    return v  # (1, D)\n",
    "\n",
    "class LMDBtoPyGSingleTask(Dataset):\n",
    "    def __init__(self,\n",
    "                 ids,                # <<< must be augmented key_ids\n",
    "                 lmdb_path,\n",
    "                 target_index=None,\n",
    "                 *,\n",
    "                 use_mixed_edges: bool = True,\n",
    "                 include_extra_atom_feats: bool = True):\n",
    "        self.ids = np.asarray(ids, dtype=np.int64)\n",
    "        self.base = LMDBDataset(self.ids, lmdb_path)\n",
    "        self.t = target_index\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        self.include_extra_atom_feats = include_extra_atom_feats\n",
    "\n",
    "    def __len__(self): return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.base[idx]\n",
    "        x  = torch.as_tensor(rec.x, dtype=torch.long)\n",
    "        ei = torch.as_tensor(rec.edge_index, dtype=torch.long)\n",
    "        ea = torch.as_tensor(rec.edge_attr)\n",
    "\n",
    "        # Mixed edges: 3 categorical + 32 RBF; categorical-only if disabled\n",
    "        edge_attr = ea.to(torch.float32) if self.use_mixed_edges else ea[:, :3].to(torch.long)\n",
    "\n",
    "        d = Data(x=x, edge_index=ei, edge_attr=edge_attr,\n",
    "                 rdkit_feats=_get_rdkit_feats_from_record(rec))  # (1, D)\n",
    "\n",
    "        if hasattr(rec, \"pos\"):                d.pos  = torch.as_tensor(rec.pos, dtype=torch.float32)\n",
    "        if self.include_extra_atom_feats and hasattr(rec, \"extra_atom_feats\"):\n",
    "                                               d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32)\n",
    "        if hasattr(rec, \"has_xyz\"):            d.has_xyz = torch.as_tensor(rec.has_xyz, dtype=torch.float32)  # (1,)\n",
    "        if hasattr(rec, \"dist\"):               d.hops = torch.as_tensor(rec.dist, dtype=torch.long).unsqueeze(0)  # (1,L,L)\n",
    "\n",
    "        if (self.t is not None) and hasattr(rec, \"y\"):\n",
    "            yv = torch.as_tensor(rec.y, dtype=torch.float32).view(-1)\n",
    "            if self.t < yv.numel(): d.y = yv[self.t:self.t+1]  # (1,)\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "694612d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell B — Token-bucket loaders with key->index mapping (drop-in) ====\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "WHALE_CUTOFF = 86  # p99 from your EDA\n",
    "\n",
    "class MapKeyBatchesToIndexBatches(Sampler):\n",
    "    \"\"\"Wrap a batch-sampler that yields key_ids; convert to dataset indices.\"\"\"\n",
    "    def __init__(self, key_batch_sampler, id2pos):\n",
    "        self.key_batch_sampler = key_batch_sampler  # yields lists of key_ids\n",
    "        self.id2pos = id2pos                        # {key_id: index_in_dataset}\n",
    "        # pre-map once for speed and safety\n",
    "        self._batches = [[self.id2pos[int(k)] for k in batch]\n",
    "                         for batch in key_batch_sampler]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batches)\n",
    "\n",
    "def make_loaders_for_task_from_pools(\n",
    "    task, task_pools, *,\n",
    "    normal_max_tokens=9000,\n",
    "    normal_max_quadratic=1080000,\n",
    "    whale_max_tokens=2500,\n",
    "    whale_max_quadratic=400000,\n",
    "    max_batch_size=1024,\n",
    "    use_mixed_edges=True,\n",
    "    include_extra_atom_feats=True,\n",
    "    num_workers=0,            # ← start with 0 on Windows for stability\n",
    "    pin_memory=False,         # pin only when you go multi-worker on CUDA\n",
    "    debug_single_process=True,\n",
    "    whale_cutoff=WHALE_CUTOFF,\n",
    "):\n",
    "    assert 'KEY_SIZES' in globals(), \"KEY_SIZES dict must be built first (Cell A).\"\n",
    "\n",
    "    t = task2idx[task]\n",
    "    tr_keys, va_keys = task_pools[task]\n",
    "    if len(tr_keys) == 0 or len(va_keys) == 0:\n",
    "        raise ValueError(f\"Empty pools for {task}. Check splits.\")\n",
    "\n",
    "    # Datasets stay in key-id order\n",
    "    tr_ds = LMDBtoPyGSingleTask(tr_keys, TRAIN_LMDB, target_index=t,\n",
    "                                use_mixed_edges=use_mixed_edges,\n",
    "                                include_extra_atom_feats=include_extra_atom_feats)\n",
    "    va_ds = LMDBtoPyGSingleTask(va_keys, TRAIN_LMDB, target_index=t,\n",
    "                                use_mixed_edges=use_mixed_edges,\n",
    "                                include_extra_atom_feats=include_extra_atom_feats)\n",
    "\n",
    "    # Map key_id -> position within each dataset\n",
    "    tr_id2pos = {int(k): i for i, k in enumerate(tr_keys)}\n",
    "    va_id2pos = {int(k): i for i, k in enumerate(va_keys)}\n",
    "\n",
    "    # Split ids by size using KEY_SIZES\n",
    "    def split_keys(keys):\n",
    "        small, whales = [], []\n",
    "        for k in keys:\n",
    "            if KEY_SIZES.get(int(k), 1) > whale_cutoff:\n",
    "                whales.append(int(k))\n",
    "            else:\n",
    "                small.append(int(k))\n",
    "        return np.array(small, dtype=np.int64), np.array(whales, dtype=np.int64)\n",
    "\n",
    "    tr_small, tr_whales = split_keys(tr_keys)\n",
    "    va_small, va_whales = split_keys(va_keys)\n",
    "\n",
    "    # Build key-id samplers (finite)\n",
    "    tr_small_s = build_bucket_sampler_for_keys(\n",
    "        tr_small, max_tokens=normal_max_tokens, max_quadratic=normal_max_quadratic,\n",
    "        max_batch_size=max_batch_size, shuffle=True, seed=42\n",
    "    )\n",
    "    tr_whale_s = build_bucket_sampler_for_keys(\n",
    "        tr_whales, max_tokens=whale_max_tokens, max_quadratic=whale_max_quadratic,\n",
    "        max_batch_size=max_batch_size, shuffle=True, seed=43\n",
    "    )\n",
    "    tr_keys_sampler = ChainBatchSamplers([tr_small_s, tr_whale_s], shuffle_order=True, seed=123)\n",
    "\n",
    "    va_small_s = build_bucket_sampler_for_keys(\n",
    "        va_small, max_tokens=normal_max_tokens, max_quadratic=normal_max_quadratic,\n",
    "        max_batch_size=max_batch_size, shuffle=False, seed=123\n",
    "    )\n",
    "    va_whale_s = build_bucket_sampler_for_keys(\n",
    "        va_whales, max_tokens=whale_max_tokens, max_quadratic=whale_max_quadratic,\n",
    "        max_batch_size=max_batch_size, shuffle=False, seed=123\n",
    "    )\n",
    "    va_keys_sampler = ChainBatchSamplers([va_small_s, va_whale_s], shuffle_order=False, seed=0)\n",
    "\n",
    "    # Wrap: key-id batches → dataset index batches\n",
    "    tr_sampler = MapKeyBatchesToIndexBatches(tr_keys_sampler, tr_id2pos)\n",
    "    va_sampler = MapKeyBatchesToIndexBatches(va_keys_sampler, va_id2pos)\n",
    "\n",
    "    # DataLoaders\n",
    "    if debug_single_process:\n",
    "        tr_loader = GeoDataLoader(tr_ds, batch_sampler=tr_sampler, num_workers=0, pin_memory=False)\n",
    "        va_loader = GeoDataLoader(va_ds, batch_sampler=va_sampler, num_workers=0, pin_memory=False)\n",
    "    else:\n",
    "        tr_loader = GeoDataLoader(\n",
    "            tr_ds, batch_sampler=tr_sampler,\n",
    "            num_workers=num_workers, pin_memory=pin_memory,\n",
    "            persistent_workers=(num_workers > 0),\n",
    "            **({} if num_workers == 0 else dict(prefetch_factor=2))\n",
    "        )\n",
    "        va_loader = GeoDataLoader(\n",
    "            va_ds, batch_sampler=va_sampler,\n",
    "            num_workers=num_workers, pin_memory=pin_memory,\n",
    "            persistent_workers=(num_workers > 0),\n",
    "            **({} if num_workers == 0 else dict(prefetch_factor=2))\n",
    "        )\n",
    "    return tr_loader, va_loader\n",
    "\n",
    "# Build loaders for all tasks (start with debug_single_process=True)\n",
    "train_loader_tg,  val_loader_tg  = make_loaders_for_task_from_pools(\"Tg\",      task_pools, debug_single_process=True)\n",
    "train_loader_den, val_loader_den = make_loaders_for_task_from_pools(\"Density\", task_pools, debug_single_process=True)\n",
    "train_loader_rg,  val_loader_rg  = make_loaders_for_task_from_pools(\"Rg\",      task_pools, debug_single_process=True)\n",
    "train_loader_ffv, val_loader_ffv = make_loaders_for_task_from_pools(\"FFV\",     task_pools, debug_single_process=True)\n",
    "train_loader_tc,  val_loader_tc  = make_loaders_for_task_from_pools(\"Tc\",      task_pools, debug_single_process=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca7e255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "# from torch.utils.data import Sampler\n",
    "\n",
    "# from torch.utils.data import Sampler\n",
    "\n",
    "# class MapKeyBatchesToIndexBatches(Sampler):\n",
    "#     \"\"\"Wrap a batch-sampler that yields key_ids; convert to dataset indices.\"\"\"\n",
    "#     def __init__(self, key_batch_sampler, id2pos):\n",
    "#         self.key_batch_sampler = key_batch_sampler  # yields lists of key_ids\n",
    "#         self.id2pos = id2pos                        # {key_id: index_in_dataset}\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         for key_batch in self.key_batch_sampler:\n",
    "#             yield [self.id2pos[int(k)] for k in key_batch]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.key_batch_sampler)\n",
    "\n",
    "\n",
    "# WHALE_CUTOFF = 86  # from EDA p99\n",
    "\n",
    "# def build_datasets_and_samplers(task, task_pools, *,\n",
    "#                                 normal_max_tokens=9000,\n",
    "#                                 normal_max_quadratic=900000,\n",
    "#                                 whale_max_tokens=2500,\n",
    "#                                 whale_max_quadratic=400000,\n",
    "#                                 max_batch_size=1024,\n",
    "#                                 use_mixed_edges=True,\n",
    "#                                 include_extra_atom_feats=True):\n",
    "#     t = task2idx[task]\n",
    "#     tr_keys, va_keys = task_pools[task]\n",
    "#     if len(tr_keys) == 0 or len(va_keys) == 0:\n",
    "#         raise ValueError(f\"Empty pools for {task}. Check splits.\")\n",
    "\n",
    "#     tr_ds = LMDBtoPyGSingleTask(tr_keys, TRAIN_LMDB, target_index=t,\n",
    "#                                 use_mixed_edges=use_mixed_edges,\n",
    "#                                 include_extra_atom_feats=include_extra_atom_feats)\n",
    "#     va_ds = LMDBtoPyGSingleTask(va_keys, TRAIN_LMDB, target_index=t,\n",
    "#                                 use_mixed_edges=use_mixed_edges,\n",
    "#                                 include_extra_atom_feats=include_extra_atom_feats)\n",
    "#     tr_id2pos = {int(k): i for i, k in enumerate(tr_keys)}\n",
    "#     va_id2pos = {int(k): i for i, k in enumerate(va_keys)}\n",
    "#     # Split by size using KEY_SIZES (must be defined earlier)\n",
    "#     def split_keys(keys):\n",
    "#         small, whales = [], []\n",
    "#         for k in keys:\n",
    "#             (whales if KEY_SIZES.get(int(k), 1) > WHALE_CUTOFF else small).append(int(k))\n",
    "#         return np.array(small, dtype=np.int64), np.array(whales, dtype=np.int64)\n",
    "\n",
    "#     tr_small, tr_whales = split_keys(tr_keys)\n",
    "#     va_small, va_whales = split_keys(va_keys)\n",
    "\n",
    "#     # Train samplers\n",
    "#     tr_small_sampler = build_bucket_sampler_for_keys(\n",
    "#         tr_small, max_tokens=normal_max_tokens, max_quadratic=normal_max_quadratic,\n",
    "#         max_batch_size=max_batch_size, shuffle=True, seed=42\n",
    "#     )\n",
    "#     tr_whale_sampler = build_bucket_sampler_for_keys(\n",
    "#         tr_whales, max_tokens=whale_max_tokens, max_quadratic=whale_max_quadratic,\n",
    "#         max_batch_size=max_batch_size, shuffle=True, seed=43\n",
    "#     )\n",
    "#     tr_sampler = ChainBatchSamplers([tr_small_sampler, tr_whale_sampler],\n",
    "#                                     shuffle_order=True, seed=123)\n",
    "\n",
    "#     # Val samplers (deterministic)\n",
    "#     va_small_sampler = build_bucket_sampler_for_keys(\n",
    "#         va_small, max_tokens=normal_max_tokens, max_quadratic=normal_max_quadratic,\n",
    "#         max_batch_size=max_batch_size, shuffle=False, seed=123\n",
    "#     )\n",
    "#     va_whale_sampler = build_bucket_sampler_for_keys(\n",
    "#         va_whales, max_tokens=whale_max_tokens, max_quadratic=whale_max_quadratic,\n",
    "#         max_batch_size=max_batch_size, shuffle=False, seed=123\n",
    "#     )\n",
    "#     va_sampler = ChainBatchSamplers([va_small_sampler, va_whale_sampler],\n",
    "#                                     shuffle_order=False, seed=0)\n",
    "#     tr_sampler_idx = MapKeyBatchesToIndexBatches(tr_sampler, tr_id2pos)\n",
    "#     va_sampler_idx = MapKeyBatchesToIndexBatches(va_sampler, va_id2pos)\n",
    "#     return tr_ds, va_ds, tr_sampler_idx, va_sampler_idx\n",
    "\n",
    "\n",
    "# def make_loaders_from_handles(tr_ds, va_ds, tr_sampler, va_sampler, *,\n",
    "#                               num_workers=2, pin_memory=True):\n",
    "#     # Start conservative on Windows; bump workers after it’s stable\n",
    "#     tr = GeoDataLoader(tr_ds, batch_sampler=tr_sampler,\n",
    "#                        num_workers=num_workers,\n",
    "#                        pin_memory=pin_memory,\n",
    "#                        persistent_workers=(num_workers > 0),\n",
    "#                        prefetch_factor=(2 if num_workers > 0 else None))\n",
    "#     va = GeoDataLoader(va_ds, batch_sampler=va_sampler,\n",
    "#                        num_workers=num_workers,\n",
    "#                        pin_memory=pin_memory,\n",
    "#                        persistent_workers=(num_workers > 0),\n",
    "#                        prefetch_factor=(2 if num_workers > 0 else None))\n",
    "#     return tr, va\n",
    "\n",
    "# # Build handles for one task\n",
    "# tr_ds_tg, va_ds_tg, tr_sampler_tg, va_sampler_tg = build_datasets_and_samplers(\"Tg\", task_pools)\n",
    "\n",
    "# # Quick sampler sanity check (optional)\n",
    "# def _check_sampler(sampler, n):\n",
    "#     c = 0\n",
    "#     for batch in sampler:\n",
    "#         assert isinstance(batch, (list, tuple)) and len(batch) > 0, \"Empty batch\"\n",
    "#         for i in batch:\n",
    "#             assert 0 <= i < n, f\"Index {i} out of range 0..{n-1}\"\n",
    "#         c += 1\n",
    "#     print(f\"sampler OK, {c} batches\")\n",
    "\n",
    "# _check_sampler(tr_sampler_tg, len(tr_ds_tg))\n",
    "# _check_sampler(va_sampler_tg, len(va_ds_tg))\n",
    "\n",
    "# # DEBUG: single-process DataLoader to surface true errors\n",
    "# tr_dbg = GeoDataLoader(tr_ds_tg, batch_sampler=tr_sampler_tg, num_workers=0, pin_memory=False)\n",
    "# b = next(iter(tr_dbg))   # if anything is wrong in __getitem__/sampler, the real traceback appears here\n",
    "\n",
    "# # If debug batch works, create production loaders\n",
    "# train_loader_tg, val_loader_tg = make_loaders_from_handles(\n",
    "#     tr_ds_tg, va_ds_tg, tr_sampler_tg, va_sampler_tg, num_workers=0, pin_memory=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983db98",
   "metadata": {},
   "source": [
    "## Step 5: Define the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc992041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numpy as np, torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, RMSprop\n",
    "from torch.amp import GradScaler, autocast\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "\n",
    "def _batch_len_stats(b):\n",
    "    # counts nodes per-graph from PyG's batch vector\n",
    "    sizes = torch.bincount(b.batch, minlength=b.num_graphs)\n",
    "    s = sizes.to(torch.int32).cpu().numpy()\n",
    "    if s.size == 0:\n",
    "        return \"L: n=0\"\n",
    "    tokens = int(s.sum())\n",
    "    quad = int((sizes.to(torch.int64)**2).sum().item())\n",
    "    q50, q90, q95, q99 = np.quantile(s, [0.5, 0.9, 0.95, 0.99])\n",
    "    return (f\"L: n={len(s)} tokens={tokens} sumL2={quad} \"\n",
    "            f\"p50={int(q50)} p90={int(q90)} p95={int(q95)} p99={int(q99)} max={int(s.max())}\")\n",
    "\n",
    "\n",
    "def free_cuda_memory(tag: str = \"\"):\n",
    "    try:\n",
    "        torch.cuda.synchronize()\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Clear PyTorch CUDA caching allocator\n",
    "    torch.cuda.empty_cache()\n",
    "    # Python GC to break reference cycles\n",
    "    gc.collect()\n",
    "    if tag:\n",
    "        try:\n",
    "            alloc = torch.cuda.memory_allocated() / (1024**2)\n",
    "            reserv = torch.cuda.memory_reserved() / (1024**2)\n",
    "            print(f\"[mem:{tag}] allocated={alloc:.1f}MB reserved={reserv:.1f}MB\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def reset_cuda_stats():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "\n",
    "def train_hybrid_gnn_sota(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    *,\n",
    "    lr: float = 5e-4,\n",
    "    optimizer: str = \"AdamW\",\n",
    "    weight_decay: float = 1e-5,\n",
    "    epochs: int = 120,\n",
    "    warmup_epochs: int = 5,\n",
    "    patience: int = 15,\n",
    "    clip_norm: float = 1.0,\n",
    "    amp: bool = True,\n",
    "    loss_name: str = \"mse\",   # \"mse\" or \"huber\"\n",
    "    save_dir: str = \"saved_models/gnn\",\n",
    "    tag: str = \"model_sota\",\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    opt_name = optimizer.lower()\n",
    "    if opt_name == \"rmsprop\":\n",
    "        opt = RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.0)\n",
    "    else:\n",
    "        opt = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # cosine schedule w/ warmup\n",
    "    def lr_factor(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / max(1, warmup_epochs)\n",
    "        t = (epoch - warmup_epochs) / max(1, (epochs - warmup_epochs))\n",
    "        return 0.5 * (1 + math.cos(math.pi * t))\n",
    "    scaler = GradScaler(\"cuda\", enabled=amp)\n",
    "\n",
    "    def loss_fn(pred, target):\n",
    "        if loss_name.lower() == \"huber\":\n",
    "            return F.huber_loss(pred, target, delta=1.0)\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_once(loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.inference_mode():\n",
    "            for i, b in enumerate(loader, 1):\n",
    "                # print(_batch_len_stats(b))  # keep if you want\n",
    "                b = b.to(device, non_blocking=True)\n",
    "                p = model(b)\n",
    "                preds.append(p.cpu())\n",
    "                trues.append(b.y.view(-1,1).cpu())\n",
    "                del p, b\n",
    "        preds = torch.cat(preds).numpy(); trues = torch.cat(trues).numpy()\n",
    "        mae = np.mean(np.abs(preds - trues))\n",
    "        rmse = float(np.sqrt(np.mean((preds - trues)**2)))\n",
    "        r2 = float(1 - np.sum((preds - trues)**2) / np.sum((trues - trues.mean())**2))\n",
    "        return mae, rmse, r2\n",
    "\n",
    "    best_mae = float(\"inf\")\n",
    "    best = None\n",
    "    best_path = os.path.join(save_dir, f\"{tag}.pt\")\n",
    "    bad = 0  # <<< add\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        for g in opt.param_groups:\n",
    "            g[\"lr\"] = lr * lr_factor(ep-1)\n",
    "\n",
    "        model.train()\n",
    "        total, count = 0.0, 0\n",
    "        for step, b in enumerate(train_loader, start=1):\n",
    "            b = b.to(device, non_blocking=True)  # <<< non_blocking\n",
    "            if ep == 1 and step % 50 == 1:\n",
    "            # if (ep <= 2) or (step % 50 == 1):\n",
    "                print(_batch_len_stats(b))\n",
    "            with autocast(\"cuda\", enabled=amp):\n",
    "                pred = model(b)\n",
    "                loss = loss_fn(pred, b.y.view(-1,1))\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            if clip_norm is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "            scaler.step(opt); scaler.update()\n",
    "\n",
    "            total += loss.item() * b.num_graphs\n",
    "            count += b.num_graphs\n",
    "            del pred, loss, b  # <<< drop references promptly\n",
    "\n",
    "        free_cuda_memory(tag=f\"after_epoch_{ep}\")\n",
    "\n",
    "        tr_mse = total / max(1, count)\n",
    "        mae, rmse, r2 = eval_once(val_loader)\n",
    "        print(f\"Epoch {ep:03d} | tr_MSE {tr_mse:.5f} | val_MAE {mae:.5f} | val_RMSE {rmse:.5f} | R2 {r2:.4f}\")\n",
    "\n",
    "        if mae < best_mae - 1e-6:\n",
    "            best_mae = mae\n",
    "            best = deepcopy(model.state_dict())\n",
    "            torch.save(best, best_path)\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    if best is not None:\n",
    "        model.load_state_dict(best)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "    final_mae, final_rmse, final_r2 = eval_once(val_loader)\n",
    "    print(f\"[{tag}] Best Val — MAE {final_mae:.6f} | RMSE {final_rmse:.6f} | R2 {final_r2:.4f}\")\n",
    "    return model, best_path, {\"MAE\": final_mae, \"RMSE\": final_rmse, \"R2\": final_r2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9449bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "def _act(name: str):\n",
    "    name = (name or \"relu\").lower()\n",
    "    if name == \"gelu\": return nn.GELU()\n",
    "    if name in (\"silu\", \"swish\"): return nn.SiLU()\n",
    "    return nn.ReLU()\n",
    "\n",
    "\n",
    "class AttnBiasFull(nn.Module):\n",
    "    \"\"\"\n",
    "    Produces additive per-head attention bias of shape (B, H, L0, L0)\n",
    "    from geometry (xyz), adjacency, SPD buckets, and categorical edge types.\n",
    "\n",
    "    Accepts both old arg names (use_geo/use_adj_const/spd_max/rbf_K) and\n",
    "    new ones (use_geo_bias/use_adj_bias/spd_buckets/rbf_k/edge_cats).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_heads: int,\n",
    "        *,\n",
    "        # old names\n",
    "        use_geo: bool = None, use_adj_const: bool = None, use_spd: bool = True,\n",
    "        spd_max: int = None, rbf_K: int = None,\n",
    "        # new alias names\n",
    "        use_geo_bias: bool = None, use_adj_bias: bool = None,\n",
    "        spd_buckets: int = None, rbf_k: int = None,\n",
    "        edge_cats: tuple = (5, 6, 2),\n",
    "        use_edge_bias: bool = True,\n",
    "        # shared\n",
    "        rbf_beta: float = 5.0, activation: str = \"relu\",\n",
    "        edge_cont_dim: int = 32,  # (kept for compatibility; not used here)\n",
    "        use_headnorm: bool = True,\n",
    "        bound_scale: float = 0.1,   # tanh scale for gentle bounding\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_heads = int(n_heads)\n",
    "        self.bound_scale = float(bound_scale)\n",
    "        self.use_headnorm = bool(use_headnorm)\n",
    "\n",
    "        # ---- resolve aliases / defaults ----\n",
    "        def pick(*vals, default):\n",
    "            for v in vals:\n",
    "                if v is not None:\n",
    "                    return v\n",
    "            return default\n",
    "\n",
    "        self.use_geo = bool(pick(use_geo, use_geo_bias, default=True))\n",
    "        self.use_adj_const = bool(pick(use_adj_const, use_adj_bias, default=True))\n",
    "\n",
    "        # SPD: if spd_buckets given, use exactly that; else spd_max + 2 (0..spd_max + catch-all)\n",
    "        if spd_buckets is not None:\n",
    "            self.spd_buckets = int(spd_buckets)\n",
    "        else:\n",
    "            smax = 5 if spd_max is None else int(spd_max)\n",
    "            self.spd_buckets = smax + 2  # 0..smax + 1(>=)\n",
    "\n",
    "        K = int(pick(rbf_K, rbf_k, default=16))\n",
    "        self.rbf_beta = float(rbf_beta)\n",
    "\n",
    "        # ---- geometry → per-head bias ----\n",
    "        if self.use_geo:\n",
    "            centers = torch.linspace(0.0, 10.0, K)\n",
    "            self.register_buffer(\"centers\", centers, persistent=False)\n",
    "            self.geo_mlp = nn.Sequential(\n",
    "                nn.Linear(K, self.n_heads),  # simple per-head projection\n",
    "            )\n",
    "\n",
    "        # ---- adjacency constant per head ----\n",
    "        if self.use_adj_const:\n",
    "            self.adj_bias = nn.Parameter(torch.zeros(self.n_heads))\n",
    "\n",
    "        # ---- SPD buckets → per-head bias ----\n",
    "        self.use_spd = bool(use_spd)\n",
    "        if self.use_spd:\n",
    "            self.spd_emb = nn.Embedding(self.spd_buckets, self.n_heads)\n",
    "\n",
    "        # ---- edge categorical bias (configurable widths) ----\n",
    "        t, s, c = edge_cats\n",
    "        self.use_edge_bias = bool(use_edge_bias)\n",
    "        if self.use_edge_bias:\n",
    "            self.edge_emb0 = nn.Embedding(int(t), self.n_heads)\n",
    "            self.edge_emb1 = nn.Embedding(int(s), self.n_heads)\n",
    "            self.edge_emb2 = nn.Embedding(int(c), self.n_heads)\n",
    "        else:\n",
    "            self.edge_emb0 = self.edge_emb1 = self.edge_emb2 = None\n",
    "\n",
    "        # ---- per-component learnable scalers ----\n",
    "        self.alpha_geo  = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_spd  = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_adj  = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_edge = nn.Parameter(torch.tensor(0.2))\n",
    "\n",
    "        # ---- simple head-wise LayerNorms (normalize across H) ----\n",
    "        if self.use_headnorm:\n",
    "            self.ln_geo  = nn.LayerNorm(self.n_heads)\n",
    "            self.ln_spd  = nn.LayerNorm(self.n_heads)\n",
    "            self.ln_edge = nn.LayerNorm(self.n_heads)\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def _apply_ln_heads(self, t: torch.Tensor, ln: nn.LayerNorm) -> torch.Tensor:\n",
    "        \"\"\"Apply LayerNorm across heads for a (B,H,L,L) tensor.\"\"\"\n",
    "        # (B,H,L,L) -> (B,L,L,H) -> LN(H) -> (B,H,L,L)\n",
    "        t = t.permute(0, 2, 3, 1)\n",
    "        t = ln(t)\n",
    "        t = t.permute(0, 3, 1, 2).contiguous()\n",
    "        return t\n",
    "\n",
    "    def _bound(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Bound magnitudes to avoid dominating softmax; keeps gradients smooth.\"\"\"\n",
    "        return self.bound_scale * torch.tanh(t)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _spd_bias(self, hops: torch.Tensor, valid_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        hops: (B, MAX_NODES, MAX_NODES) or (B, L0, L0) shortest-path distances (uint8/long)\n",
    "        valid_mask: (B, L0, L0) bool, True where both tokens are real (not PAD)\n",
    "        returns: (B, H, L0, L0) additive per-head bias\n",
    "        \"\"\"\n",
    "        if hops.dim() == 2:  # (L,L) -> (1,L,L)\n",
    "            hops = hops.unsqueeze(0)\n",
    "\n",
    "        B, L0, _ = valid_mask.shape\n",
    "\n",
    "        # align SPD to current L0 (top-left block)\n",
    "        if hops.size(1) != L0 or hops.size(2) != L0:\n",
    "            hops = hops[:, :L0, :L0]\n",
    "\n",
    "        # bucketize SPD: last bucket = catch-all (>= last)\n",
    "        last = self.spd_buckets - 1\n",
    "        raw = hops.to(valid_mask.device).long().clamp_min_(0)\n",
    "        catch_all = raw >= last\n",
    "        raw = raw.clamp_max(last - 1)\n",
    "        bucket = torch.where(catch_all, raw.new_full(raw.shape, last), raw)\n",
    "\n",
    "        # wipe invalid pairs\n",
    "        bucket = torch.where(valid_mask, bucket, torch.zeros_like(bucket))\n",
    "\n",
    "        emb = self.spd_emb(bucket)              # (B, L0, L0, H)\n",
    "        return emb.permute(0, 3, 1, 2).contiguous()  # (B, H, L0, L0)\n",
    "\n",
    "    def _edge_bias(self, edge_index, edge_attr, batch, L0, ptr=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-head additive bias from categorical bond attributes.\n",
    "        Returns: (B, H, L0, L0)\n",
    "        \"\"\"\n",
    "        u, v = edge_index\n",
    "        be   = batch[u]  # graph id per edge\n",
    "\n",
    "        if ptr is None:\n",
    "            B = int(batch.max().item()) + 1\n",
    "            counts = torch.bincount(batch, minlength=B)\n",
    "            ptr = torch.zeros(B + 1, dtype=torch.long, device=batch.device)\n",
    "            ptr[1:] = torch.cumsum(counts, dim=0)\n",
    "        B = int(ptr.numel() - 1)\n",
    "\n",
    "        start = ptr[be]\n",
    "        u_loc = (u - start).long()\n",
    "        v_loc = (v - start).long()\n",
    "\n",
    "        cat = edge_attr[:, :3].long()\n",
    "        eh  = ( self.edge_emb0(cat[:, 0])\n",
    "              + self.edge_emb1(cat[:, 1])\n",
    "              + self.edge_emb2(cat[:, 2]) )  # (E,H)\n",
    "\n",
    "        H = self.n_heads\n",
    "        eb = torch.zeros((B, H, L0, L0), device=edge_attr.device, dtype=torch.float32)\n",
    "        for b in range(B):\n",
    "            m = (be == b)\n",
    "            if not torch.any(m):\n",
    "                continue\n",
    "            eb[b, :, u_loc[m], v_loc[m]] += eh[m].T\n",
    "        return eb\n",
    "\n",
    "    # ---------- forward ----------\n",
    "    def forward(self, pos, edge_index, edge_attr, batch, key_padding_mask, hops=None, ptr=None):\n",
    "        \"\"\"\n",
    "        Returns (B, H, L0, L0) additive bias. PAD rows/cols are filled with large negative.\n",
    "        \"\"\"\n",
    "        A = to_dense_adj(edge_index, batch=batch).squeeze(1)  # (B,L0,L0)\n",
    "        B, L0, _ = A.shape\n",
    "        H = self.n_heads\n",
    "        device = A.device\n",
    "\n",
    "        valid = ~key_padding_mask                             # (B,L0)\n",
    "        valid2d = valid.unsqueeze(2) & valid.unsqueeze(1)     # (B,L0,L0)\n",
    "\n",
    "        # geometry\n",
    "        if self.use_geo and (pos is not None):\n",
    "            pad_pos, _ = to_dense_batch(pos, batch)           # (B,L0,3)\n",
    "            diff = pad_pos.unsqueeze(2) - pad_pos.unsqueeze(1)      # (B,L0,L0,3)\n",
    "            dist = torch.sqrt(torch.clamp((diff**2).sum(-1), min=0.0))  # (B,L0,L0)\n",
    "            centers = self.centers.to(dist.device)\n",
    "            rbf = torch.exp(-self.rbf_beta * (dist.unsqueeze(-1) - centers)**2)\n",
    "            geo = self.geo_mlp(rbf).permute(0, 3, 1, 2).contiguous()    # (B,H,L0,L0)\n",
    "        else:\n",
    "            geo = torch.zeros((B, H, L0, L0), device=device)\n",
    "\n",
    "        # adjacency constant per head\n",
    "        if self.use_adj_const:\n",
    "            adj = A.unsqueeze(1) * self.adj_bias.view(1, H, 1, 1)       # (B,H,L0,L0)\n",
    "        else:\n",
    "            adj = torch.zeros_like(geo)\n",
    "\n",
    "        # SPD\n",
    "        if self.use_spd and (hops is not None):\n",
    "            spd = self._spd_bias(hops, valid2d)                          # (B,H,L0,L0)\n",
    "        else:\n",
    "            spd = torch.zeros_like(geo)\n",
    "\n",
    "        # edge categorical\n",
    "        if self.use_edge_bias and (edge_attr is not None):\n",
    "            edg = self._edge_bias(edge_index, edge_attr, batch, L0, ptr) # (B,H,L0,L0)\n",
    "        else:\n",
    "            edg = torch.zeros_like(geo)\n",
    "\n",
    "        # ---- normalize & bound each component, then scale ----\n",
    "        if self.use_headnorm:\n",
    "            if self.use_geo:  geo = self._apply_ln_heads(geo,  self.ln_geo)\n",
    "            if self.use_spd:  spd = self._apply_ln_heads(spd,  self.ln_spd)\n",
    "            if self.use_edge_bias: edg = self._apply_ln_heads(edg, self.ln_edge)\n",
    "\n",
    "        # gently bound to keep attention stable\n",
    "        if self.use_geo:       geo = self._bound(geo)\n",
    "        if self.use_spd:       spd = self._bound(spd)\n",
    "        if self.use_edge_bias: edg = self._bound(edg)\n",
    "        # typically don't bound adj; it’s already a small learned scalar per head\n",
    "\n",
    "        bias = (self.alpha_geo  * geo\n",
    "              + self.alpha_spd  * spd\n",
    "              + self.alpha_adj  * adj\n",
    "              + self.alpha_edge * edg)\n",
    "\n",
    "        # mask PAD rows/cols; keep diagonal 0 for valid tokens\n",
    "        pad = key_padding_mask\n",
    "        big_neg = torch.tensor(-1e4, device=bias.device, dtype=bias.dtype)\n",
    "        bias = bias.masked_fill(pad.view(B, 1, L0, 1), big_neg)\n",
    "        bias = bias.masked_fill(pad.view(B, 1, 1, L0), big_neg)\n",
    "        I = torch.eye(L0, device=device, dtype=torch.bool).view(1, 1, L0, L0)\n",
    "        bias = torch.where(I, bias.new_zeros(()), bias)\n",
    "\n",
    "        return bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "026345f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINEConv\n",
    "\n",
    "class GINEBlock(nn.Module):\n",
    "    def __init__(self, dim, activation=\"silu\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.conv = GINEConv(nn.Sequential(\n",
    "            nn.Linear(dim, dim), act, nn.Linear(dim, dim)\n",
    "        ))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(nn.Linear(dim, 2*dim), act, nn.Dropout(dropout), nn.Linear(2*dim, dim))\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_emb):\n",
    "        h = self.conv(self.norm1(x), edge_index, edge_emb)\n",
    "        x = x + self.drop1(h)\n",
    "        x = x + self.drop2(self.ffn(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class EdgeEncoderMixed(nn.Module):\n",
    "    def __init__(self, emb_dim: int, cont_dim: int = 32, activation=\"silu\"):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.emb0 = nn.Embedding(5, emb_dim)\n",
    "        self.emb1 = nn.Embedding(6, emb_dim)\n",
    "        self.emb2 = nn.Embedding(2, emb_dim)\n",
    "        self.mlp_cont = nn.Sequential(\n",
    "            nn.Linear(cont_dim, emb_dim),\n",
    "            act,\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.LayerNorm(emb_dim),       # <<< add\n",
    "        )\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        cat  = edge_attr[:, :3].long()\n",
    "        cont = edge_attr[:, 3:].float()\n",
    "        e_cat  = self.emb0(cat[:,0]) + self.emb1(cat[:,1]) + self.emb2(cat[:,2])\n",
    "        e_cont = self.mlp_cont(cont)\n",
    "        return e_cat + 0.5 * e_cont     # <<< gentle scale on cont branch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f224f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class GraphTransformerGPS(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 8,\n",
    "        nlayers: int = 6,\n",
    "        dropout: float = 0.2,\n",
    "        drop_path: float = 0.0,   # (kept for extensibility)\n",
    "        activation: str = \"silu\",\n",
    "        rdkit_dim: int = 15,\n",
    "        use_extra_atom_feats: bool = True,\n",
    "        extra_atom_dim: int = 5,\n",
    "        # local GNN (GPS) settings\n",
    "        local_layers: int = 2,\n",
    "        use_mixed_edges: bool = True,\n",
    "        cont_dim: int = 32,\n",
    "        # bias knobs\n",
    "        use_geo_bias: bool = True,\n",
    "        use_spd_bias: bool = True,\n",
    "        spd_max: int = 5,\n",
    "        use_adj_const: bool = True,\n",
    "        use_edge_bias: bool = True,\n",
    "        # readout\n",
    "        use_cls: bool = True,\n",
    "        use_has_xyz: bool = True,\n",
    "        head_hidden: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead   = nhead\n",
    "        self.use_cls = use_cls\n",
    "        self.use_has_xyz = use_has_xyz\n",
    "        self.use_extra_atom_feats = use_extra_atom_feats\n",
    "        self.bias_builder = AttnBiasFull(\n",
    "            n_heads=nhead,\n",
    "            rbf_k=32,\n",
    "            rbf_beta=5.0,\n",
    "            use_geo_bias=use_geo_bias,          # was use_geo\n",
    "            use_adj_bias=use_adj_const,         # was use_adj_const (name matches here)\n",
    "            use_spd=use_spd_bias,               # was use_spd\n",
    "            spd_buckets=(spd_max + 1),          # was spd_max; +1 gives the \">= spd_max\" bucket\n",
    "            use_edge_bias=use_edge_bias,\n",
    "            edge_cats=(5, 6, 2),\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "\n",
    "        act = _act(activation)\n",
    "\n",
    "        # encoders\n",
    "        self.atom_enc = AtomEncoder(emb_dim=d_model)\n",
    "        if use_extra_atom_feats:\n",
    "            self.extra_proj = nn.Sequential(nn.Linear(extra_atom_dim, d_model), act, nn.Linear(d_model, d_model))\n",
    "            self.extra_gate = nn.Sequential(nn.Linear(2*d_model, d_model), act)\n",
    "\n",
    "        # local GNN stack\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        if use_mixed_edges:\n",
    "            self.edge_enc = EdgeEncoderMixed(d_model, cont_dim=cont_dim, activation=activation)\n",
    "        else:\n",
    "            from ogb.graphproppred.mol_encoder import BondEncoder\n",
    "            self.edge_enc = BondEncoder(emb_dim=d_model)\n",
    "        self.local_blocks = nn.ModuleList([GINEBlock(d_model, activation=activation, dropout=dropout) \n",
    "                                           for _ in range(local_layers)])\n",
    "\n",
    "        # transformer stack (PyTorch encoder)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=4*d_model,\n",
    "                                               dropout=dropout, activation=activation, batch_first=True, \n",
    "                                               norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers, enable_nested_tensor=False)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        # readout: concat mean + max + (optional) CLS + attention pool\n",
    "        self.gate_pool = nn.Sequential(nn.Linear(d_model, d_model//2), act, nn.Linear(d_model//2, 1))\n",
    "        # features: mean(d), max(d), attn(d) = 3d, (+cls d) optional, + rdkit, + has_xyz\n",
    "        pooled_dim = 3*d_model + (d_model if use_cls else 0)\n",
    "        head_in = pooled_dim + rdkit_dim + (1 if use_has_xyz else 0)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(head_in),\n",
    "            nn.Linear(head_in, head_hidden), act, nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden, head_hidden//2), act, nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden//2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # 1) atom encoding + optional per-atom extras\n",
    "        x = self.atom_enc(data.x)  # (N,D)\n",
    "        if getattr(self, \"use_extra_atom_feats\", False) and hasattr(data, \"extra_atom_feats\"):\n",
    "            xa = self.extra_proj(data.extra_atom_feats.float())          # (N,D)\n",
    "            x  = self.extra_gate(torch.cat([x, xa], dim=1))              # (N,D)\n",
    "\n",
    "        # 2) local GNN over sparse graph\n",
    "        e = self.edge_enc(data.edge_attr)\n",
    "        for blk in self.local_blocks:\n",
    "            x = blk(x, data.edge_index, e)                               # (N,D)\n",
    "\n",
    "        # 3) pack to dense (no CLS yet)\n",
    "        x_pad, valid = to_dense_batch(x, data.batch)                     # (B,L0,D)\n",
    "        B, L0, D = x_pad.shape\n",
    "        key_padding = ~valid                                             # (B,L0) True == PAD\n",
    "\n",
    "        # 4) head-wise attention bias on L0 tokens (B,H,L0,L0), pre-CLS\n",
    "        #    Your AttnBiasFull typically supports SPD, geometry, adjacency, edges\n",
    "        hops = getattr(data, \"hops\", None)                               # (B,MAX_NODES,MAX_NODES) or None\n",
    "        ptr = getattr(data, \"ptr\", None)\n",
    "        attn_bias = self.bias_builder(\n",
    "            pos=(data.pos if hasattr(data, \"pos\") else None),\n",
    "            edge_index=data.edge_index,\n",
    "            edge_attr=(data.edge_attr if hasattr(data, \"edge_attr\") else None),\n",
    "            batch=data.batch,\n",
    "            key_padding_mask=key_padding,   # (B,L0), True=PAD\n",
    "            hops=getattr(data, \"hops\", None),\n",
    "            ptr=ptr\n",
    "        )  # (B,H,L0,L0)                                                # (B,H,L0,L0)\n",
    "\n",
    "        # 5) finalize bias (mask PAD rows/cols, keep diagonal 0), then optionally append CLS\n",
    "        B, H, L = attn_bias.shape[0], attn_bias.shape[1], attn_bias.shape[-1]\n",
    "        pad = key_padding                                                 # (B,L)\n",
    "        huge = attn_bias.new_tensor(-1e4)\n",
    "\n",
    "        # rows FROM PAD, cols TO PAD\n",
    "        attn_bias = attn_bias.masked_fill(pad.view(B, 1, L, 1), huge)\n",
    "        attn_bias = attn_bias.masked_fill(pad.view(B, 1, 1, L), huge)\n",
    "\n",
    "        # keep diagonal = 0 on valid tokens\n",
    "        I = torch.eye(L, device=attn_bias.device, dtype=torch.bool).view(1, 1, L, L)\n",
    "        attn_bias = torch.where(I, attn_bias.new_zeros(()), attn_bias)\n",
    "\n",
    "        # (optional) append CLS token at the end\n",
    "        if getattr(self, \"use_cls\", False):\n",
    "            # append CLS embedding\n",
    "            cls = self.cls_token.expand(B, 1, D)                         # (B,1,D)\n",
    "            x_pad = torch.cat([x_pad, cls], dim=1)                       # (B,L+1,D)\n",
    "\n",
    "            # extend key_padding: CLS is always valid (False)\n",
    "            key_padding = torch.cat(\n",
    "                [key_padding, torch.zeros(B, 1, dtype=torch.bool, device=x_pad.device)],\n",
    "                dim=1\n",
    "            )                                                             # (B,L+1)\n",
    "\n",
    "            # pad bias by one row/col with zeros for CLS -> (B,H,L+1,L+1)\n",
    "            attn_bias = F.pad(attn_bias, (0, 1, 0, 1), value=0.0)\n",
    "            L = L + 1\n",
    "\n",
    "        # 6) transformer encoder with 3D additive mask (B*H,L,L)\n",
    "        attn_mask_3d = attn_bias.reshape(B * H, L, L).to(x_pad.dtype)\n",
    "        h = self.encoder(                                                # returns (B,L,D) when batch_first=True\n",
    "            x_pad,\n",
    "            mask=attn_mask_3d, # additive float mask \n",
    "        )\n",
    "\n",
    "        # 7) pooling (mean + max + gated attention), plus optional CLS; then RDKit/has_xyz and head\n",
    "        # exclude CLS from token pools\n",
    "        h_tok = h[:, :L0, :]                                             # (B,L0,D)\n",
    "        mask_f = valid.float()                                           # (B,L0)\n",
    "\n",
    "        mean = (h_tok * mask_f.unsqueeze(-1)).sum(1) / (mask_f.sum(1, keepdim=True) + 1e-8)  # (B,D)\n",
    "        mmax, _ = (h_tok + (1.0 - mask_f.unsqueeze(-1)) * (-1e4)).max(dim=1)                 # (B,D)\n",
    "\n",
    "        gate_logits = self.gate_pool(h_tok).squeeze(-1)                  # (B,L0)\n",
    "        gate = torch.softmax(gate_logits.masked_fill(~valid, -1e4), dim=1)\n",
    "        attn_pool = (h_tok * gate.unsqueeze(-1)).sum(1)                  # (B,D)\n",
    "\n",
    "        parts = [mean, mmax, attn_pool]\n",
    "\n",
    "        if getattr(self, \"use_cls\", False):\n",
    "            parts.append(h[:, L-1, :])                                   # CLS vector (B,D)\n",
    "\n",
    "        # RDKit globals\n",
    "        rd = data.rdkit_feats.view(B, -1).float()                        # (B, rdkit_dim)\n",
    "        parts.append(rd)\n",
    "\n",
    "        # optional has_xyz scalar if present\n",
    "        if getattr(self, \"use_has_xyz\", False) and hasattr(data, \"has_xyz\"):\n",
    "            parts.append(data.has_xyz.view(B, 1).float())\n",
    "\n",
    "        out = torch.cat(parts, dim=1)\n",
    "        return self.head(out)                                            # (B,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5af9f3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: n=10 tokens=960 sumL2=92160 p50=96 p90=96 p95=96 p99=96 max=96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mem:after_epoch_1] allocated=105.3MB reserved=174.0MB\n",
      "Epoch 001 | tr_MSE 22771.91854 | val_MAE 100.23416 | val_RMSE 130.70927 | R2 -0.7897\n",
      "[mem:after_epoch_2] allocated=127.2MB reserved=168.0MB\n",
      "Epoch 002 | tr_MSE 22484.83189 | val_MAE 98.10220 | val_RMSE 128.20306 | R2 -0.7217\n",
      "[mem:after_epoch_3] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 003 | tr_MSE 20913.60447 | val_MAE 90.39704 | val_RMSE 118.47663 | R2 -0.4704\n",
      "[mem:after_epoch_4] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 004 | tr_MSE 16876.24435 | val_MAE 79.91265 | val_RMSE 100.89502 | R2 -0.0664\n",
      "[mem:after_epoch_5] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 005 | tr_MSE 13248.30115 | val_MAE 83.10323 | val_RMSE 99.83572 | R2 -0.0441\n",
      "[mem:after_epoch_6] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 006 | tr_MSE 13417.76134 | val_MAE 87.05917 | val_RMSE 103.53501 | R2 -0.1229\n",
      "[mem:after_epoch_7] allocated=126.8MB reserved=188.0MB\n",
      "Epoch 007 | tr_MSE 13460.40146 | val_MAE 68.70322 | val_RMSE 86.08450 | R2 0.2237\n",
      "[mem:after_epoch_8] allocated=127.2MB reserved=172.0MB\n",
      "Epoch 008 | tr_MSE 8915.72664 | val_MAE 88.98397 | val_RMSE 107.74438 | R2 -0.2161\n",
      "[mem:after_epoch_9] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 009 | tr_MSE 16010.20029 | val_MAE 64.36108 | val_RMSE 80.43851 | R2 0.3222\n",
      "[mem:after_epoch_10] allocated=126.7MB reserved=192.0MB\n",
      "Epoch 010 | tr_MSE 7824.08335 | val_MAE 65.97728 | val_RMSE 82.33582 | R2 0.2899\n",
      "[mem:after_epoch_11] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 011 | tr_MSE 7638.27585 | val_MAE 61.92658 | val_RMSE 81.54565 | R2 0.3034\n",
      "[mem:after_epoch_12] allocated=127.2MB reserved=172.0MB\n",
      "Epoch 012 | tr_MSE 6648.06962 | val_MAE 58.60330 | val_RMSE 75.83574 | R2 0.3976\n",
      "[mem:after_epoch_13] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 013 | tr_MSE 7473.91698 | val_MAE 60.84827 | val_RMSE 77.31073 | R2 0.3739\n",
      "[mem:after_epoch_14] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 014 | tr_MSE 8798.39971 | val_MAE 91.98001 | val_RMSE 112.57427 | R2 -0.3275\n",
      "[mem:after_epoch_15] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 015 | tr_MSE 7670.37246 | val_MAE 69.93371 | val_RMSE 85.34698 | R2 0.2370\n",
      "[mem:after_epoch_16] allocated=126.7MB reserved=170.0MB\n",
      "Epoch 016 | tr_MSE 7179.30784 | val_MAE 62.72006 | val_RMSE 79.02509 | R2 0.3458\n",
      "[mem:after_epoch_17] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 017 | tr_MSE 8516.71908 | val_MAE 113.67843 | val_RMSE 158.78485 | R2 -1.6411\n",
      "[mem:after_epoch_18] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 018 | tr_MSE 14257.81709 | val_MAE 63.01053 | val_RMSE 82.82143 | R2 0.2815\n",
      "[mem:after_epoch_19] allocated=126.8MB reserved=190.0MB\n",
      "Epoch 019 | tr_MSE 7835.88200 | val_MAE 60.44922 | val_RMSE 77.79648 | R2 0.3660\n",
      "[mem:after_epoch_20] allocated=126.7MB reserved=170.0MB\n",
      "Epoch 020 | tr_MSE 6756.20242 | val_MAE 60.27582 | val_RMSE 76.14690 | R2 0.3926\n",
      "[mem:after_epoch_21] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 021 | tr_MSE 7486.04330 | val_MAE 58.96669 | val_RMSE 76.67853 | R2 0.3841\n",
      "[mem:after_epoch_22] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 022 | tr_MSE 6488.40270 | val_MAE 55.19073 | val_RMSE 70.98790 | R2 0.4721\n",
      "[mem:after_epoch_23] allocated=127.2MB reserved=172.0MB\n",
      "Epoch 023 | tr_MSE 6600.63061 | val_MAE 63.17649 | val_RMSE 79.04766 | R2 0.3454\n",
      "[mem:after_epoch_24] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 024 | tr_MSE 6205.09468 | val_MAE 56.75624 | val_RMSE 69.72594 | R2 0.4907\n",
      "[mem:after_epoch_25] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 025 | tr_MSE 6739.70627 | val_MAE 54.39731 | val_RMSE 68.31837 | R2 0.5111\n",
      "[mem:after_epoch_26] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 026 | tr_MSE 6000.17637 | val_MAE 56.10762 | val_RMSE 70.75220 | R2 0.4756\n",
      "[mem:after_epoch_27] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 027 | tr_MSE 6298.02858 | val_MAE 57.73874 | val_RMSE 72.94540 | R2 0.4426\n",
      "[mem:after_epoch_28] allocated=126.8MB reserved=194.0MB\n",
      "Epoch 028 | tr_MSE 6834.93989 | val_MAE 62.72995 | val_RMSE 80.27757 | R2 0.3249\n",
      "[mem:after_epoch_29] allocated=126.7MB reserved=168.0MB\n",
      "Epoch 029 | tr_MSE 6065.14791 | val_MAE 54.87435 | val_RMSE 67.18204 | R2 0.5272\n",
      "[mem:after_epoch_30] allocated=126.7MB reserved=182.0MB\n",
      "Epoch 030 | tr_MSE 5947.99341 | val_MAE 53.62183 | val_RMSE 66.27710 | R2 0.5399\n",
      "[mem:after_epoch_31] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 031 | tr_MSE 6300.12901 | val_MAE 54.80992 | val_RMSE 67.25835 | R2 0.5261\n",
      "[mem:after_epoch_32] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 032 | tr_MSE 6651.55088 | val_MAE 59.22444 | val_RMSE 74.83594 | R2 0.4133\n",
      "[mem:after_epoch_33] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 033 | tr_MSE 6672.78731 | val_MAE 55.32677 | val_RMSE 69.26154 | R2 0.4975\n",
      "[mem:after_epoch_34] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 034 | tr_MSE 6352.79233 | val_MAE 53.65816 | val_RMSE 66.48311 | R2 0.5370\n",
      "[mem:after_epoch_35] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 035 | tr_MSE 6281.94317 | val_MAE 60.58529 | val_RMSE 77.74081 | R2 0.3669\n",
      "[mem:after_epoch_36] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 036 | tr_MSE 6481.48425 | val_MAE 53.85339 | val_RMSE 67.12299 | R2 0.5280\n",
      "[mem:after_epoch_37] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 037 | tr_MSE 5788.54904 | val_MAE 53.78403 | val_RMSE 68.05731 | R2 0.5148\n",
      "[mem:after_epoch_38] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 038 | tr_MSE 6403.30943 | val_MAE 56.13891 | val_RMSE 71.79875 | R2 0.4600\n",
      "[mem:after_epoch_39] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 039 | tr_MSE 6472.33420 | val_MAE 61.16004 | val_RMSE 77.21725 | R2 0.3754\n",
      "[mem:after_epoch_40] allocated=127.2MB reserved=184.0MB\n",
      "Epoch 040 | tr_MSE 6524.36059 | val_MAE 55.48143 | val_RMSE 70.15633 | R2 0.4844\n",
      "[mem:after_epoch_41] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 041 | tr_MSE 6062.90690 | val_MAE 55.61686 | val_RMSE 68.27855 | R2 0.5116\n",
      "[mem:after_epoch_42] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 042 | tr_MSE 6489.45530 | val_MAE 64.28886 | val_RMSE 82.91988 | R2 0.2797\n",
      "[mem:after_epoch_43] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 043 | tr_MSE 6427.70332 | val_MAE 53.32174 | val_RMSE 67.19669 | R2 0.5270\n",
      "[mem:after_epoch_44] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 044 | tr_MSE 5740.50381 | val_MAE 52.09323 | val_RMSE 64.52492 | R2 0.5639\n",
      "[mem:after_epoch_45] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 045 | tr_MSE 6177.35539 | val_MAE 51.78215 | val_RMSE 64.18133 | R2 0.5685\n",
      "[mem:after_epoch_46] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 046 | tr_MSE 6095.95400 | val_MAE 55.05943 | val_RMSE 71.19157 | R2 0.4691\n",
      "[mem:after_epoch_47] allocated=126.7MB reserved=192.0MB\n",
      "Epoch 047 | tr_MSE 6215.33681 | val_MAE 58.67226 | val_RMSE 75.19431 | R2 0.4077\n",
      "[mem:after_epoch_48] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 048 | tr_MSE 6071.40941 | val_MAE 56.58455 | val_RMSE 71.74236 | R2 0.4608\n",
      "[mem:after_epoch_49] allocated=126.7MB reserved=170.0MB\n",
      "Epoch 049 | tr_MSE 6367.33523 | val_MAE 51.48219 | val_RMSE 65.56237 | R2 0.5497\n",
      "[mem:after_epoch_50] allocated=127.2MB reserved=184.0MB\n",
      "Epoch 050 | tr_MSE 5485.48347 | val_MAE 54.09978 | val_RMSE 69.37041 | R2 0.4959\n",
      "[mem:after_epoch_51] allocated=127.4MB reserved=170.0MB\n",
      "Epoch 051 | tr_MSE 6088.58714 | val_MAE 50.22115 | val_RMSE 65.09954 | R2 0.5561\n",
      "[mem:after_epoch_52] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 052 | tr_MSE 5760.77739 | val_MAE 53.14807 | val_RMSE 67.43358 | R2 0.5237\n",
      "[mem:after_epoch_53] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 053 | tr_MSE 6081.78213 | val_MAE 50.99339 | val_RMSE 63.12603 | R2 0.5826\n",
      "[mem:after_epoch_54] allocated=126.8MB reserved=188.0MB\n",
      "Epoch 054 | tr_MSE 5645.23018 | val_MAE 52.25891 | val_RMSE 65.43826 | R2 0.5514\n",
      "[mem:after_epoch_55] allocated=126.7MB reserved=172.0MB\n",
      "Epoch 055 | tr_MSE 6074.00720 | val_MAE 54.87832 | val_RMSE 70.36749 | R2 0.4813\n",
      "[mem:after_epoch_56] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 056 | tr_MSE 5622.22883 | val_MAE 52.54794 | val_RMSE 65.93928 | R2 0.5445\n",
      "[mem:after_epoch_57] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 057 | tr_MSE 5569.98960 | val_MAE 54.73158 | val_RMSE 68.50166 | R2 0.5084\n",
      "[mem:after_epoch_58] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 058 | tr_MSE 5691.29806 | val_MAE 50.88476 | val_RMSE 63.22713 | R2 0.5812\n",
      "[mem:after_epoch_59] allocated=126.7MB reserved=172.0MB\n",
      "Epoch 059 | tr_MSE 5880.36475 | val_MAE 61.92669 | val_RMSE 76.41689 | R2 0.3883\n",
      "[mem:after_epoch_60] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 060 | tr_MSE 6128.17246 | val_MAE 59.07334 | val_RMSE 73.33392 | R2 0.4366\n",
      "[mem:after_epoch_61] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 061 | tr_MSE 6840.04058 | val_MAE 54.43192 | val_RMSE 70.15871 | R2 0.4844\n",
      "[mem:after_epoch_62] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 062 | tr_MSE 6245.75868 | val_MAE 49.55701 | val_RMSE 61.66183 | R2 0.6017\n",
      "[mem:after_epoch_63] allocated=127.2MB reserved=170.0MB\n",
      "Epoch 063 | tr_MSE 5602.91287 | val_MAE 52.93226 | val_RMSE 65.98058 | R2 0.5440\n",
      "[mem:after_epoch_64] allocated=127.2MB reserved=184.0MB\n",
      "Epoch 064 | tr_MSE 5355.33206 | val_MAE 55.80486 | val_RMSE 69.13939 | R2 0.4992\n",
      "[mem:after_epoch_65] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 065 | tr_MSE 6096.12360 | val_MAE 67.88051 | val_RMSE 90.68343 | R2 0.1386\n",
      "[mem:after_epoch_66] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 066 | tr_MSE 6312.98278 | val_MAE 58.77848 | val_RMSE 74.00340 | R2 0.4263\n",
      "[mem:after_epoch_67] allocated=127.4MB reserved=170.0MB\n",
      "Epoch 067 | tr_MSE 6910.71646 | val_MAE 52.04628 | val_RMSE 64.97708 | R2 0.5577\n",
      "[mem:after_epoch_68] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 068 | tr_MSE 6509.33662 | val_MAE 50.72480 | val_RMSE 63.37423 | R2 0.5793\n",
      "[mem:after_epoch_69] allocated=127.4MB reserved=170.0MB\n",
      "Epoch 069 | tr_MSE 6148.65636 | val_MAE 56.31845 | val_RMSE 76.69027 | R2 0.3839\n",
      "[mem:after_epoch_70] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 070 | tr_MSE 6587.12439 | val_MAE 51.57281 | val_RMSE 63.52890 | R2 0.5772\n",
      "[mem:after_epoch_71] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 071 | tr_MSE 5908.51846 | val_MAE 49.30116 | val_RMSE 62.00787 | R2 0.5972\n",
      "[mem:after_epoch_72] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 072 | tr_MSE 5723.69598 | val_MAE 68.82731 | val_RMSE 87.29597 | R2 0.2017\n",
      "[mem:after_epoch_73] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 073 | tr_MSE 6050.91602 | val_MAE 54.54102 | val_RMSE 68.33519 | R2 0.5108\n",
      "[mem:after_epoch_74] allocated=126.8MB reserved=194.0MB\n",
      "Epoch 074 | tr_MSE 6003.27848 | val_MAE 52.48363 | val_RMSE 64.19456 | R2 0.5683\n",
      "[mem:after_epoch_75] allocated=126.7MB reserved=172.0MB\n",
      "Epoch 075 | tr_MSE 6050.70908 | val_MAE 56.81645 | val_RMSE 71.93134 | R2 0.4580\n",
      "[mem:after_epoch_76] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 076 | tr_MSE 7286.10129 | val_MAE 62.59438 | val_RMSE 76.94657 | R2 0.3798\n",
      "[mem:after_epoch_77] allocated=126.7MB reserved=194.0MB\n",
      "Epoch 077 | tr_MSE 6315.18543 | val_MAE 56.61007 | val_RMSE 70.44074 | R2 0.4802\n",
      "[mem:after_epoch_78] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 078 | tr_MSE 5905.79982 | val_MAE 55.28681 | val_RMSE 70.06776 | R2 0.4857\n",
      "[mem:after_epoch_79] allocated=126.7MB reserved=176.0MB\n",
      "Epoch 079 | tr_MSE 5946.51765 | val_MAE 53.05952 | val_RMSE 66.84924 | R2 0.5319\n",
      "[mem:after_epoch_80] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 080 | tr_MSE 5719.79231 | val_MAE 60.82135 | val_RMSE 80.89847 | R2 0.3144\n",
      "[mem:after_epoch_81] allocated=126.7MB reserved=194.0MB\n",
      "Epoch 081 | tr_MSE 5858.94898 | val_MAE 62.40768 | val_RMSE 82.67085 | R2 0.2841\n",
      "[mem:after_epoch_82] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 082 | tr_MSE 7210.03894 | val_MAE 54.26023 | val_RMSE 67.66398 | R2 0.5204\n",
      "[mem:after_epoch_83] allocated=126.7MB reserved=174.0MB\n",
      "Epoch 083 | tr_MSE 6519.81468 | val_MAE 48.93810 | val_RMSE 60.40992 | R2 0.6177\n",
      "[mem:after_epoch_84] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 084 | tr_MSE 5357.56715 | val_MAE 50.64238 | val_RMSE 63.10115 | R2 0.5829\n",
      "[mem:after_epoch_85] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 085 | tr_MSE 5284.50780 | val_MAE 55.05289 | val_RMSE 69.01536 | R2 0.5010\n",
      "[mem:after_epoch_86] allocated=127.2MB reserved=182.0MB\n",
      "Epoch 086 | tr_MSE 6062.02074 | val_MAE 49.73214 | val_RMSE 62.11857 | R2 0.5958\n",
      "[mem:after_epoch_87] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 087 | tr_MSE 5433.82080 | val_MAE 57.33671 | val_RMSE 72.63416 | R2 0.4473\n",
      "[mem:after_epoch_88] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 088 | tr_MSE 5337.98971 | val_MAE 54.10834 | val_RMSE 67.29674 | R2 0.5256\n",
      "[mem:after_epoch_89] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 089 | tr_MSE 5724.26975 | val_MAE 53.07697 | val_RMSE 63.78484 | R2 0.5738\n",
      "[mem:after_epoch_90] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 090 | tr_MSE 5404.13568 | val_MAE 54.76381 | val_RMSE 66.07800 | R2 0.5426\n",
      "[mem:after_epoch_91] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 091 | tr_MSE 5024.78315 | val_MAE 53.71951 | val_RMSE 64.76582 | R2 0.5606\n",
      "[mem:after_epoch_92] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 092 | tr_MSE 5363.88560 | val_MAE 53.81314 | val_RMSE 67.11422 | R2 0.5282\n",
      "[mem:after_epoch_93] allocated=127.4MB reserved=170.0MB\n",
      "Epoch 093 | tr_MSE 5156.64745 | val_MAE 53.81431 | val_RMSE 67.07864 | R2 0.5287\n",
      "[mem:after_epoch_94] allocated=127.2MB reserved=184.0MB\n",
      "Epoch 094 | tr_MSE 5265.42328 | val_MAE 56.83682 | val_RMSE 72.25431 | R2 0.4531\n",
      "[mem:after_epoch_95] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 095 | tr_MSE 5156.85265 | val_MAE 56.05285 | val_RMSE 71.35857 | R2 0.4666\n",
      "[mem:after_epoch_96] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 096 | tr_MSE 6109.24435 | val_MAE 52.90182 | val_RMSE 66.76656 | R2 0.5330\n",
      "[mem:after_epoch_97] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 097 | tr_MSE 4906.01659 | val_MAE 53.83730 | val_RMSE 69.37329 | R2 0.4959\n",
      "[mem:after_epoch_98] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 098 | tr_MSE 5063.52942 | val_MAE 51.76871 | val_RMSE 65.61436 | R2 0.5490\n",
      "[mem:after_epoch_99] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 099 | tr_MSE 4672.98016 | val_MAE 52.49741 | val_RMSE 65.92330 | R2 0.5448\n",
      "[mem:after_epoch_100] allocated=127.2MB reserved=190.0MB\n",
      "Epoch 100 | tr_MSE 4740.34588 | val_MAE 52.27885 | val_RMSE 65.03804 | R2 0.5569\n",
      "[mem:after_epoch_101] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 101 | tr_MSE 4565.38564 | val_MAE 58.75188 | val_RMSE 74.54874 | R2 0.4178\n",
      "[mem:after_epoch_102] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 102 | tr_MSE 5747.50743 | val_MAE 52.09703 | val_RMSE 64.84029 | R2 0.5596\n",
      "[mem:after_epoch_103] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 103 | tr_MSE 4426.71316 | val_MAE 56.30799 | val_RMSE 72.22916 | R2 0.4535\n",
      "Early stopping.\n",
      "[graphtransformer_tg_spd] Best Val — MAE 48.938103 | RMSE 60.409924 | R2 0.6177\n",
      "[mem:after_Tg] allocated=16.2MB reserved=40.0MB\n",
      "L: n=520 tokens=3840 sumL2=29700 p50=8 p90=9 p95=9 p99=9 max=9\n",
      "[mem:after_epoch_1] allocated=102.7MB reserved=166.0MB\n",
      "Epoch 001 | tr_MSE 0.72563 | val_MAE 0.20168 | val_RMSE 0.24377 | R2 -2.3840\n",
      "[mem:after_epoch_2] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 002 | tr_MSE 0.03846 | val_MAE 0.10437 | val_RMSE 0.13605 | R2 -0.0541\n",
      "[mem:after_epoch_3] allocated=124.8MB reserved=174.0MB\n",
      "Epoch 003 | tr_MSE 0.03523 | val_MAE 0.11825 | val_RMSE 0.15236 | R2 -0.3219\n",
      "[mem:after_epoch_4] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 004 | tr_MSE 0.02897 | val_MAE 0.08230 | val_RMSE 0.11173 | R2 0.2891\n",
      "[mem:after_epoch_5] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 005 | tr_MSE 0.01720 | val_MAE 0.07077 | val_RMSE 0.10224 | R2 0.4048\n",
      "[mem:after_epoch_6] allocated=124.8MB reserved=186.0MB\n",
      "Epoch 006 | tr_MSE 0.01801 | val_MAE 0.09036 | val_RMSE 0.12712 | R2 0.0798\n",
      "[mem:after_epoch_7] allocated=124.8MB reserved=174.0MB\n",
      "Epoch 007 | tr_MSE 0.02643 | val_MAE 0.07328 | val_RMSE 0.10018 | R2 0.4285\n",
      "[mem:after_epoch_8] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 008 | tr_MSE 0.01422 | val_MAE 0.09727 | val_RMSE 0.13168 | R2 0.0126\n",
      "[mem:after_epoch_9] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 009 | tr_MSE 0.01953 | val_MAE 0.17922 | val_RMSE 0.21258 | R2 -1.5734\n",
      "[mem:after_epoch_10] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 010 | tr_MSE 0.02964 | val_MAE 0.10737 | val_RMSE 0.16163 | R2 -0.4877\n",
      "[mem:after_epoch_11] allocated=124.8MB reserved=178.0MB\n",
      "Epoch 011 | tr_MSE 0.02714 | val_MAE 0.09891 | val_RMSE 0.13586 | R2 -0.0511\n",
      "[mem:after_epoch_12] allocated=124.8MB reserved=184.0MB\n",
      "Epoch 012 | tr_MSE 0.01254 | val_MAE 0.08715 | val_RMSE 0.10869 | R2 0.3272\n",
      "[mem:after_epoch_13] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 013 | tr_MSE 0.01125 | val_MAE 0.13189 | val_RMSE 0.15879 | R2 -0.4359\n",
      "[mem:after_epoch_14] allocated=124.8MB reserved=178.0MB\n",
      "Epoch 014 | tr_MSE 0.01649 | val_MAE 0.14265 | val_RMSE 0.17898 | R2 -0.8242\n",
      "[mem:after_epoch_15] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 015 | tr_MSE 0.01716 | val_MAE 0.10020 | val_RMSE 0.14020 | R2 -0.1194\n",
      "[mem:after_epoch_16] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 016 | tr_MSE 0.01648 | val_MAE 0.07142 | val_RMSE 0.10079 | R2 0.4215\n",
      "[mem:after_epoch_17] allocated=124.8MB reserved=178.0MB\n",
      "Epoch 017 | tr_MSE 0.00765 | val_MAE 0.06051 | val_RMSE 0.09249 | R2 0.5128\n",
      "[mem:after_epoch_18] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 018 | tr_MSE 0.00836 | val_MAE 0.08501 | val_RMSE 0.13277 | R2 -0.0039\n",
      "[mem:after_epoch_19] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 019 | tr_MSE 0.00821 | val_MAE 0.06235 | val_RMSE 0.10887 | R2 0.3251\n",
      "[mem:after_epoch_20] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 020 | tr_MSE 0.01126 | val_MAE 0.06454 | val_RMSE 0.10340 | R2 0.3912\n",
      "[mem:after_epoch_21] allocated=124.2MB reserved=178.0MB\n",
      "Epoch 021 | tr_MSE 0.00764 | val_MAE 0.04642 | val_RMSE 0.08474 | R2 0.5911\n",
      "[mem:after_epoch_22] allocated=124.8MB reserved=178.0MB\n",
      "Epoch 022 | tr_MSE 0.00586 | val_MAE 0.04756 | val_RMSE 0.08552 | R2 0.5835\n",
      "[mem:after_epoch_23] allocated=124.8MB reserved=178.0MB\n",
      "Epoch 023 | tr_MSE 0.00550 | val_MAE 0.04244 | val_RMSE 0.08162 | R2 0.6207\n",
      "[mem:after_epoch_24] allocated=124.2MB reserved=178.0MB\n",
      "Epoch 024 | tr_MSE 0.00506 | val_MAE 0.04164 | val_RMSE 0.08011 | R2 0.6345\n",
      "[mem:after_epoch_25] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 025 | tr_MSE 0.00474 | val_MAE 0.04223 | val_RMSE 0.08209 | R2 0.6162\n",
      "[mem:after_epoch_26] allocated=124.8MB reserved=184.0MB\n",
      "Epoch 026 | tr_MSE 0.00431 | val_MAE 0.04330 | val_RMSE 0.08524 | R2 0.5862\n",
      "[mem:after_epoch_27] allocated=124.8MB reserved=174.0MB\n",
      "Epoch 027 | tr_MSE 0.00469 | val_MAE 0.04297 | val_RMSE 0.08183 | R2 0.6187\n",
      "[mem:after_epoch_28] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 028 | tr_MSE 0.00547 | val_MAE 0.05985 | val_RMSE 0.09212 | R2 0.5168\n",
      "[mem:after_epoch_29] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 029 | tr_MSE 0.00622 | val_MAE 0.08228 | val_RMSE 0.11341 | R2 0.2676\n",
      "[mem:after_epoch_30] allocated=124.8MB reserved=184.0MB\n",
      "Epoch 030 | tr_MSE 0.00790 | val_MAE 0.08683 | val_RMSE 0.11969 | R2 0.1842\n",
      "[mem:after_epoch_31] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 031 | tr_MSE 0.01158 | val_MAE 0.05929 | val_RMSE 0.10271 | R2 0.3993\n",
      "[mem:after_epoch_32] allocated=124.8MB reserved=186.0MB\n",
      "Epoch 032 | tr_MSE 0.00626 | val_MAE 0.04227 | val_RMSE 0.08391 | R2 0.5990\n",
      "[mem:after_epoch_33] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 033 | tr_MSE 0.00572 | val_MAE 0.04599 | val_RMSE 0.08507 | R2 0.5879\n",
      "[mem:after_epoch_34] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 034 | tr_MSE 0.00528 | val_MAE 0.05814 | val_RMSE 0.10249 | R2 0.4018\n",
      "[mem:after_epoch_35] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 035 | tr_MSE 0.00539 | val_MAE 0.06529 | val_RMSE 0.09904 | R2 0.4414\n",
      "[mem:after_epoch_36] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 036 | tr_MSE 0.00626 | val_MAE 0.04832 | val_RMSE 0.08993 | R2 0.5395\n",
      "[mem:after_epoch_37] allocated=124.8MB reserved=176.0MB\n",
      "Epoch 037 | tr_MSE 0.00462 | val_MAE 0.04196 | val_RMSE 0.08404 | R2 0.5978\n",
      "[mem:after_epoch_38] allocated=124.8MB reserved=184.0MB\n",
      "Epoch 038 | tr_MSE 0.00414 | val_MAE 0.04226 | val_RMSE 0.08611 | R2 0.5778\n",
      "[mem:after_epoch_39] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 039 | tr_MSE 0.00439 | val_MAE 0.06255 | val_RMSE 0.11015 | R2 0.3091\n",
      "[mem:after_epoch_40] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 040 | tr_MSE 0.00503 | val_MAE 0.09145 | val_RMSE 0.11897 | R2 0.1940\n",
      "[mem:after_epoch_41] allocated=124.8MB reserved=184.0MB\n",
      "Epoch 041 | tr_MSE 0.00813 | val_MAE 0.07571 | val_RMSE 0.11266 | R2 0.2772\n",
      "[mem:after_epoch_42] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 042 | tr_MSE 0.00675 | val_MAE 0.06334 | val_RMSE 0.09690 | R2 0.4653\n",
      "[mem:after_epoch_43] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 043 | tr_MSE 0.00642 | val_MAE 0.04492 | val_RMSE 0.10078 | R2 0.4216\n",
      "[mem:after_epoch_44] allocated=124.8MB reserved=186.0MB\n",
      "Epoch 044 | tr_MSE 0.00513 | val_MAE 0.04982 | val_RMSE 0.10236 | R2 0.4033\n",
      "Early stopping.\n",
      "[graphtransformer_den_spd] Best Val — MAE 0.041639 | RMSE 0.080110 | R2 0.6345\n",
      "[mem:after_Density] allocated=16.2MB reserved=40.0MB\n",
      "L: n=510 tokens=3830 sumL2=29950 p50=8 p90=9 p95=9 p99=9 max=9\n",
      "[mem:after_epoch_1] allocated=103.1MB reserved=178.0MB\n",
      "Epoch 001 | tr_MSE 285.31449 | val_MAE 15.97740 | val_RMSE 16.66257 | R2 -11.6150\n",
      "[mem:after_epoch_2] allocated=125.0MB reserved=370.0MB\n",
      "Epoch 002 | tr_MSE 236.37985 | val_MAE 11.06715 | val_RMSE 12.18165 | R2 -5.7424\n",
      "[mem:after_epoch_3] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 003 | tr_MSE 72.18914 | val_MAE 5.95087 | val_RMSE 6.90924 | R2 -1.1690\n",
      "[mem:after_epoch_4] allocated=124.7MB reserved=328.0MB\n",
      "Epoch 004 | tr_MSE 65.86757 | val_MAE 4.23129 | val_RMSE 4.84845 | R2 -0.0681\n",
      "[mem:after_epoch_5] allocated=124.6MB reserved=324.0MB\n",
      "Epoch 005 | tr_MSE 25.25410 | val_MAE 4.30095 | val_RMSE 4.96800 | R2 -0.1214\n",
      "[mem:after_epoch_6] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 006 | tr_MSE 30.38996 | val_MAE 4.34267 | val_RMSE 5.07326 | R2 -0.1694\n",
      "[mem:after_epoch_7] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 007 | tr_MSE 34.92482 | val_MAE 3.85002 | val_RMSE 4.53203 | R2 0.0668\n",
      "[mem:after_epoch_8] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 008 | tr_MSE 35.58306 | val_MAE 3.58967 | val_RMSE 4.58139 | R2 0.0463\n",
      "[mem:after_epoch_9] allocated=124.7MB reserved=326.0MB\n",
      "Epoch 009 | tr_MSE 25.87293 | val_MAE 3.70090 | val_RMSE 4.38321 | R2 0.1271\n",
      "[mem:after_epoch_10] allocated=124.8MB reserved=328.0MB\n",
      "Epoch 010 | tr_MSE 40.45455 | val_MAE 3.43244 | val_RMSE 4.27807 | R2 0.1684\n",
      "[mem:after_epoch_11] allocated=124.6MB reserved=328.0MB\n",
      "Epoch 011 | tr_MSE 30.92795 | val_MAE 3.41690 | val_RMSE 4.38602 | R2 0.1259\n",
      "[mem:after_epoch_12] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 012 | tr_MSE 23.76019 | val_MAE 2.91243 | val_RMSE 3.75986 | R2 0.3577\n",
      "[mem:after_epoch_13] allocated=124.7MB reserved=324.0MB\n",
      "Epoch 013 | tr_MSE 28.02171 | val_MAE 3.52588 | val_RMSE 4.28577 | R2 0.1654\n",
      "[mem:after_epoch_14] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 014 | tr_MSE 19.92670 | val_MAE 3.55043 | val_RMSE 4.24436 | R2 0.1815\n",
      "[mem:after_epoch_15] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 015 | tr_MSE 16.23270 | val_MAE 2.71706 | val_RMSE 3.64759 | R2 0.3955\n",
      "[mem:after_epoch_16] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 016 | tr_MSE 14.42028 | val_MAE 2.31833 | val_RMSE 3.10539 | R2 0.5618\n",
      "[mem:after_epoch_17] allocated=125.1MB reserved=176.0MB\n",
      "Epoch 017 | tr_MSE 13.92566 | val_MAE 3.02981 | val_RMSE 3.79084 | R2 0.3471\n",
      "[mem:after_epoch_18] allocated=125.1MB reserved=182.0MB\n",
      "Epoch 018 | tr_MSE 16.18638 | val_MAE 4.53030 | val_RMSE 5.15606 | R2 -0.2079\n",
      "[mem:after_epoch_19] allocated=125.1MB reserved=182.0MB\n",
      "Epoch 019 | tr_MSE 17.37227 | val_MAE 3.33570 | val_RMSE 4.00711 | R2 0.2704\n",
      "[mem:after_epoch_20] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 020 | tr_MSE 15.12266 | val_MAE 3.70100 | val_RMSE 4.39289 | R2 0.1232\n",
      "[mem:after_epoch_21] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 021 | tr_MSE 13.57800 | val_MAE 2.61871 | val_RMSE 3.34924 | R2 0.4903\n",
      "[mem:after_epoch_22] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 022 | tr_MSE 12.26633 | val_MAE 2.52406 | val_RMSE 3.41043 | R2 0.4715\n",
      "[mem:after_epoch_23] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 023 | tr_MSE 12.39334 | val_MAE 2.64593 | val_RMSE 3.44337 | R2 0.4613\n",
      "[mem:after_epoch_24] allocated=125.1MB reserved=178.0MB\n",
      "Epoch 024 | tr_MSE 11.92864 | val_MAE 3.07165 | val_RMSE 3.65075 | R2 0.3944\n",
      "[mem:after_epoch_25] allocated=125.1MB reserved=182.0MB\n",
      "Epoch 025 | tr_MSE 13.00010 | val_MAE 2.21729 | val_RMSE 3.04942 | R2 0.5775\n",
      "[mem:after_epoch_26] allocated=124.7MB reserved=326.0MB\n",
      "Epoch 026 | tr_MSE 13.15396 | val_MAE 2.96944 | val_RMSE 3.63179 | R2 0.4007\n",
      "[mem:after_epoch_27] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 027 | tr_MSE 13.50388 | val_MAE 2.01606 | val_RMSE 2.89673 | R2 0.6187\n",
      "[mem:after_epoch_28] allocated=124.6MB reserved=328.0MB\n",
      "Epoch 028 | tr_MSE 10.94045 | val_MAE 2.26363 | val_RMSE 2.99113 | R2 0.5935\n",
      "[mem:after_epoch_29] allocated=124.6MB reserved=324.0MB\n",
      "Epoch 029 | tr_MSE 12.60362 | val_MAE 3.08363 | val_RMSE 3.68353 | R2 0.3835\n",
      "[mem:after_epoch_30] allocated=124.6MB reserved=326.0MB\n",
      "Epoch 030 | tr_MSE 12.17081 | val_MAE 2.26716 | val_RMSE 2.93893 | R2 0.6076\n",
      "[mem:after_epoch_31] allocated=124.6MB reserved=326.0MB\n",
      "Epoch 031 | tr_MSE 9.88020 | val_MAE 2.31898 | val_RMSE 3.03574 | R2 0.5813\n",
      "[mem:after_epoch_32] allocated=124.6MB reserved=324.0MB\n",
      "Epoch 032 | tr_MSE 9.93030 | val_MAE 2.72337 | val_RMSE 3.34910 | R2 0.4904\n",
      "[mem:after_epoch_33] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 033 | tr_MSE 9.56141 | val_MAE 2.74588 | val_RMSE 3.61717 | R2 0.4055\n",
      "[mem:after_epoch_34] allocated=124.6MB reserved=324.0MB\n",
      "Epoch 034 | tr_MSE 13.37378 | val_MAE 2.58888 | val_RMSE 3.21143 | R2 0.5314\n",
      "[mem:after_epoch_35] allocated=124.6MB reserved=326.0MB\n",
      "Epoch 035 | tr_MSE 10.56799 | val_MAE 1.92699 | val_RMSE 2.75592 | R2 0.6549\n",
      "[mem:after_epoch_36] allocated=125.1MB reserved=178.0MB\n",
      "Epoch 036 | tr_MSE 8.99895 | val_MAE 2.31490 | val_RMSE 3.06306 | R2 0.5737\n",
      "[mem:after_epoch_37] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 037 | tr_MSE 11.89824 | val_MAE 2.28000 | val_RMSE 3.00899 | R2 0.5886\n",
      "[mem:after_epoch_38] allocated=125.1MB reserved=178.0MB\n",
      "Epoch 038 | tr_MSE 9.13384 | val_MAE 1.89789 | val_RMSE 2.82006 | R2 0.6387\n",
      "[mem:after_epoch_39] allocated=124.7MB reserved=322.0MB\n",
      "Epoch 039 | tr_MSE 9.31905 | val_MAE 2.67832 | val_RMSE 3.35566 | R2 0.4884\n",
      "[mem:after_epoch_40] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 040 | tr_MSE 9.48823 | val_MAE 2.00232 | val_RMSE 2.79239 | R2 0.6457\n",
      "[mem:after_epoch_41] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 041 | tr_MSE 8.30821 | val_MAE 1.99538 | val_RMSE 2.77067 | R2 0.6512\n",
      "[mem:after_epoch_42] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 042 | tr_MSE 8.32069 | val_MAE 1.98975 | val_RMSE 2.77381 | R2 0.6504\n",
      "[mem:after_epoch_43] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 043 | tr_MSE 7.25952 | val_MAE 2.24144 | val_RMSE 2.97504 | R2 0.5978\n",
      "[mem:after_epoch_44] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 044 | tr_MSE 8.36009 | val_MAE 2.37040 | val_RMSE 3.21861 | R2 0.5293\n",
      "[mem:after_epoch_45] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 045 | tr_MSE 7.70654 | val_MAE 2.37027 | val_RMSE 3.23441 | R2 0.5247\n",
      "[mem:after_epoch_46] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 046 | tr_MSE 8.38947 | val_MAE 2.14041 | val_RMSE 2.95075 | R2 0.6044\n",
      "[mem:after_epoch_47] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 047 | tr_MSE 7.49706 | val_MAE 2.21389 | val_RMSE 2.90128 | R2 0.6175\n",
      "[mem:after_epoch_48] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 048 | tr_MSE 8.92704 | val_MAE 2.72071 | val_RMSE 3.38416 | R2 0.4796\n",
      "[mem:after_epoch_49] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 049 | tr_MSE 10.03713 | val_MAE 2.97990 | val_RMSE 3.51207 | R2 0.4396\n",
      "[mem:after_epoch_50] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 050 | tr_MSE 8.16588 | val_MAE 2.05221 | val_RMSE 2.74990 | R2 0.6564\n",
      "[mem:after_epoch_51] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 051 | tr_MSE 6.56780 | val_MAE 2.60262 | val_RMSE 3.31775 | R2 0.4999\n",
      "[mem:after_epoch_52] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 052 | tr_MSE 8.83392 | val_MAE 2.85165 | val_RMSE 3.39031 | R2 0.4777\n",
      "[mem:after_epoch_53] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 053 | tr_MSE 6.83789 | val_MAE 1.96811 | val_RMSE 2.69909 | R2 0.6690\n",
      "[mem:after_epoch_54] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 054 | tr_MSE 6.89643 | val_MAE 2.42956 | val_RMSE 3.06671 | R2 0.5727\n",
      "[mem:after_epoch_55] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 055 | tr_MSE 6.62822 | val_MAE 2.30960 | val_RMSE 2.94805 | R2 0.6051\n",
      "[mem:after_epoch_56] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 056 | tr_MSE 6.64196 | val_MAE 2.93593 | val_RMSE 3.51191 | R2 0.4396\n",
      "[mem:after_epoch_57] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 057 | tr_MSE 6.17679 | val_MAE 2.27192 | val_RMSE 3.00451 | R2 0.5898\n",
      "[mem:after_epoch_58] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 058 | tr_MSE 6.48667 | val_MAE 2.10015 | val_RMSE 2.70663 | R2 0.6671\n",
      "Early stopping.\n",
      "[graphtransformer_rg_spd] Best Val — MAE 1.897893 | RMSE 2.820058 | R2 0.6387\n",
      "[mem:after_Rg] allocated=16.2MB reserved=40.0MB\n",
      "L: n=620 tokens=4700 sumL2=36920 p50=8 p90=9 p95=9 p99=9 max=9\n",
      "[mem:after_epoch_1] allocated=104.3MB reserved=146.0MB\n",
      "Epoch 001 | tr_MSE 0.02573 | val_MAE 0.14625 | val_RMSE 0.17744 | R2 -2.6871\n",
      "[mem:after_epoch_2] allocated=126.4MB reserved=358.0MB\n",
      "Epoch 002 | tr_MSE 0.02556 | val_MAE 0.13729 | val_RMSE 0.16086 | R2 -2.0304\n",
      "[mem:after_epoch_3] allocated=126.3MB reserved=358.0MB\n",
      "Epoch 003 | tr_MSE 0.01302 | val_MAE 0.10568 | val_RMSE 0.12651 | R2 -0.8742\n",
      "[mem:after_epoch_4] allocated=126.2MB reserved=348.0MB\n",
      "Epoch 004 | tr_MSE 0.00899 | val_MAE 0.09055 | val_RMSE 0.11072 | R2 -0.4355\n",
      "[mem:after_epoch_5] allocated=126.4MB reserved=348.0MB\n",
      "Epoch 005 | tr_MSE 0.00746 | val_MAE 0.07285 | val_RMSE 0.09456 | R2 -0.0472\n",
      "[mem:after_epoch_6] allocated=126.2MB reserved=170.0MB\n",
      "Epoch 006 | tr_MSE 0.00592 | val_MAE 0.05138 | val_RMSE 0.07065 | R2 0.4154\n",
      "[mem:after_epoch_7] allocated=126.4MB reserved=358.0MB\n",
      "Epoch 007 | tr_MSE 0.00413 | val_MAE 0.05511 | val_RMSE 0.07865 | R2 0.2756\n",
      "[mem:after_epoch_8] allocated=126.4MB reserved=358.0MB\n",
      "Epoch 008 | tr_MSE 0.00681 | val_MAE 0.03948 | val_RMSE 0.05428 | R2 0.6550\n",
      "[mem:after_epoch_9] allocated=126.2MB reserved=344.0MB\n",
      "Epoch 009 | tr_MSE 0.00571 | val_MAE 0.06218 | val_RMSE 0.07556 | R2 0.3315\n",
      "[mem:after_epoch_10] allocated=126.2MB reserved=170.0MB\n",
      "Epoch 010 | tr_MSE 0.00358 | val_MAE 0.04582 | val_RMSE 0.06385 | R2 0.5225\n",
      "[mem:after_epoch_11] allocated=126.3MB reserved=358.0MB\n",
      "Epoch 011 | tr_MSE 0.00385 | val_MAE 0.04613 | val_RMSE 0.06123 | R2 0.5609\n",
      "[mem:after_epoch_12] allocated=126.2MB reserved=348.0MB\n",
      "Epoch 012 | tr_MSE 0.00368 | val_MAE 0.03663 | val_RMSE 0.04813 | R2 0.7287\n",
      "[mem:after_epoch_13] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 013 | tr_MSE 0.00340 | val_MAE 0.04308 | val_RMSE 0.05723 | R2 0.6164\n",
      "[mem:after_epoch_14] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 014 | tr_MSE 0.00245 | val_MAE 0.03765 | val_RMSE 0.05271 | R2 0.6746\n",
      "[mem:after_epoch_15] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 015 | tr_MSE 0.00227 | val_MAE 0.03671 | val_RMSE 0.04925 | R2 0.7160\n",
      "[mem:after_epoch_16] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 016 | tr_MSE 0.00195 | val_MAE 0.03167 | val_RMSE 0.04527 | R2 0.7601\n",
      "[mem:after_epoch_17] allocated=126.2MB reserved=170.0MB\n",
      "Epoch 017 | tr_MSE 0.00205 | val_MAE 0.03171 | val_RMSE 0.04603 | R2 0.7519\n",
      "[mem:after_epoch_18] allocated=126.3MB reserved=360.0MB\n",
      "Epoch 018 | tr_MSE 0.00231 | val_MAE 0.03789 | val_RMSE 0.05218 | R2 0.6812\n",
      "[mem:after_epoch_19] allocated=126.2MB reserved=350.0MB\n",
      "Epoch 019 | tr_MSE 0.00201 | val_MAE 0.03400 | val_RMSE 0.04724 | R2 0.7387\n",
      "[mem:after_epoch_20] allocated=126.2MB reserved=170.0MB\n",
      "Epoch 020 | tr_MSE 0.00190 | val_MAE 0.03239 | val_RMSE 0.04687 | R2 0.7427\n",
      "[mem:after_epoch_21] allocated=126.3MB reserved=362.0MB\n",
      "Epoch 021 | tr_MSE 0.00196 | val_MAE 0.03556 | val_RMSE 0.05043 | R2 0.7022\n",
      "[mem:after_epoch_22] allocated=126.2MB reserved=348.0MB\n",
      "Epoch 022 | tr_MSE 0.00189 | val_MAE 0.03003 | val_RMSE 0.04380 | R2 0.7753\n",
      "[mem:after_epoch_23] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 023 | tr_MSE 0.00209 | val_MAE 0.03205 | val_RMSE 0.04728 | R2 0.7383\n",
      "[mem:after_epoch_24] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 024 | tr_MSE 0.00218 | val_MAE 0.03597 | val_RMSE 0.04978 | R2 0.7097\n",
      "[mem:after_epoch_25] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 025 | tr_MSE 0.00191 | val_MAE 0.03636 | val_RMSE 0.05023 | R2 0.7045\n",
      "[mem:after_epoch_26] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 026 | tr_MSE 0.00164 | val_MAE 0.03302 | val_RMSE 0.04632 | R2 0.7488\n",
      "[mem:after_epoch_27] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 027 | tr_MSE 0.00166 | val_MAE 0.03106 | val_RMSE 0.04574 | R2 0.7550\n",
      "[mem:after_epoch_28] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 028 | tr_MSE 0.00158 | val_MAE 0.02986 | val_RMSE 0.04574 | R2 0.7550\n",
      "[mem:after_epoch_29] allocated=126.2MB reserved=170.0MB\n",
      "Epoch 029 | tr_MSE 0.00172 | val_MAE 0.02913 | val_RMSE 0.04333 | R2 0.7801\n",
      "[mem:after_epoch_30] allocated=126.4MB reserved=358.0MB\n",
      "Epoch 030 | tr_MSE 0.00210 | val_MAE 0.03305 | val_RMSE 0.04736 | R2 0.7373\n",
      "[mem:after_epoch_31] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 031 | tr_MSE 0.00248 | val_MAE 0.04845 | val_RMSE 0.06791 | R2 0.4600\n",
      "[mem:after_epoch_32] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 032 | tr_MSE 0.00313 | val_MAE 0.03546 | val_RMSE 0.05212 | R2 0.6818\n",
      "[mem:after_epoch_33] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 033 | tr_MSE 0.00235 | val_MAE 0.05297 | val_RMSE 0.06348 | R2 0.5281\n",
      "[mem:after_epoch_34] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 034 | tr_MSE 0.00550 | val_MAE 0.05077 | val_RMSE 0.06928 | R2 0.4378\n",
      "[mem:after_epoch_35] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 035 | tr_MSE 0.00287 | val_MAE 0.03609 | val_RMSE 0.05156 | R2 0.6887\n",
      "[mem:after_epoch_36] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 036 | tr_MSE 0.00181 | val_MAE 0.03329 | val_RMSE 0.04836 | R2 0.7262\n",
      "[mem:after_epoch_37] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 037 | tr_MSE 0.00197 | val_MAE 0.03632 | val_RMSE 0.04975 | R2 0.7102\n",
      "[mem:after_epoch_38] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 038 | tr_MSE 0.00202 | val_MAE 0.04403 | val_RMSE 0.06006 | R2 0.5776\n",
      "[mem:after_epoch_39] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 039 | tr_MSE 0.00245 | val_MAE 0.05238 | val_RMSE 0.06318 | R2 0.5326\n",
      "[mem:after_epoch_40] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 040 | tr_MSE 0.00392 | val_MAE 0.06734 | val_RMSE 0.08818 | R2 0.0895\n",
      "[mem:after_epoch_41] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 041 | tr_MSE 0.00627 | val_MAE 0.04363 | val_RMSE 0.05810 | R2 0.6047\n",
      "[mem:after_epoch_42] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 042 | tr_MSE 0.00278 | val_MAE 0.03316 | val_RMSE 0.04696 | R2 0.7417\n",
      "[mem:after_epoch_43] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 043 | tr_MSE 0.00442 | val_MAE 0.05622 | val_RMSE 0.07257 | R2 0.3833\n",
      "[mem:after_epoch_44] allocated=126.3MB reserved=342.0MB\n",
      "Epoch 044 | tr_MSE 0.00417 | val_MAE 0.05755 | val_RMSE 0.06841 | R2 0.4520\n",
      "[mem:after_epoch_45] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 045 | tr_MSE 0.00453 | val_MAE 0.04756 | val_RMSE 0.06412 | R2 0.5185\n",
      "[mem:after_epoch_46] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 046 | tr_MSE 0.00470 | val_MAE 0.08692 | val_RMSE 0.09651 | R2 -0.0907\n",
      "[mem:after_epoch_47] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 047 | tr_MSE 0.00556 | val_MAE 0.04811 | val_RMSE 0.06639 | R2 0.4838\n",
      "[mem:after_epoch_48] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 048 | tr_MSE 0.00325 | val_MAE 0.07063 | val_RMSE 0.08052 | R2 0.2408\n",
      "[mem:after_epoch_49] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 049 | tr_MSE 0.00352 | val_MAE 0.03941 | val_RMSE 0.05618 | R2 0.6304\n",
      "Early stopping.\n",
      "[graphtransformer_tc_spd] Best Val — MAE 0.029127 | RMSE 0.043332 | R2 0.7801\n",
      "[mem:after_Tc] allocated=16.2MB reserved=40.0MB\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(train_loader_tg))\n",
    "rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "model_tg = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "    use_adj_const=False, use_edge_bias=False,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_tg, ckpt_tg, met_tg = train_hybrid_gnn_sota(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=10, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_tg_spd\", tag=\"graphtransformer_tg_spd\"\n",
    ")\n",
    "del model_tg, train_loader_tg, val_loader_tg\n",
    "free_cuda_memory(tag=\"after_Tg\")\n",
    "reset_cuda_stats()\n",
    "\n",
    "model_den = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "    use_adj_const=False, use_edge_bias=False,\n",
    "    use_cls=False, use_has_xyz=False, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_den, ckpt_den, met_den = train_hybrid_gnn_sota(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=10, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_den_spd\", tag=\"graphtransformer_den_spd\"\n",
    ")\n",
    "del model_den, train_loader_den, val_loader_den\n",
    "free_cuda_memory(tag=\"after_Density\")\n",
    "reset_cuda_stats()\n",
    "\n",
    "# Rg\n",
    "model_rg = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=False, use_edge_bias=False,\n",
    "    use_cls=False, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_rg, ckpt_rg, met_rg = train_hybrid_gnn_sota(\n",
    "    model_rg, train_loader_rg, val_loader_rg,\n",
    "    lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=10, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_rg_spd\", tag=\"graphtransformer_rg_spd\"\n",
    ")\n",
    "del model_rg, train_loader_rg, val_loader_rg\n",
    "free_cuda_memory(tag=\"after_Rg\")\n",
    "reset_cuda_stats()\n",
    "# Tc\n",
    "model_tc = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=False, spd_max=5,\n",
    "    use_adj_const=False, use_edge_bias=False,\n",
    "    use_cls=True, use_has_xyz=False, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_tc, ckpt_tc, met_tc = train_hybrid_gnn_sota(\n",
    "    model_tc, train_loader_tc, val_loader_tc,\n",
    "    lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=10, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_tc_spd\", tag=\"graphtransformer_tc_spd\"\n",
    ")\n",
    "del model_tc, train_loader_tc, val_loader_tc\n",
    "free_cuda_memory(tag=\"after_Tc\")\n",
    "reset_cuda_stats()\n",
    "\n",
    "# model_ffv = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_ffv, ckpt_ffv, met_ffv = train_hybrid_gnn_sota(\n",
    "#     model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=200, warmup_epochs=10, patience=20,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "#     save_dir=\"saved_models/gt_ffv_spd\", tag=\"graphtransformer_ffv_spd\"\n",
    "# )\n",
    "# del model_ffv, train_loader_ffv, val_loader_ffv\n",
    "# free_cuda_memory(tag=\"after_FFV\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# model_ffv = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=True, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_ffv, ckpt_ffv, met_ffv = train_hybrid_gnn_sota(\n",
    "#     model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_ffv_spd\", tag=\"graphtransformer_ffv_spd\"\n",
    "# )\n",
    "# del model_ffv, train_loader_ffv, val_loader_ffv\n",
    "# free_cuda_memory(tag=\"after_FFV\")\n",
    "# reset_cuda_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6d1b1",
   "metadata": {},
   "source": [
    "[graphtransformer_tg_spd] Best Val — MAE 48.126213 | RMSE 63.016220 | R2 0.5840\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.039303 | RMSE 0.074613 | R2 0.6830\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.179415 | RMSE 3.125070 | R2 0.5563\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.030836 | RMSE 0.046045 | R2 0.7517\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.007025 | RMSE 0.011341 | R2 0.8756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9de38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_tg,  val_loader_tg  = make_loaders_for_task_from_pools(\"Tg\",      task_pools, debug_single_process=True)\n",
    "# train_loader_den, val_loader_den = make_loaders_for_task_from_pools(\"Density\", task_pools, debug_single_process=True)\n",
    "# train_loader_rg,  val_loader_rg  = make_loaders_for_task_from_pools(\"Rg\",      task_pools, debug_single_process=True)\n",
    "# train_loader_ffv, val_loader_ffv = make_loaders_for_task_from_pools(\"FFV\",     task_pools, debug_single_process=True)\n",
    "# train_loader_tc,  val_loader_tc  = make_loaders_for_task_from_pools(\"Tc\",      task_pools, debug_single_process=True)\n",
    "\n",
    "# # introspect dims\n",
    "# b = next(iter(train_loader_tg))\n",
    "# rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "# model_tg = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=True,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_tg, ckpt_tg, met_tg = train_hybrid_gnn_sota(\n",
    "#     model_tg, train_loader_tg, val_loader_tg,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_tg_spd\", tag=\"graphtransformer_tg_spd\"\n",
    "# )\n",
    "# del model_tg, train_loader_tg, val_loader_tg\n",
    "# free_cuda_memory(tag=\"after_Tg\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# model_den = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=True,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_den, ckpt_den, met_den = train_hybrid_gnn_sota(\n",
    "#     model_den, train_loader_den, val_loader_den,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_den_spd\", tag=\"graphtransformer_den_spd\"\n",
    "# )\n",
    "# del model_den, train_loader_den, val_loader_den\n",
    "# free_cuda_memory(tag=\"after_Density\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# # Rg\n",
    "# model_rg = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=True,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_rg, ckpt_rg, met_rg = train_hybrid_gnn_sota(\n",
    "#     model_rg, train_loader_rg, val_loader_rg,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_rg_spd\", tag=\"graphtransformer_rg_spd\"\n",
    "# )\n",
    "# del model_rg, train_loader_rg, val_loader_rg\n",
    "# free_cuda_memory(tag=\"after_Rg\")\n",
    "# reset_cuda_stats()\n",
    "# # Tc\n",
    "# model_tc = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=True,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_tc, ckpt_tc, met_tc = train_hybrid_gnn_sota(\n",
    "#     model_tc, train_loader_tc, val_loader_tc,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_tc_spd\", tag=\"graphtransformer_tc_spd\"\n",
    "# )\n",
    "# del model_tc, train_loader_tc, val_loader_tc\n",
    "# free_cuda_memory(tag=\"after_Tc\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# model_ffv = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=True,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_ffv, ckpt_ffv, met_ffv = train_hybrid_gnn_sota(\n",
    "#     model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_ffv_spd\", tag=\"graphtransformer_ffv_spd\"\n",
    "# )\n",
    "# del model_ffv, train_loader_ffv, val_loader_ffv\n",
    "# free_cuda_memory(tag=\"after_FFV\")\n",
    "# reset_cuda_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9aec8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_tg,  val_loader_tg  = make_loaders_for_task_from_pools(\"Tg\",      task_pools, debug_single_process=True)\n",
    "# train_loader_den, val_loader_den = make_loaders_for_task_from_pools(\"Density\", task_pools, debug_single_process=True)\n",
    "# train_loader_rg,  val_loader_rg  = make_loaders_for_task_from_pools(\"Rg\",      task_pools, debug_single_process=True)\n",
    "# train_loader_ffv, val_loader_ffv = make_loaders_for_task_from_pools(\"FFV\",     task_pools, debug_single_process=True)\n",
    "# train_loader_tc,  val_loader_tc  = make_loaders_for_task_from_pools(\"Tc\",      task_pools, debug_single_process=True)\n",
    "# # introspect dims\n",
    "# b = next(iter(train_loader_tg))\n",
    "# rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "# model_tg = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=True, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_tg, ckpt_tg, met_tg = train_hybrid_gnn_sota(\n",
    "#     model_tg, train_loader_tg, val_loader_tg,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_tg_spd\", tag=\"graphtransformer_tg_spd\"\n",
    "# )\n",
    "# del model_tg, train_loader_tg, val_loader_tg\n",
    "# free_cuda_memory(tag=\"after_Tg\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# model_den = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=True, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_den, ckpt_den, met_den = train_hybrid_gnn_sota(\n",
    "#     model_den, train_loader_den, val_loader_den,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_den_spd\", tag=\"graphtransformer_den_spd\"\n",
    "# )\n",
    "# del model_den, train_loader_den, val_loader_den\n",
    "# free_cuda_memory(tag=\"after_Density\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# # Rg\n",
    "# model_rg = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=True, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_rg, ckpt_rg, met_rg = train_hybrid_gnn_sota(\n",
    "#     model_rg, train_loader_rg, val_loader_rg,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_rg_spd\", tag=\"graphtransformer_rg_spd\"\n",
    "# )\n",
    "# del model_rg, train_loader_rg, val_loader_rg\n",
    "# free_cuda_memory(tag=\"after_Rg\")\n",
    "# reset_cuda_stats()\n",
    "# # Tc\n",
    "# model_tc = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=True, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_tc, ckpt_tc, met_tc = train_hybrid_gnn_sota(\n",
    "#     model_tc, train_loader_tc, val_loader_tc,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_tc_spd\", tag=\"graphtransformer_tc_spd\"\n",
    "# )\n",
    "# del model_tc, train_loader_tc, val_loader_tc\n",
    "# free_cuda_memory(tag=\"after_Tc\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# model_ffv = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=True, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_ffv, ckpt_ffv, met_ffv = train_hybrid_gnn_sota(\n",
    "#     model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_ffv_spd\", tag=\"graphtransformer_ffv_spd\"\n",
    "# )\n",
    "# del model_ffv, train_loader_ffv, val_loader_ffv\n",
    "# free_cuda_memory(tag=\"after_FFV\")\n",
    "# reset_cuda_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bf3fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_tg,  val_loader_tg  = make_loaders_for_task_from_pools(\"Tg\",      task_pools, debug_single_process=True)\n",
    "# train_loader_den, val_loader_den = make_loaders_for_task_from_pools(\"Density\", task_pools, debug_single_process=True)\n",
    "# train_loader_rg,  val_loader_rg  = make_loaders_for_task_from_pools(\"Rg\",      task_pools, debug_single_process=True)\n",
    "# train_loader_ffv, val_loader_ffv = make_loaders_for_task_from_pools(\"FFV\",     task_pools, debug_single_process=True)\n",
    "# train_loader_tc,  val_loader_tc  = make_loaders_for_task_from_pools(\"Tc\",      task_pools, debug_single_process=True)\n",
    "\n",
    "# # introspect dims\n",
    "# b = next(iter(train_loader_tg))\n",
    "# rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "# model_tg = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=True, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_tg, ckpt_tg, met_tg = train_hybrid_gnn_sota(\n",
    "#     model_tg, train_loader_tg, val_loader_tg,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_tg_spd\", tag=\"graphtransformer_tg_spd\"\n",
    "# )\n",
    "# del model_tg, train_loader_tg, val_loader_tg\n",
    "# free_cuda_memory(tag=\"after_Tg\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# model_den = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=True, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_den, ckpt_den, met_den = train_hybrid_gnn_sota(\n",
    "#     model_den, train_loader_den, val_loader_den,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_den_spd\", tag=\"graphtransformer_den_spd\"\n",
    "# )\n",
    "# del model_den, train_loader_den, val_loader_den\n",
    "# free_cuda_memory(tag=\"after_Density\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# # Rg\n",
    "# model_rg = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=True, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_rg, ckpt_rg, met_rg = train_hybrid_gnn_sota(\n",
    "#     model_rg, train_loader_rg, val_loader_rg,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_rg_spd\", tag=\"graphtransformer_rg_spd\"\n",
    "# )\n",
    "# del model_rg, train_loader_rg, val_loader_rg\n",
    "# free_cuda_memory(tag=\"after_Rg\")\n",
    "# reset_cuda_stats()\n",
    "# # Tc\n",
    "# model_tc = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=True, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_tc, ckpt_tc, met_tc = train_hybrid_gnn_sota(\n",
    "#     model_tc, train_loader_tc, val_loader_tc,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_tc_spd\", tag=\"graphtransformer_tc_spd\"\n",
    "# )\n",
    "# del model_tc, train_loader_tc, val_loader_tc\n",
    "# free_cuda_memory(tag=\"after_Tc\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# model_ffv = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=True, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_ffv, ckpt_ffv, met_ffv = train_hybrid_gnn_sota(\n",
    "#     model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_ffv_spd\", tag=\"graphtransformer_ffv_spd\"\n",
    "# )\n",
    "# del model_ffv, train_loader_ffv, val_loader_ffv\n",
    "# free_cuda_memory(tag=\"after_FFV\")\n",
    "# reset_cuda_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86870bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_tg,  val_loader_tg  = make_loaders_for_task_from_pools(\"Tg\",      task_pools, debug_single_process=True)\n",
    "# train_loader_den, val_loader_den = make_loaders_for_task_from_pools(\"Density\", task_pools, debug_single_process=True)\n",
    "# train_loader_rg,  val_loader_rg  = make_loaders_for_task_from_pools(\"Rg\",      task_pools, debug_single_process=True)\n",
    "# train_loader_ffv, val_loader_ffv = make_loaders_for_task_from_pools(\"FFV\",     task_pools, debug_single_process=True)\n",
    "# train_loader_tc,  val_loader_tc  = make_loaders_for_task_from_pools(\"Tc\",      task_pools, debug_single_process=True)\n",
    "\n",
    "# # introspect dims\n",
    "# b = next(iter(train_loader_tg))\n",
    "# rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "# model_tg = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=True, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_tg, ckpt_tg, met_tg = train_hybrid_gnn_sota(\n",
    "#     model_tg, train_loader_tg, val_loader_tg,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_tg_spd\", tag=\"graphtransformer_tg_spd\"\n",
    "# )\n",
    "# del model_tg, train_loader_tg, val_loader_tg\n",
    "# free_cuda_memory(tag=\"after_Tg\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# model_den = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=True, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_den, ckpt_den, met_den = train_hybrid_gnn_sota(\n",
    "#     model_den, train_loader_den, val_loader_den,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_den_spd\", tag=\"graphtransformer_den_spd\"\n",
    "# )\n",
    "# del model_den, train_loader_den, val_loader_den\n",
    "# free_cuda_memory(tag=\"after_Density\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# # Rg\n",
    "# model_rg = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=True, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_rg, ckpt_rg, met_rg = train_hybrid_gnn_sota(\n",
    "#     model_rg, train_loader_rg, val_loader_rg,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_rg_spd\", tag=\"graphtransformer_rg_spd\"\n",
    "# )\n",
    "# del model_rg, train_loader_rg, val_loader_rg\n",
    "# free_cuda_memory(tag=\"after_Rg\")\n",
    "# reset_cuda_stats()\n",
    "# # Tc\n",
    "# model_tc = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=True, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_tc, ckpt_tc, met_tc = train_hybrid_gnn_sota(\n",
    "#     model_tc, train_loader_tc, val_loader_tc,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_tc_spd\", tag=\"graphtransformer_tc_spd\"\n",
    "# )\n",
    "# del model_tc, train_loader_tc, val_loader_tc\n",
    "# free_cuda_memory(tag=\"after_Tc\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# model_ffv = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=True, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_ffv, ckpt_ffv, met_ffv = train_hybrid_gnn_sota(\n",
    "#     model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_ffv_spd\", tag=\"graphtransformer_ffv_spd\"\n",
    "# )\n",
    "# del model_ffv, train_loader_ffv, val_loader_ffv\n",
    "# free_cuda_memory(tag=\"after_FFV\")\n",
    "# reset_cuda_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15f42c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_tg,  val_loader_tg  = make_loaders_for_task_from_pools(\"Tg\",      task_pools, debug_single_process=True)\n",
    "# train_loader_den, val_loader_den = make_loaders_for_task_from_pools(\"Density\", task_pools, debug_single_process=True)\n",
    "# train_loader_rg,  val_loader_rg  = make_loaders_for_task_from_pools(\"Rg\",      task_pools, debug_single_process=True)\n",
    "# train_loader_ffv, val_loader_ffv = make_loaders_for_task_from_pools(\"FFV\",     task_pools, debug_single_process=True)\n",
    "# train_loader_tc,  val_loader_tc  = make_loaders_for_task_from_pools(\"Tc\",      task_pools, debug_single_process=True)\n",
    "\n",
    "# # introspect dims\n",
    "# b = next(iter(train_loader_tg))\n",
    "# rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "# model_tg = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=True, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_tg, ckpt_tg, met_tg = train_hybrid_gnn_sota(\n",
    "#     model_tg, train_loader_tg, val_loader_tg,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_tg_spd\", tag=\"graphtransformer_tg_spd\"\n",
    "# )\n",
    "# del model_tg, train_loader_tg, val_loader_tg\n",
    "# free_cuda_memory(tag=\"after_Tg\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# model_den = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=True, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_den, ckpt_den, met_den = train_hybrid_gnn_sota(\n",
    "#     model_den, train_loader_den, val_loader_den,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_den_spd\", tag=\"graphtransformer_den_spd\"\n",
    "# )\n",
    "# del model_den, train_loader_den, val_loader_den\n",
    "# free_cuda_memory(tag=\"after_Density\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# # Rg\n",
    "# model_rg = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=True, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_rg, ckpt_rg, met_rg = train_hybrid_gnn_sota(\n",
    "#     model_rg, train_loader_rg, val_loader_rg,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_rg_spd\", tag=\"graphtransformer_rg_spd\"\n",
    "# )\n",
    "# del model_rg, train_loader_rg, val_loader_rg\n",
    "# free_cuda_memory(tag=\"after_Rg\")\n",
    "# reset_cuda_stats()\n",
    "# # Tc\n",
    "# model_tc = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=True, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_tc, ckpt_tc, met_tc = train_hybrid_gnn_sota(\n",
    "#     model_tc, train_loader_tc, val_loader_tc,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_tc_spd\", tag=\"graphtransformer_tc_spd\"\n",
    "# )\n",
    "# del model_tc, train_loader_tc, val_loader_tc\n",
    "# free_cuda_memory(tag=\"after_Tc\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# model_ffv = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=True, use_has_xyz=False, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_ffv, ckpt_ffv, met_ffv = train_hybrid_gnn_sota(\n",
    "#     model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_ffv_spd\", tag=\"graphtransformer_ffv_spd\"\n",
    "# )\n",
    "# del model_ffv, train_loader_ffv, val_loader_ffv\n",
    "# free_cuda_memory(tag=\"after_FFV\")\n",
    "# reset_cuda_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "412b041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_tg,  val_loader_tg  = make_loaders_for_task_from_pools(\"Tg\",      task_pools, debug_single_process=True)\n",
    "# train_loader_den, val_loader_den = make_loaders_for_task_from_pools(\"Density\", task_pools, debug_single_process=True)\n",
    "# train_loader_rg,  val_loader_rg  = make_loaders_for_task_from_pools(\"Rg\",      task_pools, debug_single_process=True)\n",
    "# train_loader_ffv, val_loader_ffv = make_loaders_for_task_from_pools(\"FFV\",     task_pools, debug_single_process=True)\n",
    "# train_loader_tc,  val_loader_tc  = make_loaders_for_task_from_pools(\"Tc\",      task_pools, debug_single_process=True)\n",
    "\n",
    "# # introspect dims\n",
    "# b = next(iter(train_loader_tg))\n",
    "# rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "# model_tg = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=True, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_tg, ckpt_tg, met_tg = train_hybrid_gnn_sota(\n",
    "#     model_tg, train_loader_tg, val_loader_tg,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_tg_spd\", tag=\"graphtransformer_tg_spd\"\n",
    "# )\n",
    "# del model_tg, train_loader_tg, val_loader_tg\n",
    "# free_cuda_memory(tag=\"after_Tg\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# model_den = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=True, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_den, ckpt_den, met_den = train_hybrid_gnn_sota(\n",
    "#     model_den, train_loader_den, val_loader_den,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_den_spd\", tag=\"graphtransformer_den_spd\"\n",
    "# )\n",
    "# del model_den, train_loader_den, val_loader_den\n",
    "# free_cuda_memory(tag=\"after_Density\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# # Rg\n",
    "# model_rg = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=True, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_rg, ckpt_rg, met_rg = train_hybrid_gnn_sota(\n",
    "#     model_rg, train_loader_rg, val_loader_rg,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_rg_spd\", tag=\"graphtransformer_rg_spd\"\n",
    "# )\n",
    "# del model_rg, train_loader_rg, val_loader_rg\n",
    "# free_cuda_memory(tag=\"after_Rg\")\n",
    "# reset_cuda_stats()\n",
    "# # Tc\n",
    "# model_tc = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=True, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_tc, ckpt_tc, met_tc = train_hybrid_gnn_sota(\n",
    "#     model_tc, train_loader_tc, val_loader_tc,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_tc_spd\", tag=\"graphtransformer_tc_spd\"\n",
    "# )\n",
    "# del model_tc, train_loader_tc, val_loader_tc\n",
    "# free_cuda_memory(tag=\"after_Tc\")\n",
    "# reset_cuda_stats()\n",
    "\n",
    "# model_ffv = GraphTransformerGPS(\n",
    "#     d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "#     rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "#     use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "#     local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "#     use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "#     use_adj_const=False, use_edge_bias=False,\n",
    "#     use_cls=False, use_has_xyz=True, head_hidden=512\n",
    "# ).to(b.x.device)\n",
    "\n",
    "# model_ffv, ckpt_ffv, met_ffv = train_hybrid_gnn_sota(\n",
    "#     model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "#     lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "#     epochs=90, warmup_epochs=6, patience=12,\n",
    "#     clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "#     save_dir=\"saved_models/gt_ffv_spd\", tag=\"graphtransformer_ffv_spd\"\n",
    "# )\n",
    "# del model_ffv, train_loader_ffv, val_loader_ffv\n",
    "# free_cuda_memory(tag=\"after_FFV\")\n",
    "# reset_cuda_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3578832",
   "metadata": {},
   "source": [
    "\n",
    "# New LMDB with canonicalize_psmiles\n",
    "Minimal baseline:\n",
    "[graphtransformer_tg_spd] Best Val — MAE 55.100464 | RMSE 71.059502 | R2 0.4711\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.030763 | RMSE 0.068418 | R2 0.7334\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.184328 | RMSE 3.163936 | R2 0.5452\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.027480 | RMSE 0.044562 | R2 0.7675\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.006242 | RMSE 0.013769 | R2 0.8166\n",
    "use_edge_bias=True:\n",
    "[graphtransformer_tg_spd] Best Val — MAE 64.235008 | RMSE 84.792068 | R2 0.2469\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.041322 | RMSE 0.079782 | R2 0.6375\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.271991 | RMSE 3.328248 | R2 0.4967\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.030659 | RMSE 0.046286 | R2 0.7491\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.012568 | RMSE 0.021181 | R2 0.5659\n",
    "use_adj_const=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 55.523029 | RMSE 77.189560 | R2 0.3759\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.030271 | RMSE 0.071522 | R2 0.7087\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.189212 | RMSE 3.194562 | R2 0.5363\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.028110 | RMSE 0.045479 | R2 0.7578\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.005767 | RMSE 0.010659 | R2 0.8901\n",
    "use_spd_bias=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 53.716434 | RMSE 71.113457 | R2 0.4702\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.044408 | RMSE 0.074214 | R2 0.6863\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.115795 | RMSE 3.086924 | R2 0.5670\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.035664 | RMSE 0.048010 | R2 0.7301\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.013945 | RMSE 0.021938 | R2 0.5344\n",
    "use_geo_bias=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 56.241505 | RMSE 71.855812 | R2 0.4591\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.044151 | RMSE 0.077316 | R2 0.6596\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.000988 | RMSE 3.044069 | R2 0.5790\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.029302 | RMSE 0.044328 | R2 0.7699\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.016998 | RMSE 0.025697 | R2 0.3611\n",
    "use_cls=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 52.629242 | RMSE 66.531052 | R2 0.5363\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.057558 | RMSE 0.089856 | R2 0.5402\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.128716 | RMSE 3.225420 | R2 0.5273\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.026689 | RMSE 0.042924 | R2 0.7842\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.006222 | RMSE 0.011992 | R2 0.8609\n",
    "use_has_xyz=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 49.267300 | RMSE 61.446640 | R2 0.6045\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.043220 | RMSE 0.081267 | R2 0.6239\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.198472 | RMSE 3.078726 | R2 0.5693\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.035201 | RMSE 0.049572 | R2 0.7122\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.005637 | RMSE 0.010194 | R2 0.8995\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65787ea6",
   "metadata": {},
   "source": [
    "# Original LMDB\n",
    "Minimal baseline:\n",
    "[graphtransformer_tg_spd] Best Val — MAE 52.682880 | RMSE 66.310356 | R2 0.5394\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.033281 | RMSE 0.069495 | R2 0.7250\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.274377 | RMSE 3.533569 | R2 0.4327\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.027580 | RMSE 0.044943 | R2 0.7635\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.005713 | RMSE 0.008959 | R2 0.9223\n",
    "use_edge_bias=True:\n",
    "[graphtransformer_tg_spd] Best Val — MAE 59.157528 | RMSE 78.540993 | R2 0.3538\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.040733 | RMSE 0.075703 | R2 0.6736\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.184664 | RMSE 3.215525 | R2 0.5302\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.029891 | RMSE 0.045550 | R2 0.7570\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.014560 | RMSE 0.024105 | R2 0.4378\n",
    "use_adj_const=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 55.685497 | RMSE 71.508026 | R2 0.4644\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.043538 | RMSE 0.076304 | R2 0.6684\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.194414 | RMSE 3.418880 | R2 0.4689\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.030531 | RMSE 0.047150 | R2 0.7397\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.015711 | RMSE 0.025118 | R2 0.3895\n",
    "use_spd_bias=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 56.409588 | RMSE 74.155319 | R2 0.4240\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.051897 | RMSE 0.086738 | R2 0.5716\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.043078 | RMSE 3.004847 | R2 0.5897\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.030650 | RMSE 0.046136 | R2 0.7507\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.014980 | RMSE 0.024944 | R2 0.3980\n",
    "use_geo_bias=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 57.246323 | RMSE 71.635178 | R2 0.4624\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.040926 | RMSE 0.073173 | R2 0.6951\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.186664 | RMSE 3.221086 | R2 0.5286\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.029472 | RMSE 0.045144 | R2 0.7613\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.010738 | RMSE 0.018613 | R2 0.6648\n",
    "use_cls=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 51.327293 | RMSE 65.371567 | R2 0.5523\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.048267 | RMSE 0.078447 | R2 0.6495\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.215581 | RMSE 3.335439 | R2 0.4945\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.031363 | RMSE 0.046542 | R2 0.7463\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.011230 | RMSE 0.019117 | R2 0.6464\n",
    "use_has_xyz=True\n",
    "\n",
    "# New LMDB with canonicalize_psmiles\n",
    "Minimal baseline:\n",
    "[graphtransformer_tg_spd] Best Val — MAE 55.100464 | RMSE 71.059502 | R2 0.4711\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.030763 | RMSE 0.068418 | R2 0.7334\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.184328 | RMSE 3.163936 | R2 0.5452\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.027480 | RMSE 0.044562 | R2 0.7675\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.006242 | RMSE 0.013769 | R2 0.8166\n",
    "use_edge_bias=True:\n",
    "[graphtransformer_tg_spd] Best Val — MAE 64.235008 | RMSE 84.792068 | R2 0.2469\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.041322 | RMSE 0.079782 | R2 0.6375\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.271991 | RMSE 3.328248 | R2 0.4967\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.030659 | RMSE 0.046286 | R2 0.7491\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.012568 | RMSE 0.021181 | R2 0.5659\n",
    "use_adj_const=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 56.366352 | RMSE 73.193703 | R2 0.4388\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.047595 | RMSE 0.079254 | R2 0.6423\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.042789 | RMSE 2.847963 | R2 0.6315\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.033112 | RMSE 0.048311 | R2 0.7267\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.014251 | RMSE 0.022048 | R2 0.5297\n",
    "use_spd_bias=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 52.129559 | RMSE 64.974182 | R2 0.5578\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.050838 | RMSE 0.088112 | R2 0.5579\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.168722 | RMSE 3.208059 | R2 0.5324\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.033525 | RMSE 0.048122 | R2 0.7288\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.018180 | RMSE 0.029006 | R2 0.1860\n",
    "use_geo_bias=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 49.387646 | RMSE 63.215427 | R2 0.5814\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.045089 | RMSE 0.074725 | R2 0.6820\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.082205 | RMSE 2.920620 | R2 0.6124\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.033126 | RMSE 0.045319 | R2 0.7595\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.012233 | RMSE 0.020871 | R2 0.5785\n",
    "use_cls=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 51.995239 | RMSE 68.995934 | R2 0.5013\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.042371 | RMSE 0.080913 | R2 0.6272\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.145705 | RMSE 3.241805 | R2 0.5225\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.031370 | RMSE 0.044972 | R2 0.7632\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.015359 | RMSE 0.023831 | R2 0.4505\n",
    "use_has_xyz=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 52.416542 | RMSE 67.310928 | R2 0.5254\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.049355 | RMSE 0.081608 | R2 0.6207\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.096542 | RMSE 2.964315 | R2 0.6007\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.031574 | RMSE 0.046309 | R2 0.7489\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.013870 | RMSE 0.020263 | R2 0.6028\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673460",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "| Model Type | Feature | MAE | RMSE | R2 |\n",
    "|---|---|---|---|---|\n",
    "| RF3D | FFV | 0.007621 | 0.017553 | 0.6605 |\n",
    "| RF3D_Aug | FFV | 0.007578 | 0.017404 | 0.6662 |\n",
    "| GNN2 | FFV | 0.013817 | 0.023902 | 0.4473 |\n",
    "| GNN2_Aug | FFV | 0.013092 | 0.022793 | 0.4974 |\n",
    "| ET | FFV | 0.006651 | 0.016818 | 0.6883 |\n",
    "| **ET_Aug** | **FFV** | **0.006635** | **0.016826** | **0.6880** |\n",
    "| GT | FFV | 0.XXXXXX | 0.XXXXXX | 0.XXXX |\n",
    "| GT_Aug | FFV | 0.XXXXXX | 0.XXXXXX | 0.XXXX |\n",
    "| RF3D | Tg | 58.315801 | 74.296699 | 0.5846 |\n",
    "| RF3D_Aug | Tg | 58.143107 | 74.521032 | 0.5821 |\n",
    "| **GNN2** | **Tg** | **47.105114** | **61.480179** | **0.6040** |\n",
    "| GNN2_Aug | Tg | 51.539692 | 70.575638 | 0.4782 |\n",
    "| ET | Tg | 58.973811 | 74.658978 | 0.5806 |\n",
    "| ET_Aug | Tg | 58.521052 | 74.475532 | 0.5826 |\n",
    "| GT | Tg | 78.903389 | 98.401192 |-0.0143 |\n",
    "| GT_Aug | Tg | 52.365578 | 67.529610 | 0.5223 |\n",
    "| RF3D | Tc | 0.029937 | 0.045036 | 0.7313 |\n",
    "| RF3D_Aug | Tc | 0.029675 | 0.044853 | 0.7335 |\n",
    "| **GNN2** | **Tc** | **0.025115** | **0.041331** | **0.8000** |\n",
    "| **GNN2_Aug** | **Tc** | **0.025252** | **0.039670** | **0.8157** |\n",
    "| ET | Tc | 0.028888 | 0.043469 | 0.7497 |\n",
    "| ET_Aug | Tc | 0.027990 | 0.042644 | 0.7591 |\n",
    "| GT | Tc | 0.032644 | 0.046613 | 0.7456 |\n",
    "| GT_Aug | Tc | 0.028590 | 0.043121 | 0.7822 |\n",
    "| RF3D | Rg | 1.648818 | 2.493712 | 0.7299 |\n",
    "| RF3D_Aug | Rg | 1.668425 | 2.517235 | 0.7248 |\n",
    "| GNN2 | Rg | 2.115880 | 2.801481 | 0.6434 |\n",
    "| GNN2_Aug | Rg | 1.532573 | 2.405382 | 0.7371 |\n",
    "| ET | Rg | 1.619464 | 2.522478 | 0.7237 |\n",
    "| **ET_Aug** | **Rg** | **1.609396** | **2.526705** | **0.7227** |\n",
    "| GT | Rg | 2.579300 | 3.521387 | 0.4366 |\n",
    "| GT_Aug | Rg | 2.134301 | 3.066199 | 0.5728 |\n",
    "| RF3D | Density | 0.037793 | 0.070932 | 0.7847 |\n",
    "| RF3D_Aug | Density | 0.037123 | 0.070212 | 0.7891 |\n",
    "| GNN2 | Density | 0.031735 | 0.067845 | 0.7379 |\n",
    "| GNN2_Aug | Density | 0.030458 | 0.070372 | 0.7180 |\n",
    "| ET | Density | 0.028492 | 0.052839 | 0.8805 |\n",
    "| **ET_Aug** | **Density** | **0.028135** | **0.051842** | **0.8850** |\n",
    "| GT | Density | 0.104749 | 0.134771 | -0.0343 |\n",
    "| GT_Aug | Density | 0.087159 | 0.126079 | 0.0948 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f53569",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
