{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61979795",
   "metadata": {},
   "source": [
    "# HOMO-LUMO Gap Predictions\n",
    "\n",
    "### Problem Statement & Motivation\n",
    "\n",
    "Accurately predicting quantum chemical properties like the HOMO–LUMO energy gap is essential for advancing materials science, drug discovery, and electronic design. The HOMO–LUMO gap is particularly informative for assessing molecular reactivity and stability. While Density Functional Theory (DFT) provides precise estimates, its high computational cost makes it impractical for large-scale screening of molecular libraries. This notebook explores machine learning alternatives that are fast, scalable, and interpretable, offering solutions that are accessible even on modest hardware.\n",
    "\n",
    "### Related Work & Key Gap\n",
    "\n",
    "Past work has shown that:\n",
    "\n",
    "* DFT is accurate but computationally intensive\n",
    "* ML models like kernel methods and GNNs show promise, but often require large models and expensive hardware\n",
    "\n",
    "Key Gap: A need for lightweight, high-performing models that can run locally and integrate with user-friendly tools for deployment in research or education.\n",
    "\n",
    "### Methodology & Evaluation\n",
    "\n",
    "This notebook:\n",
    "\n",
    "* Benchmarks a variety of 2D-based models using RDKit descriptors, Coulomb matrices, and graph neural networks (GNNs) on a 5k molecule subset\n",
    "* Progresses to a hybrid GNN architecture combining OGB-standard graphs with SMILES-derived cheminformatics features\n",
    "* Achieves **MAE = 0.159 eV**\n",
    "* Visualizes results using parity plots, error inspection, and predicted-vs-true comparisons\n",
    "* Evaluates both random and high-error cases to better understand model behavior\n",
    "\n",
    "| Metric   | Best Model (Hybrid GNN) |\n",
    "| -------- | ----------------------- |\n",
    "| **MAE**  | 0.159 eV                |\n",
    "| **RMSE** | 0.234 eV                |\n",
    "| **R²**   | 0.965                   |\n",
    "\n",
    "\n",
    "### Deployment & Accessibility\n",
    "\n",
    "To make the model practically useful, an **interactive web app** was developed:\n",
    "\n",
    "**Live App**: [HOMO–LUMO Gap Predictor on Hugging Face](https://huggingface.co/spaces/MooseML/homo-lumo-gap-predictor)\n",
    "\n",
    "Features:\n",
    "\n",
    "* **SMILES input** for any organic molecule\n",
    "* **Real-time prediction** of the HOMO–LUMO gap\n",
    "* **Molecular visualization**\n",
    "* Simple **CSV logging** for result tracking\n",
    "\n",
    "GitHub Repository: [MooseML/homo-lumo-gap-models](https://github.com/MooseML/homo-lumo-gap-models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ace_tools_open as tools\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import pickle\n",
    "import joblib\n",
    "import os \n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Module, Sequential, Dropout\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# OGB dataset \n",
    "from ogb.lsc import PygPCQM4Mv2Dataset, PCQM4Mv2Dataset\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# RDKit\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "\n",
    "# ChemML\n",
    "from chemml.chem import Molecule, RDKitFingerprint, CoulombMatrix, tensorise_molecules\n",
    "from chemml.models import MLP, NeuralGraphHidden, NeuralGraphOutput\n",
    "from chemml.utils import regression_metrics\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589db70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Built with CUDA: True\n",
      "CUDA available: True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device: /physical_device:GPU:0\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: data\n",
      "LMDB directory: data\\processed_chunks\n",
      "Train LMDB: data\\processed_chunks\\polymer_train3d_dist.lmdb\n",
      "Test LMDB: data\\processed_chunks\\polymer_test3d_dist.lmdb\n",
      "LMDBs already exist.\n"
     ]
    }
   ],
   "source": [
    "# Paths - Fixed for Kaggle environment\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  # Writable directory\n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/best_gnn_transformer_hybrid.pt'\n",
    "else:\n",
    "    DATA_ROOT = 'data'\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = 'best_gnn_transformer_hybrid.pt'\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"LMDB directory: {CHUNK_DIR}\")\n",
    "print(f\"Train LMDB: {TRAIN_LMDB}\")\n",
    "print(f\"Test LMDB: {TEST_LMDB}\")\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    # Run the LMDB builders\n",
    "    !python build_polymer_lmdb_fixed.py train\n",
    "    !python build_polymer_lmdb_fixed.py test\n",
    "    print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c34b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDB contains 7,973 train graphs\n",
      "Global pools -> train_pool=7,175  val_pool=798\n",
      "     Tg:    511 rows with labels (pre-intersection with pools)\n",
      "    FFV:   7030 rows with labels (pre-intersection with pools)\n",
      "     Tc:    737 rows with labels (pre-intersection with pools)\n",
      "Density:    613 rows with labels (pre-intersection with pools)\n",
      "     Rg:    614 rows with labels (pre-intersection with pools)\n"
     ]
    }
   ],
   "source": [
    "# LMDB+CSV wiring \n",
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "# 1) Columns / index mapping\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "\n",
    "# 2) Read the training labels (CSV is only used to know which IDs have labels)\n",
    "train_path = os.path.join(DATA_ROOT, 'train.csv')\n",
    "train_df   = pd.read_csv(train_path)\n",
    "assert {'id','SMILES'}.issubset(train_df.columns), \"train.csv must have id and SMILES\"\n",
    "train_df['id'] = train_df['id'].astype(int)\n",
    "\n",
    "# 3) Read the actual IDs that exist in the LMDB\n",
    "def read_lmdb_ids(lmdb_path: str) -> np.ndarray:\n",
    "    ids_txt = lmdb_path + \".ids.txt\"\n",
    "    if not os.path.exists(ids_txt):\n",
    "        raise FileNotFoundError(f\"Missing {ids_txt}. Rebuild LMDB or confirm paths.\")\n",
    "    ids = np.loadtxt(ids_txt, dtype=np.int64)\n",
    "    if ids.ndim == 0:  # single id edge case\n",
    "        ids = ids.reshape(1)\n",
    "    return ids\n",
    "\n",
    "lmdb_ids = read_lmdb_ids(TRAIN_LMDB)\n",
    "print(f\"LMDB contains {len(lmdb_ids):,} train graphs\")\n",
    "\n",
    "# 4) Helper: IDs that have a label for a given task (intersection with LMDB ids)\n",
    "def ids_with_label(task: str) -> np.ndarray:\n",
    "    col = task\n",
    "    have_label = train_df.loc[~train_df[col].isna(), 'id'].astype(int).values\n",
    "    # Only keep those that were actually written to the LMDB\n",
    "    keep = np.intersect1d(have_label, lmdb_ids, assume_unique=False)\n",
    "    return keep\n",
    "\n",
    "# 5) Make a global pool split once (reused for each task)\n",
    "rng = np.random.default_rng(123)\n",
    "perm = rng.permutation(len(lmdb_ids))\n",
    "split = int(0.9 * len(lmdb_ids))\n",
    "train_pool_ids = lmdb_ids[perm[:split]]\n",
    "val_pool_ids   = lmdb_ids[perm[split:]]\n",
    "\n",
    "print(f\"Global pools -> train_pool={len(train_pool_ids):,}  val_pool={len(val_pool_ids):,}\")\n",
    "\n",
    "# 6) Quick sanity: show available counts per task\n",
    "for t in label_cols:\n",
    "    n_task_ids = len(ids_with_label(t))\n",
    "    print(f\"{t:>7}: {n_task_ids:6d} rows with labels (pre-intersection with pools)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125f621",
   "metadata": {},
   "source": [
    "The only property that appears will succeed with a simple imputation strategy is FFV. All other properties contain very high percent missing. Therefore, I will impute median for FFV, train a model for FFV, and train separate models for other properties. I will attempt to filter out missing values for each property. If this yields uncessful, I may explore sampling techniques or use the trained model to impute values to train a secondaery model. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe69f3",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47dc5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Tg shape: (7973, 2)\n",
      "Initial Tg missing:\n",
      "SMILES       0\n",
      "Tg        7462\n",
      "dtype: int64\n",
      "Cleaned Tg shape: (511, 2)\n",
      "Cleaned Tg missing:\n",
      "SMILES    0\n",
      "Tg        0\n",
      "dtype: int64\n",
      "\n",
      "Initial Density shape: (7973, 2)\n",
      "Initial Density missing:\n",
      "SMILES        0\n",
      "Density    7360\n",
      "dtype: int64\n",
      "Cleaned Density shape: (613, 2)\n",
      "Cleaned Density missing:\n",
      "SMILES     0\n",
      "Density    0\n",
      "dtype: int64\n",
      "\n",
      "Initial FFV shape: (7973, 2)\n",
      "Initial FFV missing:\n",
      "SMILES      0\n",
      "FFV       943\n",
      "dtype: int64\n",
      "Cleaned FFV shape: (7030, 2)\n",
      "Cleaned FFV missing:\n",
      "SMILES    0\n",
      "FFV       0\n",
      "dtype: int64\n",
      "\n",
      "Initial Tc shape: (7973, 2)\n",
      "Initial Tc missing:\n",
      "SMILES       0\n",
      "Tc        7236\n",
      "dtype: int64\n",
      "Cleaned Tc shape: (737, 2)\n",
      "Cleaned Tc missing:\n",
      "SMILES    0\n",
      "Tc        0\n",
      "dtype: int64\n",
      "\n",
      "Initial Rg shape: (7973, 2)\n",
      "Initial Rg missing:\n",
      "SMILES       0\n",
      "Rg        7359\n",
      "dtype: int64\n",
      "Cleaned Rg shape: (614, 2)\n",
      "Cleaned Rg missing:\n",
      "SMILES    0\n",
      "Rg        0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the CSV only to know which rows have labels; keep 'id' here.\n",
    "train_df = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_df[\"id\"] = train_df[\"id\"].astype(int)\n",
    "\n",
    "def build_target_df_from_ids(df: pd.DataFrame, target_col: str, keep_ids: np.ndarray):\n",
    "    \"\"\"\n",
    "    Return DataFrame with only SMILES + target, restricted to IDs present in the LMDB\n",
    "    and dropping missing targets.\n",
    "    \"\"\"\n",
    "    out = df.loc[df[\"id\"].isin(keep_ids), [\"SMILES\", target_col]].copy()\n",
    "    print(f\"Initial {target_col} shape:\", out.shape)\n",
    "    print(f\"Initial {target_col} missing:\\n{out.isnull().sum()}\")\n",
    "    out = out.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    print(f\"Cleaned {target_col} shape:\", out.shape)\n",
    "    print(f\"Cleaned {target_col} missing:\\n{out.isnull().sum()}\\n\")\n",
    "    return out\n",
    "\n",
    "# Build all five (use same LMDB id set so we only keep rows that exist in LMDB)\n",
    "df_tg      = build_target_df_from_ids(train_df, \"Tg\",      lmdb_ids)\n",
    "df_density = build_target_df_from_ids(train_df, \"Density\", lmdb_ids)\n",
    "df_ffv     = build_target_df_from_ids(train_df, \"FFV\",     lmdb_ids)\n",
    "df_tc      = build_target_df_from_ids(train_df, \"Tc\",      lmdb_ids)\n",
    "df_rg      = build_target_df_from_ids(train_df, \"Rg\",      lmdb_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff48e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morgan FP utilities (no 3D, no external descriptors) \n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def smiles_to_morgan_fp(\n",
    "    smi: str,\n",
    "    n_bits: int = 1024,\n",
    "    radius: int = 3,\n",
    "    use_counts: bool = False,\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"Return a 1D numpy array Morgan fingerprint; None if SMILES invalid.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    if use_counts:\n",
    "        fp = rdMolDescriptors.GetMorganFingerprint(mol, radius)\n",
    "        # convert to dense count vector\n",
    "        arr = np.zeros((n_bits,), dtype=np.int32)\n",
    "        for bit_id, count in fp.GetNonzeroElements().items():\n",
    "            arr[bit_id % n_bits] += count\n",
    "        return arr.astype(np.float32)\n",
    "    else:\n",
    "        bv = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "        arr = np.zeros((n_bits,), dtype=np.int8)\n",
    "        Chem.DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "        return arr.astype(np.float32)\n",
    "\n",
    "def prepare_fp_for_target(\n",
    "    df_target: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    *,\n",
    "    fp_bits: int = 1024,\n",
    "    fp_radius: int = 3,\n",
    "    use_counts: bool = False,\n",
    "    save_csv_path: Optional[str] = None,\n",
    "    show_progress: bool = True,\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Drop missing targets, compute Morgan FPs from SMILES only.\n",
    "    Returns (df_clean, y, X_fp) where:\n",
    "      df_clean: ['SMILES', target_col]\n",
    "      y: (N,)\n",
    "      X_fp: (N, fp_bits)\n",
    "    \"\"\"\n",
    "    assert {\"SMILES\", target_col}.issubset(df_target.columns)\n",
    "\n",
    "    # 1) drop missing targets (no imputation)\n",
    "    work = df_target[[\"SMILES\", target_col]].copy()\n",
    "    before = len(work)\n",
    "    work = work.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    after = len(work)\n",
    "    print(f\"[{target_col}] dropped {before - after} missing; kept {after}\")\n",
    "\n",
    "    # 2) compute FPs; skip invalid SMILES\n",
    "    fps, ys, keep_smiles = [], [], []\n",
    "    it = work.itertuples(index=False)\n",
    "    if show_progress:\n",
    "        it = tqdm(it, total=len(work), desc=f\"FPs for {target_col}\")\n",
    "\n",
    "    for row in it:\n",
    "        smi = row.SMILES\n",
    "        yv  = getattr(row, target_col)\n",
    "        arr = smiles_to_morgan_fp(smi, n_bits=fp_bits, radius=fp_radius, use_counts=use_counts)\n",
    "        if arr is None:\n",
    "            continue\n",
    "        fps.append(arr)\n",
    "        ys.append(float(yv))\n",
    "        keep_smiles.append(smi)\n",
    "\n",
    "    X_fp = np.stack(fps, axis=0) if fps else np.zeros((0, fp_bits), dtype=np.float32)\n",
    "    y = np.asarray(ys, dtype=float)\n",
    "    df_clean = pd.DataFrame({\"SMILES\": keep_smiles, target_col: y})\n",
    "\n",
    "    if save_csv_path:\n",
    "        df_clean.to_csv(save_csv_path, index=False)\n",
    "        print(f\"[{target_col}] saved cleaned CSV -> {save_csv_path}\")\n",
    "\n",
    "    print(f\"[{target_col}] X_fp: {X_fp.shape} | y: {y.shape}\")\n",
    "    return df_clean, y, X_fp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91f37942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] dropped 0 missing; kept 511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a556c1c4e5c4ff2aad8e61d46d4e4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Tg:   0%|          | 0/511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] saved cleaned CSV -> cleaned_tg_fp.csv\n",
      "[Tg] X_fp: (511, 1024) | y: (511,)\n",
      "[Density] dropped 0 missing; kept 613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753a67959280472d903417a6792db591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Density:   0%|          | 0/613 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Density] saved cleaned CSV -> cleaned_density_fp.csv\n",
      "[Density] X_fp: (613, 1024) | y: (613,)\n",
      "[FFV] dropped 0 missing; kept 7030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a1668c524944f890ef4b94cba1293d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for FFV:   0%|          | 0/7030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FFV] saved cleaned CSV -> cleaned_ffv_fp.csv\n",
      "[FFV] X_fp: (7030, 1024) | y: (7030,)\n",
      "[Tc] dropped 0 missing; kept 737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1aaab148df743089b3a58bf05c3a45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Tc:   0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tc] saved cleaned CSV -> cleaned_tc_fp.csv\n",
      "[Tc] X_fp: (737, 1024) | y: (737,)\n",
      "[Rg] dropped 0 missing; kept 614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a054b2600204c49a9c0467624a53f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FPs for Rg:   0%|          | 0/614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rg] saved cleaned CSV -> cleaned_rg_fp.csv\n",
      "[Rg] X_fp: (614, 1024) | y: (614,)\n"
     ]
    }
   ],
   "source": [
    "# Bit vectors (1024, r=3) \n",
    "df_clean_tg,      y_tg,      X_tg      = prepare_fp_for_target(df_tg,      \"Tg\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_tg_fp.csv\")\n",
    "df_clean_density, y_density, X_density = prepare_fp_for_target(df_density, \"Density\", fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_density_fp.csv\")\n",
    "df_clean_ffv,     y_ffv,     X_ffv     = prepare_fp_for_target(df_ffv,     \"FFV\",     fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_ffv_fp.csv\")\n",
    "df_clean_tc,      y_tc,      X_tc      = prepare_fp_for_target(df_tc,      \"Tc\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_tc_fp.csv\")\n",
    "df_clean_rg,      y_rg,      X_rg      = prepare_fp_for_target(df_rg,      \"Rg\",      fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_rg_fp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff620911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "@dataclass\n",
    "class TabularSplits:\n",
    "    # unscaled (for RF)\n",
    "    X_train: np.ndarray\n",
    "    X_test:  np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    y_test:  np.ndarray\n",
    "    # scaled (for KRR/MLP)\n",
    "    X_train_scaled: Optional[np.ndarray] = None\n",
    "    X_test_scaled:  Optional[np.ndarray] = None\n",
    "    y_train_scaled: Optional[np.ndarray] = None  # shape (N,1)\n",
    "    y_test_scaled:  Optional[np.ndarray] = None\n",
    "    x_scaler: Optional[StandardScaler] = None\n",
    "    y_scaler: Optional[StandardScaler] = None\n",
    "\n",
    "def _make_regression_stratify_bins(y: np.ndarray, n_bins: int = 10) -> np.ndarray:\n",
    "    \"\"\"Return integer bins for approximate stratification in regression.\"\"\"\n",
    "    y = y.ravel()\n",
    "    # handle degenerate case\n",
    "    if np.unique(y).size < n_bins:\n",
    "        n_bins = max(2, np.unique(y).size)\n",
    "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "    bins = np.unique(np.quantile(y, quantiles))\n",
    "    # ensure strictly increasing\n",
    "    bins = np.unique(bins)\n",
    "    # np.digitize expects right-open intervals by default\n",
    "    strat = np.digitize(y, bins[1:-1], right=False)\n",
    "    return strat\n",
    "\n",
    "def make_tabular_splits(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    scale_X: bool = True,\n",
    "    scale_y: bool = True,\n",
    "    stratify_regression: bool = False,\n",
    "    n_strat_bins: int = 10,\n",
    "    # if you already decided splits (e.g., scaffold split), pass indices:\n",
    "    train_idx: Optional[np.ndarray] = None,\n",
    "    test_idx: Optional[np.ndarray] = None,\n",
    ") -> TabularSplits:\n",
    "    \"\"\"\n",
    "    Split and (optionally) scale tabular features/targets for a single target.\n",
    "    Returns both scaled and unscaled arrays, plus fitted scalers.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    if train_idx is not None and test_idx is not None:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "    else:\n",
    "        strat = None\n",
    "        if stratify_regression:\n",
    "            strat = _make_regression_stratify_bins(y, n_bins=n_strat_bins)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "        )\n",
    "\n",
    "    # Unscaled outputs (for RF, tree models)\n",
    "    splits = TabularSplits(\n",
    "        X_train=X_train, X_test=X_test,\n",
    "        y_train=y_train, y_test=y_test\n",
    "    )\n",
    "\n",
    "    # Scaled versions (for KRR/MLP)\n",
    "    if scale_X:\n",
    "        xscaler = StandardScaler()\n",
    "        splits.X_train_scaled = xscaler.fit_transform(X_train)\n",
    "        splits.X_test_scaled  = xscaler.transform(X_test)\n",
    "        splits.x_scaler = xscaler\n",
    "    if scale_y:\n",
    "        yscaler = StandardScaler()\n",
    "        splits.y_train_scaled = yscaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        splits.y_test_scaled  = yscaler.transform(y_test.reshape(-1, 1))\n",
    "        splits.y_scaler = yscaler\n",
    "\n",
    "    # Shapes summary\n",
    "    print(\"Splits:\")\n",
    "    print(\"X_train:\", splits.X_train.shape, \"| X_test:\", splits.X_test.shape)\n",
    "    if splits.X_train_scaled is not None:\n",
    "        print(\"X_train_scaled:\", splits.X_train_scaled.shape, \"| X_test_scaled:\", splits.X_test_scaled.shape)\n",
    "    print(\"y_train:\", splits.y_train.shape, \"| y_test:\", splits.y_test.shape)\n",
    "    if splits.y_train_scaled is not None:\n",
    "        print(\"y_train_scaled:\", splits.y_train_scaled.shape, \"| y_test_scaled:\", splits.y_test_scaled.shape)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c284cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def train_eval_rf(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    rf_params: Dict[str, Any],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    save_dir: str = \"saved_models/rf\",\n",
    "    tag: str = \"model\",\n",
    ") -> Tuple[RandomForestRegressor, Dict[str, float], TabularSplits, str]:\n",
    "    \"\"\"\n",
    "    Trains a RandomForest on unscaled features; returns (model, metrics, splits, path).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # Pick a safe number of bins based on dataset size\n",
    "    if stratify_regression:\n",
    "        adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "    else:\n",
    "        adaptive_bins = n_strat_bins\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False,                 # RF doesn't need scaling\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=adaptive_bins\n",
    "    )\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=random_state, n_jobs=-1, **rf_params)\n",
    "    rf.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "    pred_tr = rf.predict(splits.X_train)\n",
    "    pred_te = rf.predict(splits.X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_MAE\": mean_absolute_error(splits.y_train, pred_tr),\n",
    "        \"train_RMSE\": mean_squared_error(splits.y_train, pred_tr, squared=False),\n",
    "        \"train_R2\": r2_score(splits.y_train, pred_tr),\n",
    "        \"val_MAE\": mean_absolute_error(splits.y_test, pred_te),\n",
    "        \"val_RMSE\": mean_squared_error(splits.y_test, pred_te, squared=False),\n",
    "        \"val_R2\": r2_score(splits.y_test, pred_te),\n",
    "    }\n",
    "    print(f\"[RF/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"rf_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": rf, \"metrics\": metrics, \"rf_params\": rf_params}, path)\n",
    "    return rf, metrics, splits, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d95126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:\n",
      "X_train: (5624, 1024) | X_test: (1406, 1024)\n",
      "y_train: (5624,) | y_test: (1406,)\n",
      "[RF/FFV] val_MAE=0.009095  val_RMSE=0.019753  val_R2=0.5701\n",
      "Splits:\n",
      "X_train: (589, 1024) | X_test: (148, 1024)\n",
      "y_train: (589,) | y_test: (148,)\n",
      "[RF/Tc] val_MAE=0.029866  val_RMSE=0.045109  val_R2=0.7304\n"
     ]
    }
   ],
   "source": [
    "rf_cfg = {\n",
    "    \"FFV\": {\"n_estimators\": 100, \"max_depth\": 60},\n",
    "    \"Tc\":  {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "    \"Rg\":  {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "}\n",
    "\n",
    "rf_ffv, m_ffv, splits_ffv, p_ffv = train_eval_rf(X_ffv, y_ffv, rf_params=rf_cfg[\"FFV\"], tag=\"FFV\")\n",
    "rf_tc,  m_tc,  splits_tc,  p_tc  = train_eval_rf(X_tc,  y_tc,  rf_params=rf_cfg[\"Tc\"],  tag=\"Tc\")\n",
    "# rf_rg,  m_rg,  splits_rg,  p_rg  = train_eval_rf(X_rg,  y_rg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Rg\")\n",
    "# rf_tg,  m_tg,  splits_tg,  p_tg  = train_eval_rf(X_tg,  y_tg,  rf_params=rf_cfg[\"Rg\"],  tag=\"Tg\")\n",
    "# rf_density,  m_density,  splits_density,  p_density  = train_eval_rf(X_density,  y_density,  rf_params=rf_cfg[\"Rg\"],  tag=\"Density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77f7ec",
   "metadata": {},
   "source": [
    "## ChemML GNN Model Results\n",
    "| Model Type             | Featurization        |   MAE |  RMSE |   R² | Notes             |\n",
    "|------------------------|----------------------|-------|-------|------|-------------------|\n",
    "| GNN (Tuned)            | tensorise_molecules Graph   | 0.302 | 0.411 | 0.900 | Best performance across all metrics   |\n",
    "| GNN (Untuned)          | tensorise_molecules Graph   | 0.400 | 0.519 | 0.841 | Good overall|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42db218",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Model Training\n",
    "\n",
    "Having explored different molecular graph representations and model architectures, I am now moving to training what is expected to be the best-performing model using the full dataset. The earlier GNN model was based on `tensorise_molecules` (ChemML) graphs and had strong performance with a **mean absolute error (MAE) around 0.30**. These graphs are based on RDKit's internal descriptors and do not reflect the original PCQM4Mv2 graph structure used in the Open Graph Benchmark (OGB). Therefore, I will shift focus to the `smiles2graph` representation provided by OGB, which aligns more directly with the benchmark's evaluation setup and top-performing models on the leaderboard.\n",
    "\n",
    "\n",
    "| Source                         | Atom/Bond Features                                                 | Format                                          | Customizable?     | Alignment with PCQM4Mv2?  |\n",
    "| ------------------------------ | ------------------------------------------------------------------ | ----------------------------------------------- | ----------------- | ---------------------- |\n",
    "| `tensorise_molecules` (ChemML) | RDKit-based descriptors (ex: atom number, degree, hybridization) | NumPy tensors (`X_atoms`, `X_bonds`, `X_edges`) | Limited           |  Not aligned          |\n",
    "| `smiles2graph` (OGB / PyG)     | Predefined categorical features from PCQM4Mv2                      | PyTorch Geometric `Data` objects                |  Highly flexible |  Matches OGB standard |\n",
    "\n",
    "By using `smiles2graph`, we:\n",
    "\n",
    "* Use OGB-standard graph construction and feature encoding for fair comparisons with leaderboard models\n",
    "* Include learnable AtomEncoder and BondEncoder embeddings from `ogb.graphproppred.mol_encoder`, which improve model expressiveness\n",
    "* Maintain compatibility with PyTorch Geometric, DGL, and OGB tools\n",
    "\n",
    "I will also concatenate GNN-derived embeddings with SMILES-based RDKit descriptors, feeding this hybrid representation into MLP head. This allows you to combine structural and cheminformatics perspectives for improved prediction accuracy. With this setup, I aim to improve upon the MAE of \\~0.30 achieved earlier and push closer toward state-of-the-art performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4becc7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EdgeEncoderMixed(nn.Module):\n",
    "#     def __init__(self, emb_dim, cont_dim=32):\n",
    "#         super().__init__()\n",
    "#         from ogb.graphproppred.mol_encoder import bond_types, bond_dirs  # or hardcode sizes\n",
    "#         self.emb0 = nn.Embedding(5, emb_dim)  # bond type\n",
    "#         self.emb1 = nn.Embedding(6, emb_dim)  # stereo\n",
    "#         self.emb2 = nn.Embedding(2, emb_dim)  # conjugation\n",
    "#         self.mlp_cont = nn.Sequential(nn.Linear(cont_dim, emb_dim), nn.ReLU(), nn.Linear(emb_dim, emb_dim))\n",
    "\n",
    "#     def forward(self, edge_attr):\n",
    "#         cat = edge_attr[:, :3].long()\n",
    "#         cont = edge_attr[:, 3:].float()\n",
    "#         e_cat = self.emb0(cat[:,0]) + self.emb1(cat[:,1]) + self.emb2(cat[:,2])\n",
    "#         e_cont = self.mlp_cont(cont)\n",
    "#         return e_cat + e_cont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d599b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tg ids: (511,) Density ids: (613,)\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))  # keep 'id'!\n",
    "lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "if os.path.exists(lmdb_ids_path):\n",
    "    with open(lmdb_ids_path) as f:\n",
    "        kept_ids = set(int(x.strip()) for x in f if x.strip())\n",
    "else:\n",
    "    kept_ids = set(train_csv['id'].astype(int).tolist())\n",
    "\n",
    "def ids_for_task(task):\n",
    "    t = task2idx[task]\n",
    "    col = label_cols[t]\n",
    "    ids = train_csv.loc[~train_csv[col].isna(), 'id'].astype(int).tolist()\n",
    "    # only those that actually exist in LMDB\n",
    "    return np.array([i for i in ids if i in kept_ids], dtype=int)\n",
    "\n",
    "ids_tg  = ids_for_task(\"Tg\")\n",
    "ids_den = ids_for_task(\"Density\")\n",
    "ids_tc = ids_for_task(\"Tc\")\n",
    "ids_rg = ids_for_task(\"Rg\")\n",
    "ids_ffv = ids_for_task(\"FFV\")\n",
    "print(\"Tg ids:\", ids_tg.shape, \"Density ids:\", ids_den.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3efce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import torch, numpy as np\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def _get_rdkit_feats_from_record(rec):\n",
    "    arr = getattr(rec, \"rdkit_feats\", None)\n",
    "    if arr is None: return torch.zeros(6, dtype=torch.float32)\n",
    "    return torch.as_tensor(np.asarray(arr, np.float32).reshape(-1), dtype=torch.float32)\n",
    "\n",
    "class LMDBtoPyGSingleTask(Dataset):\n",
    "    def __init__(self, ids, lmdb_path, target_index=None):\n",
    "        self.base = LMDBDataset(ids, lmdb_path)\n",
    "        self.t = target_index  # int or None\n",
    "\n",
    "    def __len__(self): return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.base[idx]\n",
    "        x  = torch.as_tensor(rec.x, dtype=torch.long)\n",
    "        ei = torch.as_tensor(rec.edge_index, dtype=torch.long)\n",
    "        ea = torch.as_tensor(rec.edge_attr)         # (E, 3 + 32RBF)\n",
    "        ea_cat = ea[:, :3].long()                   # <-- use only 3 categorical cols\n",
    "        rdkit = _get_rdkit_feats_from_record(rec)   # (15,)\n",
    "\n",
    "        d = Data(x=x, edge_index=ei, edge_attr=ea_cat, rdkit_feats=rdkit)\n",
    "        if (self.t is not None) and hasattr(rec, \"y\"):\n",
    "            yv = torch.as_tensor(rec.y, dtype=torch.float32).view(-1)\n",
    "            if self.t < yv.numel():\n",
    "                d.y = yv[self.t:self.t+1]\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "694612d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "def make_loaders_for_task(task, ids, *, batch_size=32, seed=42):\n",
    "    t = task2idx[task]\n",
    "    tr_ids, va_ids = train_test_split(ids, test_size=0.2, random_state=seed)\n",
    "    tr_ds = LMDBtoPyGSingleTask(tr_ids, TRAIN_LMDB, target_index=t)\n",
    "    va_ds = LMDBtoPyGSingleTask(va_ids, TRAIN_LMDB, target_index=t)\n",
    "    tr = GeoDataLoader(tr_ds, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "    va = GeoDataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    return tr, va\n",
    "\n",
    "train_loader_tg,  val_loader_tg  = make_loaders_for_task(\"Tg\", ids_tg,  batch_size=32)\n",
    "train_loader_den, val_loader_den = make_loaders_for_task(\"Density\", ids_den, batch_size=32)\n",
    "# train_loader_tc,  val_loader_tc  = make_loaders_for_task(\"Tc\", ids_tc,  batch_size=32)\n",
    "train_loader_rg, val_loader_rg = make_loaders_for_task(\"Rg\", ids_rg, batch_size=32)\n",
    "# train_loader_ffv, val_loader_ffv = make_loaders_for_task(\"FFV\", ids_ffv, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983db98",
   "metadata": {},
   "source": [
    "## Step 5: Define the Hybrid GNN Model\n",
    "\n",
    "The final architecture uses both structural and cheminformatics data by combining GNN-learned graph embeddings with SMILES-derived RDKit descriptors. This Hybrid GNN model uses `smiles2graph` for graph construction and augments it with RDKit-based molecular features for improved prediction accuracy.\n",
    "\n",
    "### Model Components:\n",
    "\n",
    "* **AtomEncoder / BondEncoder**\n",
    "  Transforms categorical atom and bond features (provided by OGB) into learnable embeddings using the encoders from `ogb.graphproppred.mol_encoder`. These provide a strong foundation for expressive graph learning.\n",
    "\n",
    "* **GINEConv Layers (x2)**\n",
    "  I use two stacked GINEConv layers (Graph Isomorphism Network with Edge features). These layers perform neighborhood aggregation based on edge attributes, allowing the model to capture localized chemical environments.\n",
    "\n",
    "* **Global Mean Pooling**\n",
    "  After message passing, node level embeddings are aggregated into a fixed size graph level representation using `global_mean_pool`.\n",
    "\n",
    "* **Concatenation with RDKit Descriptors**\n",
    "  The pooled GNN embedding is concatenated with external RDKit descriptors, which capture global molecular properties not easily inferred from graph data alone.\n",
    "\n",
    "* **MLP Prediction Head**\n",
    "  A multilayer perceptron processes the combined feature vector with ReLU activations, dropout regularization, and linear layers to predict the HOMO–LUMO gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0946f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List\n",
    "\n",
    "def _act(name: str):\n",
    "    name = (name or \"ReLU\").lower()\n",
    "    if name in (\"relu\",):   return nn.ReLU()\n",
    "    if name in (\"gelu\",):   return nn.GELU()\n",
    "    if name in (\"swish\",\"silu\"): return nn.SiLU()\n",
    "    return nn.ReLU()\n",
    "\n",
    "class HybridGNN(Module):\n",
    "    def __init__(self, gnn_dim: int, rdkit_dim: int, hidden_dim: int, dropout_rate: float=0.2, activation: str=\"ReLU\"):\n",
    "        super().__init__()\n",
    "        self.gnn_dim = gnn_dim\n",
    "        self.rdkit_dim = rdkit_dim\n",
    "        act = _act(activation)\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=gnn_dim)\n",
    "        self.bond_encoder = BondEncoder(emb_dim=gnn_dim)\n",
    "\n",
    "        self.conv1 = GINEConv(Sequential(Linear(gnn_dim, gnn_dim), act, Linear(gnn_dim, gnn_dim)))\n",
    "        self.conv2 = GINEConv(Sequential(Linear(gnn_dim, gnn_dim), act, Linear(gnn_dim, gnn_dim)))\n",
    "        self.pool = global_mean_pool\n",
    "\n",
    "        self.mlp = Sequential(\n",
    "            Linear(gnn_dim + rdkit_dim, hidden_dim), act, Dropout(dropout_rate),\n",
    "            Linear(hidden_dim, hidden_dim // 2), act, Dropout(dropout_rate),\n",
    "            Linear(hidden_dim // 2, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # encode atoms and bonds\n",
    "        x = self.atom_encoder(data.x)\n",
    "        edge_attr = self.bond_encoder(data.edge_attr)\n",
    "\n",
    "        # GNN convolutions\n",
    "        x = self.conv1(x, data.edge_index, edge_attr)\n",
    "        x = self.conv2(x, data.edge_index, edge_attr)\n",
    "        x = self.pool(x, data.batch)\n",
    "\n",
    "        # handle RDKit features\n",
    "        rdkit_feats = getattr(data, 'rdkit_feats', None)\n",
    "        if rdkit_feats is not None:\n",
    "            # Reshape the RDKit features tensor to be (batch_size, rdkit_dim)\n",
    "            # The number of samples in the batch is given by x.shape[0] after pooling\n",
    "            reshaped_rdkit_feats = rdkit_feats.view(x.shape[0], self.rdkit_dim)\n",
    "            \n",
    "            if x.shape[0] != reshaped_rdkit_feats.shape[0]:\n",
    "                raise ValueError(f\"Shape mismatch: GNN output ({x.shape[0]}) vs rdkit_feats ({reshaped_rdkit_feats.shape[0]})\")\n",
    "            \n",
    "            x = torch.cat([x, reshaped_rdkit_feats], dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"RDKit features not found in the data object\")\n",
    "\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc992041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid_gnn(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    *,\n",
    "    lr: float,\n",
    "    optimizer: str = \"Adam\",\n",
    "    weight_decay: float = 0.0,\n",
    "    epochs: int = 100,\n",
    "    patience: int = 10,\n",
    "    save_dir: str = \"saved_models/gnn\",\n",
    "    tag: str = \"model\",\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "    opt_name = optimizer.lower()\n",
    "    if opt_name == \"adamw\":\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif opt_name == \"rmsprop\":\n",
    "        opt = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.0)\n",
    "    else:\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "    best, bad = float(\"inf\"), 0\n",
    "    best_path = os.path.join(save_dir, f\"{tag}.pt\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_once(loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        for b in loader:\n",
    "            b = b.to(device)\n",
    "            p = model(b)\n",
    "            preds.append(p.cpu())\n",
    "            trues.append(b.y.view(-1,1).cpu())\n",
    "        preds = torch.cat(preds).numpy(); trues = torch.cat(trues).numpy()\n",
    "        mse = np.mean((preds - trues)**2)\n",
    "        return mse, preds, trues\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total, count = 0.0, 0\n",
    "        for b in train_loader:\n",
    "            b = b.to(device)\n",
    "            pred = model(b)\n",
    "            loss = F.mse_loss(pred, b.y.view(-1,1))\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            total += loss.item() * b.num_graphs\n",
    "            count += b.num_graphs\n",
    "        tr_mse = total / max(1, count)\n",
    "        va_mse, _, _ = eval_once(val_loader)\n",
    "        print(f\"Epoch {ep:02d} | Train MSE {tr_mse:.5f} | Val MSE {va_mse:.5f}\")\n",
    "\n",
    "        if va_mse < best - 1e-7:\n",
    "            best, bad = va_mse, 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    val_mse, val_pred, val_true = eval_once(val_loader)\n",
    "    mae = np.mean(np.abs(val_pred - val_true))\n",
    "    rmse = np.sqrt(val_mse)\n",
    "    r2 = 1 - np.sum((val_pred - val_true)**2) / np.sum((val_true - val_true.mean())**2)\n",
    "    print(f\"[{tag}] Best Val — MAE {mae:.6f} | RMSE {rmse:.6f} | R2 {r2:.4f}\")\n",
    "    return model, best_path, {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7e9cb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE 41959.36079 | Val MSE 7592.69531\n",
      "Epoch 02 | Train MSE 7588.24583 | Val MSE 6447.14258\n",
      "Epoch 03 | Train MSE 7014.11868 | Val MSE 5595.39502\n",
      "Epoch 04 | Train MSE 6597.50885 | Val MSE 5446.25928\n",
      "Epoch 05 | Train MSE 6401.56805 | Val MSE 5403.22070\n",
      "Epoch 06 | Train MSE 6175.07003 | Val MSE 5421.49268\n",
      "Epoch 07 | Train MSE 6328.19759 | Val MSE 5410.92480\n",
      "Epoch 08 | Train MSE 6252.14488 | Val MSE 5200.04248\n",
      "Epoch 09 | Train MSE 6216.99782 | Val MSE 5180.54932\n",
      "Epoch 10 | Train MSE 6376.76024 | Val MSE 5086.60107\n",
      "Epoch 11 | Train MSE 6066.95448 | Val MSE 5056.37793\n",
      "Epoch 12 | Train MSE 5714.21722 | Val MSE 4869.28467\n",
      "Epoch 13 | Train MSE 5897.03215 | Val MSE 5118.38965\n",
      "Epoch 14 | Train MSE 5653.75005 | Val MSE 4949.24951\n",
      "Epoch 15 | Train MSE 5711.58905 | Val MSE 5175.50293\n",
      "Epoch 16 | Train MSE 5332.53415 | Val MSE 5128.10938\n",
      "Epoch 17 | Train MSE 5809.69658 | Val MSE 4756.61914\n",
      "Epoch 18 | Train MSE 5566.85842 | Val MSE 5304.73340\n",
      "Epoch 19 | Train MSE 5811.56748 | Val MSE 5295.26270\n",
      "Epoch 20 | Train MSE 5637.73993 | Val MSE 5625.15479\n",
      "Epoch 21 | Train MSE 5300.79788 | Val MSE 5097.67090\n",
      "Epoch 22 | Train MSE 5333.40857 | Val MSE 5566.07422\n",
      "Epoch 23 | Train MSE 5475.35026 | Val MSE 5301.71045\n",
      "Epoch 24 | Train MSE 5576.47725 | Val MSE 5826.80566\n",
      "Epoch 25 | Train MSE 5498.74347 | Val MSE 4759.49805\n",
      "Epoch 26 | Train MSE 5047.43319 | Val MSE 4870.45459\n",
      "Epoch 27 | Train MSE 5506.53425 | Val MSE 5135.58057\n",
      "Epoch 28 | Train MSE 5413.42464 | Val MSE 5893.73242\n",
      "Epoch 29 | Train MSE 4927.69079 | Val MSE 4761.86279\n",
      "Epoch 30 | Train MSE 5421.63413 | Val MSE 4466.56641\n",
      "Epoch 31 | Train MSE 5321.99204 | Val MSE 6110.44238\n",
      "Epoch 32 | Train MSE 5267.38450 | Val MSE 4498.73096\n",
      "Epoch 33 | Train MSE 5687.80407 | Val MSE 4729.38428\n",
      "Epoch 34 | Train MSE 5052.22737 | Val MSE 4479.36865\n",
      "Epoch 35 | Train MSE 5058.38537 | Val MSE 6360.80322\n",
      "Epoch 36 | Train MSE 5421.72038 | Val MSE 6299.25195\n",
      "Epoch 37 | Train MSE 5654.47798 | Val MSE 4449.49902\n",
      "Epoch 38 | Train MSE 5062.23445 | Val MSE 4766.26270\n",
      "Epoch 39 | Train MSE 5371.17415 | Val MSE 4207.47314\n",
      "Epoch 40 | Train MSE 4695.38375 | Val MSE 4280.05469\n",
      "Epoch 41 | Train MSE 5844.95577 | Val MSE 4228.72803\n",
      "Epoch 42 | Train MSE 5250.35173 | Val MSE 4769.96680\n",
      "Epoch 43 | Train MSE 4734.36368 | Val MSE 4277.57959\n",
      "Epoch 44 | Train MSE 5098.18023 | Val MSE 4399.27539\n",
      "Epoch 45 | Train MSE 5211.45830 | Val MSE 5642.12549\n",
      "Epoch 46 | Train MSE 4467.60564 | Val MSE 4259.77295\n",
      "Epoch 47 | Train MSE 4965.38040 | Val MSE 4194.13232\n",
      "Epoch 48 | Train MSE 5141.40152 | Val MSE 6352.59424\n",
      "Epoch 49 | Train MSE 4702.59461 | Val MSE 4396.99951\n",
      "Epoch 50 | Train MSE 5181.45088 | Val MSE 4985.73828\n",
      "Epoch 51 | Train MSE 5136.56570 | Val MSE 5100.73926\n",
      "Epoch 52 | Train MSE 5122.93825 | Val MSE 4184.17627\n",
      "Epoch 53 | Train MSE 4585.11926 | Val MSE 4047.89160\n",
      "Epoch 54 | Train MSE 4463.37051 | Val MSE 4068.29785\n",
      "Epoch 55 | Train MSE 4339.08025 | Val MSE 4224.02295\n",
      "Epoch 56 | Train MSE 5053.75833 | Val MSE 4208.66943\n",
      "Epoch 57 | Train MSE 5000.90013 | Val MSE 4488.23682\n",
      "Epoch 58 | Train MSE 4949.35030 | Val MSE 4369.30127\n",
      "Epoch 59 | Train MSE 4663.17295 | Val MSE 4232.05469\n",
      "Epoch 60 | Train MSE 5080.01457 | Val MSE 5000.49805\n",
      "Epoch 61 | Train MSE 4537.88484 | Val MSE 4190.34814\n",
      "Epoch 62 | Train MSE 4340.98773 | Val MSE 4196.97754\n",
      "Epoch 63 | Train MSE 4338.49967 | Val MSE 4508.88184\n",
      "Epoch 64 | Train MSE 4113.13341 | Val MSE 4488.80225\n",
      "Epoch 65 | Train MSE 4333.47939 | Val MSE 4181.99219\n",
      "Epoch 66 | Train MSE 4098.70902 | Val MSE 3887.30347\n",
      "Epoch 67 | Train MSE 4665.40705 | Val MSE 4836.71240\n",
      "Epoch 68 | Train MSE 4869.84184 | Val MSE 4028.21973\n",
      "Epoch 69 | Train MSE 4200.31530 | Val MSE 4073.96582\n",
      "Epoch 70 | Train MSE 5221.61826 | Val MSE 4086.83008\n",
      "Epoch 71 | Train MSE 4076.75704 | Val MSE 4150.23828\n",
      "Epoch 72 | Train MSE 4084.77767 | Val MSE 3679.13037\n",
      "Epoch 73 | Train MSE 4113.28599 | Val MSE 3925.78247\n",
      "Epoch 74 | Train MSE 4456.36699 | Val MSE 4726.51172\n",
      "Epoch 75 | Train MSE 4616.72266 | Val MSE 4671.52295\n",
      "Epoch 76 | Train MSE 4050.59915 | Val MSE 4150.18213\n",
      "Epoch 77 | Train MSE 4385.18215 | Val MSE 4118.74463\n",
      "Epoch 78 | Train MSE 4310.08343 | Val MSE 3562.25269\n",
      "Epoch 79 | Train MSE 3997.41820 | Val MSE 3972.39746\n",
      "Epoch 80 | Train MSE 4389.52993 | Val MSE 3753.91016\n",
      "Epoch 81 | Train MSE 3870.21296 | Val MSE 3706.30640\n",
      "Epoch 82 | Train MSE 4068.19403 | Val MSE 4149.87061\n",
      "Epoch 83 | Train MSE 4036.40798 | Val MSE 3815.08960\n",
      "Epoch 84 | Train MSE 4026.93166 | Val MSE 5376.27344\n",
      "Epoch 85 | Train MSE 3617.45475 | Val MSE 3658.68457\n",
      "Epoch 86 | Train MSE 4408.77026 | Val MSE 3878.62231\n",
      "Epoch 87 | Train MSE 3887.82600 | Val MSE 3669.51636\n",
      "Epoch 88 | Train MSE 3872.17633 | Val MSE 4514.80957\n",
      "Epoch 89 | Train MSE 3755.46802 | Val MSE 4826.69141\n",
      "Epoch 90 | Train MSE 3891.07288 | Val MSE 4456.63770\n",
      "Epoch 91 | Train MSE 4233.38780 | Val MSE 5582.58301\n",
      "Epoch 92 | Train MSE 3801.06041 | Val MSE 7538.49902\n",
      "Epoch 93 | Train MSE 4334.65643 | Val MSE 4101.70605\n",
      "Early stopping.\n",
      "[hybridgnn_tg] Best Val — MAE 46.568249 | RMSE 59.684608 | R2 0.6268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattg\\AppData\\Local\\Temp\\ipykernel_27496\\870047112.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "tg_cfg = {'gnn_dim': 256, 'hidden_dim': 512, 'dropout_rate': 0.34404144200017467, \n",
    "          'lr': 0.0005555079210176292, 'activation': 'Swish', 'optimizer': 'RMSprop', \n",
    "          'weight_decay': 9.056299733554687e-06}\n",
    "\n",
    "model_tg = HybridGNN(\n",
    "    gnn_dim=tg_cfg['gnn_dim'],\n",
    "    rdkit_dim=15,\n",
    "    hidden_dim=tg_cfg['hidden_dim'],\n",
    "    dropout_rate=tg_cfg['dropout_rate'],\n",
    "    activation=tg_cfg['activation']\n",
    ")\n",
    "\n",
    "model_tg, ckpt_tg, metrics_tg = train_hybrid_gnn(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=tg_cfg['lr'], optimizer=tg_cfg['optimizer'],\n",
    "    weight_decay=tg_cfg['weight_decay'],\n",
    "    epochs=120, patience=15,  \n",
    "    save_dir=\"saved_models/gnn_tg\", tag=\"hybridgnn_tg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a44f756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE 1.67778 | Val MSE 0.06559\n",
      "Epoch 02 | Train MSE 0.34782 | Val MSE 0.03503\n",
      "Epoch 03 | Train MSE 0.15642 | Val MSE 0.11731\n",
      "Epoch 04 | Train MSE 0.09209 | Val MSE 0.08539\n",
      "Epoch 05 | Train MSE 0.08105 | Val MSE 0.06634\n",
      "Epoch 06 | Train MSE 0.06434 | Val MSE 0.05680\n",
      "Epoch 07 | Train MSE 0.04648 | Val MSE 0.04775\n",
      "Epoch 08 | Train MSE 0.04682 | Val MSE 0.04406\n",
      "Epoch 09 | Train MSE 0.03644 | Val MSE 0.02439\n",
      "Epoch 10 | Train MSE 0.03321 | Val MSE 0.03068\n",
      "Epoch 11 | Train MSE 0.03518 | Val MSE 0.02869\n",
      "Epoch 12 | Train MSE 0.03524 | Val MSE 0.02185\n",
      "Epoch 13 | Train MSE 0.03615 | Val MSE 0.02330\n",
      "Epoch 14 | Train MSE 0.03829 | Val MSE 0.02591\n",
      "Epoch 15 | Train MSE 0.03146 | Val MSE 0.02086\n",
      "Epoch 16 | Train MSE 0.02413 | Val MSE 0.01984\n",
      "Epoch 17 | Train MSE 0.02266 | Val MSE 0.01457\n",
      "Epoch 18 | Train MSE 0.02364 | Val MSE 0.01194\n",
      "Epoch 19 | Train MSE 0.02247 | Val MSE 0.02376\n",
      "Epoch 20 | Train MSE 0.02117 | Val MSE 0.01255\n",
      "Epoch 21 | Train MSE 0.02245 | Val MSE 0.01595\n",
      "Epoch 22 | Train MSE 0.02132 | Val MSE 0.00943\n",
      "Epoch 23 | Train MSE 0.01907 | Val MSE 0.01197\n",
      "Epoch 24 | Train MSE 0.01753 | Val MSE 0.01544\n",
      "Epoch 25 | Train MSE 0.01747 | Val MSE 0.00771\n",
      "Epoch 26 | Train MSE 0.01714 | Val MSE 0.00781\n",
      "Epoch 27 | Train MSE 0.01750 | Val MSE 0.02023\n",
      "Epoch 28 | Train MSE 0.01898 | Val MSE 0.00735\n",
      "Epoch 29 | Train MSE 0.01484 | Val MSE 0.01057\n",
      "Epoch 30 | Train MSE 0.01852 | Val MSE 0.01006\n",
      "Epoch 31 | Train MSE 0.01670 | Val MSE 0.01038\n",
      "Epoch 32 | Train MSE 0.01721 | Val MSE 0.00956\n",
      "Epoch 33 | Train MSE 0.01576 | Val MSE 0.00755\n",
      "Epoch 34 | Train MSE 0.01392 | Val MSE 0.00528\n",
      "Epoch 35 | Train MSE 0.01607 | Val MSE 0.00718\n",
      "Epoch 36 | Train MSE 0.01558 | Val MSE 0.00862\n",
      "Epoch 37 | Train MSE 0.01520 | Val MSE 0.01554\n",
      "Epoch 38 | Train MSE 0.01579 | Val MSE 0.00633\n",
      "Epoch 39 | Train MSE 0.01418 | Val MSE 0.00873\n",
      "Epoch 40 | Train MSE 0.01341 | Val MSE 0.00545\n",
      "Epoch 41 | Train MSE 0.01422 | Val MSE 0.00581\n",
      "Epoch 42 | Train MSE 0.01442 | Val MSE 0.00469\n",
      "Epoch 43 | Train MSE 0.01513 | Val MSE 0.00668\n",
      "Epoch 44 | Train MSE 0.01408 | Val MSE 0.00750\n",
      "Epoch 45 | Train MSE 0.01356 | Val MSE 0.00492\n",
      "Epoch 46 | Train MSE 0.01521 | Val MSE 0.00558\n",
      "Epoch 47 | Train MSE 0.01352 | Val MSE 0.00710\n",
      "Epoch 48 | Train MSE 0.01388 | Val MSE 0.00447\n",
      "Epoch 49 | Train MSE 0.01282 | Val MSE 0.00486\n",
      "Epoch 50 | Train MSE 0.01370 | Val MSE 0.00450\n",
      "Epoch 51 | Train MSE 0.01453 | Val MSE 0.00697\n",
      "Epoch 52 | Train MSE 0.01314 | Val MSE 0.00516\n",
      "Epoch 53 | Train MSE 0.01320 | Val MSE 0.00432\n",
      "Epoch 54 | Train MSE 0.01399 | Val MSE 0.00473\n",
      "Epoch 55 | Train MSE 0.01385 | Val MSE 0.00571\n",
      "Epoch 56 | Train MSE 0.01280 | Val MSE 0.00510\n",
      "Epoch 57 | Train MSE 0.01302 | Val MSE 0.00574\n",
      "Epoch 58 | Train MSE 0.01213 | Val MSE 0.00644\n",
      "Epoch 59 | Train MSE 0.01391 | Val MSE 0.00483\n",
      "Epoch 60 | Train MSE 0.01239 | Val MSE 0.00595\n",
      "Epoch 61 | Train MSE 0.01140 | Val MSE 0.00550\n",
      "Epoch 62 | Train MSE 0.01188 | Val MSE 0.00389\n",
      "Epoch 63 | Train MSE 0.01008 | Val MSE 0.00500\n",
      "Epoch 64 | Train MSE 0.01210 | Val MSE 0.00380\n",
      "Epoch 65 | Train MSE 0.01179 | Val MSE 0.00598\n",
      "Epoch 66 | Train MSE 0.01025 | Val MSE 0.00381\n",
      "Epoch 67 | Train MSE 0.01088 | Val MSE 0.00364\n",
      "Epoch 68 | Train MSE 0.01082 | Val MSE 0.00468\n",
      "Epoch 69 | Train MSE 0.01088 | Val MSE 0.00527\n",
      "Epoch 70 | Train MSE 0.01178 | Val MSE 0.00438\n",
      "Epoch 71 | Train MSE 0.00879 | Val MSE 0.00430\n",
      "Epoch 72 | Train MSE 0.00958 | Val MSE 0.00609\n",
      "Epoch 73 | Train MSE 0.00946 | Val MSE 0.00537\n",
      "Epoch 74 | Train MSE 0.00844 | Val MSE 0.00489\n",
      "Epoch 75 | Train MSE 0.00844 | Val MSE 0.00440\n",
      "Epoch 76 | Train MSE 0.00909 | Val MSE 0.00470\n",
      "Epoch 77 | Train MSE 0.00913 | Val MSE 0.00764\n",
      "Epoch 78 | Train MSE 0.01107 | Val MSE 0.00580\n",
      "Epoch 79 | Train MSE 0.01076 | Val MSE 0.00410\n",
      "Epoch 80 | Train MSE 0.01007 | Val MSE 0.00770\n",
      "Epoch 81 | Train MSE 0.00994 | Val MSE 0.00602\n",
      "Epoch 82 | Train MSE 0.01006 | Val MSE 0.00521\n",
      "Early stopping.\n",
      "[hybridgnn_density] Best Val — MAE 0.035400 | RMSE 0.060298 | R2 0.7929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattg\\AppData\\Local\\Temp\\ipykernel_27496\\3084240870.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "den_cfg = {'gnn_dim': 1024, 'hidden_dim': 384, 'dropout_rate': 0.3735260731607324,\n",
    "           'lr': 5.956024201538505e-04, 'activation': 'Swish', 'optimizer': 'AdamW',\n",
    "           'weight_decay': 8.619671341229739e-06}\n",
    "\n",
    "model_den = HybridGNN(\n",
    "    gnn_dim=den_cfg['gnn_dim'],\n",
    "    rdkit_dim=15,\n",
    "    hidden_dim=den_cfg['hidden_dim'],\n",
    "    dropout_rate=den_cfg['dropout_rate'],\n",
    "    activation=den_cfg['activation']\n",
    ")\n",
    "model_den, ckpt_den, metrics_den = train_hybrid_gnn(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=den_cfg['lr'], optimizer=den_cfg['optimizer'],\n",
    "    weight_decay=den_cfg['weight_decay'],\n",
    "    epochs=120, patience=15,  \n",
    "    save_dir=\"saved_models/gnn_density\", tag=\"hybridgnn_density\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f404f837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE 80.92240 | Val MSE 57.16220\n",
      "Epoch 02 | Train MSE 40.70642 | Val MSE 27.66032\n",
      "Epoch 03 | Train MSE 29.69571 | Val MSE 24.59624\n",
      "Epoch 04 | Train MSE 26.10429 | Val MSE 27.73846\n",
      "Epoch 05 | Train MSE 24.36249 | Val MSE 22.10388\n",
      "Epoch 06 | Train MSE 23.64583 | Val MSE 19.42935\n",
      "Epoch 07 | Train MSE 22.98108 | Val MSE 20.28521\n",
      "Epoch 08 | Train MSE 23.47854 | Val MSE 19.16561\n",
      "Epoch 09 | Train MSE 22.11226 | Val MSE 18.07262\n",
      "Epoch 10 | Train MSE 24.15085 | Val MSE 22.59981\n",
      "Epoch 11 | Train MSE 23.79000 | Val MSE 24.73962\n",
      "Epoch 12 | Train MSE 22.79254 | Val MSE 26.35352\n",
      "Epoch 13 | Train MSE 21.19017 | Val MSE 16.78869\n",
      "Epoch 14 | Train MSE 22.03345 | Val MSE 17.55637\n",
      "Epoch 15 | Train MSE 19.98177 | Val MSE 21.54910\n",
      "Epoch 16 | Train MSE 19.83724 | Val MSE 15.54232\n",
      "Epoch 17 | Train MSE 18.04467 | Val MSE 15.57086\n",
      "Epoch 18 | Train MSE 17.16936 | Val MSE 16.09323\n",
      "Epoch 19 | Train MSE 16.80016 | Val MSE 14.00111\n",
      "Epoch 20 | Train MSE 17.62381 | Val MSE 18.94002\n",
      "Epoch 21 | Train MSE 18.33123 | Val MSE 17.20758\n",
      "Epoch 22 | Train MSE 18.30854 | Val MSE 16.88593\n",
      "Epoch 23 | Train MSE 17.43370 | Val MSE 14.70426\n",
      "Epoch 24 | Train MSE 15.92742 | Val MSE 14.69377\n",
      "Epoch 25 | Train MSE 14.61675 | Val MSE 16.73356\n",
      "Epoch 26 | Train MSE 16.37271 | Val MSE 13.05808\n",
      "Epoch 27 | Train MSE 17.09729 | Val MSE 17.05970\n",
      "Epoch 28 | Train MSE 17.07561 | Val MSE 14.99091\n",
      "Epoch 29 | Train MSE 16.20518 | Val MSE 11.86999\n",
      "Epoch 30 | Train MSE 16.44278 | Val MSE 12.99653\n",
      "Epoch 31 | Train MSE 13.77000 | Val MSE 11.68506\n",
      "Epoch 32 | Train MSE 14.51885 | Val MSE 15.95231\n",
      "Epoch 33 | Train MSE 16.90336 | Val MSE 14.60811\n",
      "Epoch 34 | Train MSE 15.66140 | Val MSE 14.70039\n",
      "Epoch 35 | Train MSE 13.58749 | Val MSE 12.57251\n",
      "Epoch 36 | Train MSE 16.14209 | Val MSE 12.31067\n",
      "Epoch 37 | Train MSE 14.20400 | Val MSE 12.09937\n",
      "Epoch 38 | Train MSE 15.22898 | Val MSE 11.59849\n",
      "Epoch 39 | Train MSE 15.27398 | Val MSE 10.38429\n",
      "Epoch 40 | Train MSE 13.06255 | Val MSE 13.73597\n",
      "Epoch 41 | Train MSE 13.37986 | Val MSE 9.44942\n",
      "Epoch 42 | Train MSE 16.54141 | Val MSE 10.25150\n",
      "Epoch 43 | Train MSE 14.62479 | Val MSE 10.85243\n",
      "Epoch 44 | Train MSE 14.35441 | Val MSE 10.96343\n",
      "Epoch 45 | Train MSE 14.63981 | Val MSE 11.84567\n",
      "Epoch 46 | Train MSE 15.04846 | Val MSE 13.12084\n",
      "Epoch 47 | Train MSE 15.63209 | Val MSE 9.51396\n",
      "Epoch 48 | Train MSE 14.15186 | Val MSE 20.45436\n",
      "Epoch 49 | Train MSE 14.11051 | Val MSE 10.27982\n",
      "Epoch 50 | Train MSE 14.24806 | Val MSE 10.77492\n",
      "Epoch 51 | Train MSE 12.15784 | Val MSE 10.34827\n",
      "Epoch 52 | Train MSE 12.37167 | Val MSE 14.41687\n",
      "Epoch 53 | Train MSE 14.42174 | Val MSE 10.28522\n",
      "Epoch 54 | Train MSE 12.52383 | Val MSE 11.96236\n",
      "Epoch 55 | Train MSE 11.92068 | Val MSE 8.55510\n",
      "Epoch 56 | Train MSE 11.61130 | Val MSE 9.40926\n",
      "Epoch 57 | Train MSE 13.32588 | Val MSE 9.00419\n",
      "Epoch 58 | Train MSE 11.66205 | Val MSE 10.09485\n",
      "Epoch 59 | Train MSE 10.95480 | Val MSE 9.39680\n",
      "Epoch 60 | Train MSE 11.73180 | Val MSE 9.94248\n",
      "Epoch 61 | Train MSE 13.39159 | Val MSE 10.31505\n",
      "Epoch 62 | Train MSE 14.26331 | Val MSE 10.40638\n",
      "Epoch 63 | Train MSE 12.35314 | Val MSE 9.64380\n",
      "Epoch 64 | Train MSE 12.01059 | Val MSE 8.85970\n",
      "Epoch 65 | Train MSE 11.95660 | Val MSE 9.41340\n",
      "Epoch 66 | Train MSE 11.70076 | Val MSE 10.34993\n",
      "Epoch 67 | Train MSE 13.66351 | Val MSE 8.62140\n",
      "Epoch 68 | Train MSE 12.52074 | Val MSE 12.27220\n",
      "Epoch 69 | Train MSE 11.63254 | Val MSE 8.07605\n",
      "Epoch 70 | Train MSE 10.99120 | Val MSE 12.34381\n",
      "Epoch 71 | Train MSE 10.68579 | Val MSE 8.19446\n",
      "Epoch 72 | Train MSE 11.15382 | Val MSE 7.57509\n",
      "Epoch 73 | Train MSE 10.94152 | Val MSE 7.95025\n",
      "Epoch 74 | Train MSE 10.97522 | Val MSE 8.58067\n",
      "Epoch 75 | Train MSE 11.90443 | Val MSE 11.73094\n",
      "Epoch 76 | Train MSE 11.32353 | Val MSE 9.53671\n",
      "Epoch 77 | Train MSE 10.79033 | Val MSE 10.21553\n",
      "Epoch 78 | Train MSE 12.70700 | Val MSE 7.95467\n",
      "Epoch 79 | Train MSE 9.95924 | Val MSE 12.18809\n",
      "Epoch 80 | Train MSE 11.14458 | Val MSE 13.06907\n",
      "Epoch 81 | Train MSE 11.13580 | Val MSE 8.07064\n",
      "Epoch 82 | Train MSE 11.46705 | Val MSE 10.35231\n",
      "Epoch 83 | Train MSE 11.56969 | Val MSE 12.24652\n",
      "Epoch 84 | Train MSE 12.22238 | Val MSE 9.19988\n",
      "Epoch 85 | Train MSE 11.70529 | Val MSE 10.21225\n",
      "Epoch 86 | Train MSE 10.63451 | Val MSE 7.89701\n",
      "Epoch 87 | Train MSE 11.27174 | Val MSE 9.18447\n",
      "Early stopping.\n",
      "[hybridgnn_rg] Best Val — MAE 1.905321 | RMSE 2.752288 | R2 0.6558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattg\\AppData\\Local\\Temp\\ipykernel_27496\\870047112.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "rg_cfg = {'gnn_dim': 1024, 'hidden_dim': 384, 'dropout_rate': 0.3735260731607324,\n",
    "           'lr': 5.956024201538505e-04, 'activation': 'Swish', 'optimizer': 'AdamW',\n",
    "           'weight_decay': 8.619671341229739e-06}\n",
    "\n",
    "model_rg = HybridGNN(\n",
    "    gnn_dim=rg_cfg['gnn_dim'],\n",
    "    rdkit_dim=15,\n",
    "    hidden_dim=rg_cfg['hidden_dim'],\n",
    "    dropout_rate=rg_cfg['dropout_rate'],\n",
    "    activation=rg_cfg['activation']\n",
    ")\n",
    "\n",
    "model_rg, ckpt_rg, metrics_rg = train_hybrid_gnn(\n",
    "    model_rg, train_loader_rg, val_loader_rg,\n",
    "    lr=rg_cfg['lr'], optimizer=rg_cfg['optimizer'],\n",
    "    weight_decay=rg_cfg['weight_decay'],\n",
    "    epochs=120, patience=15,  \n",
    "    save_dir=\"saved_models/gnn_rg\", tag=\"hybridgnn_rg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "211e4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffv_cfg = {'gnn_dim': 256, 'hidden_dim': 512, 'dropout_rate': 0.34404144200017467, \n",
    "#           'lr': 0.0005555079210176292, 'activation': 'Swish', 'optimizer': 'RMSprop', \n",
    "#           'weight_decay': 9.056299733554687e-06}\n",
    "\n",
    "# model_ffv = HybridGNN(\n",
    "#     gnn_dim=ffv_cfg['gnn_dim'],\n",
    "#     rdkit_dim=15,\n",
    "#     hidden_dim=ffv_cfg['hidden_dim'],\n",
    "#     dropout_rate=ffv_cfg['dropout_rate'],\n",
    "#     activation=ffv_cfg['activation']\n",
    "# )\n",
    "\n",
    "# model_ffv, ckpt_ffv, metrics_ffv = train_hybrid_gnn(\n",
    "#     model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "#     lr=ffv_cfg['lr'], optimizer=ffv_cfg['optimizer'],\n",
    "#     weight_decay=ffv_cfg['weight_decay'],\n",
    "#     epochs=120, patience=15,  \n",
    "#     save_dir=\"saved_models/gnn_ffv\", tag=\"hybridgnn_ffv\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2606b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv written: (3, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattg\\AppData\\Local\\Temp\\ipykernel_27496\\3210015621.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  m_tg.load_state_dict(torch.load(ckpt_tg, map_location=device))\n",
      "C:\\Users\\mattg\\AppData\\Local\\Temp\\ipykernel_27496\\3210015621.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  m_den.load_state_dict(torch.load(ckpt_den, map_location=device))\n",
      "C:\\Users\\mattg\\AppData\\Local\\Temp\\ipykernel_27496\\3210015621.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  m_rg.load_state_dict(torch.load(ckpt_rg, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1109053969</td>\n",
       "      <td>99.650856</td>\n",
       "      <td>0.369987</td>\n",
       "      <td>0.225848</td>\n",
       "      <td>1.111971</td>\n",
       "      <td>25.481236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1422188626</td>\n",
       "      <td>150.003799</td>\n",
       "      <td>0.376855</td>\n",
       "      <td>0.232995</td>\n",
       "      <td>1.041540</td>\n",
       "      <td>21.680067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2032016830</td>\n",
       "      <td>78.972870</td>\n",
       "      <td>0.356142</td>\n",
       "      <td>0.264171</td>\n",
       "      <td>1.074244</td>\n",
       "      <td>19.211025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id          Tg       FFV        Tc   Density         Rg\n",
       "0  1109053969   99.650856  0.369987  0.225848  1.111971  25.481236\n",
       "1  1422188626  150.003799  0.376855  0.232995  1.041540  21.680067\n",
       "2  2032016830   78.972870  0.356142  0.264171  1.074244  19.211025"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Final submission: RF(FFV,Tc) + GNN(Tg,Density,Rg) using LMDB =====\n",
    "import os, numpy as np, pandas as pd, joblib, torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors as rdmd, DataStructs\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "\n",
    "# ---- 0) Load test ids & smiles\n",
    "sample   = pd.read_csv(os.path.join(DATA_ROOT, 'sample_submission.csv'))\n",
    "test_df  = pd.read_csv(os.path.join(DATA_ROOT, 'test.csv'))\n",
    "test_ids = test_df['id'].astype(sample['id'].dtype).values\n",
    "test_smiles = test_df['SMILES'].astype(str).tolist()\n",
    "\n",
    "#  1) Morgan FPs for RF models (FFV, Tc)\n",
    "def morgan_bits_from_smiles(smiles_list, n_bits=1024, radius=3):\n",
    "    X = np.zeros((len(smiles_list), n_bits), dtype=np.uint8)\n",
    "    for i, s in enumerate(smiles_list):\n",
    "        arr = np.zeros((n_bits,), dtype=np.uint8)\n",
    "        mol = Chem.MolFromSmiles(s)  # no canonicalization; if parse fails, stays zeros\n",
    "        if mol is not None:\n",
    "            fp = rdmd.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=n_bits)\n",
    "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        X[i] = arr\n",
    "    return X\n",
    "\n",
    "X_test_fp = morgan_bits_from_smiles(test_smiles, n_bits=1024, radius=3)\n",
    "\n",
    "# Load trained RFs\n",
    "rf_ffv = joblib.load(p_ffv)['model']\n",
    "rf_tc  = joblib.load(p_tc)['model']\n",
    "\n",
    "pred_ffv = rf_ffv.predict(X_test_fp).astype(float)\n",
    "pred_tc  = rf_tc.predict(X_test_fp).astype(float)\n",
    "\n",
    "# ---- 2) GNN predictions (Tg, Density, Rg) via LMDB test loader\n",
    "test_ds = LMDBtoPyGSingleTask(test_ids, TEST_LMDB, target_index=None)\n",
    "test_loader = GeoDataLoader(test_ds, batch_size=128, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_gnn(model, loader, device):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    for b in loader:\n",
    "        b = b.to(device)\n",
    "        p = model(b).view(-1).cpu().numpy()\n",
    "        outs.append(p)\n",
    "    return np.concatenate(outs, axis=0) if outs else np.zeros((0,), dtype=float)\n",
    "\n",
    "\n",
    "# Tg\n",
    "m_tg = HybridGNN(gnn_dim=256, rdkit_dim=15, hidden_dim=512,\n",
    "                 dropout_rate=0.34404144200017467, activation=\"Swish\").to(device)\n",
    "m_tg.load_state_dict(torch.load(ckpt_tg, map_location=device))\n",
    "pred_tg = predict_gnn(m_tg, test_loader, device)\n",
    "\n",
    "# Density\n",
    "den_cfg = {'gnn_dim': 1024, 'hidden_dim': 384, 'dropout_rate': 0.3735260731607324,\n",
    "           'lr': 5.956024201538505e-04, 'activation': 'Swish', 'optimizer': 'AdamW',\n",
    "           'weight_decay': 8.619671341229739e-06}\n",
    "m_den = HybridGNN(gnn_dim=den_cfg['gnn_dim'], rdkit_dim=15,\n",
    "                  hidden_dim=den_cfg['hidden_dim'], dropout_rate=den_cfg['dropout_rate'],\n",
    "                  activation=den_cfg['activation']).to(device)\n",
    "m_den.load_state_dict(torch.load(ckpt_den, map_location=device))\n",
    "pred_density = predict_gnn(m_den, test_loader, device)\n",
    "\n",
    "# Rg (your tuned GNN)\n",
    "m_rg = HybridGNN(gnn_dim=rg_cfg['gnn_dim'], rdkit_dim=15,\n",
    "                 hidden_dim=rg_cfg['hidden_dim'], dropout_rate=rg_cfg['dropout_rate'],\n",
    "                 activation=rg_cfg['activation']).to(device)\n",
    "m_rg.load_state_dict(torch.load(ckpt_rg, map_location=device))\n",
    "pred_rg = predict_gnn(m_rg, test_loader, device)\n",
    "\n",
    "# ---- 3) Safety + assemble submission\n",
    "pred_tg      = np.nan_to_num(pred_tg)\n",
    "pred_density = np.nan_to_num(pred_density)\n",
    "pred_ffv     = np.nan_to_num(pred_ffv)\n",
    "pred_tc      = np.nan_to_num(pred_tc)\n",
    "pred_rg      = np.nan_to_num(pred_rg)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Tg': pred_tg,\n",
    "    'FFV': pred_ffv,\n",
    "    'Tc': pred_tc,\n",
    "    'Density': pred_density,\n",
    "    'Rg': pred_rg,\n",
    "})\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "print('submission.csv written:', sub.shape)\n",
    "sub.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673460",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656cce6e",
   "metadata": {},
   "source": [
    "## Model Performance Summary\n",
    "\n",
    "All baseline models were initially trained and evaluated on a 5,000 molecule subset of the full dataset. Below is a comparison of results across different featurization strategies and model types:\n",
    "\n",
    "### 2D Baseline Models\n",
    "\n",
    "| Model Type    | Featurization      | MAE   | RMSE  | R²    | Notes                                 |\n",
    "| ------------- | ------------------ | ----- | ----- | ----- | ------------------------------------- |\n",
    "| MLP (Tuned)   | RDKit Fingerprints | 0.426 | 0.574 | 0.798 | Strong performance across all metrics |\n",
    "| KRR (Tuned)   | RDKit Fingerprints | 0.454 | 0.593 | 0.784 | Good overall, slightly lower R²       |\n",
    "| RF (Tuned)    | RDKit Fingerprints | 0.423 | 0.583 | 0.791 | Best MAE, very competitive overall    |\n",
    "| MLP (Tuned)   | Coulomb Matrix     | 0.636 | 0.819 | 0.588 | Significantly weaker performance      |\n",
    "| MLP (Untuned) | RDKit Fingerprints | 0.467 | 0.609 | 0.772 | Solid untuned baseline                |\n",
    "| KRR (Untuned) | RDKit Fingerprints | 0.519 | 0.668 | 0.726 | Notable drop from tuned version       |\n",
    "| RF (Untuned)  | RDKit Fingerprints | 0.426 | 0.587 | 0.788 | Surprisingly close to tuned RF        |\n",
    "| MLP (Untuned) | Coulomb Matrix     | 0.663 | 0.847 | 0.559 | Consistently underperforms            |\n",
    "\n",
    "### Graph Neural Network Models (ChemML)\n",
    "\n",
    "| Model Type    | Featurization               | MAE   | RMSE  | R²    | Notes                                |\n",
    "| ------------- | --------------------------- | ----- | ----- | ----- | ------------------------------------ |\n",
    "| GNN (Tuned)   | `tensorise_molecules` Graph | 0.302 | 0.411 | 0.900 | Best results from ChemML experiments |\n",
    "| GNN (Untuned) | `tensorise_molecules` Graph | 0.400 | 0.519 | 0.841 | Strong but less optimized            |\n",
    "\n",
    "### Final Hybrid GNN Model Trained on Full Dataset (OGB-Compatible)\n",
    "\n",
    "| Model Type           | Featurization                          | MAE   | RMSE  | R²    | Notes                              |\n",
    "| -------------------- | -------------------------------------- | ----- | ----- | ----- | ---------------------------------- |\n",
    "| Hybrid GNN (Tuned)   | OGB `smiles2graph` + RDKit descriptors | 0.159 | 0.234 | 0.965 | State-of-the-art level performance |\n",
    "| Hybrid GNN (Untuned) | OGB `smiles2graph` + RDKit descriptors | 0.223 | 0.308 | 0.939 | Still very strong pre-tuning       |\n",
    "\n",
    "---\n",
    "\n",
    "## Model Error Analysis\n",
    "\n",
    "I performed qualitative evaluation by comparing predicted vs. true HOMO–LUMO gaps for both randomly selected and poorly predicted molecules. The worst performing molecules often showed rare or complex structures likely underrepresented in the training set. This highlights the importance of structural diversity and potentially more expressive 3D information to improve generalization.\n",
    "\n",
    "## Next Steps: Integrating 3D Molecular Information\n",
    "\n",
    "To push performance even further and overcome limitations of 2D graphs and hand crafted descriptors, my next step will involve:\n",
    "\n",
    "* Using **3D molecular geometries** \n",
    "* Incorporating **interatomic distances**, angles, and **spatial encoding** (SchNet, DimeNet, or SE(3)-equivariant models)\n",
    "* Comparing results against the current best MAE (\\~0.159)\n",
    "\n",
    "This direction aligns with trends in molecular property prediction where 3D aware models often outperform purely 2D approaches, especially for quantum properties like HOMO–LUMO gaps.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
