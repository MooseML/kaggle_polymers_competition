{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ace_tools_open as tools\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import pickle\n",
    "import joblib\n",
    "import os \n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Module, Sequential, Dropout\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# OGB dataset \n",
    "from ogb.lsc import PygPCQM4Mv2Dataset, PCQM4Mv2Dataset\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# RDKit\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "\n",
    "# ChemML\n",
    "from chemml.chem import Molecule, RDKitFingerprint, CoulombMatrix, tensorise_molecules\n",
    "from chemml.models import MLP, NeuralGraphHidden, NeuralGraphOutput\n",
    "from chemml.utils import regression_metrics\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589db70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = os.getcwd() \n",
    "PROJECT_ROOT = os.path.join(CWD, '..', '..', '..')\n",
    "PROJECT_ROOT = os.path.abspath(PROJECT_ROOT)\n",
    "\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  \n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/hlgap-gnn3d-transformer-pcqm4mv2-v1.pt'\n",
    "else:\n",
    "    DATA_ROOT = os.path.join(PROJECT_ROOT, 'data')\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = os.path.join(PROJECT_ROOT, 'hlgap-gnn3d-transformer-pcqm4mv2-v1.pt')\n",
    "\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    build_script_path = os.path.join(PROJECT_ROOT, 'scripts', 'data_preprocessing', 'build_lmdb.py')\n",
    "\n",
    "    if not os.path.exists(build_script_path):\n",
    "        print(f\"ERROR: LMDB building script not found at {build_script_path}. Please check file location.\")\n",
    "    else:\n",
    "        !python {build_script_path} train\n",
    "        !python {build_script_path} test\n",
    "        print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(DATA_ROOT, 'train.csv')\n",
    "train_df   = pd.read_csv(train_path)\n",
    "\n",
    "#  Keep only the columns we care about \n",
    "target_cols = ['SMILES', 'Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "train_df = train_df[target_cols]       \n",
    "n = len(train_df)\n",
    "subset_size = n                       \n",
    "subset_df   = train_df.sample(subset_size, random_state=42)\n",
    "\n",
    "#  Save the subset as a CSV \n",
    "subset_path = os.path.join(DATA_ROOT, 'train_subset.csv')\n",
    "subset_df.to_csv(subset_path, index=False)\n",
    "\n",
    "print(f\"Saved CSV with shape: {subset_df.shape}\")\n",
    "print(subset_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "mol = Molecule(subset_df['SMILES'][0], input_type='smiles')\n",
    "mol.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f557b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(subset_path)\n",
    "print(f\"Loaded {len(df)} molecules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04007d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1779d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "total_rows = len(df)\n",
    "percent_missing = (missing_values / total_rows) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Total Missing': missing_values,\n",
    "    'Percent Missing': percent_missing\n",
    "})\n",
    "\n",
    "print(\"Missing Values by Column:\")\n",
    "print(missing_df)\n",
    "print(\"\\nFeature Statistics (Min, Max, Mean, etc.):\")\n",
    "print(df[['Tg', 'FFV', 'Tc', 'Density', 'Rg']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe69f3",
   "metadata": {},
   "source": [
    "# Tg Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc711963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a new DataFrame with only the SMILES and Tg columns\n",
    "df_tg = df[['SMILES', 'Tg']].copy()\n",
    "\n",
    "print(\"Initial Tg DataFrame shape:\", df_tg.shape)\n",
    "print(\"Initial Tg Missing Values:\")\n",
    "print(df_tg.isnull().sum())\n",
    "\n",
    "# 2. Drop all rows where the 'Tg' value is missing\n",
    "df_tg.dropna(subset=['Tg'], inplace=True)\n",
    "\n",
    "print(\"\\nCleaned Tg DataFrame shape:\", df_tg.shape)\n",
    "print(\"Cleaned Tg Missing Values:\")\n",
    "print(df_tg.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Descriptors, SanitizeFlags\n",
    "def rdkit_ogb_agree(smi: str) -> bool:\n",
    "    m = Chem.MolFromSmiles(smi)\n",
    "    if m is None:\n",
    "        return False\n",
    "    return m.GetNumAtoms() == smiles2graph(smi)[\"num_nodes\"]\n",
    "\n",
    "def canonicalize_polymer_smiles(smiles: str, cap_atomic_num: int = 6) -> str:\n",
    "    \"\"\"\n",
    "    Turn every '*' (dummy atom) into a real atom (default C) in the RDKit graph,\n",
    "    preserving existing bond orders/stereo; sanitize, remove explicit Hs, and\n",
    "    return canonical isomeric SMILES.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles, sanitize=False)\n",
    "    if mol is None:\n",
    "        raise ValueError(f\"RDKit could not parse SMILES: {smiles}\")\n",
    "\n",
    "    rw = Chem.RWMol(mol)\n",
    "    for a in rw.GetAtoms():\n",
    "        if a.GetAtomicNum() == 0:   # '*'\n",
    "            a.SetAtomicNum(cap_atomic_num)  # 6 = carbon\n",
    "            a.SetFormalCharge(0)\n",
    "            a.SetIsAromatic(False)\n",
    "            a.SetNoImplicit(False)\n",
    "            a.SetNumExplicitHs(0)\n",
    "\n",
    "    mol2 = rw.GetMol()\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol2)\n",
    "    except Exception:\n",
    "        Chem.SanitizeMol(mol2, sanitizeOps=SanitizeFlags.SANITIZE_ALL ^ SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "        Chem.Kekulize(mol2, clearAromaticFlags=True)\n",
    "\n",
    "    mol2 = Chem.RemoveHs(mol2)\n",
    "    return Chem.MolToSmiles(mol2, isomericSmiles=True, canonical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff48e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the molecule list\n",
    "valid_mol_objs = []\n",
    "valid_targets = []  \n",
    "for i, row in df_tg.iterrows():\n",
    "    smi = row['SMILES']\n",
    "\n",
    "    cleaned_smiles = canonicalize_polymer_smiles(smi)\n",
    "\n",
    "    try:\n",
    "        mol = Molecule(cleaned_smiles, input_type='smiles')\n",
    "        mol.hydrogens('add')\n",
    "        mol.to_xyz(optimizer='MMFF', maxIters=200)\n",
    "\n",
    "\n",
    "        if mol.xyz is not None:\n",
    "            valid_mol_objs.append(mol)\n",
    "            \n",
    "            valid_targets.append(\n",
    "                row[['Tg']].values\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Skipped bc missing xyz: {smi}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {smi} | Reason: {e}\")\n",
    "\n",
    "print(f\"Kept {len(valid_mol_objs)} molecules after filtering.\")\n",
    "\n",
    "df_clean = pd.DataFrame({\n",
    "    'SMILES': [m.smiles for m in valid_mol_objs],\n",
    "    'Tg':     [t[0] for t in valid_targets],\n",
    "})\n",
    "print(f\"Kept {len(df_clean)} molecules after filtering.\")\n",
    "df_clean.to_csv('cleaned_tg_dataset.csv', index=False)\n",
    "print(\"Saved cleaned Tg dataset to 'cleaned_tg_dataset.csv'.\")\n",
    "\n",
    "y = np.array([t[0] for t in valid_targets])\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "fp_featurizer = RDKitFingerprint(\n",
    "    fingerprint_type='morgan', vector='bit', n_bits=1024, radius=3\n",
    ")\n",
    "X_fp = fp_featurizer.represent(valid_mol_objs)\n",
    "\n",
    "print(\"RDKit FP shape:\", X_fp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff620911",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fp, X_test_fp, y_train, y_test = train_test_split(\n",
    "    X_fp, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "xscaler_fp = StandardScaler()\n",
    "yscaler = StandardScaler()\n",
    "\n",
    "X_train_fp_scaled = xscaler_fp.fit_transform(X_train_fp)\n",
    "X_test_fp_scaled = xscaler_fp.transform(X_test_fp)\n",
    "\n",
    "y_train_scaled = yscaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = yscaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "X_train_fp_unscaled = X_train_fp\n",
    "X_test_fp_unscaled = X_test_fp\n",
    "y_train_unscaled = y_train\n",
    "y_test_unscaled = y_test\n",
    "\n",
    "tools.display_dataframe_to_user(\n",
    "    name=\"Cleaned Feature Splits\",\n",
    "    dataframe=pd.DataFrame({\n",
    "        \"Split\": [\n",
    "            \"X_train_fp_scaled\", \"X_test_fp_scaled\",\n",
    "            \"y_train_scaled\", \"y_test_scaled\",\n",
    "            \"X_train_fp_unscaled\", \"X_test_fp_unscaled\",\n",
    "            \"y_train_unscaled\", \"y_test_unscaled\"\n",
    "        ],\n",
    "        \"Shape\": [\n",
    "            X_train_fp_scaled.shape, X_test_fp_scaled.shape,\n",
    "            y_train_scaled.shape, y_test_scaled.shape,\n",
    "            X_train_fp_unscaled.shape, X_test_fp_unscaled.shape,\n",
    "            y_train_unscaled.shape, y_test_unscaled.shape\n",
    "        ]\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489dc183",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression baseline first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9778d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Ridge on RDKit fingerprints\n",
    "krr = KernelRidge(kernel='rbf', alpha=1.0)\n",
    "krr.fit(X_train_fp_scaled, y_train_scaled)\n",
    "\n",
    "# predict on scaled test set\n",
    "y_pred_krr_scaled = krr.predict(X_test_fp_scaled)\n",
    "\n",
    "# Inverse transform predictions and test targets to compare with unscaled values\n",
    "# must reshape y_pred_krr_scaled and y_test_scaled to 2D before inverse transforming\n",
    "y_pred_krr = yscaler.inverse_transform(y_pred_krr_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_krr = yscaler.inverse_transform(y_test_scaled).flatten()\n",
    "\n",
    "# Eval against true unscaled test target\n",
    "print(\"Kernel Ridge (RDKit FP)\")\n",
    "metrics_krr = regression_metrics(y_test_krr, y_pred_krr)\n",
    "print(metrics_krr[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0fedd",
   "metadata": {},
   "source": [
    "## Random Forest Regression baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest (RDKit FP) \n",
    "rfr = RandomForestRegressor(n_estimators=100, max_depth=30, random_state=42)\n",
    "rfr.fit(X_train_fp_unscaled, y_train_unscaled)\n",
    "# predict\n",
    "y_pred_rfr = rfr.predict(X_test_fp_unscaled)\n",
    "# eval\n",
    "print(\"Random Forest (RDKit FP)\")\n",
    "metrics_rfr = regression_metrics(y_test_unscaled, y_pred_rfr)\n",
    "print(metrics_rfr[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74973657",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron with Morgan Fingerprints baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bbb566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP (Fingerprint)\n",
    "mlp_fp = MLP(\n",
    "    engine='tensorflow',\n",
    "    nfeatures=X_train_fp_scaled.shape[1],\n",
    "    nneurons=[64, 128], # These are the hidden layers\n",
    "    activations=['ReLU', 'ReLU'],\n",
    "    learning_rate=0.01,\n",
    "    alpha=0.001,\n",
    "    nepochs=200,\n",
    "    batch_size=64,\n",
    "    loss='mean_squared_error',\n",
    "    is_regression=True\n",
    ")\n",
    "\n",
    "mlp_fp.fit(X=X_train_fp_scaled, y=y_train_scaled.ravel()) # Use .ravel() to convert to 1D\n",
    "y_pred_fp_scaled = mlp_fp.predict(X_test_fp_scaled)\n",
    "# Reshape the output from predict() to 2D before inverse transforming\n",
    "y_pred_fp = yscaler.inverse_transform(y_pred_fp_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_fp = yscaler.inverse_transform(y_test_scaled).flatten()\n",
    "\n",
    "# Eval against true unscaled test target\n",
    "print(\"MLP (RDKit FP)\")\n",
    "metrics_mlp = regression_metrics(y_test, y_pred_fp)\n",
    "print(metrics_mlp[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7101926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "results = {\"Kernel Ridge (RDKit FP)\": regression_metrics(y_test_krr, y_pred_krr),\n",
    "           \"Random Forest (RDKit FP)\": regression_metrics(y_test_unscaled, y_pred_rfr),\n",
    "           \"MLP (RDKit FP)\": regression_metrics(y_test_fp, y_pred_fp),\n",
    "        }\n",
    "\n",
    "# display\n",
    "print(\"Final Model Comparison\")\n",
    "for name, metrics_df in results.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(metrics_df[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e7366",
   "metadata": {},
   "source": [
    "## Parity Plots and Residuals Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff8791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_results(y_true, y_pred, title=\"Model Evaluation\", save_dir=\"plots\"):\n",
    "    residuals = y_true.flatten() - y_pred.flatten()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # parity plot\n",
    "    sns.scatterplot(ax=axes[0], x=y_true.flatten(), y=y_pred.flatten(), alpha=0.5)\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    axes[0].plot([min_val, max_val], [min_val, max_val], '--r')\n",
    "    axes[0].set_xlabel(\"Actual HOMO-LUMO Gap\")\n",
    "    axes[0].set_ylabel(\"Predicted HOMO-LUMO Gap\")\n",
    "    axes[0].set_title(\"Parity Plot\")\n",
    "    axes[0].grid(True)\n",
    "    axes[0].axis('equal')\n",
    "\n",
    "    # residuals histogram\n",
    "    sns.histplot(ax=axes[1], data=residuals, bins=30, kde=True)\n",
    "    axes[1].set_title(\"Residuals Histogram\")\n",
    "    axes[1].set_xlabel(\"Residual (Actual - Predicted)\")\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    # overall title\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filename = os.path.join(save_dir, f\"{title.lower().replace(' ', '_')}_plots.pdf\")\n",
    "    fig.savefig(filename, bbox_inches='tight')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_regression_results(y_test_fp, y_pred_fp, title=\"MLP Untuned(RDKit FP)\")\n",
    "plot_regression_results(y_test_krr, y_pred_krr, title=\"Kernel Ridge Untuned (RDKit FP)\")\n",
    "plot_regression_results(y_test_unscaled, y_pred_rfr, title=\"Random Forest Untuned (RDKit FP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dce050",
   "metadata": {},
   "source": [
    "## Tune hyperparameters for baseline models with Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66717bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_krr(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 1.0, log=True)\n",
    "    kernel = trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly'])\n",
    "    gamma = trial.suggest_float('gamma', 1e-5, 1.0, log=True) if kernel == 'rbf' else None\n",
    "    model = KernelRidge(alpha=alpha, kernel=kernel, gamma=gamma) if gamma else KernelRidge(alpha=alpha, kernel=kernel)\n",
    "    model.fit(X_train_fp_scaled, y_train_scaled)\n",
    "    preds_scaled = model.predict(X_test_fp_scaled).reshape(-1, 1)\n",
    "    preds = yscaler.inverse_transform(preds_scaled)\n",
    "    y_test_inv = yscaler.inverse_transform(y_test_scaled)\n",
    "    metrics = regression_metrics(y_test_inv, preds)\n",
    "    return metrics['MAE'][0]\n",
    "\n",
    "study_krr = optuna.create_study(direction='minimize')\n",
    "study_krr.optimize(objective_krr, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83942b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rfr(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 100, step=10)\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train_fp_unscaled, y_train_unscaled)\n",
    "    preds = model.predict(X_test_fp_unscaled)\n",
    "    metrics = regression_metrics(y_test_unscaled, preds)\n",
    "    return metrics['MAE'][0]\n",
    "\n",
    "study_rfr = optuna.create_study(direction='minimize')\n",
    "study_rfr.optimize(objective_rfr, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6bd272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_mlp_fp(trial):\n",
    "    lr = trial.suggest_float('lr', 1e-4, 2e-2, log=True)\n",
    "    alpha = trial.suggest_float('alpha', 1e-4, 5e-2, log=True)\n",
    "    act = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid', 'gelu'])\n",
    "    n1 = trial.suggest_int('n1', 128, 384, step=64)\n",
    "    n2 = trial.suggest_int('n2', 128, 384, step=64)\n",
    "\n",
    "    model = MLP(engine='tensorflow', nfeatures=X_train_fp_scaled.shape[1], nneurons=[n1, n2],\n",
    "                activations=[act, act], learning_rate=lr, alpha=alpha,\n",
    "                nepochs=100, batch_size=64, loss='mean_squared_error', is_regression=True)\n",
    "    \n",
    "    model.fit(X_train_fp_scaled, y_train_scaled)\n",
    "    preds_scaled = model.predict(X_test_fp_scaled).reshape(-1, 1)\n",
    "    preds = yscaler.inverse_transform(preds_scaled)\n",
    "    y_test_inv = yscaler.inverse_transform(y_test_scaled)\n",
    "    metrics = regression_metrics(y_test_inv, preds)\n",
    "    return metrics['MAE'][0]\n",
    "\n",
    "study_mlp_fp = optuna.create_study(direction='minimize')\n",
    "study_mlp_fp.optimize(objective_mlp_fp, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d47ff58",
   "metadata": {},
   "source": [
    "## Retrain Models with Best Parameters Found in Respective Optuna Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_krr = study_krr.best_params\n",
    "\n",
    "# build final model using best params\n",
    "if best_krr['kernel'] == 'rbf':\n",
    "    final_krr = KernelRidge(alpha=best_krr['alpha'], kernel='rbf', gamma=best_krr['gamma'])\n",
    "else:\n",
    "    final_krr = KernelRidge(alpha=best_krr['alpha'], kernel=best_krr['kernel'])\n",
    "\n",
    "# train on scaled data\n",
    "final_krr.fit(X_train_fp_scaled, y_train_scaled)\n",
    "# predict on test set (scaled)\n",
    "final_preds_krr_scaled = final_krr.predict(X_test_fp_scaled).reshape(-1, 1)\n",
    "# inverse transform both predictions and gt\n",
    "final_preds_krr = yscaler.inverse_transform(final_preds_krr_scaled)\n",
    "y_test_krr = yscaler.inverse_transform(y_test_scaled)\n",
    "# eval\n",
    "final_metrics_krr = regression_metrics(y_test_krr, final_preds_krr)\n",
    "print(\"Final Tuned Kernel Ridge (RDKit FP):\")\n",
    "print(final_metrics_krr[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89540fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild and retrain the RFR model\n",
    "best_rfr = study_rfr.best_params\n",
    "final_rfr = RandomForestRegressor(n_estimators=best_rfr['n_estimators'], max_depth=best_rfr['max_depth'], random_state=42)\n",
    "final_rfr.fit(X_train_fp_unscaled, y_train_unscaled)\n",
    "\n",
    "# predict on test set\n",
    "final_preds_rfr = final_rfr.predict(X_test_fp_unscaled)\n",
    "\n",
    "# eval using unscaled targets\n",
    "final_metrics_rfr = regression_metrics(y_test_unscaled, final_preds_rfr)\n",
    "print(\"Final Tuned Random Forest (RDKit FP):\")\n",
    "print(final_metrics_rfr[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fp = study_mlp_fp.best_params\n",
    "final_mlp_fp = MLP(\n",
    "    engine='tensorflow', \n",
    "    nfeatures=X_train_fp_scaled.shape[1], \n",
    "    nneurons=[best_fp['n1'], best_fp['n2']], \n",
    "    activations=[best_fp['activation'], best_fp['activation']], \n",
    "    learning_rate=best_fp['lr'], \n",
    "    alpha=best_fp['alpha'], \n",
    "    nepochs=100, \n",
    "    batch_size=64, \n",
    "    loss='mean_squared_error', \n",
    "    is_regression=True\n",
    "    )\n",
    "\n",
    "# train on scaled data\n",
    "final_mlp_fp.fit(X_train_fp_scaled, y_train_scaled)\n",
    "\n",
    "# predict and inverse transform\n",
    "final_preds_fp_scaled = final_mlp_fp.predict(X_test_fp_scaled).reshape(-1, 1)\n",
    "final_preds_inv_fp = yscaler.inverse_transform(final_preds_fp_scaled)\n",
    "y_test_inv_fp = yscaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "# eval in eV\n",
    "final_metrics_fp = regression_metrics(y_test_inv_fp, final_preds_inv_fp)\n",
    "print(\"Final Tuned MLP (RDKit FP):\")\n",
    "print(final_metrics_fp[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c54319",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_results(y_test_krr, final_preds_krr, title=\"Final Kernel Ridge (RDKit FP)\", save_dir=\"plots\")\n",
    "plot_regression_results(y_test_unscaled, final_preds_rfr, title=\"Final Random Forest (RDKit FP)\", save_dir=\"plots\")\n",
    "plot_regression_results(y_test_inv_fp, final_preds_inv_fp, title=\"Final MLP (RDKit FP)\", save_dir=\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR = os.path.join(PROJECT_ROOT, \"saved_models\", \"baseline\", \"MultiLayerPerceptron\", \"saved_models_Tg\")\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "keras_save_path = os.path.join(MODEL_SAVE_DIR, \"best_mlp_fp_model_keras\")\n",
    "final_mlp_fp.model.save(keras_save_path)\n",
    "\n",
    "xscaler_save_path = os.path.join(MODEL_SAVE_DIR, \"xscaler_fp.pkl\")\n",
    "yscaler_save_path = os.path.join(MODEL_SAVE_DIR, \"yscaler.pkl\")\n",
    "\n",
    "joblib.dump(xscaler_fp, xscaler_save_path)\n",
    "joblib.dump(yscaler, yscaler_save_path)\n",
    "\n",
    "# Save evaluation metrics\n",
    "metrics_save_path = os.path.join(MODEL_SAVE_DIR, \"best_mlp_fp_metrics.csv\")\n",
    "final_metrics_fp.to_csv(metrics_save_path, index=False)\n",
    "\n",
    "print(f\"Models and metrics saved successfully to: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39535b4",
   "metadata": {},
   "source": [
    "# Training a Baseline GNN with ChemML\n",
    "ChemML's `tensorise_molecules` generates its own graph. Its important to note this graph is not the official graph from PCQM4Mv2. It may miss out on features OGB uses like formal charge, aromatacity flags, atomic chirality, and explicit hydrogens. However, tensorise_molecules is a good choice for quick prototyping and it handles graph generation and tensor formatting in a numpy-friendly way which was easier for me to understand. Final training will use smiles2graph for compatability with OGB splits and better feature representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf8404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorize molecules\n",
    "X_atoms, X_bonds, X_edges = tensorise_molecules(valid_mol_objs)\n",
    "y = df_clean['Tg'].values.reshape(-1, 1)\n",
    "\n",
    "# train test split (80/20)\n",
    "split = int(0.8 * len(y))\n",
    "X_atoms_train, X_atoms_test = X_atoms[:split], X_atoms[split:]\n",
    "X_bonds_train, X_bonds_test = X_bonds[:split], X_bonds[split:]\n",
    "X_edges_train, X_edges_test = X_edges[:split], X_edges[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# scale target\n",
    "yscaler = StandardScaler()\n",
    "y_train_scaled = yscaler.fit_transform(y_train)\n",
    "\n",
    "# model input shapes\n",
    "max_atoms = X_atoms.shape[1]\n",
    "max_degree = X_bonds.shape[2]\n",
    "num_atom_features = X_atoms.shape[-1]\n",
    "num_bond_features = X_bonds.shape[-1]\n",
    "\n",
    "# input layers\n",
    "atoms_input = Input(shape=(max_atoms, num_atom_features), name=\"atom_inputs\")\n",
    "bonds_input = Input(shape=(max_atoms, max_degree, num_bond_features), name=\"bond_inputs\")\n",
    "edges_input = Input(shape=(max_atoms, max_degree), name=\"edge_inputs\", dtype=\"int32\")\n",
    "\n",
    "# GNN layers\n",
    "conv1 = NeuralGraphHidden(8, activation='relu')([atoms_input, bonds_input, edges_input])\n",
    "conv2 = NeuralGraphHidden(8, activation='relu')([conv1, bonds_input, edges_input])\n",
    "\n",
    "fp1 = NeuralGraphOutput(128, activation='relu')([atoms_input, bonds_input, edges_input])\n",
    "fp2 = NeuralGraphOutput(128, activation='relu')([conv1, bonds_input, edges_input])\n",
    "fp3 = NeuralGraphOutput(128, activation='relu')([conv2, bonds_input, edges_input])\n",
    "\n",
    "# fingerprint aggregation\n",
    "fingerprint = Add()([fp1, fp2, fp3])\n",
    "\n",
    "# dense layers\n",
    "dense1 = Dense(128, activation='relu')(fingerprint)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "output = Dense(1, activation='linear')(dense2)\n",
    "\n",
    "# model compilation\n",
    "model = Model(inputs=[atoms_input, bonds_input, edges_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# model training\n",
    "model.fit([X_atoms_train, X_bonds_train, X_edges_train], y_train_scaled, epochs=100, batch_size=64, verbose=1, validation_split=0.1)\n",
    "\n",
    "# preds and eval\n",
    "y_pred = model.predict([X_atoms_test, X_bonds_test, X_edges_test])\n",
    "y_pred = yscaler.inverse_transform(y_pred)\n",
    "metrics = regression_metrics(y_test, y_pred)\n",
    "print(\"\\nGNN Model Results:\")\n",
    "print(metrics[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ae357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "plot_regression_results(y_test, y_pred, title=\"GNN Model Untuned(ChemML)\", save_dir=\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "\n",
    "def objective_gnn(trial):\n",
    "    conv_width = trial.suggest_categorical('conv_width', [8, 16, 32])\n",
    "    fp_length = trial.suggest_categorical('fp_length', [96, 128, 160])\n",
    "    n1 = trial.suggest_int('n1', 128, 192, step=32)\n",
    "    n2 = trial.suggest_int('n2', 64, 96, step=32)\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    alpha = trial.suggest_float('alpha', 1e-8, 1e-4, log=True)\n",
    "    activation = 'relu'\n",
    "\n",
    "    # model definition\n",
    "    atoms_input = Input(shape=(max_atoms, num_atom_features), name=\"atom_inputs\")\n",
    "    bonds_input = Input(shape=(max_atoms, max_degree, num_bond_features), name=\"bond_inputs\")\n",
    "    edges_input = Input(shape=(max_atoms, max_degree), name=\"edge_inputs\", dtype=\"int32\")\n",
    "\n",
    "    conv1 = NeuralGraphHidden(conv_width, activation=activation)([atoms_input, bonds_input, edges_input])\n",
    "    conv2 = NeuralGraphHidden(conv_width, activation=activation)([conv1, bonds_input, edges_input])\n",
    "\n",
    "    fp1 = NeuralGraphOutput(fp_length, activation=activation)([atoms_input, bonds_input, edges_input])\n",
    "    fp2 = NeuralGraphOutput(fp_length, activation=activation)([conv1, bonds_input, edges_input])\n",
    "    fp3 = NeuralGraphOutput(fp_length, activation=activation)([conv2, bonds_input, edges_input])\n",
    "    fingerprint = Add()([fp1, fp2, fp3])\n",
    "\n",
    "    dense1 = Dense(n1, activation=activation, kernel_regularizer=regularizers.l2(alpha))(fingerprint)\n",
    "    dense2 = Dense(n2, activation=activation, kernel_regularizer=regularizers.l2(alpha))(dense1)\n",
    "    output = Dense(1, activation='linear')(dense2)\n",
    "\n",
    "    model = Model(inputs=[atoms_input, bonds_input, edges_input], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mean_squared_error', metrics=[MeanAbsoluteError()])\n",
    "\n",
    "    history = model.fit([X_atoms_train, X_bonds_train, X_edges_train], y_train_scaled, epochs=100, batch_size=64, verbose=0, validation_split=0.2)\n",
    "\n",
    "    # return best validation MAE\n",
    "    val_mae = min(history.history[\"val_mean_absolute_error\"])  \n",
    "    return val_mae\n",
    "\n",
    "study_gnn = optuna.create_study(direction='minimize')  \n",
    "study_gnn.optimize(objective_gnn, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf7a39",
   "metadata": {},
   "source": [
    "## Retraining ChemML GNN with Best Parameter Found in Optuna Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = study_gnn.best_params\n",
    "\n",
    "# redefine and compile using best params\n",
    "atoms_input = Input(shape=(max_atoms, num_atom_features), name=\"atom_inputs\")\n",
    "bonds_input = Input(shape=(max_atoms, max_degree, num_bond_features), name=\"bond_inputs\")\n",
    "edges_input = Input(shape=(max_atoms, max_degree), name=\"edge_inputs\", dtype=\"int32\")\n",
    "\n",
    "conv1 = NeuralGraphHidden(params['conv_width'], activation='relu')([atoms_input, bonds_input, edges_input])\n",
    "conv2 = NeuralGraphHidden(params['conv_width'], activation='relu')([conv1, bonds_input, edges_input])\n",
    "\n",
    "fp1 = NeuralGraphOutput(params['fp_length'], activation='relu')([atoms_input, bonds_input, edges_input])\n",
    "fp2 = NeuralGraphOutput(params['fp_length'],activation='relu')([conv1, bonds_input, edges_input])\n",
    "fp3 = NeuralGraphOutput(params['fp_length'], activation='relu')([conv2, bonds_input, edges_input])\n",
    "fingerprint = Add()([fp1, fp2, fp3])\n",
    "\n",
    "dense1 = Dense(params['n1'], activation='relu', kernel_regularizer=regularizers.l2(params['alpha']))(fingerprint)\n",
    "dense2 = Dense(params['n2'], activation='relu', kernel_regularizer=regularizers.l2(params['alpha']))(dense1)\n",
    "output = Dense(1, activation='linear')(dense2)\n",
    "\n",
    "final_gnn = Model(inputs=[atoms_input, bonds_input, edges_input], outputs=output)\n",
    "final_gnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=params['lr']), loss='mean_squared_error')\n",
    "\n",
    "final_gnn.fit([X_atoms_train, X_bonds_train, X_edges_train], y_train_scaled, epochs=200, batch_size=64, verbose=1)\n",
    "\n",
    "# final eval\n",
    "y_pred_final = final_gnn.predict([X_atoms_test, X_bonds_test, X_edges_test])\n",
    "y_pred_final = yscaler.inverse_transform(y_pred_final)\n",
    "final_metrics = regression_metrics(y_test, y_pred_final)\n",
    "print(\"\\nFinal Tuned GNN Results:\")\n",
    "print(final_metrics[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_results(y_test, y_pred_final, title=\"Tuned ChemML GNN\", save_dir=\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64159a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# make a directory for this specific model\n",
    "save_dir = os.path.join(PROJECT_ROOT, \"saved_models\", \"gnn\", \"gnn_tensorise_molecules_model\", \"saved_models_Tg\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 1. save the trained GNN model\n",
    "final_gnn.save(os.path.join(save_dir, \"gnn_tensorise_molecules_model_tf\"), save_format=\"tf\")\n",
    "\n",
    "# 2. save the y target scaler\n",
    "with open(os.path.join(save_dir, \"gnn_tensorise_molecules_target_scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(yscaler, f)\n",
    "\n",
    "# 3. save the final metrics\n",
    "final_metrics.to_csv(os.path.join(save_dir, \"gnn_tensorise_molecules_metrics.csv\"), index=False)\n",
    "\n",
    "# 4. save predictions\n",
    "pred_df = pd.DataFrame({\"true_gap\": y_test.flatten(), \"predicted_gap\": y_pred_final.flatten()})\n",
    "pred_df.to_csv(os.path.join(save_dir, \"gnn_tensorise_molecules_predictions.csv\"), index=False)\n",
    "\n",
    "# 5. save the best hyperparameters\n",
    "with open(os.path.join(save_dir, \"gnn_tensorise_molecules_best_params.json\"), \"w\") as f:\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42db218",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Model Training\n",
    "\n",
    "Having explored different molecular graph representations and model architectures, I am now moving to training what is expected to be the best-performing model using the full dataset. The earlier GNN model was based on `tensorise_molecules` (ChemML) graphs and had strong performance with a **mean absolute error (MAE) around 0.30**. These graphs are based on RDKit's internal descriptors and do not reflect the original PCQM4Mv2 graph structure used in the Open Graph Benchmark (OGB). Therefore, I will shift focus to the `smiles2graph` representation provided by OGB, which aligns more directly with the benchmark's evaluation setup and top-performing models on the leaderboard.\n",
    "\n",
    "\n",
    "| Source                         | Atom/Bond Features                                                 | Format                                          | Customizable?     | Alignment with PCQM4Mv2?  |\n",
    "| ------------------------------ | ------------------------------------------------------------------ | ----------------------------------------------- | ----------------- | ---------------------- |\n",
    "| `tensorise_molecules` (ChemML) | RDKit-based descriptors (ex: atom number, degree, hybridization) | NumPy tensors (`X_atoms`, `X_bonds`, `X_edges`) | Limited           |  Not aligned          |\n",
    "| `smiles2graph` (OGB / PyG)     | Predefined categorical features from PCQM4Mv2                      | PyTorch Geometric `Data` objects                |  Highly flexible |  Matches OGB standard |\n",
    "\n",
    "By using `smiles2graph`, we:\n",
    "\n",
    "* Use OGB-standard graph construction and feature encoding for fair comparisons with leaderboard models\n",
    "* Include learnable AtomEncoder and BondEncoder embeddings from `ogb.graphproppred.mol_encoder`, which improve model expressiveness\n",
    "* Maintain compatibility with PyTorch Geometric, DGL, and OGB tools\n",
    "\n",
    "I will also concatenate GNN-derived embeddings with SMILES-based RDKit descriptors, feeding this hybrid representation into MLP head. This allows you to combine structural and cheminformatics perspectives for improved prediction accuracy. With this setup, I aim to improve upon the MAE of \\~0.30 achieved earlier and push closer toward state-of-the-art performance.\n",
    "\n",
    "\n",
    "## Step 1: Load PyG-Compatible Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8037c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cuda():\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA available? \", torch.cuda.is_available())\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Device count:\", torch.cuda.device_count())\n",
    "        print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "        print(\"Current device:\", torch.cuda.current_device())\n",
    "    else:\n",
    "        print(\"Running on CPU\")\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c907b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load OGB dataset \n",
    "df_tg = pd.read_csv('cleaned_tg_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a988df2",
   "metadata": {},
   "source": [
    "#  Step 2: Extract SMILES from Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract SMILES and Tg targets\n",
    "# `df_tg` already contains the SMILES and Tg columns.\n",
    "smiles_list = df_tg['SMILES'].tolist()\n",
    "Tg_list = df_tg['Tg'].tolist()\n",
    "\n",
    "num_mols = len(smiles_list)\n",
    "print(f\"Loaded {num_mols} molecules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa26be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rdkit_features(smiles):\n",
    "    cleaned_smiles = canonicalize_polymer_smiles(smiles)\n",
    "    mol = Chem.MolFromSmiles(cleaned_smiles)\n",
    "    if mol is None:\n",
    "        return [np.nan] * 9  # Update the number of NaNs to match new features\n",
    "\n",
    "    # Check for empty molecule\n",
    "    if mol.GetNumAtoms() == 0:\n",
    "        return [np.nan] * 9\n",
    "\n",
    "    # Add features that capture size, shape, and interactions\n",
    "    return [\n",
    "        Descriptors.MolWt(mol),\n",
    "        Descriptors.NumRotatableBonds(mol),\n",
    "        Descriptors.TPSA(mol),\n",
    "        Descriptors.NumHAcceptors(mol),\n",
    "        Descriptors.NumHDonors(mol),\n",
    "        Descriptors.RingCount(mol),\n",
    "        Descriptors.FractionCSP3(mol),  # Fraction of sp3 hybridized carbons\n",
    "        Descriptors.MolLogP(mol),      # Octanol-water partition coefficient\n",
    "        Descriptors.NumSaturatedRings(mol) # Number of saturated rings\n",
    "    ]\n",
    "\n",
    "rdkit_features = np.array([compute_rdkit_features(smi) for smi in smiles_list])\n",
    "print(f\"Shape of RDKit features: {rdkit_features.shape}\") # Should be (N, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values (failed RDKit featurization)\n",
    "valid_indices = ~np.isnan(rdkit_features).any(axis=1)\n",
    "rdkit_features = rdkit_features[valid_indices]\n",
    "smiles_list = np.array(smiles_list)[valid_indices].tolist()\n",
    "Tg_list = np.array(Tg_list)[valid_indices].tolist()\n",
    "\n",
    "print(f\"Kept {len(smiles_list)} molecules with valid RDKit features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c75372",
   "metadata": {},
   "source": [
    "# Step 4: attach RDKit features to PyG data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "rdkit_features_tensor = torch.tensor(rdkit_features, dtype=torch.float32)\n",
    "ffv_targets_tensor = torch.tensor(Tg_list, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "data_list = []\n",
    "for i in range(len(smiles_list)):\n",
    "    graph_dict = smiles2graph(smiles_list[i])\n",
    "\n",
    "    data = Data(\n",
    "        x=torch.tensor(graph_dict['node_feat'], dtype=torch.long),\n",
    "        edge_index=torch.tensor(graph_dict['edge_index'], dtype=torch.long),\n",
    "        edge_attr=torch.tensor(graph_dict['edge_feat'], dtype=torch.long),\n",
    "        rdkit_feats=rdkit_features_tensor[i],\n",
    "        y=ffv_targets_tensor[i]\n",
    "    )\n",
    "    data_list.append(data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    print(\"Batch's node features shape:\", batch.x.shape)\n",
    "    print(\"Batch's RDKit features shape:\", batch.rdkit_feats.shape)\n",
    "    print(\"Batch's targets shape:\", batch.y.shape)\n",
    "    print(\"Batch's 'batch' attribute shape:\", batch.batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2984411",
   "metadata": {},
   "source": [
    "## Step 5: Define the Hybrid GNN Model\n",
    "\n",
    "The final architecture uses both structural and cheminformatics data by combining GNN-learned graph embeddings with SMILES-derived RDKit descriptors. This Hybrid GNN model uses `smiles2graph` for graph construction and augments it with RDKit-based molecular features for improved prediction accuracy.\n",
    "\n",
    "### Model Components:\n",
    "\n",
    "* **AtomEncoder / BondEncoder**\n",
    "  Transforms categorical atom and bond features (provided by OGB) into learnable embeddings using the encoders from `ogb.graphproppred.mol_encoder`. These provide a strong foundation for expressive graph learning.\n",
    "\n",
    "* **GINEConv Layers (x2)**\n",
    "  I use two stacked GINEConv layers (Graph Isomorphism Network with Edge features). These layers perform neighborhood aggregation based on edge attributes, allowing the model to capture localized chemical environments.\n",
    "\n",
    "* **Global Mean Pooling**\n",
    "  After message passing, node level embeddings are aggregated into a fixed size graph level representation using `global_mean_pool`.\n",
    "\n",
    "* **Concatenation with RDKit Descriptors**\n",
    "  The pooled GNN embedding is concatenated with external RDKit descriptors, which capture global molecular properties not easily inferred from graph data alone.\n",
    "\n",
    "* **MLP Prediction Head**\n",
    "  A multilayer perceptron processes the combined feature vector with ReLU activations, dropout regularization, and linear layers to predict the HOMOâ€“LUMO gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbf07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridGNN(Module):\n",
    "    def __init__(self, gnn_dim, rdkit_dim, hidden_dim, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.gnn_dim = gnn_dim\n",
    "        self.rdkit_dim = rdkit_dim\n",
    "\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=gnn_dim)\n",
    "        self.bond_encoder = BondEncoder(emb_dim=gnn_dim)\n",
    "\n",
    "        self.conv1 = GINEConv(Sequential(\n",
    "            Linear(gnn_dim, gnn_dim), ReLU(), \n",
    "            Linear(gnn_dim, gnn_dim))\n",
    "            )\n",
    "        self.conv2 = GINEConv(Sequential(\n",
    "            Linear(gnn_dim, gnn_dim), ReLU(), \n",
    "            Linear(gnn_dim, gnn_dim))\n",
    "            )\n",
    "        self.pool = global_mean_pool\n",
    "\n",
    "        self.mlp = Sequential(\n",
    "            Linear(gnn_dim + rdkit_dim, hidden_dim), ReLU(), \n",
    "            Dropout(dropout_rate),\n",
    "            Linear(hidden_dim, hidden_dim // 2), ReLU(), \n",
    "            Dropout(dropout_rate),\n",
    "            Linear(hidden_dim // 2, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # encode atoms and bonds\n",
    "        x = self.atom_encoder(data.x)\n",
    "        edge_attr = self.bond_encoder(data.edge_attr)\n",
    "\n",
    "        # GNN convolutions\n",
    "        x = self.conv1(x, data.edge_index, edge_attr)\n",
    "        x = self.conv2(x, data.edge_index, edge_attr)\n",
    "        x = self.pool(x, data.batch)\n",
    "\n",
    "        # handle RDKit features\n",
    "        rdkit_feats = getattr(data, 'rdkit_feats', None)\n",
    "        if rdkit_feats is not None:\n",
    "            # Reshape the RDKit features tensor to be (batch_size, rdkit_dim)\n",
    "            reshaped_rdkit_feats = rdkit_feats.view(x.shape[0], self.rdkit_dim)\n",
    "\n",
    "            if x.shape[0] != reshaped_rdkit_feats.shape[0]:\n",
    "                raise ValueError(f\"Shape mismatch: GNN output ({x.shape[0]}) vs rdkit_feats ({reshaped_rdkit_feats.shape[0]})\")\n",
    "            \n",
    "            x = torch.cat([x, reshaped_rdkit_feats], dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"RDKit features not found in the data object\")\n",
    "\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252f39e",
   "metadata": {},
   "source": [
    "# Step 7: training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = HybridGNN(gnn_dim=128, rdkit_dim=rdkit_features.shape[1], hidden_dim=256)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            preds.append(pred.cpu())\n",
    "            targets.append(batch.y.view(-1, 1).cpu())\n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    loss = F.mse_loss(preds, targets)\n",
    "    return loss.item(), preds, targets\n",
    "\n",
    "# training loop\n",
    "for epoch in range(1, 101): # long since early stopping\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch:02d}\"):\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        loss = F.mse_loss(pred, batch.y.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    val_loss, val_preds, val_targets = evaluate(model, valid_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        save_dir = os.path.join(PROJECT_ROOT, \"saved_models\", \"gnn\", \"gnn_smiles2graph_model\", \"saved_models_Tg\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"hybridgnn_untuned.pt\"))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# final eval on val set\n",
    "model.load_state_dict(torch.load(os.path.join(save_dir, \"hybridgnn_untuned.pt\")))\n",
    "model.eval()\n",
    "_, final_preds, final_targets = evaluate(model, valid_loader)\n",
    "metrics = regression_metrics(final_targets.numpy(), final_preds.numpy())\n",
    "print(\"\\nGNN Evaluation:\")\n",
    "print(metrics[['MAE', 'RMSE', 'r_squared']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34585261",
   "metadata": {},
   "source": [
    "# Step 8: Optuna tuning of Hybrid GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85485072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridGNN(Module):\n",
    "    def __init__(self, gnn_dim, rdkit_dim, hidden_dim, dropout_rate=0.2, activation='ReLU'):\n",
    "        super().__init__()\n",
    "        act_map = {'ReLU': torch.nn.ReLU(), 'ELU': torch.nn.ELU(), 'GELU': torch.nn.GELU(), 'LeakyReLU': torch.nn.LeakyReLU(), 'PReLU': torch.nn.PReLU(), 'Swish': torch.nn.SiLU()}\n",
    "        act_fn = act_map[activation]\n",
    "        self.gnn_dim = gnn_dim\n",
    "        self.rdkit_dim = rdkit_dim\n",
    "\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=gnn_dim)\n",
    "        self.bond_encoder = BondEncoder(emb_dim=gnn_dim)\n",
    "\n",
    "        self.conv1 = GINEConv(Sequential(\n",
    "            Linear(gnn_dim, gnn_dim), act_fn, \n",
    "            Linear(gnn_dim, gnn_dim))\n",
    "            )\n",
    "        self.conv2 = GINEConv(Sequential(\n",
    "            Linear(gnn_dim, gnn_dim), act_fn, \n",
    "            Linear(gnn_dim, gnn_dim))\n",
    "            )\n",
    "        self.pool = global_mean_pool\n",
    "\n",
    "        self.mlp = Sequential(\n",
    "            Linear(gnn_dim + rdkit_dim, hidden_dim), act_fn, \n",
    "            Dropout(dropout_rate), \n",
    "            Linear(hidden_dim, hidden_dim // 2), act_fn, \n",
    "            Dropout(dropout_rate), \n",
    "            Linear(hidden_dim // 2, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # encode atoms and bonds\n",
    "        x = self.atom_encoder(data.x)\n",
    "        edge_attr = self.bond_encoder(data.edge_attr)\n",
    "\n",
    "        # GNN convolutions\n",
    "        x = self.conv1(x, data.edge_index, edge_attr)\n",
    "        x = self.conv2(x, data.edge_index, edge_attr)\n",
    "        x = self.pool(x, data.batch)\n",
    "\n",
    "        # handle RDKit features\n",
    "        rdkit_feats = getattr(data, 'rdkit_feats', None)\n",
    "        if rdkit_feats is not None:\n",
    "            reshaped_rdkit_feats = rdkit_feats.view(x.shape[0], self.rdkit_dim)\n",
    "            \n",
    "            if x.shape[0] != reshaped_rdkit_feats.shape[0]:\n",
    "                raise ValueError(f\"Shape mismatch: GNN output ({x.shape[0]}) vs rdkit_feats ({reshaped_rdkit_feats.shape[0]})\")\n",
    "            \n",
    "            x = torch.cat([x, reshaped_rdkit_feats], dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"RDKit features not found in the data object\")\n",
    "\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f1fc3",
   "metadata": {},
   "source": [
    "Multiple rounds of tuning have suggested to refine my search space to ReLU, GELU, and Swish activation functions and Adam and AdamW optimizers. Therefore, I have commented out unused parameters like momentum for SGD and the unused optimizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # hyperparameter search space\n",
    "    gnn_dim = trial.suggest_categorical(\"gnn_dim\", [256, 384, 512, 1024])\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [256, 384, 512])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.15, 0.4)\n",
    "    lr = trial.suggest_float(\"lr\", 8e-6, 1e-3, log=True)\n",
    "    activation = trial.suggest_categorical(\"activation\", ['ReLU', 'GELU', 'Swish'])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\", \"SGD\", \"RMSprop\"])\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.8, 0.99, log=True) if optimizer_name == \"SGD\" else None\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True)\n",
    "\n",
    "    train_val_set, test_set = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "    train_set, val_set = train_test_split(train_val_set, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    valid_loader = DataLoader(val_set, batch_size=64)\n",
    "\n",
    "    model = HybridGNN(\n",
    "        gnn_dim=gnn_dim,\n",
    "        rdkit_dim=rdkit_features.shape[1],\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=activation\n",
    "    )\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{optimizer_name}' not supported\")\n",
    "\n",
    "    # training loop with NaN check and early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, 100):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch)\n",
    "            loss = F.mse_loss(pred, batch.y.view(-1, 1))\n",
    "\n",
    "            if torch.isnan(loss).any():\n",
    "                print(f\"Trial {trial.number} | Epoch {epoch:02d} | NaN loss detected so pruning trial\")\n",
    "                trial.report(float('inf'), epoch)\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch.num_graphs\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                batch = batch.to(device)\n",
    "                pred = model(batch)\n",
    "                val_loss += F.mse_loss(pred, batch.y.view(-1, 1)).item() * batch.num_graphs\n",
    "        val_loss /= len(valid_loader.dataset)\n",
    "\n",
    "        # logging, reporting, pruning, early stopping\n",
    "        print(f\"Trial {trial.number} | Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Optimizer: {optimizer_name}\")\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Trial {trial.number} - Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    study_name = \"final_2d_gnn_study_Tg_6\"\n",
    "    storage_name = f\"sqlite:///{study_name}.db\"\n",
    "    study = optuna.create_study(study_name=study_name, storage=storage_name, direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "    def save_study_callback(study, trial):\n",
    "        pass\n",
    "\n",
    "    study.optimize(objective, n_trials=1500, callbacks=[save_study_callback])\n",
    "    print(study.best_params)\n",
    "    joblib.dump(study, f\"{study_name}_final.pkl\")\n",
    "    \n",
    "    # final plots\n",
    "    vis = optuna.visualization\n",
    "    fig = vis.plot_optimization_history(study)\n",
    "    fig.show()\n",
    "    fig_params = vis.plot_param_importances(study)\n",
    "    fig_params.show()\n",
    "    fig_intermediate = vis.plot_intermediate_values(study)\n",
    "    fig_intermediate.show()\n",
    "    fig_parallel_coordinate = vis.plot_parallel_coordinate(study)\n",
    "    fig_parallel_coordinate.show()\n",
    "    fig_slice = vis.plot_slice(study)\n",
    "    fig_slice.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad23af4",
   "metadata": {},
   "source": [
    "# Step 9: Retrain with best prameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "train_val_set, test_set = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "train_set, val_set = train_test_split(train_val_set, test_size=0.25, random_state=42)\n",
    " \n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(val_set, batch_size=64)\n",
    "test_loader = DataLoader(test_set, batch_size=64)\n",
    "\n",
    "# reinitialize model\n",
    "model = HybridGNN(\n",
    "    gnn_dim=best_params['gnn_dim'],\n",
    "    rdkit_dim=rdkit_features.shape[1],\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    activation=best_params['activation']\n",
    ")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# instantiate Optimizer based on Optuna's choice\n",
    "optimizer_name = best_params['optimizer']\n",
    "lr = best_params['lr']\n",
    "weight_decay = best_params['weight_decay']\n",
    "\n",
    "if optimizer_name == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "elif optimizer_name == \"AdamW\":\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "elif optimizer_name == \"SGD\":\n",
    "    momentum = best_params['momentum']\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "elif optimizer_name == \"RMSprop\":\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "else:\n",
    "    raise ValueError(f\"Optimizer '{optimizer_name}' not supported.\")\n",
    "\n",
    "num_epochs = 100\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "num_warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# early stopping training loop w loss tracking and plotting\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            loss = F.mse_loss(pred, batch.y.view(-1, 1))\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "            preds.append(pred.cpu())\n",
    "            targets.append(batch.y.view(-1, 1).cpu())\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    return avg_loss, preds, targets\n",
    "\n",
    "for epoch in range(1, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        loss = F.mse_loss(pred, batch.y.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    val_loss, val_preds, val_targets = evaluate(model, valid_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        save_dir = os.path.join(PROJECT_ROOT, \"saved_models\", \"gnn\", \"gnn_smiles2graph_model\", \"saved_models_Tg\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"best_hybridgnn.pt\"))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# load best model and final evaluation on the TEST set\n",
    "model.load_state_dict(torch.load(\"best_hybridgnn.pt\"))\n",
    "model.eval()\n",
    "final_test_loss, test_preds, test_targets = evaluate(model, test_loader)\n",
    "metrics = regression_metrics(test_targets.numpy(), test_preds.numpy())\n",
    "print(\"\\nFinal Test Set Evaluation:\")\n",
    "print(metrics[['MAE', 'RMSE', 'r_squared']])\n",
    "print(f\"Final Test Loss: {final_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef0f88",
   "metadata": {},
   "source": [
    "# Step 10: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting training and validation loss \n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
