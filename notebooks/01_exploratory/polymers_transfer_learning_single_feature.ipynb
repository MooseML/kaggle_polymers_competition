{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c615e0e2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-23T05:01:54.324857Z",
     "iopub.status.busy": "2025-07-23T05:01:54.324606Z",
     "iopub.status.idle": "2025-07-23T05:01:56.365975Z",
     "shell.execute_reply": "2025-07-23T05:01:56.365092Z"
    },
    "papermill": {
     "duration": 2.048016,
     "end_time": "2025-07-23T05:01:56.367363",
     "exception": false,
     "start_time": "2025-07-23T05:01:54.319347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb22f53",
   "metadata": {
    "papermill": {
     "duration": 0.002914,
     "end_time": "2025-07-23T05:01:56.374004",
     "exception": false,
     "start_time": "2025-07-23T05:01:56.371090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NeurIPS 2025 Open Polymer Prediction: Baseline Pipeline\n",
    "\n",
    "This notebook runs the full baseline pipeline for the competition, including:\n",
    "- Data preparation (LMDB creation)\n",
    "- Model training and validation\n",
    "- Test prediction and submission generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc8011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T05:01:56.381100Z",
     "iopub.status.busy": "2025-07-23T05:01:56.380771Z",
     "iopub.status.idle": "2025-07-23T05:03:41.036564Z",
     "shell.execute_reply": "2025-07-23T05:03:41.035578Z"
    },
    "papermill": {
     "duration": 104.661117,
     "end_time": "2025-07-23T05:03:41.038168",
     "exception": false,
     "start_time": "2025-07-23T05:01:56.377051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torch_geometric\n",
    "# !pip install rdkit \n",
    "# !pip install ogb\n",
    "# !pip install lmdb\n",
    "# !pip install lz4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47963b58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T05:03:41.090980Z",
     "iopub.status.busy": "2025-07-23T05:03:41.090724Z",
     "iopub.status.idle": "2025-07-23T05:03:58.507303Z",
     "shell.execute_reply": "2025-07-23T05:03:58.506190Z"
    },
    "papermill": {
     "duration": 17.444366,
     "end_time": "2025-07-23T05:03:58.508474",
     "exception": true,
     "start_time": "2025-07-23T05:03:41.064108",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/polymer')\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "from polymer_model import PolymerPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08cd22",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2. Check/Create LMDBs (train & test)\n",
    "If LMDBs are missing, run the builder scripts. Comment out after first run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = os.getcwd() \n",
    "PROJECT_ROOT = os.path.join(CWD, '..', '..')\n",
    "PROJECT_ROOT = os.path.abspath(PROJECT_ROOT)\n",
    "\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  \n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/hlgap-gnn3d-transformer-pcqm4mv2-v1.pt'\n",
    "else:\n",
    "    DATA_ROOT = os.path.join(PROJECT_ROOT, 'data')\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = os.path.join(PROJECT_ROOT, 'hlgap-gnn3d-transformer-pcqm4mv2-v1.pt')\n",
    "\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    build_script_path = os.path.join(PROJECT_ROOT, 'scripts', 'data_preprocessing', 'build_lmdb.py')\n",
    "\n",
    "    if not os.path.exists(build_script_path):\n",
    "        print(f\"ERROR: LMDB building script not found at {build_script_path}. Please check file location.\")\n",
    "    else:\n",
    "        !python {build_script_path} train\n",
    "        !python {build_script_path} test\n",
    "        print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fcc8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(DATA_ROOT, \"test.csv\"))\n",
    "\n",
    "def ids_for_task(task):\n",
    "    tcol = label_cols[task2idx[task]]\n",
    "    df = train_df[['id', tcol]].copy()\n",
    "    df = df[~df[tcol].isna()]\n",
    "    return df['id'].values.astype(train_df['id'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fcea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def make_single_task_model(backbone_ckpt, use_gap=False, freeze=True):\n",
    "    m = PolymerPredictor(\n",
    "        backbone_ckpt=backbone_ckpt,\n",
    "        n_out=1,\n",
    "        freeze=freeze,\n",
    "        use_gap=use_gap \n",
    "    ).to(device)\n",
    "    if freeze:\n",
    "        m.backbone.eval() # deterministic trunk during head-only stage\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cccba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "def filter_ids_with_label(task: str, ids):\n",
    "    \"\"\"Return only those ids (from the given pool) that have a label for this task.\"\"\"\n",
    "    tcol = label_cols[task2idx[task]]\n",
    "    sub = train_df.loc[train_df['id'].isin(ids), ['id', tcol]]\n",
    "    return sub.loc[~sub[tcol].isna(), 'id'].values\n",
    "\n",
    "def make_random_pools(train_ratio=0.9, seed=42):\n",
    "    \"\"\"Create a global pool split; you can reuse these pools for every task.\"\"\"\n",
    "    all_ids = train_df['id'].values\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm = rng.permutation(len(all_ids))\n",
    "    split = int(train_ratio * len(all_ids))\n",
    "    return all_ids[perm[:split]], all_ids[perm[split:]]\n",
    "\n",
    "def make_task_loaders(task, train_pool_ids, val_pool_ids,\n",
    "                      batch_size=128, num_workers=4, shuffle=True):\n",
    "    \"\"\"\n",
    "    Build loaders for a single task.\n",
    "    Both datasets read from TRAIN_LMDB (validation is a subset of training set).\n",
    "    \"\"\"\n",
    "    # Keep only rows that actually have this task’s label\n",
    "    train_ids = filter_ids_with_label(task, train_pool_ids)\n",
    "    val_ids   = filter_ids_with_label(task, val_pool_ids)\n",
    "\n",
    "    # Fallback if val set ends up empty or tiny\n",
    "    if len(val_ids) < max(32, int(0.05 * len(train_ids))):\n",
    "        n_val = min(max(32, int(0.1 * len(train_ids))), len(train_ids)//5 or 1)\n",
    "        val_ids = train_ids[:n_val]\n",
    "        train_ids = train_ids[n_val:]\n",
    "\n",
    "    # Build datasets from the training LMDB for both splits\n",
    "    train_ds = LMDBDataset(train_ids, TRAIN_LMDB)\n",
    "    val_ds   = LMDBDataset(val_ids,   TRAIN_LMDB)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, val_loader, train_ids, val_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ded208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Make global pools once (reuse for all tasks)\n",
    "train_pool_ids, val_pool_ids = make_random_pools(train_ratio=0.9, seed=123)\n",
    "\n",
    "# 2) Build loaders per task \n",
    "train_loader_tc, val_loader_tc, tr_ids_tc, va_ids_tc = make_task_loaders(\n",
    "    \"Tc\", train_pool_ids, val_pool_ids, batch_size=256, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60ded8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3. Train/Validation Split\n",
    "Create a stratified split based on label availability (how many non-NaN values each row has).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mae_1d(pred, true):\n",
    "    # pred: [B,1], true: [B,5] (we’ll slice the right column before calling)\n",
    "    return (pred.squeeze(-1) - true).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca56a9e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_single_task(task, backbone_ckpt, train_loader, val_loader, use_gap=False,\n",
    "                      epochs_head=10, epochs_ft=0, lr_head=1e-3, lr_bb=1e-6, clip=0.5):\n",
    "    t = task2idx[task]\n",
    "    model = make_single_task_model(backbone_ckpt, use_gap=use_gap, freeze=True)\n",
    "    opt   = torch.optim.AdamW(model.head.parameters(), lr=lr_head, weight_decay=1e-5)\n",
    "    best = {\"mae\": float(\"inf\"), \"path\": f\"best_{task}.pt\", \"epoch\": -1}\n",
    "\n",
    "    # ------------- Stage 1: head-only -------------\n",
    "    for epoch in range(epochs_head):\n",
    "        model.train(); model.backbone.eval()\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            y = batch.y[:, t]; mask = ~torch.isnan(y)\n",
    "            if mask.sum() == 0: continue\n",
    "            pred = model(batch).squeeze(-1)[mask]\n",
    "            loss = (pred - y[mask]).abs().mean()\n",
    "            opt.zero_grad(set_to_none=True); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.head.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "        # val\n",
    "        model.eval(); s=n=0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                y = batch.y[:, t]; mask = ~torch.isnan(y)\n",
    "                if mask.sum() == 0: continue\n",
    "                pred = model(batch).squeeze(-1)[mask]\n",
    "                s += (pred - y[mask]).abs().sum().item(); n += mask.sum().item()\n",
    "        val_mae = s / max(1, n)\n",
    "        if val_mae < best[\"mae\"]:\n",
    "            best.update(mae=val_mae, epoch=epoch)\n",
    "            torch.save({\"model\": model.state_dict()}, best[\"path\"])\n",
    "        print(f\"[{task}] Stage1 Ep{epoch+1}: val_MAE={val_mae:.6f}\")\n",
    "\n",
    "    # ------------- Stage 2: unfreeze last block -------------\n",
    "    if epochs_ft > 0:\n",
    "        layers = getattr(model.backbone.transformer, \"layers\", None) or getattr(model.backbone.transformer, \"encoder_layers\", None)\n",
    "        if layers is not None:\n",
    "            for p in model.backbone.parameters(): p.requires_grad = False\n",
    "            for p in layers[-1].parameters(): p.requires_grad = True\n",
    "            for m in model.backbone.modules():\n",
    "                if isinstance(m, torch.nn.LayerNorm):\n",
    "                    for p in m.parameters(): p.requires_grad = True\n",
    "\n",
    "            opt = torch.optim.AdamW([\n",
    "                {\"params\": model.head.parameters(), \"lr\": lr_head, \"weight_decay\": 1e-5},\n",
    "                {\"params\": layers[-1].parameters(), \"lr\": lr_bb, \"weight_decay\": 1e-5},\n",
    "            ])\n",
    "\n",
    "            for epoch in range(epochs_ft):\n",
    "                model.train()\n",
    "                for batch in train_loader:\n",
    "                    batch = batch.to(device)\n",
    "                    y = batch.y[:, t]; mask = ~torch.isnan(y)\n",
    "                    if mask.sum() == 0: continue\n",
    "                    pred = model(batch).squeeze(-1)[mask]\n",
    "                    loss = (pred - y[mask]).abs().mean()\n",
    "                    opt.zero_grad(set_to_none=True); loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                    opt.step()\n",
    "\n",
    "                model.eval(); s=n=0\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_loader:\n",
    "                        batch = batch.to(device)\n",
    "                        y = batch.y[:, t]; mask = ~torch.isnan(y)\n",
    "                        if mask.sum() == 0: continue\n",
    "                        pred = model(batch).squeeze(-1)[mask]\n",
    "                        s += (pred - y[mask]).abs().sum().item(); n += mask.sum().item()\n",
    "                val_mae = s / max(1, n)\n",
    "                if val_mae < best[\"mae\"]:\n",
    "                    best.update(mae=val_mae, epoch=epoch+epochs_head)\n",
    "                    torch.save({\"model\": model.state_dict()}, best[\"path\"])\n",
    "                print(f\"[{task}] Stage2 Ep{epoch+1}: val_MAE={val_mae:.6f}\")\n",
    "\n",
    "    print(f\"[{task}] Best val_MAE={best['mae']:.6f} (epoch {best['epoch']+1})  -> {best['path']}\")\n",
    "    # free memory before returning\n",
    "    del opt, model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return best[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fdd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE  = 256\n",
    "\n",
    "# make global pools once\n",
    "train_pool_ids, val_pool_ids = make_random_pools(train_ratio=0.9, seed=123)\n",
    "\n",
    "task_cfg = {\n",
    "    \"Tg\": {\"use_gap\": True,  \"epochs_head\": 35, \"epochs_ft\": 15, \"lr_head\": 3e-4, \"lr_bb\": 1e-6},\n",
    "    \"Density\": {\"use_gap\": True,  \"epochs_head\": 15, \"epochs_ft\": 0,  \"lr_head\": 5e-4, \"lr_bb\": 0.0},\n",
    "    \"FFV\": {\"use_gap\": False, \"epochs_head\": 8,  \"epochs_ft\": 0,  \"lr_head\": 1e-3, \"lr_bb\": 0.0},\n",
    "    \"Tc\": {\"use_gap\": False, \"epochs_head\": 18, \"epochs_ft\": 5,  \"lr_head\": 5e-4, \"lr_bb\": 1e-6},\n",
    "    \"Rg\": {\"use_gap\": False, \"epochs_head\": 25, \"epochs_ft\": 6,  \"lr_head\": 5e-4, \"lr_bb\": 1e-6},\n",
    "}\n",
    "\n",
    "\n",
    "best_ckpts = {}\n",
    "for task, cfg in task_cfg.items():\n",
    "    print(f\"\\n=== {task} ===\")\n",
    "    train_loader, val_loader, _, _ = make_task_loaders(\n",
    "        task, train_pool_ids, val_pool_ids,\n",
    "        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True\n",
    "    )\n",
    "    ckpt = train_single_task(\n",
    "        task, BACKBONE_PATH, train_loader, val_loader,\n",
    "        use_gap=cfg[\"use_gap\"],\n",
    "        epochs_head=cfg[\"epochs_head\"], epochs_ft=cfg[\"epochs_ft\"],\n",
    "        lr_head=cfg[\"lr_head\"], lr_bb=cfg[\"lr_bb\"],\n",
    "        clip=0.5,\n",
    "    )\n",
    "    best_ckpts[task] = ckpt\n",
    "\n",
    "print(\"Best checkpoints:\", best_ckpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7d142",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4. Data Loading\n",
    "Create DataLoaders using the fast LMDB datasets for both train and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebcffae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_task(task, ckpt_path, test_loader, backbone_ckpt, use_gap=False):\n",
    "    model = make_single_task_model(backbone_ckpt, use_gap=use_gap, freeze=True)\n",
    "    state = torch.load(ckpt_path, map_location=device)[\"model\"]\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    with torch.inference_mode():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch).squeeze(-1).cpu().numpy()\n",
    "            outs.append(pred)\n",
    "    return np.concatenate(outs, axis=0)\n",
    "\n",
    "# test loader (PyG)\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_loader = DataLoader(\n",
    "    LMDBDataset(test_ids, TEST_LMDB),\n",
    "    batch_size=256, shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "preds = {}\n",
    "for task in label_cols:\n",
    "    ckpt = f\"best_{task}.pt\"\n",
    "    preds[task] = predict_task(task, ckpt, test_loader, BACKBONE_PATH, use_gap=task_cfg[task][\"use_gap\"])  \n",
    "\n",
    "sample = pd.read_csv(os.path.join(DATA_ROOT, \"sample_submission.csv\"))\n",
    "sub = sample.copy()\n",
    "for k in label_cols:\n",
    "    sub[k] = preds[k].astype(float)\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(sub.head(), sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(os.path.join(DATA_ROOT, \"sample_submission.csv\"))\n",
    "sub = sample[[\"id\"]].copy() # preserve Kaggle's id order\n",
    "\n",
    "# If ids match already, this is just assignment; otherwise merge handles it.\n",
    "pred_df = pd.DataFrame({\"id\": test_ids})\n",
    "for k in label_cols:\n",
    "    pred_df[k] = preds[k].astype(float)\n",
    "\n",
    "sub = sub.merge(pred_df, on=\"id\", how=\"left\")\n",
    "\n",
    "# clip to train ranges to prevent wild outliers\n",
    "for k in label_cols:\n",
    "    lo, hi = np.nanmin(train_df[k].values), np.nanmax(train_df[k].values)\n",
    "    sub[k] = np.clip(sub[k].values, lo, hi)\n",
    "\n",
    "# Sanity checks\n",
    "assert sub[label_cols].notna().all().all(), \"Found NaNs in predictions.\"\n",
    "assert len(sub) == len(sample), \"Submission length mismatch.\"\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12966160,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "datasetId": 7923968,
     "sourceId": 12550608,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 132.263567,
   "end_time": "2025-07-23T05:04:00.360011",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-23T05:01:48.096444",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
