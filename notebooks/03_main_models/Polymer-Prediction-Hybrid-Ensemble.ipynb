{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61979795",
   "metadata": {},
   "source": [
    "# Polymer Property Predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a32b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numpy as np, torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, RMSprop\n",
    "from torch.amp import GradScaler, autocast\n",
    "from copy import deepcopy\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, GlobalAttention\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GINEConv\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import torch, numpy as np\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from src.data.polymer_dataset import LMDBDataset\n",
    "from typing import Dict, Any, Tuple, NamedTuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import inspect\n",
    "# Import the new libraries for LightGBM and XGBoost\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors as rdmd, DataStructs\n",
    "import numpy as np, torch\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ace_tools_open as tools\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import pickle\n",
    "import joblib\n",
    "import os \n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Module, Sequential, Dropout\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# OGB dataset \n",
    "from ogb.lsc import PygPCQM4Mv2Dataset, PCQM4Mv2Dataset\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# RDKit\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "\n",
    "# ChemML\n",
    "from chemml.chem import Molecule, RDKitFingerprint, CoulombMatrix, tensorise_molecules\n",
    "from chemml.models import MLP, NeuralGraphHidden, NeuralGraphOutput\n",
    "from chemml.utils import regression_metrics\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589db70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = os.getcwd() \n",
    "PROJECT_ROOT = os.path.join(CWD, '..', '..')\n",
    "PROJECT_ROOT = os.path.abspath(PROJECT_ROOT)\n",
    "\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  \n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/hlgap-gnn3d-transformer-pcqm4mv2-v1.pt'\n",
    "else:\n",
    "    DATA_ROOT = os.path.join(PROJECT_ROOT, 'data')\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = os.path.join(PROJECT_ROOT, 'hlgap-gnn3d-transformer-pcqm4mv2-v1.pt')\n",
    "\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    build_script_path = os.path.join(PROJECT_ROOT, 'scripts', 'data_preprocessing', 'build_lmdb.py')\n",
    "\n",
    "    if not os.path.exists(build_script_path):\n",
    "        print(f\"ERROR: LMDB building script not found at {build_script_path}. Please check file location.\")\n",
    "    else:\n",
    "        !python {build_script_path} train\n",
    "        !python {build_script_path} test\n",
    "        print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed2ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMDB+CSV wiring (augmented-aware)\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx = {k:i for i,k in enumerate(label_cols)}\n",
    "AUG_KEY_MULT = 1000 # must match the builder\n",
    "\n",
    "# 1) Read training labels (CSV = ground truth, parent ids)\n",
    "train_path = os.path.join(DATA_ROOT, 'train.csv')\n",
    "train_df = pd.read_csv(train_path)\n",
    "assert {'id','SMILES'}.issubset(train_df.columns), \"train.csv must have id and SMILES\"\n",
    "train_df['id'] = train_df['id'].astype(int)\n",
    "\n",
    "# 2) LMDB ids (these are augmented key_ids)\n",
    "def read_lmdb_ids(lmdb_path: str) -> np.ndarray:\n",
    "    ids_txt = lmdb_path + \".ids.txt\"\n",
    "    if not os.path.exists(ids_txt):\n",
    "        raise FileNotFoundError(f\"Missing {ids_txt}. Rebuild LMDB or confirm paths.\")\n",
    "    ids = np.loadtxt(ids_txt, dtype=np.int64)\n",
    "    if ids.ndim == 0:\n",
    "        ids = ids.reshape(1)\n",
    "    return ids\n",
    "\n",
    "lmdb_ids = read_lmdb_ids(TRAIN_LMDB)\n",
    "print(f\"LMDB contains {len(lmdb_ids):,} train graphs (augmented key_ids)\")\n",
    "\n",
    "# 3) parent map (better) or derive from key_ids\n",
    "pmap_path = TRAIN_LMDB + \".parent_map.tsv\"\n",
    "if os.path.exists(pmap_path):\n",
    "    pmap = pd.read_csv(pmap_path, sep=\"\\t\") # cols: key_id, parent_id, aug_idx, seed\n",
    "    pmap['key_id'] = pmap['key_id'].astype(np.int64)\n",
    "    pmap['parent_id'] = pmap['parent_id'].astype(np.int64)\n",
    "else:\n",
    "    # Fallback: derive parents from integer division\n",
    "    pmap = pd.DataFrame({\n",
    "        'key_id': lmdb_ids.astype(np.int64),\n",
    "        'parent_id': (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "    })\n",
    "\n",
    "# 4) parents that actually exist in LMDB\n",
    "parents_in_lmdb = np.sort(pmap['parent_id'].unique().astype(np.int64))\n",
    "\n",
    "# 5) helper: parent ids that have a label for a given task\n",
    "def parents_with_label(task: str) -> np.ndarray:\n",
    "    col = task\n",
    "    have_label = train_df.loc[~train_df[col].isna(), 'id'].astype(int).values\n",
    "    keep = np.intersect1d(have_label, parents_in_lmdb, assume_unique=False)\n",
    "    return keep\n",
    "\n",
    "# 6) Make a parent-level global split once (reused for each task)\n",
    "rng = np.random.default_rng(123)\n",
    "perm = rng.permutation(len(parents_in_lmdb))\n",
    "split = int(0.9 * len(parents_in_lmdb))\n",
    "parents_train = parents_in_lmdb[perm[:split]]\n",
    "parents_val = parents_in_lmdb[perm[split:]]\n",
    "\n",
    "# Map parent split to augmented key_ids for the GNN\n",
    "train_pool_key_ids = pmap.loc[pmap.parent_id.isin(parents_train), 'key_id'].astype(np.int64).values\n",
    "val_pool_key_ids   = pmap.loc[pmap.parent_id.isin(parents_val), 'key_id'].astype(np.int64).values\n",
    "\n",
    "print(f\"Global pools -> train_pool={len(train_pool_key_ids):,}  val_pool={len(val_pool_key_ids):,}\")\n",
    "for t in label_cols:\n",
    "    n = len(parents_with_label(t))\n",
    "    print(f\"{t:>7}: {n:6d} parents with labels (pre-intersection-by-task)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056bd8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOMO_CSV = os.path.join(DATA_ROOT, \"homolumo_parent.csv\")\n",
    "homo = pd.read_csv(HOMO_CSV).drop_duplicates(\"parent_id\").set_index(\"parent_id\").sort_index()\n",
    "embed_cols = [c for c in homo.columns if c.startswith(\"h_embed_\")]\n",
    "\n",
    "def append_homolumo_features(\n",
    "        X_base: np.ndarray,\n",
    "        parents_vec: np.ndarray,\n",
    "        homo_df: pd.DataFrame,\n",
    "        *,\n",
    "        use_gap: bool = True,\n",
    "        use_embed: bool = False,\n",
    "        ) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Append HOMO-LUMO features to X_base using parent_ids in parents_vec.\n",
    "    Toggles:\n",
    "      - use_gap:    adds 1 col 'gap_pred'\n",
    "      - use_embed:  adds all 'h_embed_*' cols\n",
    "    \"\"\"\n",
    "    cols: List[str] = []\n",
    "    blocks: List[np.ndarray] = []\n",
    "\n",
    "    if use_gap:\n",
    "        g = homo_df.reindex(parents_vec)[\"gap_pred\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "        blocks.append(g)\n",
    "        cols.append(\"gap_pred\")\n",
    "\n",
    "    if use_embed:\n",
    "        embed_cols = [c for c in homo_df.columns if c.startswith(\"h_embed_\")]\n",
    "        if embed_cols:\n",
    "            E = homo_df.reindex(parents_vec)[embed_cols].to_numpy(dtype=np.float32)\n",
    "            blocks.append(E)\n",
    "            cols.extend(embed_cols)\n",
    "\n",
    "    if blocks:\n",
    "        H = np.hstack(blocks)\n",
    "        H = np.nan_to_num(H, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        X_aug = np.hstack([X_base.astype(np.float32), H]).astype(np.float32)\n",
    "        return X_aug, cols\n",
    "    else:\n",
    "        return X_base.astype(np.float32), cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_numpy(x, default_shape=None, dtype=np.float32):\n",
    "    try:\n",
    "        return torch.as_tensor(x).detach().cpu().numpy().astype(dtype)\n",
    "    except Exception:\n",
    "        if default_shape is None:\n",
    "            return np.array([], dtype=dtype)\n",
    "        return np.zeros(default_shape, dtype=dtype)\n",
    "\n",
    "def geom_features_from_rec(rec, rdkit_dim_expected=15, rbf_K=32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a fixed-length vector from a single LMDB record:\n",
    "      [rdkit(15), n_atoms, n_bonds, deg_mean, deg_max, has_xyz, eig3(3), bbox_extents(3), radius_stats(3), hop_hist(3), extra_atom_mean(5), edge_rbf_mean(32)]\n",
    "    ~ total len = 15 + 5 + 3 + 3 + 3 + 3 + 5 + 32 = 69\n",
    "    \"\"\"\n",
    "    # 15 RDKit descriptors stored in LMDB \n",
    "    rd = getattr(rec, \"rdkit_feats\", None)\n",
    "    rd = _safe_numpy(rd, default_shape=(1, rdkit_dim_expected)).reshape(-1)\n",
    "    if rd.size != rdkit_dim_expected:\n",
    "        rd = np.zeros((rdkit_dim_expected,), dtype=np.float32)\n",
    "\n",
    "    # basic graph sizes & degree\n",
    "    x = torch.as_tensor(rec.x) # [N, ...]\n",
    "    ei = torch.as_tensor(rec.edge_index) # [2, E]\n",
    "    n = x.shape[0]\n",
    "    e = ei.shape[1] if ei.ndim == 2 else 0\n",
    "    deg = torch.bincount(ei[0], minlength=n) if e > 0 else torch.zeros(n, dtype=torch.long)\n",
    "    deg_mean = deg.float().mean().item() if n > 0 else 0.0\n",
    "    deg_max  = deg.max().item() if n > 0 else 0.0\n",
    "\n",
    "    # has_xyz flag\n",
    "    has_xyz = int(bool(getattr(rec, \"has_xyz\", torch.zeros(1, dtype=torch.bool))[0].item())) if hasattr(rec, \"has_xyz\") else 0\n",
    "\n",
    "    # pos-based features\n",
    "    eig3 = np.zeros(3, dtype=np.float32)\n",
    "    extents = np.zeros(3, dtype=np.float32)\n",
    "    rad_stats = np.zeros(3, dtype=np.float32)\n",
    "    pos = getattr(rec, \"pos\", None)\n",
    "    if pos is not None and n > 0 and has_xyz:\n",
    "        P = torch.as_tensor(pos).float() # [N,3]\n",
    "        center = P.mean(dim=0, keepdim=True)\n",
    "        C = P - center\n",
    "        cov = (C.T @ C) / max(1, n-1) # [3,3]\n",
    "        vals = torch.linalg.eigvalsh(cov).clamp_min(0).sqrt() # length scales\n",
    "        eig3 = vals.detach().cpu().numpy()\n",
    "        mn, mx = P.min(0).values, P.max(0).values\n",
    "        extents = (mx - mn).detach().cpu().numpy()\n",
    "        r = C.norm(dim=1)\n",
    "        rad_stats = np.array([r.mean().item(), r.std().item(), r.max().item()], dtype=np.float32)\n",
    "\n",
    "    # hop-distance histogram (1,2,3 hops)\n",
    "    hop_hist = np.zeros(3, dtype=np.float32)\n",
    "    D = getattr(rec, \"dist\", None)\n",
    "    if D is not None and n > 0:\n",
    "        Dn = torch.as_tensor(D).float()[:n, :n]\n",
    "        hop_hist = np.array([\n",
    "            (Dn == 1).float().mean().item(),\n",
    "            (Dn == 2).float().mean().item(),\n",
    "            (Dn == 3).float().mean().item()\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    # extra atom features (mean over atoms, 5 dims if present)\n",
    "    extra_atom = getattr(rec, \"extra_atom_feats\", None)\n",
    "    extra_mean = np.zeros(5, dtype=np.float32)\n",
    "    if extra_atom is not None and hasattr(extra_atom, \"shape\") and extra_atom.shape[-1] == 5:\n",
    "        extra_mean = torch.as_tensor(extra_atom).float().mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    # edge RBF (last 32 channels of edge_attr were RBF(d))\n",
    "    rbf_mean = np.zeros(rbf_K, dtype=np.float32)\n",
    "    ea = getattr(rec, \"edge_attr\", None)\n",
    "    if ea is not None:\n",
    "        EA = torch.as_tensor(ea)\n",
    "        if EA.ndim == 2 and EA.shape[1] >= (3 + rbf_K):\n",
    "            rbf = EA[:, -rbf_K:].float()\n",
    "            rbf_mean = rbf.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    scalars = np.array([n, e, deg_mean, deg_max, has_xyz], dtype=np.float32)\n",
    "    return np.concatenate([rd, scalars, eig3, extents, rad_stats, hop_hist, extra_mean, rbf_mean], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def morgan_bits(smiles_list, n_bits=1024, radius=3):\n",
    "    X = np.zeros((len(smiles_list), n_bits), dtype=np.uint8)\n",
    "    for i, s in enumerate(smiles_list):\n",
    "        arr = np.zeros((n_bits,), dtype=np.uint8)\n",
    "        m = Chem.MolFromSmiles(s)\n",
    "        if m is not None:\n",
    "            fp = rdmd.GetMorganFingerprintAsBitVect(m, radius=radius, nBits=n_bits)\n",
    "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        X[i] = arr\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "def build_rf_features_from_lmdb(ids: np.ndarray, lmdb_path: str, smiles_list: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns X = [Morgan1024 | LMDB-3D-global(69)] for each id/smiles.\n",
    "    Assumes ids and smiles_list are aligned with the CSV used to build LMDB.\n",
    "    \"\"\"\n",
    "    base = LMDBDataset(ids, lmdb_path)\n",
    "    # 3D/global block\n",
    "    feats3d = []\n",
    "    for i in range(len(base)):\n",
    "        rec = base[i]\n",
    "        feats3d.append(geom_features_from_rec(rec)) # shape (69,)\n",
    "    X3d = np.vstack(feats3d).astype(np.float32) if feats3d else np.zeros((0, 69), dtype=np.float32)\n",
    "\n",
    "    # Morgan FP block (2D)\n",
    "    Xfp = morgan_bits(smiles_list, n_bits=1024, radius=3)# (N,1024)\n",
    "\n",
    "    # concat\n",
    "    X = np.hstack([Xfp, X3d]).astype(np.float32)# (N, 1024+69)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe69f3",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc5c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build parent-level target DataFrames for RF/tabular\n",
    "train_df = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_df[\"id\"] = train_df[\"id\"].astype(int)\n",
    "\n",
    "# Reuse parents_in_lmdb computed above\n",
    "def build_target_df_from_parents(df: pd.DataFrame, target_col: str, keep_parent_ids: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns DataFrame with ['id','SMILES', target_col] restricted to PARENT ids that\n",
    "    exist in the LMDB; drops missing targets.\n",
    "    \"\"\"\n",
    "    out = df.loc[df[\"id\"].isin(keep_parent_ids), [\"id\", \"SMILES\", target_col]].copy()\n",
    "    print(f\"Initial {target_col} shape:\", out.shape)\n",
    "    print(f\"Initial {target_col} missing:\\n{out.isnull().sum()}\")\n",
    "    out = out.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    print(f\"Cleaned {target_col} shape:\", out.shape)\n",
    "    print(f\"Cleaned {target_col} missing:\\n{out.isnull().sum()}\\n\")\n",
    "    return out\n",
    "\n",
    "# Build all five on PARENTS that exist in LMDB\n",
    "df_tg = build_target_df_from_parents(train_df, \"Tg\", parents_in_lmdb)\n",
    "df_density = build_target_df_from_parents(train_df, \"Density\", parents_in_lmdb)\n",
    "df_ffv = build_target_df_from_parents(train_df, \"FFV\", parents_in_lmdb)\n",
    "df_tc = build_target_df_from_parents(train_df, \"Tc\", parents_in_lmdb)\n",
    "df_rg = build_target_df_from_parents(train_df, \"Rg\", parents_in_lmdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff48e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morgan FP utilities (no 3D, no external descriptors) \n",
    "def smiles_to_morgan_fp(\n",
    "        smi: str,\n",
    "        n_bits: int = 1024,\n",
    "        radius: int = 3,\n",
    "        use_counts: bool = False,\n",
    "        ) -> Optional[np.ndarray]:\n",
    "    \"\"\"Return a 1D numpy array Morgan fingerprint; None if SMILES invalid.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    if use_counts:\n",
    "        fp = rdmd.GetMorganFingerprint(mol, radius)\n",
    "        # convert to dense count vector\n",
    "        arr = np.zeros((n_bits,), dtype=np.int32)\n",
    "        for bit_id, count in fp.GetNonzeroElements().items():\n",
    "            arr[bit_id % n_bits] += count\n",
    "        return arr.astype(np.float32)\n",
    "    else:\n",
    "        bv = rdmd.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "        arr = np.zeros((n_bits,), dtype=np.int8)\n",
    "        Chem.DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "        return arr.astype(np.float32)\n",
    "\n",
    "def prepare_fp_for_target(\n",
    "        df_target: pd.DataFrame,\n",
    "        target_col: str,\n",
    "        *,\n",
    "        fp_bits: int = 1024,\n",
    "        fp_radius: int = 3,\n",
    "        use_counts: bool = False,\n",
    "        save_csv_path: Optional[str] = None,\n",
    "        show_progress: bool = True,\n",
    "        ) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Drop missing targets, compute Morgan FPs from SMILES only.\n",
    "    Returns (df_clean, y, X_fp) where:\n",
    "      df_clean: ['SMILES', target_col]\n",
    "      y: (N,)\n",
    "      X_fp: (N, fp_bits)\n",
    "    \"\"\"\n",
    "    assert {\"SMILES\", target_col}.issubset(df_target.columns)\n",
    "\n",
    "    # 1) drop missing targets \n",
    "    work = df_target[[\"SMILES\", target_col]].copy()\n",
    "    before = len(work)\n",
    "    work = work.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    after = len(work)\n",
    "    print(f\"[{target_col}] dropped {before - after} missing; kept {after}\")\n",
    "\n",
    "    # 2) compute FPs and skip invalid SMILES\n",
    "    fps, ys, keep_smiles = [], [], []\n",
    "    it = work.itertuples(index=False)\n",
    "    if show_progress:\n",
    "        it = tqdm(it, total=len(work), desc=f\"FPs for {target_col}\")\n",
    "\n",
    "    for row in it:\n",
    "        smi = row.SMILES\n",
    "        yv  = getattr(row, target_col)\n",
    "        arr = smiles_to_morgan_fp(smi, n_bits=fp_bits, radius=fp_radius, use_counts=use_counts)\n",
    "        if arr is None:\n",
    "            continue\n",
    "        fps.append(arr)\n",
    "        ys.append(float(yv))\n",
    "        keep_smiles.append(smi)\n",
    "\n",
    "    X_fp = np.stack(fps, axis=0) if fps else np.zeros((0, fp_bits), dtype=np.float32)\n",
    "    y = np.asarray(ys, dtype=float)\n",
    "    df_clean = pd.DataFrame({\"SMILES\": keep_smiles, target_col: y})\n",
    "\n",
    "    if save_csv_path:\n",
    "        df_clean.to_csv(save_csv_path, index=False)\n",
    "        print(f\"[{target_col}] saved cleaned CSV -> {save_csv_path}\")\n",
    "\n",
    "    print(f\"[{target_col}] X_fp: {X_fp.shape} | y: {y.shape}\")\n",
    "    return df_clean, y, X_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f37942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bit vectors (1024, r=3) \n",
    "df_clean_tg, y_tg, X_tg = prepare_fp_for_target(df_tg, \"Tg\", fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_tg_fp.csv\")\n",
    "df_clean_density, y_density, X_density = prepare_fp_for_target(df_density, \"Density\", fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_density_fp.csv\")\n",
    "df_clean_ffv, y_ffv, X_ffv = prepare_fp_for_target(df_ffv, \"FFV\", fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_ffv_fp.csv\")\n",
    "df_clean_tc, y_tc, X_tc = prepare_fp_for_target(df_tc, \"Tc\", fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_tc_fp.csv\")\n",
    "df_clean_rg, y_rg, X_rg = prepare_fp_for_target(df_rg, \"Rg\", fp_bits=1024, fp_radius=3, use_counts=False, save_csv_path=\"cleaned_rg_fp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff620911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class TabularSplits:\n",
    "    # unscaled (for RF)\n",
    "    X_train: np.ndarray\n",
    "    X_test:  np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    y_test:  np.ndarray\n",
    "    # scaled (for KRR/MLP)\n",
    "    X_train_scaled: Optional[np.ndarray] = None\n",
    "    X_test_scaled:  Optional[np.ndarray] = None\n",
    "    y_train_scaled: Optional[np.ndarray] = None  # shape (N,1)\n",
    "    y_test_scaled:  Optional[np.ndarray] = None\n",
    "    x_scaler: Optional[StandardScaler] = None\n",
    "    y_scaler: Optional[StandardScaler] = None\n",
    "\n",
    "def _make_regression_stratify_bins(y: np.ndarray, n_bins: int = 10) -> np.ndarray:\n",
    "    \"\"\"Return integer bins for approximate stratification in regression.\"\"\"\n",
    "    y = y.ravel()\n",
    "    # handle degenerate case\n",
    "    if np.unique(y).size < n_bins:\n",
    "        n_bins = max(2, np.unique(y).size)\n",
    "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "    bins = np.unique(np.quantile(y, quantiles))\n",
    "    # ensure strictly increasing\n",
    "    bins = np.unique(bins)\n",
    "    # np.digitize expects right-open intervals by default\n",
    "    strat = np.digitize(y, bins[1:-1], right=False)\n",
    "    return strat\n",
    "\n",
    "def make_tabular_splits(\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        *,\n",
    "        test_size: float = 0.2,\n",
    "        random_state: int = 42,\n",
    "        scale_X: bool = True,\n",
    "        scale_y: bool = True,\n",
    "        stratify_regression: bool = False,\n",
    "        n_strat_bins: int = 10,\n",
    "        train_idx: Optional[np.ndarray] = None,\n",
    "        test_idx: Optional[np.ndarray] = None,\n",
    "        ) -> TabularSplits:\n",
    "    \"\"\"\n",
    "    Split and (optionally) scale tabular features/targets for a single target.\n",
    "    Returns both scaled and unscaled arrays, plus fitted scalers.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    if train_idx is not None and test_idx is not None:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "    else:\n",
    "        strat = None\n",
    "        if stratify_regression:\n",
    "            strat = _make_regression_stratify_bins(y, n_bins=n_strat_bins)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "        )\n",
    "\n",
    "    # Unscaled outputs (for RF, tree models)\n",
    "    splits = TabularSplits(\n",
    "        X_train=X_train, X_test=X_test,\n",
    "        y_train=y_train, y_test=y_test\n",
    "    )\n",
    "\n",
    "    # Scaled versions (for KRR/MLP)\n",
    "    if scale_X:\n",
    "        xscaler = StandardScaler()\n",
    "        splits.X_train_scaled = xscaler.fit_transform(X_train)\n",
    "        splits.X_test_scaled  = xscaler.transform(X_test)\n",
    "        splits.x_scaler = xscaler\n",
    "    if scale_y:\n",
    "        yscaler = StandardScaler()\n",
    "        splits.y_train_scaled = yscaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        splits.y_test_scaled  = yscaler.transform(y_test.reshape(-1, 1))\n",
    "        splits.y_scaler = yscaler\n",
    "\n",
    "    print(\"Splits:\")\n",
    "    print(\"X_train:\", splits.X_train.shape, \"| X_test:\", splits.X_test.shape)\n",
    "    if splits.X_train_scaled is not None:\n",
    "        print(\"X_train_scaled:\", splits.X_train_scaled.shape, \"| X_test_scaled:\", splits.X_test_scaled.shape)\n",
    "    print(\"y_train:\", splits.y_train.shape, \"| y_test:\", splits.y_test.shape)\n",
    "    if splits.y_train_scaled is not None:\n",
    "        print(\"y_train_scaled:\", splits.y_train_scaled.shape, \"| y_test_scaled:\", splits.y_test_scaled.shape)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reg_metrics(y_tr, p_tr, y_va, p_va):\n",
    "    return {\n",
    "        \"train_MAE\": mean_absolute_error(y_tr, p_tr),\n",
    "        \"train_RMSE\": mean_squared_error(y_tr, p_tr),\n",
    "        \"train_R2\": r2_score(y_tr, p_tr),\n",
    "        \"val_MAE\": mean_absolute_error(y_va, p_va),\n",
    "        \"val_RMSE\": mean_squared_error(y_va, p_va),\n",
    "        \"val_R2\": r2_score(y_va, p_va),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_rf(\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        *,\n",
    "        rf_params: Dict[str, Any],\n",
    "        test_size: float = 0.2,\n",
    "        random_state: int = 42,\n",
    "        stratify_regression: bool = True,\n",
    "        n_strat_bins: int = 10,\n",
    "        save_dir: str = os.path.join(PROJECT_ROOT, \"saved_models\", \"baseline\", \"RandomForest\", \"rf\"),\n",
    "        tag: str = \"model\",\n",
    "        ) -> Tuple[RandomForestRegressor, Dict[str, float], TabularSplits, str]:\n",
    "    \"\"\"\n",
    "    Trains a RandomForest on unscaled features; returns (model, metrics, splits, path).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    if stratify_regression:\n",
    "        adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "    else:\n",
    "        adaptive_bins = n_strat_bins\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False, # RF doesn't need scaling\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=adaptive_bins\n",
    "    )\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=random_state, n_jobs=-1, **rf_params)\n",
    "    rf.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "    pred_tr = rf.predict(splits.X_train)\n",
    "    pred_te = rf.predict(splits.X_test)\n",
    "    metrics = _reg_metrics(splits.y_train, pred_tr, splits.y_test, pred_te)\n",
    "    print(f\"[RF/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"rf_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": rf, \"metrics\": metrics, \"rf_params\": rf_params}, path)\n",
    "    return rf, metrics, splits, path\n",
    "\n",
    "def train_eval_lgbm(\n",
    "        X, y, *,\n",
    "        lgbm_params,\n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify_regression=True, \n",
    "        n_strat_bins=10,\n",
    "        save_dir=os.path.join(PROJECT_ROOT, \"saved_models\", \"baseline\", \"LightGBM\", \"lgbm\"),\n",
    "        tag=\"model\",\n",
    "        early_stopping_rounds=400,\n",
    "        extra_dump: Optional[Dict[str,Any]]=None\n",
    "        ):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y))))) if stratify_regression else n_strat_bins\n",
    "    splits = make_tabular_splits(\n",
    "        X, y, test_size=test_size, random_state=random_state,\n",
    "        scale_X=False, scale_y=False,\n",
    "        stratify_regression=stratify_regression, n_strat_bins=adaptive_bins\n",
    "    )\n",
    "\n",
    "    Xtr = np.asarray(splits.X_train, dtype=np.float32)\n",
    "    Ytr = np.asarray(splits.y_train, dtype=np.float32)\n",
    "    Xva = np.asarray(splits.X_test, dtype=np.float32)\n",
    "    Yva = np.asarray(splits.y_test, dtype=np.float32)\n",
    "\n",
    "    base = dict(\n",
    "        n_estimators=4000,\n",
    "        learning_rate=0.03,\n",
    "        objective=\"l1\",            \n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbosity=-1,             \n",
    "    )\n",
    "    lgb_params = {k: v for k, v in lgbm_params.items() if k not in (\"colsample_bytree\", \"subsample\", \"subsample_freq\")}\n",
    "    # if no bagging, drop bagging_freq to avoid warning\n",
    "    if lgb_params.get(\"bagging_fraction\", 1.0) >= 1.0:\n",
    "        lgb_params.pop(\"bagging_freq\", None)\n",
    "    base.update(lgb_params)\n",
    "\n",
    "    lgbm = lgb.LGBMRegressor(**base)\n",
    "    lgbm.fit(\n",
    "        Xtr, Ytr,\n",
    "        eval_set=[(Xva, Yva)],\n",
    "        eval_metric=\"l1\",\n",
    "        callbacks=[lgb.early_stopping(early_stopping_rounds, verbose=False),\n",
    "                   lgb.log_evaluation(period=0)]\n",
    "    )\n",
    "\n",
    "    p_tr = lgbm.predict(Xtr, num_iteration=lgbm.best_iteration_)\n",
    "    p_va = lgbm.predict(Xva, num_iteration=lgbm.best_iteration_)\n",
    "    metrics = _reg_metrics(Ytr, p_tr, Yva, p_va)\n",
    "    print(f\"[LGBM/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"lgbm_{tag}.joblib\")\n",
    "    payload = {\"model\": lgbm, \"metrics\": metrics, \"lgbm_params\": base}\n",
    "    if extra_dump: payload.update(extra_dump)       # <<<\n",
    "    joblib.dump(payload, path)\n",
    "    return lgbm, metrics, splits, path\n",
    "\n",
    "\n",
    "def train_eval_xgb(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    xgb_params: Dict[str, Any],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    save_dir: str=os.path.join(PROJECT_ROOT, \"saved_models\", \"baseline\", \"XGBoost\", \"xgb\"),\n",
    "    tag: str = \"model\",\n",
    ") -> Tuple[xgb.XGBRegressor, Dict[str, float], TabularSplits, str]:\n",
    "    \"\"\"\n",
    "    Trains an XGBoost on unscaled features; returns (model, metrics, splits, path).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    if stratify_regression:\n",
    "        adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "    else:\n",
    "        adaptive_bins = n_strat_bins\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False,\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=adaptive_bins\n",
    "    )\n",
    "    \n",
    "    # Base parameters as provided by the user\n",
    "    base_params = dict(\n",
    "        n_estimators=6000,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        colsample_bylevel=0.8,\n",
    "        colsample_bynode=0.8,\n",
    "        reg_lambda=2.0,\n",
    "        reg_alpha=0.0,\n",
    "        min_child_weight=2.0,\n",
    "        gamma=0.0,\n",
    "        # Check for GPU availability and set tree_method accordingly\n",
    "        # This requires the 'xgboost' library to be installed with GPU support\n",
    "        tree_method=\"gpu_hist\" if \"cuda\" in xgb_params.get(\"device\", \"cpu\") else \"hist\",\n",
    "        max_bin=512,\n",
    "        objective=\"reg:squarederror\",\n",
    "        eval_metric=\"mae\",\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    \n",
    "    # Merge base and custom parameters, with custom ones taking precedence\n",
    "    full_params = {**base_params, **xgb_params}\n",
    "\n",
    "    xgbr = xgb.XGBRegressor(**full_params)\n",
    "    \n",
    "    # Removed early stopping to avoid `TypeError` due to library version incompatibility.\n",
    "    xgbr.fit(\n",
    "        splits.X_train,\n",
    "        splits.y_train,\n",
    "        eval_set=[(splits.X_test, splits.y_test)],\n",
    "    )\n",
    "\n",
    "    pred_tr = xgbr.predict(splits.X_train)\n",
    "    pred_te = xgbr.predict(splits.X_test)\n",
    "    metrics = _reg_metrics(splits.y_train, pred_tr, splits.y_test, pred_te)\n",
    "    print(f\"[XGB/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"xgb_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": xgbr, \"metrics\": metrics, \"xgb_params\": xgb_params}, path)\n",
    "    return xgbr, metrics, splits, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cfg = {\n",
    "\"FFV\": {\"n_estimators\": 100, \"max_depth\": 60},\n",
    "\"Tc\": {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "\"Rg\": {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "}\n",
    "\n",
    "rf_cfg[\"Tg\"] = rf_cfg[\"Rg\"]\n",
    "rf_cfg[\"Density\"] = rf_cfg[\"Rg\"]\n",
    "\n",
    "lgbm_cfg = {\n",
    "\"FFV\": {\"num_leaves\": 127, \"min_child_samples\": 20, \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 1},\n",
    "\"Tc\": {\"num_leaves\": 63,  \"min_child_samples\": 15, \"feature_fraction\": 0.7, \"bagging_fraction\": 0.8, \"bagging_freq\": 1},\n",
    "\"Rg\": {\"num_leaves\": 127, \"min_child_samples\": 10, \"feature_fraction\": 1.0, \"bagging_fraction\": 0.8, \"bagging_freq\": 1},\n",
    "\"Tg\": {\"num_leaves\": 127, \"min_child_samples\": 20, \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 1},\n",
    "\"Density\": {\"num_leaves\": 63,  \"min_child_samples\": 10, \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 1},\n",
    "}\n",
    "\n",
    "xgb_cfg = {\n",
    "\"FFV\": {\"max_depth\": 6,  \"min_child_weight\": 2.0, \"gamma\": 0.0, \"reg_lambda\": 2.0, \"reg_alpha\": 0.0, \"colsample_bytree\": 0.85},\n",
    "\"Tc\": {\"max_depth\": 6,  \"min_child_weight\": 4.0, \"gamma\": 0.1, \"reg_lambda\": 3.0, \"reg_alpha\": 0.1, \"colsample_bytree\": 0.8},\n",
    "\"Rg\": {\"max_depth\": 10, \"min_child_weight\": 2.0, \"gamma\": 0.0, \"reg_lambda\": 2.0, \"reg_alpha\": 0.0, \"colsample_bytree\": 0.9},\n",
    "\"Tg\": {\"max_depth\": 10, \"min_child_weight\": 4.0, \"gamma\": 0.2, \"reg_lambda\": 3.0, \"reg_alpha\": 0.1, \"colsample_bytree\": 0.85},\n",
    "\"Density\": {\"max_depth\": 6,  \"min_child_weight\": 1.0, \"gamma\": 0.0, \"reg_lambda\": 2.0, \"reg_alpha\": 0.0, \"colsample_bytree\": 0.8},\n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\n--- Training Random Forest Models ---\")\n",
    "rf_ffv, m_ffv, splits_ffv, p_ffv = train_eval_rf(X_ffv, y_ffv, rf_params=rf_cfg[\"FFV\"], tag=\"FFV\")\n",
    "rf_tc, m_tc, splits_tc, p_tc = train_eval_rf(X_tc, y_tc, rf_params=rf_cfg[\"Tc\"], tag=\"Tc\")\n",
    "rf_rg, m_rg, splits_rg, p_rg = train_eval_rf(X_rg, y_rg, rf_params=rf_cfg[\"Rg\"], tag=\"Rg\")\n",
    "rf_tg, m_tg, splits_tg, p_tg = train_eval_rf(X_tg, y_tg, rf_params=rf_cfg[\"Tg\"], tag=\"Tg\")\n",
    "rf_density, m_density, splits_density, p_density = train_eval_rf(X_density, y_density, rf_params=rf_cfg[\"Density\"], tag=\"Density\")\n",
    "\n",
    "print(\"\\n--- Training LightGBM Models ---\")\n",
    "lgbm_ffv, m_lgbm_ffv, splits_lgbm_ffv, p_lgbm_ffv = train_eval_lgbm(X_ffv, y_ffv, lgbm_params=lgbm_cfg[\"FFV\"], tag=\"FFV\")\n",
    "lgbm_tc, m_lgbm_tc, splits_lgbm_tc, p_lgbm_tc = train_eval_lgbm(X_tc, y_tc, lgbm_params=lgbm_cfg[\"Tc\"], tag=\"Tc\")\n",
    "lgbm_rg, m_lgbm_rg, splits_lgbm_rg, p_lgbm_rg = train_eval_lgbm(X_rg, y_rg, lgbm_params=lgbm_cfg[\"Rg\"], tag=\"Rg\")\n",
    "lgbm_tg, m_lgbm_tg, splits_lgbm_tg, p_lgbm_tg = train_eval_lgbm(X_tg, y_tg, lgbm_params=lgbm_cfg[\"Tg\"], tag=\"Tg\")\n",
    "lgbm_density, m_lgbm_density, splits_lgbm_density, p_lgbm_density = train_eval_lgbm(X_density, y_density, lgbm_params=lgbm_cfg[\"Density\"], tag=\"Density\")\n",
    "\n",
    "print(\"\\n--- Training XGBoost Models ---\")\n",
    "xgb_ffv, m_xgb_ffv, splits_xgb_ffv, p_xgb_ffv = train_eval_xgb(X_ffv, y_ffv, xgb_params=xgb_cfg[\"FFV\"], tag=\"FFV\")\n",
    "xgb_tc, m_xgb_tc, splits_xgb_tc, p_xgb_tc = train_eval_xgb(X_tc, y_tc, xgb_params=xgb_cfg[\"Tc\"], tag=\"Tc\")\n",
    "xgb_rg, m_xgb_rg, splits_xgb_rg, p_xgb_rg = train_eval_xgb(X_rg, y_rg, xgb_params=xgb_cfg[\"Rg\"], tag=\"Rg\")\n",
    "xgb_tg, m_xgb_tg, splits_xgb_tg, p_xgb_tg = train_eval_xgb(X_tg, y_tg, xgb_params=xgb_cfg[\"Tg\"], tag=\"Tg\")\n",
    "xgb_density, m_xgb_density, splits_xgb_density, p_xgb_density = train_eval_xgb(X_density, y_density, xgb_params=xgb_cfg[\"Density\"], tag=\"Density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c86343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rf_features_from_lmdb_parents(parent_ids, lmdb_path, smiles_list, *, agg=\"mean\"):\n",
    "    \"\"\"\n",
    "    Uses your EXISTING build_rf_features_from_lmdb(ids, lmdb_path, smiles_list)\n",
    "    but expands each parent id -> its augmented key_ids, extracts features for all,\n",
    "    then reduces (mean/median) back to ONE row per parent (so shapes match y).\n",
    "\n",
    "    Returns:\n",
    "        X_parent  : (N_parents_kept, D)   aggregated features per parent\n",
    "        keep_idx  : indices into the input arrays (parent_ids/smiles_list/y)\n",
    "                    that were actually kept (in order)\n",
    "    \"\"\"\n",
    "    pmap_path = lmdb_path + \".parent_map.tsv\"\n",
    "    if os.path.exists(pmap_path):\n",
    "        pmap = pd.read_csv(pmap_path, sep=\"\\t\")  # cols: key_id, parent_id, aug_idx, seed\n",
    "        pmap['key_id'] = pmap['key_id'].astype(np.int64)\n",
    "        pmap['parent_id'] = pmap['parent_id'].astype(np.int64)\n",
    "        group = pmap.groupby('parent_id')['key_id'].apply(list).to_dict()\n",
    "    else:\n",
    "        # derive from ids.txt if parent_map.tsv is missing\n",
    "        lmdb_ids = np.loadtxt(lmdb_path + \".ids.txt\", dtype=np.int64)\n",
    "        if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "        dfmap = pd.DataFrame({\n",
    "            'parent_id': (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "            'key_id': lmdb_ids.astype(np.int64),\n",
    "        })\n",
    "        group = dfmap.groupby('parent_id')['key_id'].apply(list).to_dict()\n",
    "\n",
    "    # 2) expand to augmented keys while tracking slices\n",
    "    flat_keys, flat_smiles, seg_sizes = [], [], []\n",
    "    for pid, smi in zip(parent_ids, smiles_list):\n",
    "        keys = group.get(int(pid), [])\n",
    "        seg_sizes.append(len(keys))\n",
    "        if len(keys):\n",
    "            flat_keys.extend(keys)\n",
    "            flat_smiles.extend([smi] * len(keys))\n",
    "\n",
    "    if len(flat_keys) == 0:\n",
    "        raise ValueError(\"No augmented key_ids found for provided parent ids. \"\n",
    "                         \"Check that LMDB matches this CSV and AUG_KEY_MULT.\")\n",
    "\n",
    "    X_all = build_rf_features_from_lmdb(np.array(flat_keys, dtype=np.int64), lmdb_path, flat_smiles)\n",
    "    # 4) fold back to parents by aggregation\n",
    "    out_rows, keep_idx = [], []\n",
    "    i0 = 0\n",
    "    for i, k in enumerate(seg_sizes):\n",
    "        if k == 0: # parent not present in LMDB (should be rare)\n",
    "            continue\n",
    "        Xi = X_all[i0:i0+k]\n",
    "        i0 += k\n",
    "        if agg == \"mean\":\n",
    "            out_rows.append(Xi.mean(axis=0))\n",
    "        elif agg == \"median\":\n",
    "            out_rows.append(np.median(Xi, axis=0))\n",
    "        elif agg == \"max\":\n",
    "            out_rows.append(Xi.max(axis=0))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported agg={agg}\")\n",
    "        keep_idx.append(i)\n",
    "\n",
    "    X_parent = np.vstack(out_rows).astype(np.float32)\n",
    "    keep_idx = np.asarray(keep_idx, dtype=int)\n",
    "\n",
    "    assert X_parent.ndim == 2 and X_parent.shape[0] == keep_idx.size, \"bad aggregation folding\"\n",
    "\n",
    "    n_drop = (len(parent_ids) - keep_idx.size)\n",
    "    if n_drop:\n",
    "        print(f\"[build_rf_features_from_lmdb_parents] Dropped {n_drop} parents with 0 aug rows in LMDB\")\n",
    "\n",
    "    return X_parent, keep_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd86f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_aug3d_for_target(\n",
    "        target_col: str,\n",
    "        rf_params: dict,\n",
    "        *,\n",
    "        train_csv_path: str,\n",
    "        lmdb_path: str,\n",
    "        save_dir: str = os.path.join(PROJECT_ROOT, \"saved_models\", \"baseline\", \"RandomForest\", \"rf_aug3d\"),\n",
    "        tag_prefix: str = \"aug3D\",\n",
    "        test_size: float = 0.2,\n",
    "        random_state: int = 42,\n",
    "        stratify_regression: bool = True,\n",
    "        n_strat_bins: int = 10,\n",
    "        agg: str = \"mean\",\n",
    "        homo_df: Optional[pd.DataFrame] = None,\n",
    "        use_gap: bool = False,\n",
    "        use_embed: bool = False,\n",
    "        ):\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    mask = ~df[target_col].isna()\n",
    "    parent_ids = df.loc[mask, 'id'].astype(int).values\n",
    "    smiles_tr = df.loc[mask, 'SMILES'].astype(str).tolist()\n",
    "    y = df.loc[mask, target_col].astype(float).values\n",
    "\n",
    "    # LMDB features aggregated over augmentations\n",
    "    X_parent, keep_idx = build_rf_features_from_lmdb_parents(parent_ids, lmdb_path, smiles_tr, agg=agg)\n",
    "    y_keep = y[keep_idx]\n",
    "\n",
    "    # append HOMO–LUMO (toggle gap/embed)\n",
    "    parents_kept = parent_ids[keep_idx]\n",
    "    if homo_df is not None and (use_gap or use_embed):\n",
    "        X_parent, added_cols = append_homolumo_features(\n",
    "            X_parent, parents_kept, homo_df, use_gap=use_gap, use_embed=use_embed\n",
    "        )\n",
    "        print(f\"[{target_col}] appended HOMO cols: {len(added_cols)} -> X={X_parent.shape}\")\n",
    "\n",
    "    model, metrics, splits, path = train_eval_rf(\n",
    "        X_parent, y_keep,\n",
    "        rf_params=rf_params,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=n_strat_bins,\n",
    "        save_dir=save_dir,\n",
    "        tag=f\"{target_col}_{tag_prefix}_{agg}\" + (\"+gap\" if use_gap else \"\") + (\"+emb\" if use_embed else \"\")\n",
    "        )\n",
    "    return model, metrics, splits, path\n",
    "\n",
    "\n",
    "def train_lgbm_aug3d_for_target(\n",
    "        target_col: str,\n",
    "        lgbm_params: dict,\n",
    "        *,\n",
    "        train_csv_path: str,\n",
    "        lmdb_path: str,\n",
    "        save_dir: str=os.path.join(PROJECT_ROOT, \"saved_models\", \"baseline\", \"LightGBM\", \"lgbm_aug3d\"),\n",
    "        tag_prefix: str = \"aug3D\",\n",
    "        test_size: float = 0.2,\n",
    "        random_state: int = 42,\n",
    "        stratify_regression: bool = True,\n",
    "        n_strat_bins: int = 10,\n",
    "        agg: str = \"mean\",\n",
    "        homo_df: Optional[pd.DataFrame] = None,\n",
    "        use_gap: bool = False,\n",
    "        use_embed: bool = False,\n",
    "        ):\n",
    "    \"\"\"Load rows with target, build X=[FP|3D], train LGBM.\"\"\"\n",
    "\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    mask = ~df[target_col].isna()\n",
    "    parent_ids = df.loc[mask, 'id'].astype(int).values\n",
    "    smiles_tr = df.loc[mask, 'SMILES'].astype(str).tolist()\n",
    "    y = df.loc[mask, target_col].astype(float).values\n",
    "\n",
    "    # LMDB features aggregated over augmentations\n",
    "    X_parent, keep_idx = build_rf_features_from_lmdb_parents(parent_ids, lmdb_path, smiles_tr, agg=agg)\n",
    "    y_keep = y[keep_idx]\n",
    "\n",
    "    # append HOMO–LUMO (toggle gap/embed)\n",
    "    parents_kept = parent_ids[keep_idx]\n",
    "    if homo_df is not None and (use_gap or use_embed):\n",
    "        X_parent, added_cols = append_homolumo_features(\n",
    "            X_parent, parents_kept, homo_df, use_gap=use_gap, use_embed=use_embed\n",
    "        )\n",
    "        print(f\"[{target_col}] appended HOMO cols: {len(added_cols)} -> X={X_parent.shape}\")\n",
    "\n",
    "\n",
    "    model, metrics, splits, path = train_eval_lgbm(\n",
    "        X_parent, y_keep,\n",
    "        lgbm_params=lgbm_params,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=n_strat_bins,\n",
    "        save_dir=save_dir,\n",
    "        tag=f\"{target_col}_{tag_prefix}_{agg}\" + (\"+gap\" if use_gap else \"\") + (\"+emb\" if use_embed else \"\")\n",
    "    )\n",
    "    return model, metrics, splits, path\n",
    "\n",
    "def train_xgb_aug3d_for_target(\n",
    "        target_col: str,\n",
    "        xgb_params: dict,\n",
    "        *,\n",
    "        train_csv_path: str,\n",
    "        lmdb_path: str,\n",
    "        save_dir: str=os.path.join(PROJECT_ROOT, \"saved_models\", \"baseline\", \"XGBoost\", \"xgb_aug3d\"),\n",
    "        tag_prefix: str = \"aug3D\",\n",
    "        test_size: float = 0.2,\n",
    "        random_state: int = 42,\n",
    "        stratify_regression: bool = True,\n",
    "        n_strat_bins: int = 10,\n",
    "        agg: str = \"mean\",\n",
    "        homo_df: Optional[pd.DataFrame] = None,\n",
    "        use_gap: bool = False,\n",
    "        use_embed: bool = False,\n",
    "        ):\n",
    "    \"\"\"Load rows with target, build X=[FP|3D], train XGB.\"\"\"\n",
    "\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    mask = ~df[target_col].isna()\n",
    "    parent_ids = df.loc[mask, 'id'].astype(int).values\n",
    "    smiles_tr = df.loc[mask, 'SMILES'].astype(str).tolist()\n",
    "    y = df.loc[mask, target_col].astype(float).values\n",
    "\n",
    "    # LMDB features aggregated over augmentations\n",
    "    X_parent, keep_idx = build_rf_features_from_lmdb_parents(parent_ids, lmdb_path, smiles_tr, agg=agg)\n",
    "    y_keep = y[keep_idx]\n",
    "\n",
    "    # append HOMO–LUMO (toggle gap/embed)\n",
    "    parents_kept = parent_ids[keep_idx]\n",
    "    if homo_df is not None and (use_gap or use_embed):\n",
    "        X_parent, added_cols = append_homolumo_features(\n",
    "            X_parent, parents_kept, homo_df, use_gap=use_gap, use_embed=use_embed\n",
    "        )\n",
    "        print(f\"[{target_col}] appended HOMO cols: {len(added_cols)} -> X={X_parent.shape}\")\n",
    "\n",
    "    model, metrics, splits, path = train_eval_xgb(\n",
    "        X_parent, y_keep,\n",
    "        xgb_params=xgb_params,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=n_strat_bins,\n",
    "        save_dir=save_dir,\n",
    "        tag=f\"{target_col}_{tag_prefix}_{agg}\" + (\"+gap\" if use_gap else \"\") + (\"+emb\" if use_embed else \"\")\n",
    "    )\n",
    "    return model, metrics, splits, path\n",
    "\n",
    "rf_cfg_aug = {\n",
    "    \"FFV\": {\"n_estimators\": 800, \"max_depth\": 30, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "    \"Tc\": {'n_estimators': 800, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False},\n",
    "    \"Rg\": {'n_estimators': 400, 'max_depth': 260, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 1.0, 'bootstrap': True},\n",
    "    \"Tg\": {\"n_estimators\": 600, \"max_depth\": 60, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "    \"Density\": {\"n_estimators\": 600, \"max_depth\": 40, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "}\n",
    "\n",
    "lgbm_cfg_aug = {\n",
    "  \"FFV\": {\"num_leaves\": 127, \"min_child_samples\": 20, \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 1},\n",
    "  \"Tc\": {\"num_leaves\": 63,  \"min_child_samples\": 15, \"feature_fraction\": 0.7, \"bagging_fraction\": 0.8, \"bagging_freq\": 1},\n",
    "  \"Rg\": {\"num_leaves\": 127, \"min_child_samples\": 10, \"feature_fraction\": 1.0, \"bagging_fraction\": 0.8, \"bagging_freq\": 1},\n",
    "  \"Tg\": {\"num_leaves\": 127, \"min_child_samples\": 20, \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 1},\n",
    "  \"Density\": {\"num_leaves\": 63,  \"min_child_samples\": 10, \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 1},\n",
    "}\n",
    "xgb_cfg_aug = {\n",
    "  \"FFV\": {\"max_depth\": 6,  \"min_child_weight\": 2.0, \"gamma\": 0.0, \"reg_lambda\": 2.0, \"reg_alpha\": 0.0, \"colsample_bytree\": 0.85},\n",
    "  \"Tc\": {\"max_depth\": 6,  \"min_child_weight\": 4.0, \"gamma\": 0.1, \"reg_lambda\": 3.0, \"reg_alpha\": 0.1, \"colsample_bytree\": 0.8},\n",
    "  \"Rg\": {\"max_depth\": 10, \"min_child_weight\": 2.0, \"gamma\": 0.0, \"reg_lambda\": 2.0, \"reg_alpha\": 0.0, \"colsample_bytree\": 0.9},\n",
    "  \"Tg\": {\"max_depth\": 10, \"min_child_weight\": 4.0, \"gamma\": 0.2, \"reg_lambda\": 3.0, \"reg_alpha\": 0.1, \"colsample_bytree\": 0.85},\n",
    "  \"Density\": {\"max_depth\": 6,  \"min_child_weight\": 1.0, \"gamma\": 0.0, \"reg_lambda\": 2.0, \"reg_alpha\": 0.0, \"colsample_bytree\": 0.8},\n",
    "}\n",
    "\n",
    "\n",
    "TRAIN_CSV = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "targets = [\"FFV\", \"Tc\", \"Rg\", \"Tg\", \"Density\"]\n",
    "print(\"--- Training Random Forest (+3D) Models ---\")\n",
    "rf_models, rf_metrics, rf_splits, rf_paths = {}, {}, {}, {}\n",
    "for t in targets:\n",
    "    print(f\"\\n>>> Training RF(+3D) for {t}\")\n",
    "    m, met, sp, p = train_rf_aug3d_for_target(\n",
    "        t, rf_cfg_aug[t],\n",
    "        train_csv_path=TRAIN_CSV,\n",
    "        lmdb_path=TRAIN_LMDB,\n",
    "        save_dir=os.path.join(PROJECT_ROOT, \"saved_models\", \"baseline\", \"RandomForest\", \"rf_aug3d\"),\n",
    "        tag_prefix=\"aug3D\"\n",
    "    )\n",
    "    rf_models[t], rf_metrics[t], rf_splits[t], rf_paths[t] = m, met, sp, p\n",
    "    print(f\"[RF+3D/{t}]  val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n--- Training LightGBM (+3D) Models ---\")\n",
    "lgbm_models, lgbm_metrics, lgbm_splits, lgbm_paths = {}, {}, {}, {}\n",
    "for t in targets:\n",
    "    print(f\"\\n>>> Training LGBM(+3D) for {t}\")\n",
    "    m, met, sp, p = train_lgbm_aug3d_for_target(\n",
    "        t, lgbm_cfg_aug[t],\n",
    "        train_csv_path=TRAIN_CSV,\n",
    "        lmdb_path=TRAIN_LMDB,\n",
    "        save_dir=os.path.join(PROJECT_ROOT, \"saved_models\", \"baseline\", \"LightGBM\", \"lgbm_aug3d\"),\n",
    "        tag_prefix=\"aug3D\"\n",
    "    )\n",
    "    lgbm_models[t], lgbm_metrics[t], lgbm_splits[t], lgbm_paths[t] = m, met, sp, p\n",
    "    print(f\"[LGBM+3D/{t}]  val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n--- Training XGBoost (+3D) Models ---\")\n",
    "xgb_models, xgb_metrics, xgb_splits, xgb_paths = {}, {}, {}, {}\n",
    "for t in targets:\n",
    "    print(f\"\\n>>> Training XGB(+3D) for {t}\")\n",
    "    m, met, sp, p = train_xgb_aug3d_for_target(\n",
    "        t, xgb_cfg_aug[t],\n",
    "        train_csv_path=TRAIN_CSV,\n",
    "        lmdb_path=TRAIN_LMDB,\n",
    "        save_dir=os.path.join(PROJECT_ROOT, \"saved_models\", \"baseline\", \"XGBoost\", \"xgb_aug3d\"),\n",
    "        tag_prefix=\"aug3D\"\n",
    "    )\n",
    "    xgb_models[t], xgb_metrics[t], xgb_splits[t], xgb_paths[t] = m, met, sp, p\n",
    "    print(f\"[XGB+3D/{t}]  val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4186d",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d599b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed version - use the parent mapping approach\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "AUG_KEY_MULT = 1000  \n",
    "\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_csv[\"id\"] = train_csv[\"id\"].astype(int)\n",
    "\n",
    "# Load parent mapping (key_id -> parent_id)\n",
    "pmap_path = TRAIN_LMDB + \".parent_map.tsv\"\n",
    "if os.path.exists(pmap_path):\n",
    "    pmap = pd.read_csv(pmap_path, sep=\"\\t\") # cols: key_id, parent_id, aug_idx, seed\n",
    "    pmap[\"key_id\"] = pmap[\"key_id\"].astype(np.int64)\n",
    "    pmap[\"parent_id\"] = pmap[\"parent_id\"].astype(np.int64)\n",
    "    parents_in_lmdb = np.sort(pmap[\"parent_id\"].unique().astype(np.int64))\n",
    "else:\n",
    "    # fallback: derive parent IDs from synthetic IDs\n",
    "    lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "    lmdb_ids = np.loadtxt(lmdb_ids_path, dtype=np.int64)\n",
    "    if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "    parents_in_lmdb = np.sort((lmdb_ids // AUG_KEY_MULT).unique().astype(np.int64))\n",
    "\n",
    "def ids_for_task(task):\n",
    "    t = task2idx[task]\n",
    "    col = label_cols[t]\n",
    "    # Get parent IDs that have the target and exist in LMDB\n",
    "    parent_ids = train_csv.loc[~train_csv[col].isna(), 'id'].astype(int).tolist()\n",
    "    # Filter to only those that exist in LMDB\n",
    "    valid_parent_ids = [pid for pid in parent_ids if pid in parents_in_lmdb]\n",
    "    \n",
    "    # Convert to synthetic IDs (use first conformer: aug_idx=0)\n",
    "    synthetic_ids = [pid * AUG_KEY_MULT + 0 for pid in valid_parent_ids]\n",
    "    return np.array(synthetic_ids, dtype=np.int64)\n",
    "\n",
    "ids_tg = ids_for_task(\"Tg\")\n",
    "ids_den = ids_for_task(\"Density\")\n",
    "ids_tc = ids_for_task(\"Tc\")\n",
    "ids_rg = ids_for_task(\"Rg\")\n",
    "ids_ffv = ids_for_task(\"FFV\")\n",
    "print(\"Tg ids:\", ids_tg.shape, \"Density ids:\", ids_den.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3efce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rdkit_feats_from_record(rec):\n",
    "    arr = getattr(rec, \"rdkit_feats\", None)\n",
    "    if arr is None:\n",
    "        return torch.zeros(15, dtype=torch.float32) # or 6 if depending on LMDB build\n",
    "    v = torch.as_tensor(np.asarray(arr, np.float32).reshape(-1), dtype=torch.float32)\n",
    "    return v.unsqueeze(0) # (1, D) so batch -> (B, D)\n",
    "\n",
    "\n",
    "class LMDBtoPyGSingleTask(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            ids,\n",
    "            lmdb_path,\n",
    "            target_index=None,\n",
    "            *,\n",
    "            use_mixed_edges: bool = True, # <— 3 cat + 32 RBF continuous\n",
    "            include_extra_atom_feats: bool = True, \n",
    "            ):\n",
    "        self.base = LMDBDataset(ids, lmdb_path)\n",
    "        self.t = target_index\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        self.include_extra_atom_feats = include_extra_atom_feats\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.base[idx]\n",
    "\n",
    "        x = torch.as_tensor(rec.x, dtype=torch.long)\n",
    "        ei = torch.as_tensor(rec.edge_index, dtype=torch.long)\n",
    "\n",
    "        ea = torch.as_tensor(rec.edge_attr) # (E, 3 + 32)\n",
    "        if self.use_mixed_edges:\n",
    "            # keep all columns, EdgeEncoderMixed will split cat vs cont\n",
    "            edge_attr = ea.to(torch.float32)\n",
    "        else:\n",
    "            # categorical-only for BondEncoder\n",
    "            edge_attr = ea[:, :3].to(torch.long)\n",
    "\n",
    "        # rdkit globals: Keep as (1, D) so PyG collates to (B, D)\n",
    "        rdkit_feats = _get_rdkit_feats_from_record(rec) # (1, D)\n",
    "        d = Data(x=x, edge_index=ei, edge_attr=edge_attr, rdkit_feats=rdkit_feats)\n",
    "\n",
    "        if self.include_extra_atom_feats and hasattr(rec, \"extra_atom_feats\"):\n",
    "            d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32) # (N,5)\n",
    "\n",
    "        if hasattr(rec, \"has_xyz\"):\n",
    "            # collates to (B,1), handy as a gating/global indicator\n",
    "            hz = np.asarray(rec.has_xyz, np.uint8).reshape(-1)\n",
    "            d.has_xyz = torch.from_numpy(hz.astype(np.float32))\n",
    "\n",
    "        if (self.t is not None) and hasattr(rec, \"y\"):\n",
    "            yv = torch.as_tensor(rec.y, dtype=torch.float32).view(-1)\n",
    "            if self.t < yv.numel():\n",
    "                d.y = yv[self.t:self.t+1] # (1,)\n",
    "\n",
    "        # geometry & extras from LMDB (if present)\n",
    "        if hasattr(rec, \"pos\"): # (N,3) float\n",
    "            d.pos = torch.as_tensor(rec.pos, dtype=torch.float32)\n",
    "        if hasattr(rec, \"extra_atom_feats\"):# (N,5) float\n",
    "            d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32)\n",
    "        if hasattr(rec, \"has_xyz\"): # (1,) bool/uint8\n",
    "            d.has_xyz = torch.as_tensor(rec.has_xyz, dtype=torch.float32)\n",
    "        if hasattr(rec, \"dist\"):\n",
    "            # rec.dist is (L, L) (uint8) \n",
    "            d.hops = torch.as_tensor(rec.dist, dtype=torch.long).unsqueeze(0)  # (1, L, L)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694612d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders_for_task(\n",
    "        task, \n",
    "        ids, \n",
    "        *, \n",
    "        batch_size=64, \n",
    "        seed=42,\n",
    "        use_mixed_edges=True, \n",
    "        include_extra_atom_feats=True\n",
    "        ):\n",
    "    t = task2idx[task]\n",
    "    tr_ids, va_ids = train_test_split(ids, test_size=0.2, random_state=seed)\n",
    "    tr_ds = LMDBtoPyGSingleTask(\n",
    "        tr_ids, \n",
    "        TRAIN_LMDB, \n",
    "        target_index=t, \n",
    "        use_mixed_edges=use_mixed_edges, \n",
    "        include_extra_atom_feats=include_extra_atom_feats\n",
    "        )\n",
    "    va_ds = LMDBtoPyGSingleTask(\n",
    "        va_ids, \n",
    "        TRAIN_LMDB, \n",
    "        target_index=t,\n",
    "        use_mixed_edges=use_mixed_edges,\n",
    "        include_extra_atom_feats=include_extra_atom_feats\n",
    "        )\n",
    "    tr = GeoDataLoader(tr_ds, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "    va = GeoDataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    return tr, va"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983db98",
   "metadata": {},
   "source": [
    "## Step 5: Define the Hybrid GNN Model\n",
    "\n",
    "The final architecture uses both structural and cheminformatics data by combining GNN-learned graph embeddings with SMILES-derived RDKit descriptors. This Hybrid GNN model uses `smiles2graph` for graph construction and augments it with RDKit-based molecular features for improved prediction accuracy.\n",
    "\n",
    "### Model Components:\n",
    "\n",
    "* **AtomEncoder / BondEncoder**\n",
    "  Transforms categorical atom and bond features (provided by OGB) into learnable embeddings using the encoders from `ogb.graphproppred.mol_encoder`. These provide a strong foundation for expressive graph learning.\n",
    "\n",
    "* **GINEConv Layers (x2)**\n",
    "  I use two stacked GINEConv layers (Graph Isomorphism Network with Edge features). These layers perform neighborhood aggregation based on edge attributes, allowing the model to capture localized chemical environments.\n",
    "\n",
    "* **Global Mean Pooling**\n",
    "  After message passing, node level embeddings are aggregated into a fixed size graph level representation using `global_mean_pool`.\n",
    "\n",
    "* **Concatenation with RDKit Descriptors**\n",
    "  The pooled GNN embedding is concatenated with external RDKit descriptors, which capture global molecular properties not easily inferred from graph data alone.\n",
    "\n",
    "* **MLP Prediction Head**\n",
    "  A multilayer perceptron processes the combined feature vector with ReLU activations, dropout regularization, and linear layers to predict the HOMO–LUMO gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dad355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = float(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        rand = keep + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        rand.floor_() # 0/1\n",
    "        return x.div(keep) * rand\n",
    "\n",
    "def _act(name: str):\n",
    "    name = (name or \"ReLU\").lower()\n",
    "    if name == \"relu\": return nn.ReLU()\n",
    "    if name == \"gelu\": return nn.GELU()\n",
    "    if name in (\"swish\", \"silu\"): return nn.SiLU()\n",
    "    return nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EdgeEncoderMixed(nn.Module):\n",
    "    def __init__(self, emb_dim: int, cont_dim: int = 32, activation=\"GeLU\"):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        # OGB bond categorical widths: type(5), stereo(6), conjugation(2)\n",
    "        self.emb0 = nn.Embedding(5, emb_dim)\n",
    "        self.emb1 = nn.Embedding(6, emb_dim)\n",
    "        self.emb2 = nn.Embedding(2, emb_dim)\n",
    "        self.mlp_cont = nn.Sequential(\n",
    "            nn.Linear(cont_dim, emb_dim), act,\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            )\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        # edge_attr: (E, 3+K)\n",
    "        cat = edge_attr[:, :3].long()\n",
    "        cont = edge_attr[:, 3:].float()\n",
    "        e_cat  = self.emb0(cat[:,0]) + self.emb1(cat[:,1]) + self.emb2(cat[:,2])\n",
    "        e_cont = self.mlp_cont(cont)\n",
    "        return e_cat + e_cont\n",
    "\n",
    "\n",
    "class ExtraAtomEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, activation=\"GeLU\"):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim), act,\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            )\n",
    "\n",
    "    def forward(self, extra):\n",
    "        return self.proj(extra) # (N, out_dim)\n",
    "\n",
    "\n",
    "class GINEBlock_GNN(nn.Module):\n",
    "    def __init__(self, dim, activation=\"GeLU\", dropout=0.1, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.conv = GINEConv(nn.Sequential(\n",
    "            nn.Linear(dim, dim), act,\n",
    "            nn.Linear(dim, dim),)\n",
    "            )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dp1 = DropPath(drop_path)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, 2*dim), act,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2*dim, dim))\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dp2 = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_emb):\n",
    "        # pre-norm transformer style\n",
    "        h = self.norm1(x)\n",
    "        h = self.conv(h, edge_index, edge_emb)\n",
    "        h = self.dropout1(h)\n",
    "        x = x + self.dp1(h)\n",
    "        h2 = self.norm2(x)\n",
    "        h2 = self.ffn(h2)\n",
    "        h2 = self.dropout2(h2)\n",
    "        x = x + self.dp2(h2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridGNNv2(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            gnn_dim: int,\n",
    "            rdkit_dim: int,\n",
    "            hidden_dim: int,\n",
    "            *,\n",
    "            num_layers: int = 8,\n",
    "            activation: str = \"Swish\",\n",
    "            dropout: float = 0.2,\n",
    "            drop_path_rate: float = 0.1,\n",
    "            use_mixed_edges: bool = True,\n",
    "            cont_dim: int = 32,\n",
    "            use_extra_atom_feats: bool = True,\n",
    "            extra_atom_dim: int = 5\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.gnn_dim = gnn_dim\n",
    "        self.rdkit_dim = rdkit_dim\n",
    "        self.use_extra_atom_feats = use_extra_atom_feats\n",
    "\n",
    "        # encoders\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=gnn_dim)\n",
    "        if use_mixed_edges:\n",
    "            self.edge_encoder = EdgeEncoderMixed(emb_dim=gnn_dim, cont_dim=cont_dim, activation=activation)\n",
    "        else:\n",
    "            self.edge_encoder = BondEncoder(emb_dim=gnn_dim)\n",
    "\n",
    "        if use_extra_atom_feats:\n",
    "            self.extra_atom = ExtraAtomEncoder(in_dim=extra_atom_dim, out_dim=gnn_dim, activation=activation)\n",
    "            self.extra_gate = nn.Sequential(\n",
    "                nn.Linear(2*gnn_dim, gnn_dim), _act(activation)\n",
    "                )\n",
    "\n",
    "        # backbone\n",
    "        dpr = [drop_path_rate * i / max(1, num_layers - 1) for i in range(num_layers)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GINEBlock_GNN(gnn_dim, activation=activation, dropout=dropout, drop_path=dpr[i])\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # pooling (concat of mean/max/attention)\n",
    "        self.att_pool = GlobalAttention(\n",
    "            gate_nn=nn.Sequential(\n",
    "                nn.Linear(gnn_dim, gnn_dim // 2), _act(activation),\n",
    "                nn.Linear(gnn_dim // 2, 1),\n",
    "            )\n",
    "        )\n",
    "        pooled_dim = 3 * gnn_dim  # mean + max + attention\n",
    "        # plus rdkit globals (+ optional has_xyz scalar)\n",
    "        self.with_has_xyz = True\n",
    "        head_in = pooled_dim + rdkit_dim + (1 if self.with_has_xyz else 0)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(head_in),\n",
    "            nn.Linear(head_in, hidden_dim), _act(activation),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2), _act(activation),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.atom_encoder(data.x) # (N, D)\n",
    "\n",
    "        if self.use_extra_atom_feats and hasattr(data, \"extra_atom_feats\"):\n",
    "            xa = self.extra_atom(data.extra_atom_feats)  # (N, D)\n",
    "            x = self.extra_gate(torch.cat([x, xa], dim=1))\n",
    "\n",
    "        e = self.edge_encoder(data.edge_attr)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, data.edge_index, e)\n",
    "\n",
    "        # pool\n",
    "        mean = global_mean_pool(x, data.batch)\n",
    "        mmax = global_max_pool(x, data.batch)\n",
    "        attn = self.att_pool(x, data.batch)\n",
    "        g = torch.cat([mean, mmax, attn], dim=1)\n",
    "\n",
    "        rd = data.rdkit_feats.view(g.size(0), -1)\n",
    "        extras = [g, rd]\n",
    "\n",
    "        if self.with_has_xyz and hasattr(data, \"has_xyz\"):\n",
    "            # has_xyz collates to (B,1)\n",
    "            extras.append(data.has_xyz.view(-1, 1).float())\n",
    "\n",
    "        out = torch.cat(extras, dim=1)\n",
    "        return self.head(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc992041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid_gnn_sota(\n",
    "        model: nn.Module,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        *,\n",
    "        lr: float = 5e-4,\n",
    "        optimizer: str = \"AdamW\",\n",
    "        weight_decay: float = 1e-5,\n",
    "        epochs: int = 120,\n",
    "        warmup_epochs: int = 5,\n",
    "        patience: int = 15,\n",
    "        clip_norm: float = 1.0,\n",
    "        amp: bool = True,\n",
    "        loss_name: str = \"mse\",   # \"mse\" or \"huber\"\n",
    "        save_dir: str=os.path.join(PROJECT_ROOT, \"saved_models\", \"gnn\"),\n",
    "        tag: str = \"model_sota\",\n",
    "        device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        ):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    opt_name = optimizer.lower()\n",
    "    if opt_name == \"rmsprop\":\n",
    "        opt = RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.0)\n",
    "    else:\n",
    "        opt = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # cosine schedule w/ warmup\n",
    "    def lr_factor(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / max(1, warmup_epochs)\n",
    "        t = (epoch - warmup_epochs) / max(1, (epochs - warmup_epochs))\n",
    "        return 0.5 * (1 + math.cos(math.pi * t))\n",
    "    scaler = GradScaler(\"cuda\", enabled=amp)\n",
    "\n",
    "    def loss_fn(pred, target):\n",
    "        if loss_name.lower() == \"huber\":\n",
    "            return F.huber_loss(pred, target, delta=1.0)\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_once(loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        for b in loader:\n",
    "            b = b.to(device)\n",
    "            p = model(b)\n",
    "            preds.append(p.detach().cpu())\n",
    "            trues.append(b.y.view(-1,1).cpu())\n",
    "        preds = torch.cat(preds).numpy(); trues = torch.cat(trues).numpy()\n",
    "        mae = np.mean(np.abs(preds - trues))\n",
    "        rmse = float(np.sqrt(np.mean((preds - trues)**2)))\n",
    "        r2 = float(1 - np.sum((preds - trues)**2) / np.sum((trues - trues.mean())**2))\n",
    "        return mae, rmse, r2\n",
    "\n",
    "    best_mae = float(\"inf\")\n",
    "    best = None\n",
    "    best_path = os.path.join(save_dir, f\"{tag}.pt\")\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # schedule\n",
    "        for g in opt.param_groups:\n",
    "            g[\"lr\"] = lr * lr_factor(ep-1)\n",
    "\n",
    "        model.train()\n",
    "        total, count = 0.0, 0\n",
    "        for b in train_loader:\n",
    "            b = b.to(device)\n",
    "            with autocast(\"cuda\", enabled=amp):\n",
    "                pred = model(b)\n",
    "                loss = loss_fn(pred, b.y.view(-1,1))\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            if clip_norm is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "            scaler.step(opt); scaler.update()\n",
    "\n",
    "            total += loss.item() * b.num_graphs\n",
    "            count += b.num_graphs\n",
    "\n",
    "        tr_mse = total / max(1, count)\n",
    "        mae, rmse, r2 = eval_once(val_loader)\n",
    "        print(f\"Epoch {ep:03d} | tr_MSE {tr_mse:.5f} | val_MAE {mae:.5f} | val_RMSE {rmse:.5f} | R2 {r2:.4f}\")\n",
    "\n",
    "        if mae < best_mae - 1e-6:\n",
    "            best_mae = mae\n",
    "            best = deepcopy(model.state_dict())\n",
    "            torch.save(best, best_path)\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best is not None:\n",
    "        model.load_state_dict(best)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "    final_mae, final_rmse, final_r2 = eval_once(val_loader)\n",
    "    print(f\"[{tag}] Best Val — MAE {final_mae:.6f} | RMSE {final_rmse:.6f} | R2 {final_r2:.4f}\")\n",
    "    return model, best_path, {\"MAE\": final_mae, \"RMSE\": final_rmse, \"R2\": final_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0813a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loaders (now feeding mixed edges + extra atom feats)\n",
    "train_loader_tg, val_loader_tg  = make_loaders_for_task(\"Tg\", ids_tg, batch_size=64, use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "train_loader_ffv,  val_loader_ffv  = make_loaders_for_task(\"FFV\", ids_ffv, batch_size=64, use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "train_loader_tc,  val_loader_tc  = make_loaders_for_task(\"Tc\", ids_tc, batch_size=64, use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "train_loader_den, val_loader_den = make_loaders_for_task(\"Density\", ids_den, batch_size=64, use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "train_loader_rg,  val_loader_rg  = make_loaders_for_task(\"Rg\", ids_rg, batch_size=64, use_mixed_edges=True, include_extra_atom_feats=True)\n",
    "\n",
    "b_tg = next(iter(train_loader_tg))\n",
    "rd_dim = b_tg.rdkit_feats.shape[-1]        \n",
    "print(\"rdkit_dim =\", rd_dim)\n",
    "\n",
    "# Tg \n",
    "model_tg = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "\n",
    "model_tg, ckpt_tg, metrics_tg = train_hybrid_gnn_sota(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=0.0005555079210176292, optimizer=\"RMSprop\", weight_decay=9.056299733554687e-06,\n",
    "    epochs=200, warmup_epochs=5, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=os.path.join(PROJECT_ROOT, \"saved_models\", \"gnn\", \"gnn_tg_v2\"), tag=\"hybridgnn_tg_v2\"\n",
    ")\n",
    "# # FFV\n",
    "model_ffv = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "\n",
    "model_ffv, ckpt_ffv, metrics_ffv = train_hybrid_gnn_sota(\n",
    "    model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "    lr=0.0005555079210176292, optimizer=\"RMSprop\", weight_decay=9.056299733554687e-06,\n",
    "    epochs=200, warmup_epochs=5, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=os.path.join(PROJECT_ROOT, \"saved_models\", \"gnn\", \"gnn_ffv_v2\"), tag=\"hybridgnn_ffv_v2\"\n",
    ")\n",
    "\n",
    "# Tc\n",
    "model_tc = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "\n",
    "model_tc, ckpt_tc, metrics_tc = train_hybrid_gnn_sota(\n",
    "    model_tc, train_loader_tc, val_loader_tc,\n",
    "    lr=0.0005555079210176292, optimizer=\"RMSprop\", weight_decay=9.056299733554687e-06,\n",
    "    epochs=200, warmup_epochs=5, patience=30,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=os.path.join(PROJECT_ROOT, \"saved_models\", \"gnn\", \"gnn_tc_v2\"), tag=\"hybridgnn_tc_v2\"\n",
    ")\n",
    "\n",
    "# Density \n",
    "model_den = HybridGNNv2(\n",
    "    gnn_dim=1024, rdkit_dim=rd_dim, hidden_dim=384,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.1, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "model_den, ckpt_den, metrics_den = train_hybrid_gnn_sota(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=5.956024201538505e-04, optimizer=\"AdamW\", weight_decay=8.619671341229739e-06,\n",
    "    epochs=200, warmup_epochs=8, patience=30,\n",
    "    clip_norm=0.5, amp=True, loss_name=\"mse\",\n",
    "    save_dir=os.path.join(PROJECT_ROOT, \"saved_models\", \"gnn\", \"gnn_density_v2\"), tag=\"hybridgnn_density_v2\"\n",
    ")\n",
    "\n",
    "# Rg \n",
    "model_rg = HybridGNNv2(\n",
    "    gnn_dim=256, rdkit_dim=rd_dim, hidden_dim=512,\n",
    "    num_layers=12, activation=\"Swish\", dropout=0.2, drop_path_rate=0.2,\n",
    "    use_mixed_edges=True, cont_dim=32,\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    ")\n",
    "model_rg, ckpt_rg, metrics_rg = train_hybrid_gnn_sota(\n",
    "    model_rg, train_loader_rg, val_loader_rg,\n",
    "    lr=5.6e-4, optimizer=\"RMSprop\", weight_decay=9.0e-6,\n",
    "    epochs=120, warmup_epochs=6, patience=20,\n",
    "    clip_norm=0.5, amp=True, loss_name=\"huber\",  # Huber often helps Rg\n",
    "    save_dir=os.path.join(PROJECT_ROOT, \"saved_models\", \"gnn\", \"gnn_rg_v2\"), tag=\"hybridgnn_rg_v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1bc6e",
   "metadata": {},
   "source": [
    "\n",
    "| Model Type | Feature | MAE | RMSE | R2 |\n",
    "|---|---|---|---|---|\n",
    "| RF3D | Tg | 57.499522 | 74.296699 | 0.5867 |\n",
    "| **GNN2** | **Tg** | **47.105114** | **61.480179** | **0.6040** |\n",
    "| XGB  | Tg | 56.246650 | 70.835005 | 0.6224 |\n",
    "| LGBM | Tg | 54.66728  | 68.213869 | 0.6499 |\n",
    "| RF3D | Tc | 0.029678 | 0.002014 | 0.7331 |\n",
    "| **GNN2** | **Tc** | **0.025115** | **0.041331** | **0.8000** |\n",
    "| XGB  | Tc | 0.036923  | 0.002652 | 0.6486 |\n",
    "| LGBM | Tc | 0.028768 | 0.002198 | 0.7088 |\n",
    "| RF3D | Density | 0.037793 | 0.070932 | 0.7847 |\n",
    "| GNN2 | Density | 0.031735 | 0.067845 | 0.7379 |\n",
    "| **XGB**  | **Density** | **0.024079**  | **0.001544** | **0.9339** |\n",
    "| LGBM | Density | 0.029702 | 0.002772 | 0.8814 |\n",
    "| RF3D | FFV | 0.007621 | 0.017553 | 0.6605 |\n",
    "| GNN2 | FFV | 0.013817 | 0.023902 | 0.4473 |\n",
    "| **XGB**  | **FFV** | **0.005842**  | **0.000198** | **0.7816** |\n",
    "| LGBM | FFV | 0.006494 | 0.000316 | 0.6515 |\n",
    "| RF3D | Rg | 1.639309 | 2.493712 | 0.7296 |\n",
    "| GNN2 | Rg | 2.083034 | 2.801481 | 0.6434 |\n",
    "| XGB  | Rg | 1.581447 | 2.442744 | 0.7408 |\n",
    "| **LGBM** | **Rg** | **1.557495** | **2.477095** | **0.7335** |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673460",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
