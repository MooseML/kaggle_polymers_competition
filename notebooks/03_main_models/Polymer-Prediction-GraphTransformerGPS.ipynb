{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61979795",
   "metadata": {},
   "source": [
    "# Polymer Property Predictions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "from typing import Dict, Optional\n",
    "import os \n",
    "import random \n",
    "from copy import deepcopy\n",
    "import gc\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Sampler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import AdamW, RMSprop\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# # PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "# # OGB dataset \n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Local imports \n",
    "from dataset_polymer_fixed import LMDBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589db70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Built with CUDA: True\n",
      "CUDA available: True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device: /physical_device:GPU:0\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: data\n",
      "LMDB directory: data\\processed_chunks\n",
      "Train LMDB: data\\processed_chunks\\polymer_train3d_dist.lmdb\n",
      "Test LMDB: data\\processed_chunks\\polymer_test3d_dist.lmdb\n",
      "LMDBs already exist.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  # Writable directory\n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/hlgap-gnn3d-transformer-pcqm4mv2-v1.pt'\n",
    "else:\n",
    "    DATA_ROOT = 'data'\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = 'hlgap-gnn3d-transformer-pcqm4mv2-v1.pt'\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"LMDB directory: {CHUNK_DIR}\")\n",
    "print(f\"Train LMDB: {TRAIN_LMDB}\")\n",
    "print(f\"Test LMDB: {TEST_LMDB}\")\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    # Run the LMDB builders\n",
    "    !python build_polymer_lmdb_fixed.py train\n",
    "    !python build_polymer_lmdb_fixed.py test\n",
    "    print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a0004",
   "metadata": {},
   "source": [
    "# Graph Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d599b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Tg → parents train=  408 val=  103 | aug rows train=  4080 val=  1030\n",
      "    FFV → parents train= 5624 val= 1406 | aug rows train= 56240 val= 14060\n",
      "     Tc → parents train=  589 val=  148 | aug rows train=  5890 val=  1480\n",
      "Density → parents train=  490 val=  123 | aug rows train=  4900 val=  1230\n",
      "     Rg → parents train=  491 val=  123 | aug rows train=  4910 val=  1230\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Parent-aware wiring (CSV parents -> augmented LMDB key_ids) --------------------\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "AUG_KEY_MULT = 1000  \n",
    "\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_csv[\"id\"] = train_csv[\"id\"].astype(int)\n",
    "\n",
    "# LMDB ids (augmented key_ids)\n",
    "lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "lmdb_ids = np.loadtxt(lmdb_ids_path, dtype=np.int64)\n",
    "if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "\n",
    "# Parent map (preferred) -> key_id list per parent\n",
    "pmap_path = TRAIN_LMDB + \".parent_map.tsv\"\n",
    "if os.path.exists(pmap_path):\n",
    "    pmap = pd.read_csv(pmap_path, sep=\"\\t\") # cols: key_id, parent_id, aug_idx, seed\n",
    "    pmap[\"key_id\"] = pmap[\"key_id\"].astype(np.int64)\n",
    "    pmap[\"parent_id\"] = pmap[\"parent_id\"].astype(np.int64)\n",
    "else:\n",
    "    # fallback if parent_map missing: derive parent by integer division\n",
    "    pmap = pd.DataFrame({\n",
    "        \"key_id\": lmdb_ids.astype(np.int64),\n",
    "        \"parent_id\": (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "    })\n",
    "parents_in_lmdb = np.sort(pmap[\"parent_id\"].unique().astype(np.int64))\n",
    "\n",
    "def parents_with_label(task: str) -> np.ndarray:\n",
    "    m = ~train_csv[task].isna()\n",
    "    have = train_csv.loc[m, \"id\"].astype(int).values # parents that have this label\n",
    "    return np.intersect1d(have, parents_in_lmdb, assume_unique=False)\n",
    "\n",
    "# Split by parents so we have no leakage and then we can expand to augmented key_ids \n",
    "def task_parent_split_keys(task: str, test_size=0.2, seed=42):\n",
    "    parents_labeled = parents_with_label(task)\n",
    "    if parents_labeled.size == 0:\n",
    "        raise ValueError(f\"No parents with labels for {task}\")\n",
    "    p_tr, p_va = train_test_split(parents_labeled, test_size=test_size, random_state=seed)\n",
    "    tr_keys = pmap.loc[pmap.parent_id.isin(p_tr), \"key_id\"].astype(np.int64).values\n",
    "    va_keys = pmap.loc[pmap.parent_id.isin(p_va), \"key_id\"].astype(np.int64).values\n",
    "    return np.sort(tr_keys), np.sort(va_keys), np.sort(p_tr), np.sort(p_va)\n",
    "\n",
    "# Build pools (augmented key_ids) per task\n",
    "task_pools = {}\n",
    "task_parent_splits = {}\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys, p_tr, p_va = task_parent_split_keys(t, test_size=0.2, seed=42)\n",
    "    task_pools[t] = (tr_keys, va_keys)\n",
    "    task_parent_splits[t] = (p_tr, p_va)\n",
    "\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys = task_pools[t]\n",
    "    p_tr, p_va = task_parent_splits[t]\n",
    "    print(f\"{t:>7} → parents train={len(p_tr):5d} val={len(p_va):5d} | aug rows train={len(tr_keys):6d} val={len(va_keys):6d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- KEY_SIZES (key_id -> parent n_atoms_2d) and simple finite samplers --------------------\n",
    "parent_meta_path = TRAIN_LMDB + \".parent_meta.tsv\"\n",
    "if not os.path.exists(parent_meta_path):\n",
    "    raise FileNotFoundError(f\"Missing {parent_meta_path}. Rebuild LMDB with parent_meta.tsv enabled.\")\n",
    "\n",
    "parent_meta = pd.read_csv(parent_meta_path, sep=\"\\t\") # cols: parent_id, n_atoms_2d, star_count, replacement_Z\n",
    "parent_meta[\"parent_id\"] = parent_meta[\"parent_id\"].astype(np.int64)\n",
    "parent_meta = parent_meta.drop_duplicates(subset=[\"parent_id\"])\n",
    "\n",
    "key_sizes_df = pmap.merge(parent_meta[[\"parent_id\",\"n_atoms_2d\"]], on=\"parent_id\", how=\"left\")\n",
    "\n",
    "if key_sizes_df[\"n_atoms_2d\"].isna().any():\n",
    "    med = int(key_sizes_df[\"n_atoms_2d\"].median())\n",
    "    key_sizes_df[\"n_atoms_2d\"] = key_sizes_df[\"n_atoms_2d\"].fillna(med)\n",
    "\n",
    "KEY_SIZES: Dict[int,int] = dict(\n",
    "    zip(key_sizes_df[\"key_id\"].astype(np.int64).tolist(),\n",
    "        key_sizes_df[\"n_atoms_2d\"].astype(int).tolist())\n",
    ")\n",
    "\n",
    "class EmptyBatchSampler(Sampler):\n",
    "    def __iter__(self):\n",
    "        return iter(())           \n",
    "    def __len__(self):\n",
    "        return 0\n",
    "\n",
    "class TokenBucketBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Precompute a finite list of batches from (keys, sizes) under token/quadratic/max_batch constraints.\n",
    "    __iter__ just yields those batches. __len__ returns exact count.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys, sizes_dict: Dict[int,int], *, max_tokens: int, max_quadratic: int, max_batch_size: int, shuffle: bool, seed: int, bins: int = 8):\n",
    "        self.keys = np.asarray(keys, dtype=np.int64)\n",
    "        self.sizes_dict = sizes_dict\n",
    "        self.max_tokens = int(max_tokens)\n",
    "        self.max_quadratic = int(max_quadratic)\n",
    "        self.max_batch_size = int(max_batch_size)\n",
    "        self.shuffle = bool(shuffle)\n",
    "        self.seed = int(seed)\n",
    "        self.bins = int(max(1, bins))\n",
    "        self._batches = self._pack_once() # precompute finite list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batches)\n",
    "\n",
    "    def _pack_once(self):\n",
    "        if self.keys.size == 0:\n",
    "            return []\n",
    "\n",
    "        # materialize (key,size), guard size>=1\n",
    "        pairs = [(int(k), max(1, int(self.sizes_dict.get(int(k), 1)))) for k in self.keys]\n",
    "\n",
    "        # shuffle or deterministic order\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        if self.shuffle:\n",
    "            rng.shuffle(pairs)\n",
    "        else:\n",
    "            pairs.sort(key=lambda t: (t[1], t[0]))\n",
    "\n",
    "        sizes = np.array([s for _, s in pairs], dtype=np.int32)\n",
    "        # Bins by size to reduce padding \n",
    "        B = int(min(self.bins, max(1, sizes.size)))\n",
    "        try:\n",
    "            qs = np.quantile(sizes, np.linspace(0, 1, B + 1)) if sizes.size > 1 else np.array([sizes[0], sizes[0]])\n",
    "        except Exception:\n",
    "            qs = np.array([sizes.min(), sizes.max()])\n",
    "\n",
    "        bins = [[] for _ in range(B)]\n",
    "        for i, (k, s) in enumerate(pairs):\n",
    "            b = int(np.searchsorted(qs, s, side=\"right\")) - 1\n",
    "            b = max(0, min(B - 1, b))\n",
    "            bins[b].append((k, s))\n",
    "\n",
    "        if self.shuffle:\n",
    "            for b in range(B):\n",
    "                rng.shuffle(bins[b])\n",
    "\n",
    "        batches = []\n",
    "        # Round robin draw from bins so each item is used once...finite\n",
    "        progress = True\n",
    "        while progress:\n",
    "            progress = False\n",
    "            for b in range(B):\n",
    "                if not bins[b]:\n",
    "                    continue\n",
    "                progress = True\n",
    "                cur, cur_tokens, cur_quad = [], 0, 0\n",
    "                while bins[b]:\n",
    "                    k, s = bins[b][0]\n",
    "                    next_len = len(cur) + 1\n",
    "                    next_tokens = cur_tokens + s\n",
    "                    next_quad = cur_quad + s * s\n",
    "                    if (next_len <= self.max_batch_size\n",
    "                        and next_tokens <= self.max_tokens\n",
    "                        and next_quad  <= self.max_quadratic):\n",
    "                        cur.append(k)\n",
    "                        cur_tokens = next_tokens\n",
    "                        cur_quad = next_quad\n",
    "                        bins[b].pop(0)\n",
    "                    else:\n",
    "                        break\n",
    "                if cur:\n",
    "                    batches.append(cur)\n",
    "\n",
    "        if not batches: # Safety \n",
    "            batches = [[int(k)] for (k, _) in pairs]\n",
    "        return batches\n",
    "\n",
    "class ChainBatchSamplers(Sampler):\n",
    "    \"\"\"Concatenate multiple precomputed batch samplers into one finite sequence.\"\"\"\n",
    "    def __init__(self, samplers, *, shuffle_order=False, seed=0):\n",
    "        self.samplers = list(samplers)\n",
    "        self.shuffle_order = bool(shuffle_order)\n",
    "        self.seed = int(seed)\n",
    "        # precompute concatenation so __len__ is exact\n",
    "        order = list(range(len(self.samplers)))\n",
    "        if self.shuffle_order:\n",
    "            rnd = random.Random(self.seed)\n",
    "            rnd.shuffle(order)\n",
    "        self._batches = []\n",
    "        for k in order:\n",
    "            s = self.samplers[k]\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            self._batches.extend(list(s))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batches)\n",
    "\n",
    "def build_bucket_sampler_for_keys(\n",
    "    keys: np.ndarray, *,\n",
    "    max_tokens: int,\n",
    "    max_quadratic: Optional[int],\n",
    "    max_batch_size: int,\n",
    "    shuffle: bool,\n",
    "    seed: int,\n",
    "    ) -> Sampler:\n",
    "    keys = np.asarray(keys, dtype=np.int64)\n",
    "\n",
    "    if keys.size == 0:\n",
    "        return EmptyBatchSampler()\n",
    "    \n",
    "    return TokenBucketBatchSampler(\n",
    "        keys, \n",
    "        KEY_SIZES,\n",
    "        max_tokens=max_tokens,\n",
    "        max_quadratic=max_quadratic if max_quadratic is not None else 1_200_000,\n",
    "        max_batch_size=max_batch_size,\n",
    "        shuffle=shuffle, \n",
    "        seed=seed, \n",
    "        bins=8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3efce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rdkit_feats_from_record(rec):\n",
    "    arr = getattr(rec, \"rdkit_feats\", None)\n",
    "    if arr is None:\n",
    "        return torch.zeros(1, 15, dtype=torch.float32) # keep (1, D)\n",
    "    v = torch.as_tensor(np.asarray(arr, np.float32).reshape(1, -1), dtype=torch.float32)\n",
    "    return v # (1, D)\n",
    "\n",
    "class LMDBtoPyGSingleTask(Dataset):\n",
    "    def __init__(self, ids, lmdb_path, target_index=None, *, use_mixed_edges: bool = True, include_extra_atom_feats: bool = True):\n",
    "        self.ids = np.asarray(ids, dtype=np.int64)\n",
    "        self.base = LMDBDataset(self.ids, lmdb_path)\n",
    "        self.t = target_index\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        self.include_extra_atom_feats = include_extra_atom_feats\n",
    "\n",
    "    def __len__(self): return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.base[idx]\n",
    "        x = torch.as_tensor(rec.x, dtype=torch.long)\n",
    "        ei = torch.as_tensor(rec.edge_index, dtype=torch.long)\n",
    "        ea = torch.as_tensor(rec.edge_attr)\n",
    "\n",
    "        # Mixed edges: 3 categorical + 32 RBF. Categorical-only if disabled\n",
    "        edge_attr = ea.to(torch.float32) if self.use_mixed_edges else ea[:, :3].to(torch.long)\n",
    "\n",
    "        d = Data(x=x, edge_index=ei, edge_attr=edge_attr, rdkit_feats=_get_rdkit_feats_from_record(rec)) # (1, D)\n",
    "\n",
    "        if hasattr(rec, \"pos\"):\n",
    "                d.pos  = torch.as_tensor(rec.pos, dtype=torch.float32)\n",
    "        if self.include_extra_atom_feats and hasattr(rec, \"extra_atom_feats\"):\n",
    "             d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32)\n",
    "        if hasattr(rec, \"has_xyz\"):\n",
    "             d.has_xyz = torch.as_tensor(rec.has_xyz, dtype=torch.float32) # (1,)\n",
    "        if hasattr(rec, \"dist\"):\n",
    "             d.hops = torch.as_tensor(rec.dist, dtype=torch.long).unsqueeze(0) # (1,L,L)\n",
    "\n",
    "        if (self.t is not None) and hasattr(rec, \"y\"):\n",
    "            yv = torch.as_tensor(rec.y, dtype=torch.float32).view(-1)\n",
    "            if self.t < yv.numel(): \n",
    "                 d.y = yv[self.t:self.t+1] # (1,)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694612d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Token-bucket loaders with key->index mapping --------------------\n",
    "WHALE_CUTOFF = 86 # p99 from EDA\n",
    "\n",
    "class MapKeyBatchesToIndexBatches(Sampler):\n",
    "    \"\"\"Wrap a batch-sampler that yields key_ids; convert to dataset indices.\"\"\"\n",
    "    def __init__(self, key_batch_sampler, id2pos):\n",
    "        self.key_batch_sampler = key_batch_sampler # yields lists of key_ids\n",
    "        self.id2pos = id2pos # {key_id: index_in_dataset}\n",
    "        # pre-map once for speed and safety\n",
    "        self._batches = [[self.id2pos[int(k)] for k in batch] for batch in key_batch_sampler]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batches)\n",
    "\n",
    "def make_loaders_for_task_from_pools(\n",
    "        task, \n",
    "        task_pools,\n",
    "        *,\n",
    "        normal_max_tokens=9000,\n",
    "        normal_max_quadratic=1080000,\n",
    "        whale_max_tokens=2500,\n",
    "        whale_max_quadratic=400000,\n",
    "        max_batch_size=1024,\n",
    "        use_mixed_edges=True,\n",
    "        include_extra_atom_feats=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        debug_single_process=True,\n",
    "        whale_cutoff=WHALE_CUTOFF,\n",
    "        ):\n",
    "    assert 'KEY_SIZES' in globals(), \"KEY_SIZES dict must be built first\"\n",
    "\n",
    "    t = task2idx[task]\n",
    "    tr_keys, va_keys = task_pools[task]\n",
    "    if len(tr_keys) == 0 or len(va_keys) == 0:\n",
    "        raise ValueError(f\"Empty pools for {task}. Check splits.\")\n",
    "\n",
    "    # Datasets stay in key-id order\n",
    "    tr_ds = LMDBtoPyGSingleTask(tr_keys, TRAIN_LMDB, target_index=t, use_mixed_edges=use_mixed_edges, include_extra_atom_feats=include_extra_atom_feats)\n",
    "    va_ds = LMDBtoPyGSingleTask(va_keys, TRAIN_LMDB, target_index=t, use_mixed_edges=use_mixed_edges, include_extra_atom_feats=include_extra_atom_feats)\n",
    "\n",
    "    # Map key_id -> position within each dataset\n",
    "    tr_id2pos = {int(k): i for i, k in enumerate(tr_keys)}\n",
    "    va_id2pos = {int(k): i for i, k in enumerate(va_keys)}\n",
    "\n",
    "    # Split ids by size using KEY_SIZES\n",
    "    def split_keys(keys):\n",
    "        small, whales = [], []\n",
    "        for k in keys:\n",
    "            if KEY_SIZES.get(int(k), 1) > whale_cutoff:\n",
    "                whales.append(int(k))\n",
    "            else:\n",
    "                small.append(int(k))\n",
    "        return np.array(small, dtype=np.int64), np.array(whales, dtype=np.int64)\n",
    "\n",
    "    tr_small, tr_whales = split_keys(tr_keys)\n",
    "    va_small, va_whales = split_keys(va_keys)\n",
    "\n",
    "    # Build key-id samplers (finite)\n",
    "    tr_small_s = build_bucket_sampler_for_keys(tr_small, max_tokens=normal_max_tokens, max_quadratic=normal_max_quadratic, max_batch_size=max_batch_size, shuffle=True, seed=42)\n",
    "    tr_whale_s = build_bucket_sampler_for_keys(tr_whales, max_tokens=whale_max_tokens, max_quadratic=whale_max_quadratic, max_batch_size=max_batch_size, shuffle=True, seed=43)\n",
    "    tr_keys_sampler = ChainBatchSamplers([tr_small_s, tr_whale_s], shuffle_order=True, seed=123)\n",
    "\n",
    "    va_small_s = build_bucket_sampler_for_keys(va_small, max_tokens=normal_max_tokens, max_quadratic=normal_max_quadratic, max_batch_size=max_batch_size, shuffle=False, seed=123)\n",
    "    va_whale_s = build_bucket_sampler_for_keys(va_whales, max_tokens=whale_max_tokens, max_quadratic=whale_max_quadratic, max_batch_size=max_batch_size, shuffle=False, seed=123)\n",
    "    va_keys_sampler = ChainBatchSamplers([va_small_s, va_whale_s], shuffle_order=False, seed=0)\n",
    "\n",
    "    # Wrap: key-id batches -> dataset index batches\n",
    "    tr_sampler = MapKeyBatchesToIndexBatches(tr_keys_sampler, tr_id2pos)\n",
    "    va_sampler = MapKeyBatchesToIndexBatches(va_keys_sampler, va_id2pos)\n",
    "\n",
    "    # DataLoaders\n",
    "    if debug_single_process:\n",
    "        tr_loader = GeoDataLoader(tr_ds, batch_sampler=tr_sampler, num_workers=0, pin_memory=False)\n",
    "        va_loader = GeoDataLoader(va_ds, batch_sampler=va_sampler, num_workers=0, pin_memory=False)\n",
    "    else:\n",
    "        tr_loader = GeoDataLoader(\n",
    "            tr_ds, \n",
    "            batch_sampler=tr_sampler,\n",
    "            num_workers=num_workers, \n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=(num_workers > 0),\n",
    "            **({} if num_workers == 0 else dict(prefetch_factor=2))\n",
    "        )\n",
    "        va_loader = GeoDataLoader(\n",
    "            va_ds, \n",
    "            batch_sampler=va_sampler,\n",
    "            num_workers=num_workers, \n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=(num_workers > 0),\n",
    "            **({} if num_workers == 0 else dict(prefetch_factor=2))\n",
    "        )\n",
    "    return tr_loader, va_loader\n",
    "\n",
    "train_loader_tg,  val_loader_tg  = make_loaders_for_task_from_pools(\"Tg\", task_pools, debug_single_process=True)\n",
    "train_loader_den, val_loader_den = make_loaders_for_task_from_pools(\"Density\", task_pools, debug_single_process=True)\n",
    "train_loader_rg,  val_loader_rg  = make_loaders_for_task_from_pools(\"Rg\", task_pools, debug_single_process=True)\n",
    "train_loader_ffv, val_loader_ffv = make_loaders_for_task_from_pools(\"FFV\", task_pools, debug_single_process=True)\n",
    "train_loader_tc,  val_loader_tc  = make_loaders_for_task_from_pools(\"Tc\", task_pools, debug_single_process=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983db98",
   "metadata": {},
   "source": [
    "## Step 5: Define the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc992041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_len_stats(b):\n",
    "    # counts nodes per-graph from PyG's batch vector\n",
    "    sizes = torch.bincount(b.batch, minlength=b.num_graphs)\n",
    "    s = sizes.to(torch.int32).cpu().numpy()\n",
    "    if s.size == 0:\n",
    "        return \"L: n=0\"\n",
    "    tokens = int(s.sum())\n",
    "    quad = int((sizes.to(torch.int64)**2).sum().item())\n",
    "    q50, q90, q95, q99 = np.quantile(s, [0.5, 0.9, 0.95, 0.99])\n",
    "    return (f\"L: n={len(s)} tokens={tokens} sumL2={quad}\"\n",
    "            f\"p50={int(q50)} p90={int(q90)} p95={int(q95)} p99={int(q99)} max={int(s.max())}\")\n",
    "\n",
    "\n",
    "def free_cuda_memory(tag: str = \"\"):\n",
    "    try:\n",
    "        torch.cuda.synchronize()\n",
    "    except Exception:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    if tag:\n",
    "        try:\n",
    "            alloc = torch.cuda.memory_allocated() / (1024**2)\n",
    "            reserv = torch.cuda.memory_reserved() / (1024**2)\n",
    "            print(f\"[mem:{tag}] allocated={alloc:.1f}MB reserved={reserv:.1f}MB\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def reset_cuda_stats():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def train_hybrid_gnn_sota(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    *,\n",
    "    lr: float = 5e-4,\n",
    "    optimizer: str = \"AdamW\",\n",
    "    weight_decay: float = 1e-5,\n",
    "    epochs: int = 120,\n",
    "    warmup_epochs: int = 5,\n",
    "    patience: int = 15,\n",
    "    clip_norm: float = 1.0,\n",
    "    amp: bool = True,\n",
    "    loss_name: str = \"mse\",   # \"mse\" or \"huber\"\n",
    "    save_dir: str = \"saved_models/gnn\",\n",
    "    tag: str = \"model_sota\",\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    opt_name = optimizer.lower()\n",
    "    if opt_name == \"rmsprop\":\n",
    "        opt = RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.0)\n",
    "    else:\n",
    "        opt = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # cosine schedule w/ warmup\n",
    "    def lr_factor(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / max(1, warmup_epochs)\n",
    "        t = (epoch - warmup_epochs) / max(1, (epochs - warmup_epochs))\n",
    "        return 0.5 * (1 + math.cos(math.pi * t))\n",
    "    scaler = GradScaler(\"cuda\", enabled=amp)\n",
    "\n",
    "    def loss_fn(pred, target):\n",
    "        if loss_name.lower() == \"huber\":\n",
    "            return F.huber_loss(pred, target, delta=1.0)\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_once(loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.inference_mode():\n",
    "            for i, b in enumerate(loader, 1):\n",
    "                # print(_batch_len_stats(b))  # keep if you want\n",
    "                b = b.to(device, non_blocking=True)\n",
    "                p = model(b)\n",
    "                preds.append(p.cpu())\n",
    "                trues.append(b.y.view(-1,1).cpu())\n",
    "                del p, b\n",
    "        preds = torch.cat(preds).numpy(); trues = torch.cat(trues).numpy()\n",
    "        mae = np.mean(np.abs(preds - trues))\n",
    "        rmse = float(np.sqrt(np.mean((preds - trues)**2)))\n",
    "        r2 = float(1 - np.sum((preds - trues)**2) / np.sum((trues - trues.mean())**2))\n",
    "        return mae, rmse, r2\n",
    "\n",
    "    best_mae = float(\"inf\")\n",
    "    best = None\n",
    "    best_path = os.path.join(save_dir, f\"{tag}.pt\")\n",
    "    bad = 0 \n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        for g in opt.param_groups:\n",
    "            g[\"lr\"] = lr * lr_factor(ep-1)\n",
    "\n",
    "        model.train()\n",
    "        total, count = 0.0, 0\n",
    "        for step, b in enumerate(train_loader, start=1):\n",
    "            b = b.to(device, non_blocking=True)\n",
    "            if ep == 1 and step % 50 == 1:\n",
    "            # if (ep <= 2) or (step % 50 == 1):\n",
    "                print(_batch_len_stats(b))\n",
    "            with autocast(\"cuda\", enabled=amp):\n",
    "                pred = model(b)\n",
    "                loss = loss_fn(pred, b.y.view(-1,1))\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            if clip_norm is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "            scaler.step(opt); scaler.update()\n",
    "\n",
    "            total += loss.item() * b.num_graphs\n",
    "            count += b.num_graphs\n",
    "            del pred, loss, b  \n",
    "\n",
    "        free_cuda_memory(tag=f\"after_epoch_{ep}\")\n",
    "\n",
    "        tr_mse = total / max(1, count)\n",
    "        mae, rmse, r2 = eval_once(val_loader)\n",
    "        print(f\"Epoch {ep:03d} | tr_MSE {tr_mse:.5f} | val_MAE {mae:.5f} | val_RMSE {rmse:.5f} | R2 {r2:.4f}\")\n",
    "\n",
    "        if mae < best_mae - 1e-6:\n",
    "            best_mae = mae\n",
    "            best = deepcopy(model.state_dict())\n",
    "            torch.save(best, best_path)\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    if best is not None:\n",
    "        model.load_state_dict(best)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "    final_mae, final_rmse, final_r2 = eval_once(val_loader)\n",
    "    print(f\"[{tag}] Best Val — MAE {final_mae:.6f} | RMSE {final_rmse:.6f} | R2 {final_r2:.4f}\")\n",
    "    return model, best_path, {\"MAE\": final_mae, \"RMSE\": final_rmse, \"R2\": final_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9449bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _act(name: str):\n",
    "    name = (name or \"relu\").lower()\n",
    "    if name == \"gelu\": \n",
    "        return nn.GELU()\n",
    "    if name in (\"silu\", \"swish\"): \n",
    "        return nn.SiLU()\n",
    "    return nn.ReLU()\n",
    "\n",
    "\n",
    "class AttnBiasFull(nn.Module):\n",
    "    \"\"\"\n",
    "    Produces additive per-head attention bias of shape (B, H, L0, L0)\n",
    "    from geometry (xyz), adjacency, SPD buckets, and categorical edge types.\n",
    "\n",
    "    Accepts both old arg names (use_geo/use_adj_const/spd_max/rbf_K) and\n",
    "    new ones (use_geo_bias/use_adj_bias/spd_buckets/rbf_k/edge_cats).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_heads: int,\n",
    "        *,\n",
    "        # older names I used\n",
    "        use_geo: bool = None, \n",
    "        use_adj_const: bool = None, \n",
    "        use_spd: bool = True,\n",
    "        spd_max: int = None, \n",
    "        rbf_K: int = None,\n",
    "        # new names\n",
    "        use_geo_bias: bool = None, \n",
    "        use_adj_bias: bool = None,\n",
    "        spd_buckets: int = None, \n",
    "        rbf_k: int = None,\n",
    "        edge_cats: tuple = (5, 6, 2),\n",
    "        use_edge_bias: bool = True,\n",
    "        # shared\n",
    "        rbf_beta: float = 5.0, \n",
    "        activation: str = \"relu\",\n",
    "        edge_cont_dim: int = 32,  # (kept for compatibility but not currently being used here)\n",
    "        use_headnorm: bool = True,\n",
    "        bound_scale: float = 0.1, # tanh scale for gentle bounding\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_heads = int(n_heads)\n",
    "        self.bound_scale = float(bound_scale)\n",
    "        self.use_headnorm = bool(use_headnorm)\n",
    "\n",
    "        def pick(*vals, default):\n",
    "            for v in vals:\n",
    "                if v is not None:\n",
    "                    return v\n",
    "            return default\n",
    "\n",
    "        self.use_geo = bool(pick(use_geo, use_geo_bias, default=True))\n",
    "        self.use_adj_const = bool(pick(use_adj_const, use_adj_bias, default=True))\n",
    "\n",
    "        # SPD: if spd_buckets given then we will use exactly that; otherwise, we will use spd_max + 2 (0..spd_max + catch-all)\n",
    "        if spd_buckets is not None:\n",
    "            self.spd_buckets = int(spd_buckets)\n",
    "        else:\n",
    "            smax = 5 if spd_max is None else int(spd_max)\n",
    "            self.spd_buckets = smax + 2 # 0..smax + 1(>=)\n",
    "\n",
    "        K = int(pick(rbf_K, rbf_k, default=16))\n",
    "        self.rbf_beta = float(rbf_beta)\n",
    "\n",
    "        #  geometry -> per-head bias \n",
    "        if self.use_geo:\n",
    "            centers = torch.linspace(0.0, 10.0, K)\n",
    "            self.register_buffer(\"centers\", centers, persistent=False)\n",
    "            self.geo_mlp = nn.Sequential(\n",
    "                nn.Linear(K, self.n_heads), # simple per-head projection\n",
    "            )\n",
    "\n",
    "        if self.use_adj_const:\n",
    "            self.adj_bias = nn.Parameter(torch.zeros(self.n_heads))\n",
    "\n",
    "        # SPD buckets -> per-head bias \n",
    "        self.use_spd = bool(use_spd)\n",
    "        if self.use_spd:\n",
    "            self.spd_emb = nn.Embedding(self.spd_buckets, self.n_heads)\n",
    "\n",
    "        # edge categorical bias (configurable widths)\n",
    "        t, s, c = edge_cats\n",
    "        self.use_edge_bias = bool(use_edge_bias)\n",
    "        if self.use_edge_bias:\n",
    "            self.edge_emb0 = nn.Embedding(int(t), self.n_heads)\n",
    "            self.edge_emb1 = nn.Embedding(int(s), self.n_heads)\n",
    "            self.edge_emb2 = nn.Embedding(int(c), self.n_heads)\n",
    "        else:\n",
    "            self.edge_emb0 = self.edge_emb1 = self.edge_emb2 = None\n",
    "\n",
    "        # per-component learnable scalers \n",
    "        self.alpha_geo = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_spd = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_adj = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_edge = nn.Parameter(torch.tensor(0.2))\n",
    "\n",
    "        # simple head-wise LayerNorms (normalize across H)\n",
    "        if self.use_headnorm:\n",
    "            self.ln_geo = nn.LayerNorm(self.n_heads)\n",
    "            self.ln_spd = nn.LayerNorm(self.n_heads)\n",
    "            self.ln_edge = nn.LayerNorm(self.n_heads)\n",
    "\n",
    "    # ------------------------------------------ helpers ------------------------------------------\n",
    "    def _apply_ln_heads(self, t: torch.Tensor, ln: nn.LayerNorm) -> torch.Tensor:\n",
    "        \"\"\"Apply LayerNorm across heads for a (B,H,L,L) tensor.\"\"\"\n",
    "        # (B,H,L,L) -> (B,L,L,H) -> LN(H) -> (B,H,L,L)\n",
    "        t = t.permute(0, 2, 3, 1)\n",
    "        t = ln(t)\n",
    "        t = t.permute(0, 3, 1, 2).contiguous()\n",
    "        return t\n",
    "\n",
    "    def _bound(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Bound magnitudes to avoid dominating softmax; keeps gradients smooth.\"\"\"\n",
    "        return self.bound_scale * torch.tanh(t)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _spd_bias(self, hops: torch.Tensor, valid_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        hops: (B, MAX_NODES, MAX_NODES) or (B, L0, L0) shortest-path distances (uint8/long)\n",
    "        valid_mask: (B, L0, L0) bool, True where both tokens are real (not PAD)\n",
    "        returns: (B, H, L0, L0) additive per-head bias\n",
    "        \"\"\"\n",
    "        if hops.dim() == 2: # (L,L) -> (1,L,L)\n",
    "            hops = hops.unsqueeze(0)\n",
    "\n",
    "        B, L0, _ = valid_mask.shape\n",
    "\n",
    "        # align SPD to current L0 (top-left block)\n",
    "        if hops.size(1) != L0 or hops.size(2) != L0:\n",
    "            hops = hops[:, :L0, :L0]\n",
    "\n",
    "        # bucketize SPD: last bucket = catch-all (>= last)\n",
    "        last = self.spd_buckets - 1\n",
    "        raw = hops.to(valid_mask.device).long().clamp_min_(0)\n",
    "        catch_all = raw >= last\n",
    "        raw = raw.clamp_max(last - 1)\n",
    "        bucket = torch.where(catch_all, raw.new_full(raw.shape, last), raw)\n",
    "\n",
    "        # wipe invalid pairs\n",
    "        bucket = torch.where(valid_mask, bucket, torch.zeros_like(bucket))\n",
    "\n",
    "        emb = self.spd_emb(bucket) # (B, L0, L0, H)\n",
    "        return emb.permute(0, 3, 1, 2).contiguous() # (B, H, L0, L0)\n",
    "\n",
    "    def _edge_bias(self, edge_index, edge_attr, batch, L0, ptr=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-head additive bias from categorical bond attributes.\n",
    "        Returns: (B, H, L0, L0)\n",
    "        \"\"\"\n",
    "        u, v = edge_index\n",
    "        be   = batch[u] # graph id per edge\n",
    "\n",
    "        if ptr is None:\n",
    "            B = int(batch.max().item()) + 1\n",
    "            counts = torch.bincount(batch, minlength=B)\n",
    "            ptr = torch.zeros(B + 1, dtype=torch.long, device=batch.device)\n",
    "            ptr[1:] = torch.cumsum(counts, dim=0)\n",
    "        B = int(ptr.numel() - 1)\n",
    "\n",
    "        start = ptr[be]\n",
    "        u_loc = (u - start).long()\n",
    "        v_loc = (v - start).long()\n",
    "\n",
    "        cat = edge_attr[:, :3].long()\n",
    "        eh  = ( self.edge_emb0(cat[:, 0])\n",
    "              + self.edge_emb1(cat[:, 1])\n",
    "              + self.edge_emb2(cat[:, 2]) )  # (E,H)\n",
    "\n",
    "        H = self.n_heads\n",
    "        eb = torch.zeros((B, H, L0, L0), device=edge_attr.device, dtype=torch.float32)\n",
    "        for b in range(B):\n",
    "            m = (be == b)\n",
    "            if not torch.any(m):\n",
    "                continue\n",
    "            eb[b, :, u_loc[m], v_loc[m]] += eh[m].T\n",
    "        return eb\n",
    "\n",
    "    # ------------------------------------------ forward ------------------------------------------\n",
    "    def forward(self, pos, edge_index, edge_attr, batch, key_padding_mask, hops=None, ptr=None):\n",
    "        \"\"\"\n",
    "        Returns (B, H, L0, L0) additive bias. PAD rows/cols are filled with large negative.\n",
    "        \"\"\"\n",
    "        A = to_dense_adj(edge_index, batch=batch).squeeze(1) # (B,L0,L0)\n",
    "        B, L0, _ = A.shape\n",
    "        H = self.n_heads\n",
    "        device = A.device\n",
    "\n",
    "        valid = ~key_padding_mask# (B,L0)\n",
    "        valid2d = valid.unsqueeze(2) & valid.unsqueeze(1) # (B,L0,L0)\n",
    "\n",
    "        # geometry\n",
    "        if self.use_geo and (pos is not None):\n",
    "            pad_pos, _ = to_dense_batch(pos, batch) # (B,L0,3)\n",
    "            diff = pad_pos.unsqueeze(2) - pad_pos.unsqueeze(1) # (B,L0,L0,3)\n",
    "            dist = torch.sqrt(torch.clamp((diff**2).sum(-1), min=0.0)) # (B,L0,L0)\n",
    "            centers = self.centers.to(dist.device)\n",
    "            rbf = torch.exp(-self.rbf_beta * (dist.unsqueeze(-1) - centers)**2)\n",
    "            geo = self.geo_mlp(rbf).permute(0, 3, 1, 2).contiguous() # (B,H,L0,L0)\n",
    "        else:\n",
    "            geo = torch.zeros((B, H, L0, L0), device=device)\n",
    "\n",
    "        # adjacency constant per head\n",
    "        if self.use_adj_const:\n",
    "            adj = A.unsqueeze(1) * self.adj_bias.view(1, H, 1, 1) # (B,H,L0,L0)\n",
    "        else:\n",
    "            adj = torch.zeros_like(geo)\n",
    "\n",
    "        # SPD\n",
    "        if self.use_spd and (hops is not None):\n",
    "            spd = self._spd_bias(hops, valid2d) # (B,H,L0,L0)\n",
    "        else:\n",
    "            spd = torch.zeros_like(geo)\n",
    "\n",
    "        # edge categorical\n",
    "        if self.use_edge_bias and (edge_attr is not None):\n",
    "            edg = self._edge_bias(edge_index, edge_attr, batch, L0, ptr) # (B,H,L0,L0)\n",
    "        else:\n",
    "            edg = torch.zeros_like(geo)\n",
    "\n",
    "        # normalize and bound each component, then scale \n",
    "        if self.use_headnorm:\n",
    "            if self.use_geo:\n",
    "                geo = self._apply_ln_heads(geo,  self.ln_geo)\n",
    "            if self.use_spd:  \n",
    "                spd = self._apply_ln_heads(spd,  self.ln_spd)\n",
    "            if self.use_edge_bias: \n",
    "                edg = self._apply_ln_heads(edg, self.ln_edge)\n",
    "\n",
    "        # gently bound to keep attention stable\n",
    "        if self.use_geo:       \n",
    "            geo = self._bound(geo)\n",
    "        if self.use_spd:       \n",
    "            spd = self._bound(spd)\n",
    "        if self.use_edge_bias: \n",
    "            edg = self._bound(edg)\n",
    "        # typically don't bound adj because it’s already a small learned scalar per head\n",
    "\n",
    "        bias = (self.alpha_geo  * geo + self.alpha_spd  * spd + self.alpha_adj  * adj + self.alpha_edge * edg)\n",
    "\n",
    "        # mask PAD rows/cols. keep diagonal 0 for valid tokens\n",
    "        pad = key_padding_mask\n",
    "        big_neg = torch.tensor(-1e4, device=bias.device, dtype=bias.dtype)\n",
    "        bias = bias.masked_fill(pad.view(B, 1, L0, 1), big_neg)\n",
    "        bias = bias.masked_fill(pad.view(B, 1, 1, L0), big_neg)\n",
    "        I = torch.eye(L0, device=device, dtype=torch.bool).view(1, 1, L0, L0)\n",
    "        bias = torch.where(I, bias.new_zeros(()), bias)\n",
    "\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026345f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINEBlock(nn.Module):\n",
    "    def __init__(self, dim, activation=\"silu\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.conv = GINEConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim, dim), act, \n",
    "                nn.Linear(dim, dim)\n",
    "                ))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, 2*dim), act, \n",
    "            nn.Dropout(dropout), \n",
    "            nn.Linear(2*dim, dim)\n",
    "            )\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_emb):\n",
    "        h = self.conv(self.norm1(x), edge_index, edge_emb)\n",
    "        x = x + self.drop1(h)\n",
    "        x = x + self.drop2(self.ffn(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class EdgeEncoderMixed(nn.Module):\n",
    "    def __init__(self, emb_dim: int, cont_dim: int = 32, activation=\"silu\"):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.emb0 = nn.Embedding(5, emb_dim)\n",
    "        self.emb1 = nn.Embedding(6, emb_dim)\n",
    "        self.emb2 = nn.Embedding(2, emb_dim)\n",
    "        self.mlp_cont = nn.Sequential(\n",
    "            nn.Linear(cont_dim, emb_dim), act,\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.LayerNorm(emb_dim),      \n",
    "        )\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        cat  = edge_attr[:, :3].long()\n",
    "        cont = edge_attr[:, 3:].float()\n",
    "        e_cat  = self.emb0(cat[:,0]) + self.emb1(cat[:,1]) + self.emb2(cat[:,2])\n",
    "        e_cont = self.mlp_cont(cont)\n",
    "        return e_cat + 0.5 * e_cont # gentle scale on cont branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTransformerGPS(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 8,\n",
    "        nlayers: int = 6,\n",
    "        dropout: float = 0.2,\n",
    "        drop_path: float = 0.0, # (kept for future work)\n",
    "        activation: str = \"silu\",\n",
    "        rdkit_dim: int = 15,\n",
    "        use_extra_atom_feats: bool = True,\n",
    "        extra_atom_dim: int = 5,\n",
    "        # local GNN (GPS) settings\n",
    "        local_layers: int = 2,\n",
    "        use_mixed_edges: bool = True,\n",
    "        cont_dim: int = 32,\n",
    "        # bias knobs\n",
    "        use_geo_bias: bool = True,\n",
    "        use_spd_bias: bool = True,\n",
    "        spd_max: int = 5,\n",
    "        use_adj_const: bool = True,\n",
    "        use_edge_bias: bool = True,\n",
    "        # readout\n",
    "        use_cls: bool = True,\n",
    "        use_has_xyz: bool = True,\n",
    "        head_hidden: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.use_cls = use_cls\n",
    "        self.use_has_xyz = use_has_xyz\n",
    "        self.use_extra_atom_feats = use_extra_atom_feats\n",
    "        self.bias_builder = AttnBiasFull(\n",
    "            n_heads=nhead,\n",
    "            rbf_k=32,\n",
    "            rbf_beta=5.0,\n",
    "            use_geo_bias=use_geo_bias,          \n",
    "            use_adj_bias=use_adj_const,        \n",
    "            use_spd=use_spd_bias,             \n",
    "            spd_buckets=(spd_max + 1), # was spd_max; +1 gives the \">= spd_max\" bucket\n",
    "            use_edge_bias=use_edge_bias,\n",
    "            edge_cats=(5, 6, 2),\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "\n",
    "        act = _act(activation)\n",
    "\n",
    "        # encoders\n",
    "        self.atom_enc = AtomEncoder(emb_dim=d_model)\n",
    "        if use_extra_atom_feats:\n",
    "            self.extra_proj = nn.Sequential(\n",
    "                nn.Linear(extra_atom_dim, d_model), act, \n",
    "                nn.Linear(d_model, d_model)\n",
    "                )\n",
    "            self.extra_gate = nn.Sequential(\n",
    "                nn.Linear(2*d_model, d_model), act\n",
    "                )\n",
    "\n",
    "        # local GNN stack\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        if use_mixed_edges:\n",
    "            self.edge_enc = EdgeEncoderMixed(d_model, cont_dim=cont_dim, activation=activation)\n",
    "        else:\n",
    "            self.edge_enc = BondEncoder(emb_dim=d_model)\n",
    "        self.local_blocks = nn.ModuleList([GINEBlock(d_model, activation=activation, dropout=dropout) for _ in range(local_layers)])\n",
    "\n",
    "        # transformer stack (PyTorch encoder)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=4*d_model,\n",
    "            dropout=dropout, \n",
    "            activation=activation, \n",
    "            batch_first=True, \n",
    "            norm_first=True\n",
    "            )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers, enable_nested_tensor=False)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        # readout: concat mean + max + (optional) CLS + attention pool\n",
    "        self.gate_pool = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model//2), act, \n",
    "            nn.Linear(d_model//2, 1)\n",
    "            )\n",
    "        # features: mean(d), max(d), attn(d) = 3d, (+cls d) optional, + rdkit, + has_xyz\n",
    "        pooled_dim = 3*d_model + (d_model if use_cls else 0)\n",
    "        head_in = pooled_dim + rdkit_dim + (1 if use_has_xyz else 0)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(head_in),\n",
    "            nn.Linear(head_in, head_hidden), act, \n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden, head_hidden//2), act, \n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden//2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # 1. atom encoding  and optional per-atom extras\n",
    "        x = self.atom_enc(data.x) # (N,D)\n",
    "        if getattr(self, \"use_extra_atom_feats\", False) and hasattr(data, \"extra_atom_feats\"):\n",
    "            xa = self.extra_proj(data.extra_atom_feats.float()) # (N,D)\n",
    "            x  = self.extra_gate(torch.cat([x, xa], dim=1)) # (N,D)\n",
    "\n",
    "        # 2. local GNN over sparse graph\n",
    "        e = self.edge_enc(data.edge_attr)\n",
    "        for blk in self.local_blocks:\n",
    "            x = blk(x, data.edge_index, e) # (N,D)\n",
    "\n",
    "        # 3. pack to dense (no CLS yet)\n",
    "        x_pad, valid = to_dense_batch(x, data.batch) # (B,L0,D)\n",
    "        B, L0, D = x_pad.shape\n",
    "        key_padding = ~valid # (B,L0) True == PAD\n",
    "\n",
    "        # 4, head-wise attention bias on L0 tokens (B,H,L0,L0), pre-CLS\n",
    "        # AttnBiasFull supports SPD, geometry, adjacency, edges\n",
    "        hops = getattr(data, \"hops\", None) # (B,MAX_NODES,MAX_NODES) or None\n",
    "        ptr = getattr(data, \"ptr\", None)\n",
    "        attn_bias = self.bias_builder(\n",
    "            pos=(data.pos if hasattr(data, \"pos\") else None),\n",
    "            edge_index=data.edge_index,\n",
    "            edge_attr=(data.edge_attr if hasattr(data, \"edge_attr\") else None),\n",
    "            batch=data.batch,\n",
    "            key_padding_mask=key_padding, # (B,L0), True=PAD\n",
    "            hops=getattr(data, \"hops\", None),\n",
    "            ptr=ptr\n",
    "        )  # (B,H,L0,L0)                                              \n",
    "\n",
    "        # 5. finalize bias (mask PAD rows/cols, keep diagonal 0), then optionally append CLS\n",
    "        B, H, L = attn_bias.shape[0], attn_bias.shape[1], attn_bias.shape[-1]\n",
    "        pad = key_padding # (B,L)\n",
    "        huge = attn_bias.new_tensor(-1e4)\n",
    "\n",
    "        # rows FROM PAD, cols TO PAD\n",
    "        attn_bias = attn_bias.masked_fill(pad.view(B, 1, L, 1), huge)\n",
    "        attn_bias = attn_bias.masked_fill(pad.view(B, 1, 1, L), huge)\n",
    "\n",
    "        # keep diagonal = 0 on valid tokens\n",
    "        I = torch.eye(L, device=attn_bias.device, dtype=torch.bool).view(1, 1, L, L)\n",
    "        attn_bias = torch.where(I, attn_bias.new_zeros(()), attn_bias)\n",
    "\n",
    "        # (optional) append CLS token at the end\n",
    "        if getattr(self, \"use_cls\", False):\n",
    "            # append CLS embedding\n",
    "            cls = self.cls_token.expand(B, 1, D)# (B,1,D)\n",
    "            x_pad = torch.cat([x_pad, cls], dim=1) # (B,L+1,D)\n",
    "\n",
    "            # extend key_padding: CLS is always valid (False)\n",
    "            key_padding = torch.cat(\n",
    "                [key_padding, torch.zeros(B, 1, dtype=torch.bool, device=x_pad.device)],\n",
    "                dim=1\n",
    "            ) # (B,L+1)\n",
    "\n",
    "            # pad bias by one row/col with zeros for CLS -> (B,H,L+1,L+1)\n",
    "            attn_bias = F.pad(attn_bias, (0, 1, 0, 1), value=0.0)\n",
    "            L = L + 1\n",
    "\n",
    "        # 6. transformer encoder with 3D additive mask (B*H,L,L)\n",
    "        attn_mask_3d = attn_bias.reshape(B * H, L, L).to(x_pad.dtype)\n",
    "        h = self.encoder( # returns (B,L,D) when batch_first=True\n",
    "            x_pad,\n",
    "            mask=attn_mask_3d, # additive float mask \n",
    "        )\n",
    "\n",
    "        # 7. pooling (mean + max + gated attention), plus optional CLS and then RDKit/has_xyz and head \n",
    "        # exclude CLS from token pools\n",
    "        h_tok = h[:, :L0, :] # (B,L0,D)\n",
    "        mask_f = valid.float()# (B,L0)\n",
    "\n",
    "        mean = (h_tok * mask_f.unsqueeze(-1)).sum(1) / (mask_f.sum(1, keepdim=True) + 1e-8)  # (B,D)\n",
    "        mmax, _ = (h_tok + (1.0 - mask_f.unsqueeze(-1)) * (-1e4)).max(dim=1) # (B,D)\n",
    "\n",
    "        gate_logits = self.gate_pool(h_tok).squeeze(-1)# (B,L0)\n",
    "        gate = torch.softmax(gate_logits.masked_fill(~valid, -1e4), dim=1)\n",
    "        attn_pool = (h_tok * gate.unsqueeze(-1)).sum(1) # (B,D)\n",
    "\n",
    "        parts = [mean, mmax, attn_pool]\n",
    "\n",
    "        if getattr(self, \"use_cls\", False):\n",
    "            parts.append(h[:, L-1, :]) # CLS vector (B,D)\n",
    "\n",
    "        # RDKit globals\n",
    "        rd = data.rdkit_feats.view(B, -1).float() # (B, rdkit_dim)\n",
    "        parts.append(rd)\n",
    "\n",
    "        # optional has_xyz scalar if present\n",
    "        if getattr(self, \"use_has_xyz\", False) and hasattr(data, \"has_xyz\"):\n",
    "            parts.append(data.has_xyz.view(B, 1).float())\n",
    "\n",
    "        out = torch.cat(parts, dim=1)\n",
    "        return self.head(out) # (B,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9f3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: n=10 tokens=960 sumL2=92160 p50=96 p90=96 p95=96 p99=96 max=96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mem:after_epoch_1] allocated=105.3MB reserved=174.0MB\n",
      "Epoch 001 | tr_MSE 22771.91854 | val_MAE 100.23416 | val_RMSE 130.70927 | R2 -0.7897\n",
      "[mem:after_epoch_2] allocated=127.2MB reserved=168.0MB\n",
      "Epoch 002 | tr_MSE 22484.83189 | val_MAE 98.10220 | val_RMSE 128.20306 | R2 -0.7217\n",
      "[mem:after_epoch_3] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 003 | tr_MSE 20913.60447 | val_MAE 90.39704 | val_RMSE 118.47663 | R2 -0.4704\n",
      "[mem:after_epoch_4] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 004 | tr_MSE 16876.24435 | val_MAE 79.91265 | val_RMSE 100.89502 | R2 -0.0664\n",
      "[mem:after_epoch_5] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 005 | tr_MSE 13248.30115 | val_MAE 83.10323 | val_RMSE 99.83572 | R2 -0.0441\n",
      "[mem:after_epoch_6] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 006 | tr_MSE 13417.76134 | val_MAE 87.05917 | val_RMSE 103.53501 | R2 -0.1229\n",
      "[mem:after_epoch_7] allocated=126.8MB reserved=188.0MB\n",
      "Epoch 007 | tr_MSE 13460.40146 | val_MAE 68.70322 | val_RMSE 86.08450 | R2 0.2237\n",
      "[mem:after_epoch_8] allocated=127.2MB reserved=172.0MB\n",
      "Epoch 008 | tr_MSE 8915.72664 | val_MAE 88.98397 | val_RMSE 107.74438 | R2 -0.2161\n",
      "[mem:after_epoch_9] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 009 | tr_MSE 16010.20029 | val_MAE 64.36108 | val_RMSE 80.43851 | R2 0.3222\n",
      "[mem:after_epoch_10] allocated=126.7MB reserved=192.0MB\n",
      "Epoch 010 | tr_MSE 7824.08335 | val_MAE 65.97728 | val_RMSE 82.33582 | R2 0.2899\n",
      "[mem:after_epoch_11] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 011 | tr_MSE 7638.27585 | val_MAE 61.92658 | val_RMSE 81.54565 | R2 0.3034\n",
      "[mem:after_epoch_12] allocated=127.2MB reserved=172.0MB\n",
      "Epoch 012 | tr_MSE 6648.06962 | val_MAE 58.60330 | val_RMSE 75.83574 | R2 0.3976\n",
      "[mem:after_epoch_13] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 013 | tr_MSE 7473.91698 | val_MAE 60.84827 | val_RMSE 77.31073 | R2 0.3739\n",
      "[mem:after_epoch_14] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 014 | tr_MSE 8798.39971 | val_MAE 91.98001 | val_RMSE 112.57427 | R2 -0.3275\n",
      "[mem:after_epoch_15] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 015 | tr_MSE 7670.37246 | val_MAE 69.93371 | val_RMSE 85.34698 | R2 0.2370\n",
      "[mem:after_epoch_16] allocated=126.7MB reserved=170.0MB\n",
      "Epoch 016 | tr_MSE 7179.30784 | val_MAE 62.72006 | val_RMSE 79.02509 | R2 0.3458\n",
      "[mem:after_epoch_17] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 017 | tr_MSE 8516.71908 | val_MAE 113.67843 | val_RMSE 158.78485 | R2 -1.6411\n",
      "[mem:after_epoch_18] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 018 | tr_MSE 14257.81709 | val_MAE 63.01053 | val_RMSE 82.82143 | R2 0.2815\n",
      "[mem:after_epoch_19] allocated=126.8MB reserved=190.0MB\n",
      "Epoch 019 | tr_MSE 7835.88200 | val_MAE 60.44922 | val_RMSE 77.79648 | R2 0.3660\n",
      "[mem:after_epoch_20] allocated=126.7MB reserved=170.0MB\n",
      "Epoch 020 | tr_MSE 6756.20242 | val_MAE 60.27582 | val_RMSE 76.14690 | R2 0.3926\n",
      "[mem:after_epoch_21] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 021 | tr_MSE 7486.04330 | val_MAE 58.96669 | val_RMSE 76.67853 | R2 0.3841\n",
      "[mem:after_epoch_22] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 022 | tr_MSE 6488.40270 | val_MAE 55.19073 | val_RMSE 70.98790 | R2 0.4721\n",
      "[mem:after_epoch_23] allocated=127.2MB reserved=172.0MB\n",
      "Epoch 023 | tr_MSE 6600.63061 | val_MAE 63.17649 | val_RMSE 79.04766 | R2 0.3454\n",
      "[mem:after_epoch_24] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 024 | tr_MSE 6205.09468 | val_MAE 56.75624 | val_RMSE 69.72594 | R2 0.4907\n",
      "[mem:after_epoch_25] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 025 | tr_MSE 6739.70627 | val_MAE 54.39731 | val_RMSE 68.31837 | R2 0.5111\n",
      "[mem:after_epoch_26] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 026 | tr_MSE 6000.17637 | val_MAE 56.10762 | val_RMSE 70.75220 | R2 0.4756\n",
      "[mem:after_epoch_27] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 027 | tr_MSE 6298.02858 | val_MAE 57.73874 | val_RMSE 72.94540 | R2 0.4426\n",
      "[mem:after_epoch_28] allocated=126.8MB reserved=194.0MB\n",
      "Epoch 028 | tr_MSE 6834.93989 | val_MAE 62.72995 | val_RMSE 80.27757 | R2 0.3249\n",
      "[mem:after_epoch_29] allocated=126.7MB reserved=168.0MB\n",
      "Epoch 029 | tr_MSE 6065.14791 | val_MAE 54.87435 | val_RMSE 67.18204 | R2 0.5272\n",
      "[mem:after_epoch_30] allocated=126.7MB reserved=182.0MB\n",
      "Epoch 030 | tr_MSE 5947.99341 | val_MAE 53.62183 | val_RMSE 66.27710 | R2 0.5399\n",
      "[mem:after_epoch_31] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 031 | tr_MSE 6300.12901 | val_MAE 54.80992 | val_RMSE 67.25835 | R2 0.5261\n",
      "[mem:after_epoch_32] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 032 | tr_MSE 6651.55088 | val_MAE 59.22444 | val_RMSE 74.83594 | R2 0.4133\n",
      "[mem:after_epoch_33] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 033 | tr_MSE 6672.78731 | val_MAE 55.32677 | val_RMSE 69.26154 | R2 0.4975\n",
      "[mem:after_epoch_34] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 034 | tr_MSE 6352.79233 | val_MAE 53.65816 | val_RMSE 66.48311 | R2 0.5370\n",
      "[mem:after_epoch_35] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 035 | tr_MSE 6281.94317 | val_MAE 60.58529 | val_RMSE 77.74081 | R2 0.3669\n",
      "[mem:after_epoch_36] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 036 | tr_MSE 6481.48425 | val_MAE 53.85339 | val_RMSE 67.12299 | R2 0.5280\n",
      "[mem:after_epoch_37] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 037 | tr_MSE 5788.54904 | val_MAE 53.78403 | val_RMSE 68.05731 | R2 0.5148\n",
      "[mem:after_epoch_38] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 038 | tr_MSE 6403.30943 | val_MAE 56.13891 | val_RMSE 71.79875 | R2 0.4600\n",
      "[mem:after_epoch_39] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 039 | tr_MSE 6472.33420 | val_MAE 61.16004 | val_RMSE 77.21725 | R2 0.3754\n",
      "[mem:after_epoch_40] allocated=127.2MB reserved=184.0MB\n",
      "Epoch 040 | tr_MSE 6524.36059 | val_MAE 55.48143 | val_RMSE 70.15633 | R2 0.4844\n",
      "[mem:after_epoch_41] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 041 | tr_MSE 6062.90690 | val_MAE 55.61686 | val_RMSE 68.27855 | R2 0.5116\n",
      "[mem:after_epoch_42] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 042 | tr_MSE 6489.45530 | val_MAE 64.28886 | val_RMSE 82.91988 | R2 0.2797\n",
      "[mem:after_epoch_43] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 043 | tr_MSE 6427.70332 | val_MAE 53.32174 | val_RMSE 67.19669 | R2 0.5270\n",
      "[mem:after_epoch_44] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 044 | tr_MSE 5740.50381 | val_MAE 52.09323 | val_RMSE 64.52492 | R2 0.5639\n",
      "[mem:after_epoch_45] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 045 | tr_MSE 6177.35539 | val_MAE 51.78215 | val_RMSE 64.18133 | R2 0.5685\n",
      "[mem:after_epoch_46] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 046 | tr_MSE 6095.95400 | val_MAE 55.05943 | val_RMSE 71.19157 | R2 0.4691\n",
      "[mem:after_epoch_47] allocated=126.7MB reserved=192.0MB\n",
      "Epoch 047 | tr_MSE 6215.33681 | val_MAE 58.67226 | val_RMSE 75.19431 | R2 0.4077\n",
      "[mem:after_epoch_48] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 048 | tr_MSE 6071.40941 | val_MAE 56.58455 | val_RMSE 71.74236 | R2 0.4608\n",
      "[mem:after_epoch_49] allocated=126.7MB reserved=170.0MB\n",
      "Epoch 049 | tr_MSE 6367.33523 | val_MAE 51.48219 | val_RMSE 65.56237 | R2 0.5497\n",
      "[mem:after_epoch_50] allocated=127.2MB reserved=184.0MB\n",
      "Epoch 050 | tr_MSE 5485.48347 | val_MAE 54.09978 | val_RMSE 69.37041 | R2 0.4959\n",
      "[mem:after_epoch_51] allocated=127.4MB reserved=170.0MB\n",
      "Epoch 051 | tr_MSE 6088.58714 | val_MAE 50.22115 | val_RMSE 65.09954 | R2 0.5561\n",
      "[mem:after_epoch_52] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 052 | tr_MSE 5760.77739 | val_MAE 53.14807 | val_RMSE 67.43358 | R2 0.5237\n",
      "[mem:after_epoch_53] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 053 | tr_MSE 6081.78213 | val_MAE 50.99339 | val_RMSE 63.12603 | R2 0.5826\n",
      "[mem:after_epoch_54] allocated=126.8MB reserved=188.0MB\n",
      "Epoch 054 | tr_MSE 5645.23018 | val_MAE 52.25891 | val_RMSE 65.43826 | R2 0.5514\n",
      "[mem:after_epoch_55] allocated=126.7MB reserved=172.0MB\n",
      "Epoch 055 | tr_MSE 6074.00720 | val_MAE 54.87832 | val_RMSE 70.36749 | R2 0.4813\n",
      "[mem:after_epoch_56] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 056 | tr_MSE 5622.22883 | val_MAE 52.54794 | val_RMSE 65.93928 | R2 0.5445\n",
      "[mem:after_epoch_57] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 057 | tr_MSE 5569.98960 | val_MAE 54.73158 | val_RMSE 68.50166 | R2 0.5084\n",
      "[mem:after_epoch_58] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 058 | tr_MSE 5691.29806 | val_MAE 50.88476 | val_RMSE 63.22713 | R2 0.5812\n",
      "[mem:after_epoch_59] allocated=126.7MB reserved=172.0MB\n",
      "Epoch 059 | tr_MSE 5880.36475 | val_MAE 61.92669 | val_RMSE 76.41689 | R2 0.3883\n",
      "[mem:after_epoch_60] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 060 | tr_MSE 6128.17246 | val_MAE 59.07334 | val_RMSE 73.33392 | R2 0.4366\n",
      "[mem:after_epoch_61] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 061 | tr_MSE 6840.04058 | val_MAE 54.43192 | val_RMSE 70.15871 | R2 0.4844\n",
      "[mem:after_epoch_62] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 062 | tr_MSE 6245.75868 | val_MAE 49.55701 | val_RMSE 61.66183 | R2 0.6017\n",
      "[mem:after_epoch_63] allocated=127.2MB reserved=170.0MB\n",
      "Epoch 063 | tr_MSE 5602.91287 | val_MAE 52.93226 | val_RMSE 65.98058 | R2 0.5440\n",
      "[mem:after_epoch_64] allocated=127.2MB reserved=184.0MB\n",
      "Epoch 064 | tr_MSE 5355.33206 | val_MAE 55.80486 | val_RMSE 69.13939 | R2 0.4992\n",
      "[mem:after_epoch_65] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 065 | tr_MSE 6096.12360 | val_MAE 67.88051 | val_RMSE 90.68343 | R2 0.1386\n",
      "[mem:after_epoch_66] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 066 | tr_MSE 6312.98278 | val_MAE 58.77848 | val_RMSE 74.00340 | R2 0.4263\n",
      "[mem:after_epoch_67] allocated=127.4MB reserved=170.0MB\n",
      "Epoch 067 | tr_MSE 6910.71646 | val_MAE 52.04628 | val_RMSE 64.97708 | R2 0.5577\n",
      "[mem:after_epoch_68] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 068 | tr_MSE 6509.33662 | val_MAE 50.72480 | val_RMSE 63.37423 | R2 0.5793\n",
      "[mem:after_epoch_69] allocated=127.4MB reserved=170.0MB\n",
      "Epoch 069 | tr_MSE 6148.65636 | val_MAE 56.31845 | val_RMSE 76.69027 | R2 0.3839\n",
      "[mem:after_epoch_70] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 070 | tr_MSE 6587.12439 | val_MAE 51.57281 | val_RMSE 63.52890 | R2 0.5772\n",
      "[mem:after_epoch_71] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 071 | tr_MSE 5908.51846 | val_MAE 49.30116 | val_RMSE 62.00787 | R2 0.5972\n",
      "[mem:after_epoch_72] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 072 | tr_MSE 5723.69598 | val_MAE 68.82731 | val_RMSE 87.29597 | R2 0.2017\n",
      "[mem:after_epoch_73] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 073 | tr_MSE 6050.91602 | val_MAE 54.54102 | val_RMSE 68.33519 | R2 0.5108\n",
      "[mem:after_epoch_74] allocated=126.8MB reserved=194.0MB\n",
      "Epoch 074 | tr_MSE 6003.27848 | val_MAE 52.48363 | val_RMSE 64.19456 | R2 0.5683\n",
      "[mem:after_epoch_75] allocated=126.7MB reserved=172.0MB\n",
      "Epoch 075 | tr_MSE 6050.70908 | val_MAE 56.81645 | val_RMSE 71.93134 | R2 0.4580\n",
      "[mem:after_epoch_76] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 076 | tr_MSE 7286.10129 | val_MAE 62.59438 | val_RMSE 76.94657 | R2 0.3798\n",
      "[mem:after_epoch_77] allocated=126.7MB reserved=194.0MB\n",
      "Epoch 077 | tr_MSE 6315.18543 | val_MAE 56.61007 | val_RMSE 70.44074 | R2 0.4802\n",
      "[mem:after_epoch_78] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 078 | tr_MSE 5905.79982 | val_MAE 55.28681 | val_RMSE 70.06776 | R2 0.4857\n",
      "[mem:after_epoch_79] allocated=126.7MB reserved=176.0MB\n",
      "Epoch 079 | tr_MSE 5946.51765 | val_MAE 53.05952 | val_RMSE 66.84924 | R2 0.5319\n",
      "[mem:after_epoch_80] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 080 | tr_MSE 5719.79231 | val_MAE 60.82135 | val_RMSE 80.89847 | R2 0.3144\n",
      "[mem:after_epoch_81] allocated=126.7MB reserved=194.0MB\n",
      "Epoch 081 | tr_MSE 5858.94898 | val_MAE 62.40768 | val_RMSE 82.67085 | R2 0.2841\n",
      "[mem:after_epoch_82] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 082 | tr_MSE 7210.03894 | val_MAE 54.26023 | val_RMSE 67.66398 | R2 0.5204\n",
      "[mem:after_epoch_83] allocated=126.7MB reserved=174.0MB\n",
      "Epoch 083 | tr_MSE 6519.81468 | val_MAE 48.93810 | val_RMSE 60.40992 | R2 0.6177\n",
      "[mem:after_epoch_84] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 084 | tr_MSE 5357.56715 | val_MAE 50.64238 | val_RMSE 63.10115 | R2 0.5829\n",
      "[mem:after_epoch_85] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 085 | tr_MSE 5284.50780 | val_MAE 55.05289 | val_RMSE 69.01536 | R2 0.5010\n",
      "[mem:after_epoch_86] allocated=127.2MB reserved=182.0MB\n",
      "Epoch 086 | tr_MSE 6062.02074 | val_MAE 49.73214 | val_RMSE 62.11857 | R2 0.5958\n",
      "[mem:after_epoch_87] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 087 | tr_MSE 5433.82080 | val_MAE 57.33671 | val_RMSE 72.63416 | R2 0.4473\n",
      "[mem:after_epoch_88] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 088 | tr_MSE 5337.98971 | val_MAE 54.10834 | val_RMSE 67.29674 | R2 0.5256\n",
      "[mem:after_epoch_89] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 089 | tr_MSE 5724.26975 | val_MAE 53.07697 | val_RMSE 63.78484 | R2 0.5738\n",
      "[mem:after_epoch_90] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 090 | tr_MSE 5404.13568 | val_MAE 54.76381 | val_RMSE 66.07800 | R2 0.5426\n",
      "[mem:after_epoch_91] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 091 | tr_MSE 5024.78315 | val_MAE 53.71951 | val_RMSE 64.76582 | R2 0.5606\n",
      "[mem:after_epoch_92] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 092 | tr_MSE 5363.88560 | val_MAE 53.81314 | val_RMSE 67.11422 | R2 0.5282\n",
      "[mem:after_epoch_93] allocated=127.4MB reserved=170.0MB\n",
      "Epoch 093 | tr_MSE 5156.64745 | val_MAE 53.81431 | val_RMSE 67.07864 | R2 0.5287\n",
      "[mem:after_epoch_94] allocated=127.2MB reserved=184.0MB\n",
      "Epoch 094 | tr_MSE 5265.42328 | val_MAE 56.83682 | val_RMSE 72.25431 | R2 0.4531\n",
      "[mem:after_epoch_95] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 095 | tr_MSE 5156.85265 | val_MAE 56.05285 | val_RMSE 71.35857 | R2 0.4666\n",
      "[mem:after_epoch_96] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 096 | tr_MSE 6109.24435 | val_MAE 52.90182 | val_RMSE 66.76656 | R2 0.5330\n",
      "[mem:after_epoch_97] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 097 | tr_MSE 4906.01659 | val_MAE 53.83730 | val_RMSE 69.37329 | R2 0.4959\n",
      "[mem:after_epoch_98] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 098 | tr_MSE 5063.52942 | val_MAE 51.76871 | val_RMSE 65.61436 | R2 0.5490\n",
      "[mem:after_epoch_99] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 099 | tr_MSE 4672.98016 | val_MAE 52.49741 | val_RMSE 65.92330 | R2 0.5448\n",
      "[mem:after_epoch_100] allocated=127.2MB reserved=190.0MB\n",
      "Epoch 100 | tr_MSE 4740.34588 | val_MAE 52.27885 | val_RMSE 65.03804 | R2 0.5569\n",
      "[mem:after_epoch_101] allocated=127.4MB reserved=174.0MB\n",
      "Epoch 101 | tr_MSE 4565.38564 | val_MAE 58.75188 | val_RMSE 74.54874 | R2 0.4178\n",
      "[mem:after_epoch_102] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 102 | tr_MSE 5747.50743 | val_MAE 52.09703 | val_RMSE 64.84029 | R2 0.5596\n",
      "[mem:after_epoch_103] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 103 | tr_MSE 4426.71316 | val_MAE 56.30799 | val_RMSE 72.22916 | R2 0.4535\n",
      "Early stopping.\n",
      "[graphtransformer_tg_spd] Best Val — MAE 48.938103 | RMSE 60.409924 | R2 0.6177\n",
      "[mem:after_Tg] allocated=16.2MB reserved=40.0MB\n",
      "L: n=520 tokens=3840 sumL2=29700 p50=8 p90=9 p95=9 p99=9 max=9\n",
      "[mem:after_epoch_1] allocated=102.7MB reserved=166.0MB\n",
      "Epoch 001 | tr_MSE 0.72563 | val_MAE 0.20168 | val_RMSE 0.24377 | R2 -2.3840\n",
      "[mem:after_epoch_2] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 002 | tr_MSE 0.03846 | val_MAE 0.10437 | val_RMSE 0.13605 | R2 -0.0541\n",
      "[mem:after_epoch_3] allocated=124.8MB reserved=174.0MB\n",
      "Epoch 003 | tr_MSE 0.03523 | val_MAE 0.11825 | val_RMSE 0.15236 | R2 -0.3219\n",
      "[mem:after_epoch_4] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 004 | tr_MSE 0.02897 | val_MAE 0.08230 | val_RMSE 0.11173 | R2 0.2891\n",
      "[mem:after_epoch_5] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 005 | tr_MSE 0.01720 | val_MAE 0.07077 | val_RMSE 0.10224 | R2 0.4048\n",
      "[mem:after_epoch_6] allocated=124.8MB reserved=186.0MB\n",
      "Epoch 006 | tr_MSE 0.01801 | val_MAE 0.09036 | val_RMSE 0.12712 | R2 0.0798\n",
      "[mem:after_epoch_7] allocated=124.8MB reserved=174.0MB\n",
      "Epoch 007 | tr_MSE 0.02643 | val_MAE 0.07328 | val_RMSE 0.10018 | R2 0.4285\n",
      "[mem:after_epoch_8] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 008 | tr_MSE 0.01422 | val_MAE 0.09727 | val_RMSE 0.13168 | R2 0.0126\n",
      "[mem:after_epoch_9] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 009 | tr_MSE 0.01953 | val_MAE 0.17922 | val_RMSE 0.21258 | R2 -1.5734\n",
      "[mem:after_epoch_10] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 010 | tr_MSE 0.02964 | val_MAE 0.10737 | val_RMSE 0.16163 | R2 -0.4877\n",
      "[mem:after_epoch_11] allocated=124.8MB reserved=178.0MB\n",
      "Epoch 011 | tr_MSE 0.02714 | val_MAE 0.09891 | val_RMSE 0.13586 | R2 -0.0511\n",
      "[mem:after_epoch_12] allocated=124.8MB reserved=184.0MB\n",
      "Epoch 012 | tr_MSE 0.01254 | val_MAE 0.08715 | val_RMSE 0.10869 | R2 0.3272\n",
      "[mem:after_epoch_13] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 013 | tr_MSE 0.01125 | val_MAE 0.13189 | val_RMSE 0.15879 | R2 -0.4359\n",
      "[mem:after_epoch_14] allocated=124.8MB reserved=178.0MB\n",
      "Epoch 014 | tr_MSE 0.01649 | val_MAE 0.14265 | val_RMSE 0.17898 | R2 -0.8242\n",
      "[mem:after_epoch_15] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 015 | tr_MSE 0.01716 | val_MAE 0.10020 | val_RMSE 0.14020 | R2 -0.1194\n",
      "[mem:after_epoch_16] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 016 | tr_MSE 0.01648 | val_MAE 0.07142 | val_RMSE 0.10079 | R2 0.4215\n",
      "[mem:after_epoch_17] allocated=124.8MB reserved=178.0MB\n",
      "Epoch 017 | tr_MSE 0.00765 | val_MAE 0.06051 | val_RMSE 0.09249 | R2 0.5128\n",
      "[mem:after_epoch_18] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 018 | tr_MSE 0.00836 | val_MAE 0.08501 | val_RMSE 0.13277 | R2 -0.0039\n",
      "[mem:after_epoch_19] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 019 | tr_MSE 0.00821 | val_MAE 0.06235 | val_RMSE 0.10887 | R2 0.3251\n",
      "[mem:after_epoch_20] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 020 | tr_MSE 0.01126 | val_MAE 0.06454 | val_RMSE 0.10340 | R2 0.3912\n",
      "[mem:after_epoch_21] allocated=124.2MB reserved=178.0MB\n",
      "Epoch 021 | tr_MSE 0.00764 | val_MAE 0.04642 | val_RMSE 0.08474 | R2 0.5911\n",
      "[mem:after_epoch_22] allocated=124.8MB reserved=178.0MB\n",
      "Epoch 022 | tr_MSE 0.00586 | val_MAE 0.04756 | val_RMSE 0.08552 | R2 0.5835\n",
      "[mem:after_epoch_23] allocated=124.8MB reserved=178.0MB\n",
      "Epoch 023 | tr_MSE 0.00550 | val_MAE 0.04244 | val_RMSE 0.08162 | R2 0.6207\n",
      "[mem:after_epoch_24] allocated=124.2MB reserved=178.0MB\n",
      "Epoch 024 | tr_MSE 0.00506 | val_MAE 0.04164 | val_RMSE 0.08011 | R2 0.6345\n",
      "[mem:after_epoch_25] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 025 | tr_MSE 0.00474 | val_MAE 0.04223 | val_RMSE 0.08209 | R2 0.6162\n",
      "[mem:after_epoch_26] allocated=124.8MB reserved=184.0MB\n",
      "Epoch 026 | tr_MSE 0.00431 | val_MAE 0.04330 | val_RMSE 0.08524 | R2 0.5862\n",
      "[mem:after_epoch_27] allocated=124.8MB reserved=174.0MB\n",
      "Epoch 027 | tr_MSE 0.00469 | val_MAE 0.04297 | val_RMSE 0.08183 | R2 0.6187\n",
      "[mem:after_epoch_28] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 028 | tr_MSE 0.00547 | val_MAE 0.05985 | val_RMSE 0.09212 | R2 0.5168\n",
      "[mem:after_epoch_29] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 029 | tr_MSE 0.00622 | val_MAE 0.08228 | val_RMSE 0.11341 | R2 0.2676\n",
      "[mem:after_epoch_30] allocated=124.8MB reserved=184.0MB\n",
      "Epoch 030 | tr_MSE 0.00790 | val_MAE 0.08683 | val_RMSE 0.11969 | R2 0.1842\n",
      "[mem:after_epoch_31] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 031 | tr_MSE 0.01158 | val_MAE 0.05929 | val_RMSE 0.10271 | R2 0.3993\n",
      "[mem:after_epoch_32] allocated=124.8MB reserved=186.0MB\n",
      "Epoch 032 | tr_MSE 0.00626 | val_MAE 0.04227 | val_RMSE 0.08391 | R2 0.5990\n",
      "[mem:after_epoch_33] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 033 | tr_MSE 0.00572 | val_MAE 0.04599 | val_RMSE 0.08507 | R2 0.5879\n",
      "[mem:after_epoch_34] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 034 | tr_MSE 0.00528 | val_MAE 0.05814 | val_RMSE 0.10249 | R2 0.4018\n",
      "[mem:after_epoch_35] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 035 | tr_MSE 0.00539 | val_MAE 0.06529 | val_RMSE 0.09904 | R2 0.4414\n",
      "[mem:after_epoch_36] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 036 | tr_MSE 0.00626 | val_MAE 0.04832 | val_RMSE 0.08993 | R2 0.5395\n",
      "[mem:after_epoch_37] allocated=124.8MB reserved=176.0MB\n",
      "Epoch 037 | tr_MSE 0.00462 | val_MAE 0.04196 | val_RMSE 0.08404 | R2 0.5978\n",
      "[mem:after_epoch_38] allocated=124.8MB reserved=184.0MB\n",
      "Epoch 038 | tr_MSE 0.00414 | val_MAE 0.04226 | val_RMSE 0.08611 | R2 0.5778\n",
      "[mem:after_epoch_39] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 039 | tr_MSE 0.00439 | val_MAE 0.06255 | val_RMSE 0.11015 | R2 0.3091\n",
      "[mem:after_epoch_40] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 040 | tr_MSE 0.00503 | val_MAE 0.09145 | val_RMSE 0.11897 | R2 0.1940\n",
      "[mem:after_epoch_41] allocated=124.8MB reserved=184.0MB\n",
      "Epoch 041 | tr_MSE 0.00813 | val_MAE 0.07571 | val_RMSE 0.11266 | R2 0.2772\n",
      "[mem:after_epoch_42] allocated=124.8MB reserved=182.0MB\n",
      "Epoch 042 | tr_MSE 0.00675 | val_MAE 0.06334 | val_RMSE 0.09690 | R2 0.4653\n",
      "[mem:after_epoch_43] allocated=124.8MB reserved=180.0MB\n",
      "Epoch 043 | tr_MSE 0.00642 | val_MAE 0.04492 | val_RMSE 0.10078 | R2 0.4216\n",
      "[mem:after_epoch_44] allocated=124.8MB reserved=186.0MB\n",
      "Epoch 044 | tr_MSE 0.00513 | val_MAE 0.04982 | val_RMSE 0.10236 | R2 0.4033\n",
      "Early stopping.\n",
      "[graphtransformer_den_spd] Best Val — MAE 0.041639 | RMSE 0.080110 | R2 0.6345\n",
      "[mem:after_Density] allocated=16.2MB reserved=40.0MB\n",
      "L: n=510 tokens=3830 sumL2=29950 p50=8 p90=9 p95=9 p99=9 max=9\n",
      "[mem:after_epoch_1] allocated=103.1MB reserved=178.0MB\n",
      "Epoch 001 | tr_MSE 285.31449 | val_MAE 15.97740 | val_RMSE 16.66257 | R2 -11.6150\n",
      "[mem:after_epoch_2] allocated=125.0MB reserved=370.0MB\n",
      "Epoch 002 | tr_MSE 236.37985 | val_MAE 11.06715 | val_RMSE 12.18165 | R2 -5.7424\n",
      "[mem:after_epoch_3] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 003 | tr_MSE 72.18914 | val_MAE 5.95087 | val_RMSE 6.90924 | R2 -1.1690\n",
      "[mem:after_epoch_4] allocated=124.7MB reserved=328.0MB\n",
      "Epoch 004 | tr_MSE 65.86757 | val_MAE 4.23129 | val_RMSE 4.84845 | R2 -0.0681\n",
      "[mem:after_epoch_5] allocated=124.6MB reserved=324.0MB\n",
      "Epoch 005 | tr_MSE 25.25410 | val_MAE 4.30095 | val_RMSE 4.96800 | R2 -0.1214\n",
      "[mem:after_epoch_6] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 006 | tr_MSE 30.38996 | val_MAE 4.34267 | val_RMSE 5.07326 | R2 -0.1694\n",
      "[mem:after_epoch_7] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 007 | tr_MSE 34.92482 | val_MAE 3.85002 | val_RMSE 4.53203 | R2 0.0668\n",
      "[mem:after_epoch_8] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 008 | tr_MSE 35.58306 | val_MAE 3.58967 | val_RMSE 4.58139 | R2 0.0463\n",
      "[mem:after_epoch_9] allocated=124.7MB reserved=326.0MB\n",
      "Epoch 009 | tr_MSE 25.87293 | val_MAE 3.70090 | val_RMSE 4.38321 | R2 0.1271\n",
      "[mem:after_epoch_10] allocated=124.8MB reserved=328.0MB\n",
      "Epoch 010 | tr_MSE 40.45455 | val_MAE 3.43244 | val_RMSE 4.27807 | R2 0.1684\n",
      "[mem:after_epoch_11] allocated=124.6MB reserved=328.0MB\n",
      "Epoch 011 | tr_MSE 30.92795 | val_MAE 3.41690 | val_RMSE 4.38602 | R2 0.1259\n",
      "[mem:after_epoch_12] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 012 | tr_MSE 23.76019 | val_MAE 2.91243 | val_RMSE 3.75986 | R2 0.3577\n",
      "[mem:after_epoch_13] allocated=124.7MB reserved=324.0MB\n",
      "Epoch 013 | tr_MSE 28.02171 | val_MAE 3.52588 | val_RMSE 4.28577 | R2 0.1654\n",
      "[mem:after_epoch_14] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 014 | tr_MSE 19.92670 | val_MAE 3.55043 | val_RMSE 4.24436 | R2 0.1815\n",
      "[mem:after_epoch_15] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 015 | tr_MSE 16.23270 | val_MAE 2.71706 | val_RMSE 3.64759 | R2 0.3955\n",
      "[mem:after_epoch_16] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 016 | tr_MSE 14.42028 | val_MAE 2.31833 | val_RMSE 3.10539 | R2 0.5618\n",
      "[mem:after_epoch_17] allocated=125.1MB reserved=176.0MB\n",
      "Epoch 017 | tr_MSE 13.92566 | val_MAE 3.02981 | val_RMSE 3.79084 | R2 0.3471\n",
      "[mem:after_epoch_18] allocated=125.1MB reserved=182.0MB\n",
      "Epoch 018 | tr_MSE 16.18638 | val_MAE 4.53030 | val_RMSE 5.15606 | R2 -0.2079\n",
      "[mem:after_epoch_19] allocated=125.1MB reserved=182.0MB\n",
      "Epoch 019 | tr_MSE 17.37227 | val_MAE 3.33570 | val_RMSE 4.00711 | R2 0.2704\n",
      "[mem:after_epoch_20] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 020 | tr_MSE 15.12266 | val_MAE 3.70100 | val_RMSE 4.39289 | R2 0.1232\n",
      "[mem:after_epoch_21] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 021 | tr_MSE 13.57800 | val_MAE 2.61871 | val_RMSE 3.34924 | R2 0.4903\n",
      "[mem:after_epoch_22] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 022 | tr_MSE 12.26633 | val_MAE 2.52406 | val_RMSE 3.41043 | R2 0.4715\n",
      "[mem:after_epoch_23] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 023 | tr_MSE 12.39334 | val_MAE 2.64593 | val_RMSE 3.44337 | R2 0.4613\n",
      "[mem:after_epoch_24] allocated=125.1MB reserved=178.0MB\n",
      "Epoch 024 | tr_MSE 11.92864 | val_MAE 3.07165 | val_RMSE 3.65075 | R2 0.3944\n",
      "[mem:after_epoch_25] allocated=125.1MB reserved=182.0MB\n",
      "Epoch 025 | tr_MSE 13.00010 | val_MAE 2.21729 | val_RMSE 3.04942 | R2 0.5775\n",
      "[mem:after_epoch_26] allocated=124.7MB reserved=326.0MB\n",
      "Epoch 026 | tr_MSE 13.15396 | val_MAE 2.96944 | val_RMSE 3.63179 | R2 0.4007\n",
      "[mem:after_epoch_27] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 027 | tr_MSE 13.50388 | val_MAE 2.01606 | val_RMSE 2.89673 | R2 0.6187\n",
      "[mem:after_epoch_28] allocated=124.6MB reserved=328.0MB\n",
      "Epoch 028 | tr_MSE 10.94045 | val_MAE 2.26363 | val_RMSE 2.99113 | R2 0.5935\n",
      "[mem:after_epoch_29] allocated=124.6MB reserved=324.0MB\n",
      "Epoch 029 | tr_MSE 12.60362 | val_MAE 3.08363 | val_RMSE 3.68353 | R2 0.3835\n",
      "[mem:after_epoch_30] allocated=124.6MB reserved=326.0MB\n",
      "Epoch 030 | tr_MSE 12.17081 | val_MAE 2.26716 | val_RMSE 2.93893 | R2 0.6076\n",
      "[mem:after_epoch_31] allocated=124.6MB reserved=326.0MB\n",
      "Epoch 031 | tr_MSE 9.88020 | val_MAE 2.31898 | val_RMSE 3.03574 | R2 0.5813\n",
      "[mem:after_epoch_32] allocated=124.6MB reserved=324.0MB\n",
      "Epoch 032 | tr_MSE 9.93030 | val_MAE 2.72337 | val_RMSE 3.34910 | R2 0.4904\n",
      "[mem:after_epoch_33] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 033 | tr_MSE 9.56141 | val_MAE 2.74588 | val_RMSE 3.61717 | R2 0.4055\n",
      "[mem:after_epoch_34] allocated=124.6MB reserved=324.0MB\n",
      "Epoch 034 | tr_MSE 13.37378 | val_MAE 2.58888 | val_RMSE 3.21143 | R2 0.5314\n",
      "[mem:after_epoch_35] allocated=124.6MB reserved=326.0MB\n",
      "Epoch 035 | tr_MSE 10.56799 | val_MAE 1.92699 | val_RMSE 2.75592 | R2 0.6549\n",
      "[mem:after_epoch_36] allocated=125.1MB reserved=178.0MB\n",
      "Epoch 036 | tr_MSE 8.99895 | val_MAE 2.31490 | val_RMSE 3.06306 | R2 0.5737\n",
      "[mem:after_epoch_37] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 037 | tr_MSE 11.89824 | val_MAE 2.28000 | val_RMSE 3.00899 | R2 0.5886\n",
      "[mem:after_epoch_38] allocated=125.1MB reserved=178.0MB\n",
      "Epoch 038 | tr_MSE 9.13384 | val_MAE 1.89789 | val_RMSE 2.82006 | R2 0.6387\n",
      "[mem:after_epoch_39] allocated=124.7MB reserved=322.0MB\n",
      "Epoch 039 | tr_MSE 9.31905 | val_MAE 2.67832 | val_RMSE 3.35566 | R2 0.4884\n",
      "[mem:after_epoch_40] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 040 | tr_MSE 9.48823 | val_MAE 2.00232 | val_RMSE 2.79239 | R2 0.6457\n",
      "[mem:after_epoch_41] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 041 | tr_MSE 8.30821 | val_MAE 1.99538 | val_RMSE 2.77067 | R2 0.6512\n",
      "[mem:after_epoch_42] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 042 | tr_MSE 8.32069 | val_MAE 1.98975 | val_RMSE 2.77381 | R2 0.6504\n",
      "[mem:after_epoch_43] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 043 | tr_MSE 7.25952 | val_MAE 2.24144 | val_RMSE 2.97504 | R2 0.5978\n",
      "[mem:after_epoch_44] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 044 | tr_MSE 8.36009 | val_MAE 2.37040 | val_RMSE 3.21861 | R2 0.5293\n",
      "[mem:after_epoch_45] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 045 | tr_MSE 7.70654 | val_MAE 2.37027 | val_RMSE 3.23441 | R2 0.5247\n",
      "[mem:after_epoch_46] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 046 | tr_MSE 8.38947 | val_MAE 2.14041 | val_RMSE 2.95075 | R2 0.6044\n",
      "[mem:after_epoch_47] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 047 | tr_MSE 7.49706 | val_MAE 2.21389 | val_RMSE 2.90128 | R2 0.6175\n",
      "[mem:after_epoch_48] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 048 | tr_MSE 8.92704 | val_MAE 2.72071 | val_RMSE 3.38416 | R2 0.4796\n",
      "[mem:after_epoch_49] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 049 | tr_MSE 10.03713 | val_MAE 2.97990 | val_RMSE 3.51207 | R2 0.4396\n",
      "[mem:after_epoch_50] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 050 | tr_MSE 8.16588 | val_MAE 2.05221 | val_RMSE 2.74990 | R2 0.6564\n",
      "[mem:after_epoch_51] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 051 | tr_MSE 6.56780 | val_MAE 2.60262 | val_RMSE 3.31775 | R2 0.4999\n",
      "[mem:after_epoch_52] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 052 | tr_MSE 8.83392 | val_MAE 2.85165 | val_RMSE 3.39031 | R2 0.4777\n",
      "[mem:after_epoch_53] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 053 | tr_MSE 6.83789 | val_MAE 1.96811 | val_RMSE 2.69909 | R2 0.6690\n",
      "[mem:after_epoch_54] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 054 | tr_MSE 6.89643 | val_MAE 2.42956 | val_RMSE 3.06671 | R2 0.5727\n",
      "[mem:after_epoch_55] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 055 | tr_MSE 6.62822 | val_MAE 2.30960 | val_RMSE 2.94805 | R2 0.6051\n",
      "[mem:after_epoch_56] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 056 | tr_MSE 6.64196 | val_MAE 2.93593 | val_RMSE 3.51191 | R2 0.4396\n",
      "[mem:after_epoch_57] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 057 | tr_MSE 6.17679 | val_MAE 2.27192 | val_RMSE 3.00451 | R2 0.5898\n",
      "[mem:after_epoch_58] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 058 | tr_MSE 6.48667 | val_MAE 2.10015 | val_RMSE 2.70663 | R2 0.6671\n",
      "Early stopping.\n",
      "[graphtransformer_rg_spd] Best Val — MAE 1.897893 | RMSE 2.820058 | R2 0.6387\n",
      "[mem:after_Rg] allocated=16.2MB reserved=40.0MB\n",
      "L: n=620 tokens=4700 sumL2=36920 p50=8 p90=9 p95=9 p99=9 max=9\n",
      "[mem:after_epoch_1] allocated=104.3MB reserved=146.0MB\n",
      "Epoch 001 | tr_MSE 0.02573 | val_MAE 0.14625 | val_RMSE 0.17744 | R2 -2.6871\n",
      "[mem:after_epoch_2] allocated=126.4MB reserved=358.0MB\n",
      "Epoch 002 | tr_MSE 0.02556 | val_MAE 0.13729 | val_RMSE 0.16086 | R2 -2.0304\n",
      "[mem:after_epoch_3] allocated=126.3MB reserved=358.0MB\n",
      "Epoch 003 | tr_MSE 0.01302 | val_MAE 0.10568 | val_RMSE 0.12651 | R2 -0.8742\n",
      "[mem:after_epoch_4] allocated=126.2MB reserved=348.0MB\n",
      "Epoch 004 | tr_MSE 0.00899 | val_MAE 0.09055 | val_RMSE 0.11072 | R2 -0.4355\n",
      "[mem:after_epoch_5] allocated=126.4MB reserved=348.0MB\n",
      "Epoch 005 | tr_MSE 0.00746 | val_MAE 0.07285 | val_RMSE 0.09456 | R2 -0.0472\n",
      "[mem:after_epoch_6] allocated=126.2MB reserved=170.0MB\n",
      "Epoch 006 | tr_MSE 0.00592 | val_MAE 0.05138 | val_RMSE 0.07065 | R2 0.4154\n",
      "[mem:after_epoch_7] allocated=126.4MB reserved=358.0MB\n",
      "Epoch 007 | tr_MSE 0.00413 | val_MAE 0.05511 | val_RMSE 0.07865 | R2 0.2756\n",
      "[mem:after_epoch_8] allocated=126.4MB reserved=358.0MB\n",
      "Epoch 008 | tr_MSE 0.00681 | val_MAE 0.03948 | val_RMSE 0.05428 | R2 0.6550\n",
      "[mem:after_epoch_9] allocated=126.2MB reserved=344.0MB\n",
      "Epoch 009 | tr_MSE 0.00571 | val_MAE 0.06218 | val_RMSE 0.07556 | R2 0.3315\n",
      "[mem:after_epoch_10] allocated=126.2MB reserved=170.0MB\n",
      "Epoch 010 | tr_MSE 0.00358 | val_MAE 0.04582 | val_RMSE 0.06385 | R2 0.5225\n",
      "[mem:after_epoch_11] allocated=126.3MB reserved=358.0MB\n",
      "Epoch 011 | tr_MSE 0.00385 | val_MAE 0.04613 | val_RMSE 0.06123 | R2 0.5609\n",
      "[mem:after_epoch_12] allocated=126.2MB reserved=348.0MB\n",
      "Epoch 012 | tr_MSE 0.00368 | val_MAE 0.03663 | val_RMSE 0.04813 | R2 0.7287\n",
      "[mem:after_epoch_13] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 013 | tr_MSE 0.00340 | val_MAE 0.04308 | val_RMSE 0.05723 | R2 0.6164\n",
      "[mem:after_epoch_14] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 014 | tr_MSE 0.00245 | val_MAE 0.03765 | val_RMSE 0.05271 | R2 0.6746\n",
      "[mem:after_epoch_15] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 015 | tr_MSE 0.00227 | val_MAE 0.03671 | val_RMSE 0.04925 | R2 0.7160\n",
      "[mem:after_epoch_16] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 016 | tr_MSE 0.00195 | val_MAE 0.03167 | val_RMSE 0.04527 | R2 0.7601\n",
      "[mem:after_epoch_17] allocated=126.2MB reserved=170.0MB\n",
      "Epoch 017 | tr_MSE 0.00205 | val_MAE 0.03171 | val_RMSE 0.04603 | R2 0.7519\n",
      "[mem:after_epoch_18] allocated=126.3MB reserved=360.0MB\n",
      "Epoch 018 | tr_MSE 0.00231 | val_MAE 0.03789 | val_RMSE 0.05218 | R2 0.6812\n",
      "[mem:after_epoch_19] allocated=126.2MB reserved=350.0MB\n",
      "Epoch 019 | tr_MSE 0.00201 | val_MAE 0.03400 | val_RMSE 0.04724 | R2 0.7387\n",
      "[mem:after_epoch_20] allocated=126.2MB reserved=170.0MB\n",
      "Epoch 020 | tr_MSE 0.00190 | val_MAE 0.03239 | val_RMSE 0.04687 | R2 0.7427\n",
      "[mem:after_epoch_21] allocated=126.3MB reserved=362.0MB\n",
      "Epoch 021 | tr_MSE 0.00196 | val_MAE 0.03556 | val_RMSE 0.05043 | R2 0.7022\n",
      "[mem:after_epoch_22] allocated=126.2MB reserved=348.0MB\n",
      "Epoch 022 | tr_MSE 0.00189 | val_MAE 0.03003 | val_RMSE 0.04380 | R2 0.7753\n",
      "[mem:after_epoch_23] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 023 | tr_MSE 0.00209 | val_MAE 0.03205 | val_RMSE 0.04728 | R2 0.7383\n",
      "[mem:after_epoch_24] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 024 | tr_MSE 0.00218 | val_MAE 0.03597 | val_RMSE 0.04978 | R2 0.7097\n",
      "[mem:after_epoch_25] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 025 | tr_MSE 0.00191 | val_MAE 0.03636 | val_RMSE 0.05023 | R2 0.7045\n",
      "[mem:after_epoch_26] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 026 | tr_MSE 0.00164 | val_MAE 0.03302 | val_RMSE 0.04632 | R2 0.7488\n",
      "[mem:after_epoch_27] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 027 | tr_MSE 0.00166 | val_MAE 0.03106 | val_RMSE 0.04574 | R2 0.7550\n",
      "[mem:after_epoch_28] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 028 | tr_MSE 0.00158 | val_MAE 0.02986 | val_RMSE 0.04574 | R2 0.7550\n",
      "[mem:after_epoch_29] allocated=126.2MB reserved=170.0MB\n",
      "Epoch 029 | tr_MSE 0.00172 | val_MAE 0.02913 | val_RMSE 0.04333 | R2 0.7801\n",
      "[mem:after_epoch_30] allocated=126.4MB reserved=358.0MB\n",
      "Epoch 030 | tr_MSE 0.00210 | val_MAE 0.03305 | val_RMSE 0.04736 | R2 0.7373\n",
      "[mem:after_epoch_31] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 031 | tr_MSE 0.00248 | val_MAE 0.04845 | val_RMSE 0.06791 | R2 0.4600\n",
      "[mem:after_epoch_32] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 032 | tr_MSE 0.00313 | val_MAE 0.03546 | val_RMSE 0.05212 | R2 0.6818\n",
      "[mem:after_epoch_33] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 033 | tr_MSE 0.00235 | val_MAE 0.05297 | val_RMSE 0.06348 | R2 0.5281\n",
      "[mem:after_epoch_34] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 034 | tr_MSE 0.00550 | val_MAE 0.05077 | val_RMSE 0.06928 | R2 0.4378\n",
      "[mem:after_epoch_35] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 035 | tr_MSE 0.00287 | val_MAE 0.03609 | val_RMSE 0.05156 | R2 0.6887\n",
      "[mem:after_epoch_36] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 036 | tr_MSE 0.00181 | val_MAE 0.03329 | val_RMSE 0.04836 | R2 0.7262\n",
      "[mem:after_epoch_37] allocated=126.3MB reserved=346.0MB\n",
      "Epoch 037 | tr_MSE 0.00197 | val_MAE 0.03632 | val_RMSE 0.04975 | R2 0.7102\n",
      "[mem:after_epoch_38] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 038 | tr_MSE 0.00202 | val_MAE 0.04403 | val_RMSE 0.06006 | R2 0.5776\n",
      "[mem:after_epoch_39] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 039 | tr_MSE 0.00245 | val_MAE 0.05238 | val_RMSE 0.06318 | R2 0.5326\n",
      "[mem:after_epoch_40] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 040 | tr_MSE 0.00392 | val_MAE 0.06734 | val_RMSE 0.08818 | R2 0.0895\n",
      "[mem:after_epoch_41] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 041 | tr_MSE 0.00627 | val_MAE 0.04363 | val_RMSE 0.05810 | R2 0.6047\n",
      "[mem:after_epoch_42] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 042 | tr_MSE 0.00278 | val_MAE 0.03316 | val_RMSE 0.04696 | R2 0.7417\n",
      "[mem:after_epoch_43] allocated=126.3MB reserved=344.0MB\n",
      "Epoch 043 | tr_MSE 0.00442 | val_MAE 0.05622 | val_RMSE 0.07257 | R2 0.3833\n",
      "[mem:after_epoch_44] allocated=126.3MB reserved=342.0MB\n",
      "Epoch 044 | tr_MSE 0.00417 | val_MAE 0.05755 | val_RMSE 0.06841 | R2 0.4520\n",
      "[mem:after_epoch_45] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 045 | tr_MSE 0.00453 | val_MAE 0.04756 | val_RMSE 0.06412 | R2 0.5185\n",
      "[mem:after_epoch_46] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 046 | tr_MSE 0.00470 | val_MAE 0.08692 | val_RMSE 0.09651 | R2 -0.0907\n",
      "[mem:after_epoch_47] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 047 | tr_MSE 0.00556 | val_MAE 0.04811 | val_RMSE 0.06639 | R2 0.4838\n",
      "[mem:after_epoch_48] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 048 | tr_MSE 0.00325 | val_MAE 0.07063 | val_RMSE 0.08052 | R2 0.2408\n",
      "[mem:after_epoch_49] allocated=126.3MB reserved=348.0MB\n",
      "Epoch 049 | tr_MSE 0.00352 | val_MAE 0.03941 | val_RMSE 0.05618 | R2 0.6304\n",
      "Early stopping.\n",
      "[graphtransformer_tc_spd] Best Val — MAE 0.029127 | RMSE 0.043332 | R2 0.7801\n",
      "[mem:after_Tc] allocated=16.2MB reserved=40.0MB\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(train_loader_tg))\n",
    "rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "model_tg = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "    use_adj_const=False, use_edge_bias=False,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_tg, ckpt_tg, met_tg = train_hybrid_gnn_sota(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=10, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_tg_spd\", tag=\"graphtransformer_tg_spd\"\n",
    ")\n",
    "del model_tg, train_loader_tg, val_loader_tg\n",
    "free_cuda_memory(tag=\"after_Tg\")\n",
    "reset_cuda_stats()\n",
    "\n",
    "model_den = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "    use_adj_const=False, use_edge_bias=False,\n",
    "    use_cls=False, use_has_xyz=False, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_den, ckpt_den, met_den = train_hybrid_gnn_sota(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=10, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_den_spd\", tag=\"graphtransformer_den_spd\"\n",
    ")\n",
    "del model_den, train_loader_den, val_loader_den\n",
    "free_cuda_memory(tag=\"after_Density\")\n",
    "reset_cuda_stats()\n",
    "\n",
    "# Rg\n",
    "model_rg = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=False, use_edge_bias=False,\n",
    "    use_cls=False, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_rg, ckpt_rg, met_rg = train_hybrid_gnn_sota(\n",
    "    model_rg, train_loader_rg, val_loader_rg,\n",
    "    lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=10, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_rg_spd\", tag=\"graphtransformer_rg_spd\"\n",
    ")\n",
    "del model_rg, train_loader_rg, val_loader_rg\n",
    "free_cuda_memory(tag=\"after_Rg\")\n",
    "reset_cuda_stats()\n",
    "# Tc\n",
    "model_tc = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=False, spd_max=5,\n",
    "    use_adj_const=False, use_edge_bias=False,\n",
    "    use_cls=True, use_has_xyz=False, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_tc, ckpt_tc, met_tc = train_hybrid_gnn_sota(\n",
    "    model_tc, train_loader_tc, val_loader_tc,\n",
    "    lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=10, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=\"saved_models/gt_tc_spd\", tag=\"graphtransformer_tc_spd\"\n",
    ")\n",
    "del model_tc, train_loader_tc, val_loader_tc\n",
    "free_cuda_memory(tag=\"after_Tc\")\n",
    "reset_cuda_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6d1b1",
   "metadata": {},
   "source": [
    "[graphtransformer_tg_spd] Best Val — MAE 48.938103 | RMSE 60.409924 | R2 0.6177\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.041639 | RMSE 0.080110 | R2 0.6345\n",
    "[graphtransformer_rg_spd] Best Val — MAE 1.897893 | RMSE 2.820058 | R2 0.6387\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.029127 | RMSE 0.043332 | R2 0.7801\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.007025 | RMSE 0.011341 | R2 0.8756"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65787ea6",
   "metadata": {},
   "source": [
    "# Original LMDB\n",
    "Minimal baseline:\n",
    "[graphtransformer_tg_spd] Best Val — MAE 52.682880 | RMSE 66.310356 | R2 0.5394\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.033281 | RMSE 0.069495 | R2 0.7250\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.274377 | RMSE 3.533569 | R2 0.4327\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.027580 | RMSE 0.044943 | R2 0.7635\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.005713 | RMSE 0.008959 | R2 0.9223\n",
    "use_edge_bias=True:\n",
    "[graphtransformer_tg_spd] Best Val — MAE 59.157528 | RMSE 78.540993 | R2 0.3538\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.040733 | RMSE 0.075703 | R2 0.6736\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.184664 | RMSE 3.215525 | R2 0.5302\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.029891 | RMSE 0.045550 | R2 0.7570\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.014560 | RMSE 0.024105 | R2 0.4378\n",
    "use_adj_const=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 55.685497 | RMSE 71.508026 | R2 0.4644\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.043538 | RMSE 0.076304 | R2 0.6684\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.194414 | RMSE 3.418880 | R2 0.4689\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.030531 | RMSE 0.047150 | R2 0.7397\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.015711 | RMSE 0.025118 | R2 0.3895\n",
    "use_spd_bias=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 56.409588 | RMSE 74.155319 | R2 0.4240\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.051897 | RMSE 0.086738 | R2 0.5716\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.043078 | RMSE 3.004847 | R2 0.5897\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.030650 | RMSE 0.046136 | R2 0.7507\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.014980 | RMSE 0.024944 | R2 0.3980\n",
    "use_geo_bias=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 57.246323 | RMSE 71.635178 | R2 0.4624\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.040926 | RMSE 0.073173 | R2 0.6951\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.186664 | RMSE 3.221086 | R2 0.5286\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.029472 | RMSE 0.045144 | R2 0.7613\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.010738 | RMSE 0.018613 | R2 0.6648\n",
    "use_cls=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 51.327293 | RMSE 65.371567 | R2 0.5523\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.048267 | RMSE 0.078447 | R2 0.6495\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.215581 | RMSE 3.335439 | R2 0.4945\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.031363 | RMSE 0.046542 | R2 0.7463\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.011230 | RMSE 0.019117 | R2 0.6464\n",
    "use_has_xyz=True\n",
    "\n",
    "# New LMDB with canonicalize_psmiles\n",
    "Minimal baseline:\n",
    "[graphtransformer_tg_spd] Best Val — MAE 55.100464 | RMSE 71.059502 | R2 0.4711\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.030763 | RMSE 0.068418 | R2 0.7334\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.184328 | RMSE 3.163936 | R2 0.5452\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.027480 | RMSE 0.044562 | R2 0.7675\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.006242 | RMSE 0.013769 | R2 0.8166\n",
    "use_edge_bias=True:\n",
    "[graphtransformer_tg_spd] Best Val — MAE 64.235008 | RMSE 84.792068 | R2 0.2469\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.041322 | RMSE 0.079782 | R2 0.6375\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.271991 | RMSE 3.328248 | R2 0.4967\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.030659 | RMSE 0.046286 | R2 0.7491\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.012568 | RMSE 0.021181 | R2 0.5659\n",
    "use_adj_const=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 56.366352 | RMSE 73.193703 | R2 0.4388\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.047595 | RMSE 0.079254 | R2 0.6423\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.042789 | RMSE 2.847963 | R2 0.6315\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.033112 | RMSE 0.048311 | R2 0.7267\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.014251 | RMSE 0.022048 | R2 0.5297\n",
    "use_spd_bias=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 52.129559 | RMSE 64.974182 | R2 0.5578\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.050838 | RMSE 0.088112 | R2 0.5579\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.168722 | RMSE 3.208059 | R2 0.5324\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.033525 | RMSE 0.048122 | R2 0.7288\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.018180 | RMSE 0.029006 | R2 0.1860\n",
    "use_geo_bias=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 49.387646 | RMSE 63.215427 | R2 0.5814\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.045089 | RMSE 0.074725 | R2 0.6820\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.082205 | RMSE 2.920620 | R2 0.6124\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.033126 | RMSE 0.045319 | R2 0.7595\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.012233 | RMSE 0.020871 | R2 0.5785\n",
    "use_cls=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 51.995239 | RMSE 68.995934 | R2 0.5013\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.042371 | RMSE 0.080913 | R2 0.6272\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.145705 | RMSE 3.241805 | R2 0.5225\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.031370 | RMSE 0.044972 | R2 0.7632\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.015359 | RMSE 0.023831 | R2 0.4505\n",
    "use_has_xyz=True\n",
    "[graphtransformer_tg_spd] Best Val — MAE 52.416542 | RMSE 67.310928 | R2 0.5254\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.049355 | RMSE 0.081608 | R2 0.6207\n",
    "[graphtransformer_rg_spd] Best Val — MAE 2.096542 | RMSE 2.964315 | R2 0.6007\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.031574 | RMSE 0.046309 | R2 0.7489\n",
    "[graphtransformer_ffv_spd] Best Val — MAE 0.013870 | RMSE 0.020263 | R2 0.6028\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673460",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "| Model Type | Feature | MAE | RMSE | R2 |\n",
    "|---|---|---|---|---|\n",
    "| RF3D | FFV | 0.007621 | 0.017553 | 0.6605 |\n",
    "| RF3D_Aug | FFV | 0.007578 | 0.017404 | 0.6662 |\n",
    "| GNN2 | FFV | 0.013817 | 0.023902 | 0.4473 |\n",
    "| GNN2_Aug | FFV | 0.013092 | 0.022793 | 0.4974 |\n",
    "| ET | FFV | 0.006651 | 0.016818 | 0.6883 |\n",
    "| ET_Aug | FFV | 0.006635 | 0.016826 | 0.6880 |\n",
    "| **GT** | **FFV** | **0.005713** | **0.008959** | **0.9223** |\n",
    "| GT_Aug | FFV | 0.007025 | 0.011341 | 0.8756 |\n",
    "| RF3D | Tg | 58.315801 | 74.296699 | 0.5846 |\n",
    "| RF3D_Aug | Tg | 58.143107 | 74.521032 | 0.5821 |\n",
    "| **GNN2** | **Tg** | **47.105114** | **61.480179** | **0.6040** |\n",
    "| GNN2_Aug | Tg | 51.539692 | 70.575638 | 0.4782 |\n",
    "| ET | Tg | 58.973811 | 74.658978 | 0.5806 |\n",
    "| ET_Aug | Tg | 58.521052 | 74.475532 | 0.5826 |\n",
    "| GT | Tg | 78.903389 | 98.401192 |-0.0143 |\n",
    "| GT_Aug | Tg | 52.365578 | 67.529610 | 0.5223 |\n",
    "| RF3D | Tc | 0.029937 | 0.045036 | 0.7313 |\n",
    "| RF3D_Aug | Tc | 0.029675 | 0.044853 | 0.7335 |\n",
    "| **GNN2** | **Tc** | **0.025115** | **0.041331** | **0.8000** |\n",
    "| **GNN2_Aug** | **Tc** | **0.025252** | **0.039670** | **0.8157** |\n",
    "| ET | Tc | 0.028888 | 0.043469 | 0.7497 |\n",
    "| ET_Aug | Tc | 0.027990 | 0.042644 | 0.7591 |\n",
    "| GT | Tc | 0.032644 | 0.046613 | 0.7456 |\n",
    "| GT_Aug | Tc | 0.028590 | 0.043121 | 0.7822 |\n",
    "| RF3D | Rg | 1.648818 | 2.493712 | 0.7299 |\n",
    "| RF3D_Aug | Rg | 1.668425 | 2.517235 | 0.7248 |\n",
    "| GNN2 | Rg | 2.115880 | 2.801481 | 0.6434 |\n",
    "| GNN2_Aug | Rg | 1.532573 | 2.405382 | 0.7371 |\n",
    "| ET | Rg | 1.619464 | 2.522478 | 0.7237 |\n",
    "| **ET_Aug** | **Rg** | **1.609396** | **2.526705** | **0.7227** |\n",
    "| GT | Rg | 2.579300 | 3.521387 | 0.4366 |\n",
    "| GT_Aug | Rg | 2.134301 | 3.066199 | 0.5728 |\n",
    "| RF3D | Density | 0.037793 | 0.070932 | 0.7847 |\n",
    "| RF3D_Aug | Density | 0.037123 | 0.070212 | 0.7891 |\n",
    "| GNN2 | Density | 0.031735 | 0.067845 | 0.7379 |\n",
    "| GNN2_Aug | Density | 0.030458 | 0.070372 | 0.7180 |\n",
    "| ET | Density | 0.028492 | 0.052839 | 0.8805 |\n",
    "| **ET_Aug** | **Density** | **0.028135** | **0.051842** | **0.8850** |\n",
    "| GT | Density | 0.104749 | 0.134771 | -0.0343 |\n",
    "| GT_Aug | Density | 0.087159 | 0.126079 | 0.0948 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a95722",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
