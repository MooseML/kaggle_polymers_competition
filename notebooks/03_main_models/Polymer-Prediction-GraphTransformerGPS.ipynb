{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61979795",
   "metadata": {},
   "source": [
    "# Polymer Property Predictions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "from typing import Dict, Optional\n",
    "import os \n",
    "import random \n",
    "from copy import deepcopy\n",
    "import gc\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Sampler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import AdamW, RMSprop\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# # PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "# # OGB dataset \n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Local imports \n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from src.data.polymer_dataset import LMDBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589db70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Built with CUDA: True\n",
      "CUDA available: True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device: /physical_device:GPU:0\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDBs already exist.\n"
     ]
    }
   ],
   "source": [
    "CWD = os.getcwd() \n",
    "PROJECT_ROOT = os.path.join(CWD, '..', '..')\n",
    "PROJECT_ROOT = os.path.abspath(PROJECT_ROOT)\n",
    "\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  \n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/hlgap-gnn3d-transformer-pcqm4mv2-v1.pt'\n",
    "else:\n",
    "    DATA_ROOT = os.path.join(PROJECT_ROOT, 'data')\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = os.path.join(PROJECT_ROOT, 'hlgap-gnn3d-transformer-pcqm4mv2-v1.pt')\n",
    "\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    build_script_path = os.path.join(PROJECT_ROOT, 'scripts', 'data_preprocessing', 'build_lmdb.py')\n",
    "\n",
    "    if not os.path.exists(build_script_path):\n",
    "        print(f\"ERROR: LMDB building script not found at {build_script_path}. Please check file location.\")\n",
    "    else:\n",
    "        !python {build_script_path} train\n",
    "        !python {build_script_path} test\n",
    "        print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a0004",
   "metadata": {},
   "source": [
    "# Graph Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d599b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Tg → parents train=  408 val=  103 | aug rows train=  4080 val=  1030\n",
      "    FFV → parents train= 5624 val= 1406 | aug rows train= 56240 val= 14060\n",
      "     Tc → parents train=  589 val=  148 | aug rows train=  5890 val=  1480\n",
      "Density → parents train=  490 val=  123 | aug rows train=  4900 val=  1230\n",
      "     Rg → parents train=  491 val=  123 | aug rows train=  4910 val=  1230\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Parent-aware wiring (CSV parents -> augmented LMDB key_ids) --------------------\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "AUG_KEY_MULT = 1000  \n",
    "\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_csv[\"id\"] = train_csv[\"id\"].astype(int)\n",
    "\n",
    "# LMDB ids (augmented key_ids)\n",
    "lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "lmdb_ids = np.loadtxt(lmdb_ids_path, dtype=np.int64)\n",
    "if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "\n",
    "# Parent map (preferred) -> key_id list per parent\n",
    "pmap_path = TRAIN_LMDB + \".parent_map.tsv\"\n",
    "if os.path.exists(pmap_path):\n",
    "    pmap = pd.read_csv(pmap_path, sep=\"\\t\") # cols: key_id, parent_id, aug_idx, seed\n",
    "    pmap[\"key_id\"] = pmap[\"key_id\"].astype(np.int64)\n",
    "    pmap[\"parent_id\"] = pmap[\"parent_id\"].astype(np.int64)\n",
    "else:\n",
    "    # fallback if parent_map missing: derive parent by integer division\n",
    "    pmap = pd.DataFrame({\n",
    "        \"key_id\": lmdb_ids.astype(np.int64),\n",
    "        \"parent_id\": (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "    })\n",
    "parents_in_lmdb = np.sort(pmap[\"parent_id\"].unique().astype(np.int64))\n",
    "\n",
    "def parents_with_label(task: str) -> np.ndarray:\n",
    "    m = ~train_csv[task].isna()\n",
    "    have = train_csv.loc[m, \"id\"].astype(int).values # parents that have this label\n",
    "    return np.intersect1d(have, parents_in_lmdb, assume_unique=False)\n",
    "\n",
    "# Split by parents so we have no leakage and then we can expand to augmented key_ids \n",
    "def task_parent_split_keys(task: str, test_size=0.2, seed=42):\n",
    "    parents_labeled = parents_with_label(task)\n",
    "    if parents_labeled.size == 0:\n",
    "        raise ValueError(f\"No parents with labels for {task}\")\n",
    "    p_tr, p_va = train_test_split(parents_labeled, test_size=test_size, random_state=seed)\n",
    "    tr_keys = pmap.loc[pmap.parent_id.isin(p_tr), \"key_id\"].astype(np.int64).values\n",
    "    va_keys = pmap.loc[pmap.parent_id.isin(p_va), \"key_id\"].astype(np.int64).values\n",
    "    return np.sort(tr_keys), np.sort(va_keys), np.sort(p_tr), np.sort(p_va)\n",
    "\n",
    "# Build pools (augmented key_ids) per task\n",
    "task_pools = {}\n",
    "task_parent_splits = {}\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys, p_tr, p_va = task_parent_split_keys(t, test_size=0.2, seed=42)\n",
    "    task_pools[t] = (tr_keys, va_keys)\n",
    "    task_parent_splits[t] = (p_tr, p_va)\n",
    "\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys = task_pools[t]\n",
    "    p_tr, p_va = task_parent_splits[t]\n",
    "    print(f\"{t:>7} → parents train={len(p_tr):5d} val={len(p_va):5d} | aug rows train={len(tr_keys):6d} val={len(va_keys):6d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9647b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- KEY_SIZES (key_id -> parent n_atoms_2d) and simple finite samplers --------------------\n",
    "parent_meta_path = TRAIN_LMDB + \".parent_meta.tsv\"\n",
    "if not os.path.exists(parent_meta_path):\n",
    "    raise FileNotFoundError(f\"Missing {parent_meta_path}. Rebuild LMDB with parent_meta.tsv enabled.\")\n",
    "\n",
    "parent_meta = pd.read_csv(parent_meta_path, sep=\"\\t\") # cols: parent_id, n_atoms_2d, star_count, replacement_Z\n",
    "parent_meta[\"parent_id\"] = parent_meta[\"parent_id\"].astype(np.int64)\n",
    "parent_meta = parent_meta.drop_duplicates(subset=[\"parent_id\"])\n",
    "\n",
    "key_sizes_df = pmap.merge(parent_meta[[\"parent_id\",\"n_atoms_2d\"]], on=\"parent_id\", how=\"left\")\n",
    "\n",
    "if key_sizes_df[\"n_atoms_2d\"].isna().any():\n",
    "    med = int(key_sizes_df[\"n_atoms_2d\"].median())\n",
    "    key_sizes_df[\"n_atoms_2d\"] = key_sizes_df[\"n_atoms_2d\"].fillna(med)\n",
    "\n",
    "KEY_SIZES: Dict[int,int] = dict(\n",
    "    zip(key_sizes_df[\"key_id\"].astype(np.int64).tolist(),\n",
    "        key_sizes_df[\"n_atoms_2d\"].astype(int).tolist())\n",
    ")\n",
    "\n",
    "class EmptyBatchSampler(Sampler):\n",
    "    def __iter__(self):\n",
    "        return iter(())           \n",
    "    def __len__(self):\n",
    "        return 0\n",
    "\n",
    "class TokenBucketBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Precompute a finite list of batches from (keys, sizes) under token/quadratic/max_batch constraints.\n",
    "    __iter__ just yields those batches. __len__ returns exact count.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys, sizes_dict: Dict[int,int], *, max_tokens: int, max_quadratic: int, max_batch_size: int, shuffle: bool, seed: int, bins: int = 8):\n",
    "        self.keys = np.asarray(keys, dtype=np.int64)\n",
    "        self.sizes_dict = sizes_dict\n",
    "        self.max_tokens = int(max_tokens)\n",
    "        self.max_quadratic = int(max_quadratic)\n",
    "        self.max_batch_size = int(max_batch_size)\n",
    "        self.shuffle = bool(shuffle)\n",
    "        self.seed = int(seed)\n",
    "        self.bins = int(max(1, bins))\n",
    "        self._batches = self._pack_once() # precompute finite list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batches)\n",
    "\n",
    "    def _pack_once(self):\n",
    "        if self.keys.size == 0:\n",
    "            return []\n",
    "\n",
    "        # materialize (key,size), guard size>=1\n",
    "        pairs = [(int(k), max(1, int(self.sizes_dict.get(int(k), 1)))) for k in self.keys]\n",
    "\n",
    "        # shuffle or deterministic order\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        if self.shuffle:\n",
    "            rng.shuffle(pairs)\n",
    "        else:\n",
    "            pairs.sort(key=lambda t: (t[1], t[0]))\n",
    "\n",
    "        sizes = np.array([s for _, s in pairs], dtype=np.int32)\n",
    "        # Bins by size to reduce padding \n",
    "        B = int(min(self.bins, max(1, sizes.size)))\n",
    "        try:\n",
    "            qs = np.quantile(sizes, np.linspace(0, 1, B + 1)) if sizes.size > 1 else np.array([sizes[0], sizes[0]])\n",
    "        except Exception:\n",
    "            qs = np.array([sizes.min(), sizes.max()])\n",
    "\n",
    "        bins = [[] for _ in range(B)]\n",
    "        for i, (k, s) in enumerate(pairs):\n",
    "            b = int(np.searchsorted(qs, s, side=\"right\")) - 1\n",
    "            b = max(0, min(B - 1, b))\n",
    "            bins[b].append((k, s))\n",
    "\n",
    "        if self.shuffle:\n",
    "            for b in range(B):\n",
    "                rng.shuffle(bins[b])\n",
    "\n",
    "        batches = []\n",
    "        # Round robin draw from bins so each item is used once...finite\n",
    "        progress = True\n",
    "        while progress:\n",
    "            progress = False\n",
    "            for b in range(B):\n",
    "                if not bins[b]:\n",
    "                    continue\n",
    "                progress = True\n",
    "                cur, cur_tokens, cur_quad = [], 0, 0\n",
    "                while bins[b]:\n",
    "                    k, s = bins[b][0]\n",
    "                    next_len = len(cur) + 1\n",
    "                    next_tokens = cur_tokens + s\n",
    "                    next_quad = cur_quad + s * s\n",
    "                    if (next_len <= self.max_batch_size\n",
    "                        and next_tokens <= self.max_tokens\n",
    "                        and next_quad  <= self.max_quadratic):\n",
    "                        cur.append(k)\n",
    "                        cur_tokens = next_tokens\n",
    "                        cur_quad = next_quad\n",
    "                        bins[b].pop(0)\n",
    "                    else:\n",
    "                        break\n",
    "                if cur:\n",
    "                    batches.append(cur)\n",
    "\n",
    "        if not batches: # Safety \n",
    "            batches = [[int(k)] for (k, _) in pairs]\n",
    "        return batches\n",
    "\n",
    "class ChainBatchSamplers(Sampler):\n",
    "    \"\"\"Concatenate multiple precomputed batch samplers into one finite sequence.\"\"\"\n",
    "    def __init__(self, samplers, *, shuffle_order=False, seed=0):\n",
    "        self.samplers = list(samplers)\n",
    "        self.shuffle_order = bool(shuffle_order)\n",
    "        self.seed = int(seed)\n",
    "        # precompute concatenation so __len__ is exact\n",
    "        order = list(range(len(self.samplers)))\n",
    "        if self.shuffle_order:\n",
    "            rnd = random.Random(self.seed)\n",
    "            rnd.shuffle(order)\n",
    "        self._batches = []\n",
    "        for k in order:\n",
    "            s = self.samplers[k]\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            self._batches.extend(list(s))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batches)\n",
    "\n",
    "def build_bucket_sampler_for_keys(\n",
    "    keys: np.ndarray, *,\n",
    "    max_tokens: int,\n",
    "    max_quadratic: Optional[int],\n",
    "    max_batch_size: int,\n",
    "    shuffle: bool,\n",
    "    seed: int,\n",
    "    ) -> Sampler:\n",
    "    keys = np.asarray(keys, dtype=np.int64)\n",
    "\n",
    "    if keys.size == 0:\n",
    "        return EmptyBatchSampler()\n",
    "    \n",
    "    return TokenBucketBatchSampler(\n",
    "        keys, \n",
    "        KEY_SIZES,\n",
    "        max_tokens=max_tokens,\n",
    "        max_quadratic=max_quadratic if max_quadratic is not None else 1_200_000,\n",
    "        max_batch_size=max_batch_size,\n",
    "        shuffle=shuffle, \n",
    "        seed=seed, \n",
    "        bins=8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3efce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rdkit_feats_from_record(rec):\n",
    "    arr = getattr(rec, \"rdkit_feats\", None)\n",
    "    if arr is None:\n",
    "        return torch.zeros(1, 15, dtype=torch.float32) # keep (1, D)\n",
    "    v = torch.as_tensor(np.asarray(arr, np.float32).reshape(1, -1), dtype=torch.float32)\n",
    "    return v # (1, D)\n",
    "\n",
    "class LMDBtoPyGSingleTask(Dataset):\n",
    "    def __init__(self, ids, lmdb_path, target_index=None, *, use_mixed_edges: bool = True, include_extra_atom_feats: bool = True):\n",
    "        self.ids = np.asarray(ids, dtype=np.int64)\n",
    "        self.base = LMDBDataset(self.ids, lmdb_path)\n",
    "        self.t = target_index\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        self.include_extra_atom_feats = include_extra_atom_feats\n",
    "\n",
    "    def __len__(self): return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.base[idx]\n",
    "        x = torch.as_tensor(rec.x, dtype=torch.long)\n",
    "        ei = torch.as_tensor(rec.edge_index, dtype=torch.long)\n",
    "        ea = torch.as_tensor(rec.edge_attr)\n",
    "\n",
    "        # Mixed edges: 3 categorical + 32 RBF. Categorical-only if disabled\n",
    "        edge_attr = ea.to(torch.float32) if self.use_mixed_edges else ea[:, :3].to(torch.long)\n",
    "\n",
    "        d = Data(x=x, edge_index=ei, edge_attr=edge_attr, rdkit_feats=_get_rdkit_feats_from_record(rec)) # (1, D)\n",
    "\n",
    "        if hasattr(rec, \"pos\"):\n",
    "                d.pos  = torch.as_tensor(rec.pos, dtype=torch.float32)\n",
    "        if self.include_extra_atom_feats and hasattr(rec, \"extra_atom_feats\"):\n",
    "             d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32)\n",
    "        if hasattr(rec, \"has_xyz\"):\n",
    "             d.has_xyz = torch.as_tensor(rec.has_xyz, dtype=torch.float32) # (1,)\n",
    "        if hasattr(rec, \"dist\"):\n",
    "             d.hops = torch.as_tensor(rec.dist, dtype=torch.long).unsqueeze(0) # (1,L,L)\n",
    "\n",
    "        if (self.t is not None) and hasattr(rec, \"y\"):\n",
    "            yv = torch.as_tensor(rec.y, dtype=torch.float32).view(-1)\n",
    "            if self.t < yv.numel(): \n",
    "                 d.y = yv[self.t:self.t+1] # (1,)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694612d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Token-bucket loaders with key->index mapping --------------------\n",
    "WHALE_CUTOFF = 86 # p99 from EDA\n",
    "\n",
    "class MapKeyBatchesToIndexBatches(Sampler):\n",
    "    \"\"\"Wrap a batch-sampler that yields key_ids; convert to dataset indices.\"\"\"\n",
    "    def __init__(self, key_batch_sampler, id2pos):\n",
    "        self.key_batch_sampler = key_batch_sampler # yields lists of key_ids\n",
    "        self.id2pos = id2pos # {key_id: index_in_dataset}\n",
    "        # pre-map once for speed and safety\n",
    "        self._batches = [[self.id2pos[int(k)] for k in batch] for batch in key_batch_sampler]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batches)\n",
    "\n",
    "def make_loaders_for_task_from_pools(\n",
    "        task, \n",
    "        task_pools,\n",
    "        *,\n",
    "        normal_max_tokens=9000,\n",
    "        normal_max_quadratic=1080000,\n",
    "        whale_max_tokens=2500,\n",
    "        whale_max_quadratic=400000,\n",
    "        max_batch_size=1024,\n",
    "        use_mixed_edges=True,\n",
    "        include_extra_atom_feats=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        debug_single_process=True,\n",
    "        whale_cutoff=WHALE_CUTOFF,\n",
    "        ):\n",
    "    assert 'KEY_SIZES' in globals(), \"KEY_SIZES dict must be built first\"\n",
    "\n",
    "    t = task2idx[task]\n",
    "    tr_keys, va_keys = task_pools[task]\n",
    "    if len(tr_keys) == 0 or len(va_keys) == 0:\n",
    "        raise ValueError(f\"Empty pools for {task}. Check splits.\")\n",
    "\n",
    "    # Datasets stay in key-id order\n",
    "    tr_ds = LMDBtoPyGSingleTask(tr_keys, TRAIN_LMDB, target_index=t, use_mixed_edges=use_mixed_edges, include_extra_atom_feats=include_extra_atom_feats)\n",
    "    va_ds = LMDBtoPyGSingleTask(va_keys, TRAIN_LMDB, target_index=t, use_mixed_edges=use_mixed_edges, include_extra_atom_feats=include_extra_atom_feats)\n",
    "\n",
    "    # Map key_id -> position within each dataset\n",
    "    tr_id2pos = {int(k): i for i, k in enumerate(tr_keys)}\n",
    "    va_id2pos = {int(k): i for i, k in enumerate(va_keys)}\n",
    "\n",
    "    # Split ids by size using KEY_SIZES\n",
    "    def split_keys(keys):\n",
    "        small, whales = [], []\n",
    "        for k in keys:\n",
    "            if KEY_SIZES.get(int(k), 1) > whale_cutoff:\n",
    "                whales.append(int(k))\n",
    "            else:\n",
    "                small.append(int(k))\n",
    "        return np.array(small, dtype=np.int64), np.array(whales, dtype=np.int64)\n",
    "\n",
    "    tr_small, tr_whales = split_keys(tr_keys)\n",
    "    va_small, va_whales = split_keys(va_keys)\n",
    "\n",
    "    # Build key-id samplers (finite)\n",
    "    tr_small_s = build_bucket_sampler_for_keys(tr_small, max_tokens=normal_max_tokens, max_quadratic=normal_max_quadratic, max_batch_size=max_batch_size, shuffle=True, seed=42)\n",
    "    tr_whale_s = build_bucket_sampler_for_keys(tr_whales, max_tokens=whale_max_tokens, max_quadratic=whale_max_quadratic, max_batch_size=max_batch_size, shuffle=True, seed=43)\n",
    "    tr_keys_sampler = ChainBatchSamplers([tr_small_s, tr_whale_s], shuffle_order=True, seed=123)\n",
    "\n",
    "    va_small_s = build_bucket_sampler_for_keys(va_small, max_tokens=normal_max_tokens, max_quadratic=normal_max_quadratic, max_batch_size=max_batch_size, shuffle=False, seed=123)\n",
    "    va_whale_s = build_bucket_sampler_for_keys(va_whales, max_tokens=whale_max_tokens, max_quadratic=whale_max_quadratic, max_batch_size=max_batch_size, shuffle=False, seed=123)\n",
    "    va_keys_sampler = ChainBatchSamplers([va_small_s, va_whale_s], shuffle_order=False, seed=0)\n",
    "\n",
    "    # Wrap: key-id batches -> dataset index batches\n",
    "    tr_sampler = MapKeyBatchesToIndexBatches(tr_keys_sampler, tr_id2pos)\n",
    "    va_sampler = MapKeyBatchesToIndexBatches(va_keys_sampler, va_id2pos)\n",
    "\n",
    "    # DataLoaders\n",
    "    if debug_single_process:\n",
    "        tr_loader = GeoDataLoader(tr_ds, batch_sampler=tr_sampler, num_workers=0, pin_memory=False)\n",
    "        va_loader = GeoDataLoader(va_ds, batch_sampler=va_sampler, num_workers=0, pin_memory=False)\n",
    "    else:\n",
    "        tr_loader = GeoDataLoader(\n",
    "            tr_ds, \n",
    "            batch_sampler=tr_sampler,\n",
    "            num_workers=num_workers, \n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=(num_workers > 0),\n",
    "            **({} if num_workers == 0 else dict(prefetch_factor=2))\n",
    "        )\n",
    "        va_loader = GeoDataLoader(\n",
    "            va_ds, \n",
    "            batch_sampler=va_sampler,\n",
    "            num_workers=num_workers, \n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=(num_workers > 0),\n",
    "            **({} if num_workers == 0 else dict(prefetch_factor=2))\n",
    "        )\n",
    "    return tr_loader, va_loader\n",
    "\n",
    "train_loader_tg,  val_loader_tg  = make_loaders_for_task_from_pools(\"Tg\", task_pools, debug_single_process=True)\n",
    "train_loader_den, val_loader_den = make_loaders_for_task_from_pools(\"Density\", task_pools, debug_single_process=True)\n",
    "train_loader_rg,  val_loader_rg  = make_loaders_for_task_from_pools(\"Rg\", task_pools, debug_single_process=True)\n",
    "train_loader_ffv, val_loader_ffv = make_loaders_for_task_from_pools(\"FFV\", task_pools, debug_single_process=True)\n",
    "train_loader_tc,  val_loader_tc  = make_loaders_for_task_from_pools(\"Tc\", task_pools, debug_single_process=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983db98",
   "metadata": {},
   "source": [
    "## Step 5: Define the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc992041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_len_stats(b):\n",
    "    # counts nodes per-graph from PyG's batch vector\n",
    "    sizes = torch.bincount(b.batch, minlength=b.num_graphs)\n",
    "    s = sizes.to(torch.int32).cpu().numpy()\n",
    "    if s.size == 0:\n",
    "        return \"L: n=0\"\n",
    "    tokens = int(s.sum())\n",
    "    quad = int((sizes.to(torch.int64)**2).sum().item())\n",
    "    q50, q90, q95, q99 = np.quantile(s, [0.5, 0.9, 0.95, 0.99])\n",
    "    return (f\"L: n={len(s)} tokens={tokens} sumL2={quad}\"\n",
    "            f\"p50={int(q50)} p90={int(q90)} p95={int(q95)} p99={int(q99)} max={int(s.max())}\")\n",
    "\n",
    "\n",
    "def free_cuda_memory(tag: str = \"\"):\n",
    "    try:\n",
    "        torch.cuda.synchronize()\n",
    "    except Exception:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    if tag:\n",
    "        try:\n",
    "            alloc = torch.cuda.memory_allocated() / (1024**2)\n",
    "            reserv = torch.cuda.memory_reserved() / (1024**2)\n",
    "            print(f\"[mem:{tag}] allocated={alloc:.1f}MB reserved={reserv:.1f}MB\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def reset_cuda_stats():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def train_hybrid_gnn_sota(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    *,\n",
    "    lr: float = 5e-4,\n",
    "    optimizer: str = \"AdamW\",\n",
    "    weight_decay: float = 1e-5,\n",
    "    epochs: int = 120,\n",
    "    warmup_epochs: int = 5,\n",
    "    patience: int = 15,\n",
    "    clip_norm: float = 1.0,\n",
    "    amp: bool = True,\n",
    "    loss_name: str = \"mse\",   # \"mse\" or \"huber\"\n",
    "    save_dir: str = os.path.join(PROJECT_ROOT, \"saved_models\", \"gnn\"),\n",
    "    tag: str = \"model_sota\",\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    opt_name = optimizer.lower()\n",
    "    if opt_name == \"rmsprop\":\n",
    "        opt = RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.0)\n",
    "    else:\n",
    "        opt = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # cosine schedule w/ warmup\n",
    "    def lr_factor(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / max(1, warmup_epochs)\n",
    "        t = (epoch - warmup_epochs) / max(1, (epochs - warmup_epochs))\n",
    "        return 0.5 * (1 + math.cos(math.pi * t))\n",
    "    scaler = GradScaler(\"cuda\", enabled=amp)\n",
    "\n",
    "    def loss_fn(pred, target):\n",
    "        if loss_name.lower() == \"huber\":\n",
    "            return F.huber_loss(pred, target, delta=1.0)\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_once(loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.inference_mode():\n",
    "            for i, b in enumerate(loader, 1):\n",
    "                # print(_batch_len_stats(b))  # keep if you want\n",
    "                b = b.to(device, non_blocking=True)\n",
    "                p = model(b)\n",
    "                preds.append(p.cpu())\n",
    "                trues.append(b.y.view(-1,1).cpu())\n",
    "                del p, b\n",
    "        preds = torch.cat(preds).numpy(); trues = torch.cat(trues).numpy()\n",
    "        mae = np.mean(np.abs(preds - trues))\n",
    "        rmse = float(np.sqrt(np.mean((preds - trues)**2)))\n",
    "        r2 = float(1 - np.sum((preds - trues)**2) / np.sum((trues - trues.mean())**2))\n",
    "        return mae, rmse, r2\n",
    "\n",
    "    best_mae = float(\"inf\")\n",
    "    best = None\n",
    "    best_path = os.path.join(save_dir, f\"{tag}.pt\")\n",
    "    bad = 0 \n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        for g in opt.param_groups:\n",
    "            g[\"lr\"] = lr * lr_factor(ep-1)\n",
    "\n",
    "        model.train()\n",
    "        total, count = 0.0, 0\n",
    "        for step, b in enumerate(train_loader, start=1):\n",
    "            b = b.to(device, non_blocking=True)\n",
    "            if ep == 1 and step % 50 == 1:\n",
    "            # if (ep <= 2) or (step % 50 == 1):\n",
    "                print(_batch_len_stats(b))\n",
    "            with autocast(\"cuda\", enabled=amp):\n",
    "                pred = model(b)\n",
    "                loss = loss_fn(pred, b.y.view(-1,1))\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            if clip_norm is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "            scaler.step(opt); scaler.update()\n",
    "\n",
    "            total += loss.item() * b.num_graphs\n",
    "            count += b.num_graphs\n",
    "            del pred, loss, b  \n",
    "\n",
    "        free_cuda_memory(tag=f\"after_epoch_{ep}\")\n",
    "\n",
    "        tr_mse = total / max(1, count)\n",
    "        mae, rmse, r2 = eval_once(val_loader)\n",
    "        print(f\"Epoch {ep:03d} | tr_MSE {tr_mse:.5f} | val_MAE {mae:.5f} | val_RMSE {rmse:.5f} | R2 {r2:.4f}\")\n",
    "\n",
    "        if mae < best_mae - 1e-6:\n",
    "            best_mae = mae\n",
    "            best = deepcopy(model.state_dict())\n",
    "            torch.save(best, best_path)\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    if best is not None:\n",
    "        model.load_state_dict(best)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "    final_mae, final_rmse, final_r2 = eval_once(val_loader)\n",
    "    print(f\"[{tag}] Best Val — MAE {final_mae:.6f} | RMSE {final_rmse:.6f} | R2 {final_r2:.4f}\")\n",
    "    return model, best_path, {\"MAE\": final_mae, \"RMSE\": final_rmse, \"R2\": final_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9449bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _act(name: str):\n",
    "    name = (name or \"relu\").lower()\n",
    "    if name == \"gelu\": \n",
    "        return nn.GELU()\n",
    "    if name in (\"silu\", \"swish\"): \n",
    "        return nn.SiLU()\n",
    "    return nn.ReLU()\n",
    "\n",
    "\n",
    "class AttnBiasFull(nn.Module):\n",
    "    \"\"\"\n",
    "    Produces additive per-head attention bias of shape (B, H, L0, L0)\n",
    "    from geometry (xyz), adjacency, SPD buckets, and categorical edge types.\n",
    "\n",
    "    Accepts both old arg names (use_geo/use_adj_const/spd_max/rbf_K) and\n",
    "    new ones (use_geo_bias/use_adj_bias/spd_buckets/rbf_k/edge_cats).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_heads: int,\n",
    "        *,\n",
    "        # older names I used\n",
    "        use_geo: bool = None, \n",
    "        use_adj_const: bool = None, \n",
    "        use_spd: bool = True,\n",
    "        spd_max: int = None, \n",
    "        rbf_K: int = None,\n",
    "        # new names\n",
    "        use_geo_bias: bool = None, \n",
    "        use_adj_bias: bool = None,\n",
    "        spd_buckets: int = None, \n",
    "        rbf_k: int = None,\n",
    "        edge_cats: tuple = (5, 6, 2),\n",
    "        use_edge_bias: bool = True,\n",
    "        # shared\n",
    "        rbf_beta: float = 5.0, \n",
    "        activation: str = \"relu\",\n",
    "        edge_cont_dim: int = 32,  # (kept for compatibility but not currently being used here)\n",
    "        use_headnorm: bool = True,\n",
    "        bound_scale: float = 0.1, # tanh scale for gentle bounding\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_heads = int(n_heads)\n",
    "        self.bound_scale = float(bound_scale)\n",
    "        self.use_headnorm = bool(use_headnorm)\n",
    "\n",
    "        def pick(*vals, default):\n",
    "            for v in vals:\n",
    "                if v is not None:\n",
    "                    return v\n",
    "            return default\n",
    "\n",
    "        self.use_geo = bool(pick(use_geo, use_geo_bias, default=True))\n",
    "        self.use_adj_const = bool(pick(use_adj_const, use_adj_bias, default=True))\n",
    "\n",
    "        # SPD: if spd_buckets given then we will use exactly that; otherwise, we will use spd_max + 2 (0..spd_max + catch-all)\n",
    "        if spd_buckets is not None:\n",
    "            self.spd_buckets = int(spd_buckets)\n",
    "        else:\n",
    "            smax = 5 if spd_max is None else int(spd_max)\n",
    "            self.spd_buckets = smax + 2 # 0..smax + 1(>=)\n",
    "\n",
    "        K = int(pick(rbf_K, rbf_k, default=16))\n",
    "        self.rbf_beta = float(rbf_beta)\n",
    "\n",
    "        #  geometry -> per-head bias \n",
    "        if self.use_geo:\n",
    "            centers = torch.linspace(0.0, 10.0, K)\n",
    "            self.register_buffer(\"centers\", centers, persistent=False)\n",
    "            self.geo_mlp = nn.Sequential(\n",
    "                nn.Linear(K, self.n_heads), # simple per-head projection\n",
    "            )\n",
    "\n",
    "        if self.use_adj_const:\n",
    "            self.adj_bias = nn.Parameter(torch.zeros(self.n_heads))\n",
    "\n",
    "        # SPD buckets -> per-head bias \n",
    "        self.use_spd = bool(use_spd)\n",
    "        if self.use_spd:\n",
    "            self.spd_emb = nn.Embedding(self.spd_buckets, self.n_heads)\n",
    "\n",
    "        # edge categorical bias (configurable widths)\n",
    "        t, s, c = edge_cats\n",
    "        self.use_edge_bias = bool(use_edge_bias)\n",
    "        if self.use_edge_bias:\n",
    "            self.edge_emb0 = nn.Embedding(int(t), self.n_heads)\n",
    "            self.edge_emb1 = nn.Embedding(int(s), self.n_heads)\n",
    "            self.edge_emb2 = nn.Embedding(int(c), self.n_heads)\n",
    "        else:\n",
    "            self.edge_emb0 = self.edge_emb1 = self.edge_emb2 = None\n",
    "\n",
    "        # per-component learnable scalers \n",
    "        self.alpha_geo = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_spd = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_adj = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_edge = nn.Parameter(torch.tensor(0.2))\n",
    "\n",
    "        # simple head-wise LayerNorms (normalize across H)\n",
    "        if self.use_headnorm:\n",
    "            self.ln_geo = nn.LayerNorm(self.n_heads)\n",
    "            self.ln_spd = nn.LayerNorm(self.n_heads)\n",
    "            self.ln_edge = nn.LayerNorm(self.n_heads)\n",
    "\n",
    "    # ------------------------------------------ helpers ------------------------------------------\n",
    "    def _apply_ln_heads(self, t: torch.Tensor, ln: nn.LayerNorm) -> torch.Tensor:\n",
    "        \"\"\"Apply LayerNorm across heads for a (B,H,L,L) tensor.\"\"\"\n",
    "        # (B,H,L,L) -> (B,L,L,H) -> LN(H) -> (B,H,L,L)\n",
    "        t = t.permute(0, 2, 3, 1)\n",
    "        t = ln(t)\n",
    "        t = t.permute(0, 3, 1, 2).contiguous()\n",
    "        return t\n",
    "\n",
    "    def _bound(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Bound magnitudes to avoid dominating softmax; keeps gradients smooth.\"\"\"\n",
    "        return self.bound_scale * torch.tanh(t)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _spd_bias(self, hops: torch.Tensor, valid_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        hops: (B, MAX_NODES, MAX_NODES) or (B, L0, L0) shortest-path distances (uint8/long)\n",
    "        valid_mask: (B, L0, L0) bool, True where both tokens are real (not PAD)\n",
    "        returns: (B, H, L0, L0) additive per-head bias\n",
    "        \"\"\"\n",
    "        if hops.dim() == 2: # (L,L) -> (1,L,L)\n",
    "            hops = hops.unsqueeze(0)\n",
    "\n",
    "        B, L0, _ = valid_mask.shape\n",
    "\n",
    "        # align SPD to current L0 (top-left block)\n",
    "        if hops.size(1) != L0 or hops.size(2) != L0:\n",
    "            hops = hops[:, :L0, :L0]\n",
    "\n",
    "        # bucketize SPD: last bucket = catch-all (>= last)\n",
    "        last = self.spd_buckets - 1\n",
    "        raw = hops.to(valid_mask.device).long().clamp_min_(0)\n",
    "        catch_all = raw >= last\n",
    "        raw = raw.clamp_max(last - 1)\n",
    "        bucket = torch.where(catch_all, raw.new_full(raw.shape, last), raw)\n",
    "\n",
    "        # wipe invalid pairs\n",
    "        bucket = torch.where(valid_mask, bucket, torch.zeros_like(bucket))\n",
    "\n",
    "        emb = self.spd_emb(bucket) # (B, L0, L0, H)\n",
    "        return emb.permute(0, 3, 1, 2).contiguous() # (B, H, L0, L0)\n",
    "\n",
    "    def _edge_bias(self, edge_index, edge_attr, batch, L0, ptr=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-head additive bias from categorical bond attributes.\n",
    "        Returns: (B, H, L0, L0)\n",
    "        \"\"\"\n",
    "        u, v = edge_index\n",
    "        be   = batch[u] # graph id per edge\n",
    "\n",
    "        if ptr is None:\n",
    "            B = int(batch.max().item()) + 1\n",
    "            counts = torch.bincount(batch, minlength=B)\n",
    "            ptr = torch.zeros(B + 1, dtype=torch.long, device=batch.device)\n",
    "            ptr[1:] = torch.cumsum(counts, dim=0)\n",
    "        B = int(ptr.numel() - 1)\n",
    "\n",
    "        start = ptr[be]\n",
    "        u_loc = (u - start).long()\n",
    "        v_loc = (v - start).long()\n",
    "\n",
    "        cat = edge_attr[:, :3].long()\n",
    "        eh  = ( self.edge_emb0(cat[:, 0])\n",
    "              + self.edge_emb1(cat[:, 1])\n",
    "              + self.edge_emb2(cat[:, 2]) )  # (E,H)\n",
    "\n",
    "        H = self.n_heads\n",
    "        eb = torch.zeros((B, H, L0, L0), device=edge_attr.device, dtype=torch.float32)\n",
    "        for b in range(B):\n",
    "            m = (be == b)\n",
    "            if not torch.any(m):\n",
    "                continue\n",
    "            eb[b, :, u_loc[m], v_loc[m]] += eh[m].T\n",
    "        return eb\n",
    "\n",
    "    # ------------------------------------------ forward ------------------------------------------\n",
    "    def forward(self, pos, edge_index, edge_attr, batch, key_padding_mask, hops=None, ptr=None):\n",
    "        \"\"\"\n",
    "        Returns (B, H, L0, L0) additive bias. PAD rows/cols are filled with large negative.\n",
    "        \"\"\"\n",
    "        A = to_dense_adj(edge_index, batch=batch).squeeze(1) # (B,L0,L0)\n",
    "        B, L0, _ = A.shape\n",
    "        H = self.n_heads\n",
    "        device = A.device\n",
    "\n",
    "        valid = ~key_padding_mask# (B,L0)\n",
    "        valid2d = valid.unsqueeze(2) & valid.unsqueeze(1) # (B,L0,L0)\n",
    "\n",
    "        # geometry\n",
    "        if self.use_geo and (pos is not None):\n",
    "            pad_pos, _ = to_dense_batch(pos, batch) # (B,L0,3)\n",
    "            diff = pad_pos.unsqueeze(2) - pad_pos.unsqueeze(1) # (B,L0,L0,3)\n",
    "            dist = torch.sqrt(torch.clamp((diff**2).sum(-1), min=0.0)) # (B,L0,L0)\n",
    "            centers = self.centers.to(dist.device)\n",
    "            rbf = torch.exp(-self.rbf_beta * (dist.unsqueeze(-1) - centers)**2)\n",
    "            geo = self.geo_mlp(rbf).permute(0, 3, 1, 2).contiguous() # (B,H,L0,L0)\n",
    "        else:\n",
    "            geo = torch.zeros((B, H, L0, L0), device=device)\n",
    "\n",
    "        # adjacency constant per head\n",
    "        if self.use_adj_const:\n",
    "            adj = A.unsqueeze(1) * self.adj_bias.view(1, H, 1, 1) # (B,H,L0,L0)\n",
    "        else:\n",
    "            adj = torch.zeros_like(geo)\n",
    "\n",
    "        # SPD\n",
    "        if self.use_spd and (hops is not None):\n",
    "            spd = self._spd_bias(hops, valid2d) # (B,H,L0,L0)\n",
    "        else:\n",
    "            spd = torch.zeros_like(geo)\n",
    "\n",
    "        # edge categorical\n",
    "        if self.use_edge_bias and (edge_attr is not None):\n",
    "            edg = self._edge_bias(edge_index, edge_attr, batch, L0, ptr) # (B,H,L0,L0)\n",
    "        else:\n",
    "            edg = torch.zeros_like(geo)\n",
    "\n",
    "        # normalize and bound each component, then scale \n",
    "        if self.use_headnorm:\n",
    "            if self.use_geo:\n",
    "                geo = self._apply_ln_heads(geo,  self.ln_geo)\n",
    "            if self.use_spd:  \n",
    "                spd = self._apply_ln_heads(spd,  self.ln_spd)\n",
    "            if self.use_edge_bias: \n",
    "                edg = self._apply_ln_heads(edg, self.ln_edge)\n",
    "\n",
    "        # gently bound to keep attention stable\n",
    "        if self.use_geo:       \n",
    "            geo = self._bound(geo)\n",
    "        if self.use_spd:       \n",
    "            spd = self._bound(spd)\n",
    "        if self.use_edge_bias: \n",
    "            edg = self._bound(edg)\n",
    "        # typically don't bound adj because it’s already a small learned scalar per head\n",
    "\n",
    "        bias = (self.alpha_geo  * geo + self.alpha_spd  * spd + self.alpha_adj  * adj + self.alpha_edge * edg)\n",
    "\n",
    "        # mask PAD rows/cols. keep diagonal 0 for valid tokens\n",
    "        pad = key_padding_mask\n",
    "        big_neg = torch.tensor(-1e4, device=bias.device, dtype=bias.dtype)\n",
    "        bias = bias.masked_fill(pad.view(B, 1, L0, 1), big_neg)\n",
    "        bias = bias.masked_fill(pad.view(B, 1, 1, L0), big_neg)\n",
    "        I = torch.eye(L0, device=device, dtype=torch.bool).view(1, 1, L0, L0)\n",
    "        bias = torch.where(I, bias.new_zeros(()), bias)\n",
    "\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "026345f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINEBlock(nn.Module):\n",
    "    def __init__(self, dim, activation=\"silu\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.conv = GINEConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim, dim), act, \n",
    "                nn.Linear(dim, dim)\n",
    "                ))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, 2*dim), act, \n",
    "            nn.Dropout(dropout), \n",
    "            nn.Linear(2*dim, dim)\n",
    "            )\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_emb):\n",
    "        h = self.conv(self.norm1(x), edge_index, edge_emb)\n",
    "        x = x + self.drop1(h)\n",
    "        x = x + self.drop2(self.ffn(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class EdgeEncoderMixed(nn.Module):\n",
    "    def __init__(self, emb_dim: int, cont_dim: int = 32, activation=\"silu\"):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.emb0 = nn.Embedding(5, emb_dim)\n",
    "        self.emb1 = nn.Embedding(6, emb_dim)\n",
    "        self.emb2 = nn.Embedding(2, emb_dim)\n",
    "        self.mlp_cont = nn.Sequential(\n",
    "            nn.Linear(cont_dim, emb_dim), act,\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.LayerNorm(emb_dim),      \n",
    "        )\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        cat  = edge_attr[:, :3].long()\n",
    "        cont = edge_attr[:, 3:].float()\n",
    "        e_cat  = self.emb0(cat[:,0]) + self.emb1(cat[:,1]) + self.emb2(cat[:,2])\n",
    "        e_cont = self.mlp_cont(cont)\n",
    "        return e_cat + 0.5 * e_cont # gentle scale on cont branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f224f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTransformerGPS(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 8,\n",
    "        nlayers: int = 6,\n",
    "        dropout: float = 0.2,\n",
    "        drop_path: float = 0.0, # (kept for future work)\n",
    "        activation: str = \"silu\",\n",
    "        rdkit_dim: int = 15,\n",
    "        use_extra_atom_feats: bool = True,\n",
    "        extra_atom_dim: int = 5,\n",
    "        # local GNN (GPS) settings\n",
    "        local_layers: int = 2,\n",
    "        use_mixed_edges: bool = True,\n",
    "        cont_dim: int = 32,\n",
    "        # bias knobs\n",
    "        use_geo_bias: bool = True,\n",
    "        use_spd_bias: bool = True,\n",
    "        spd_max: int = 5,\n",
    "        use_adj_const: bool = True,\n",
    "        use_edge_bias: bool = True,\n",
    "        # readout\n",
    "        use_cls: bool = True,\n",
    "        use_has_xyz: bool = True,\n",
    "        head_hidden: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.use_cls = use_cls\n",
    "        self.use_has_xyz = use_has_xyz\n",
    "        self.use_extra_atom_feats = use_extra_atom_feats\n",
    "        self.bias_builder = AttnBiasFull(\n",
    "            n_heads=nhead,\n",
    "            rbf_k=32,\n",
    "            rbf_beta=5.0,\n",
    "            use_geo_bias=use_geo_bias,          \n",
    "            use_adj_bias=use_adj_const,        \n",
    "            use_spd=use_spd_bias,             \n",
    "            spd_buckets=(spd_max + 1), # was spd_max; +1 gives the \">= spd_max\" bucket\n",
    "            use_edge_bias=use_edge_bias,\n",
    "            edge_cats=(5, 6, 2),\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "\n",
    "        act = _act(activation)\n",
    "\n",
    "        # encoders\n",
    "        self.atom_enc = AtomEncoder(emb_dim=d_model)\n",
    "        if use_extra_atom_feats:\n",
    "            self.extra_proj = nn.Sequential(\n",
    "                nn.Linear(extra_atom_dim, d_model), act, \n",
    "                nn.Linear(d_model, d_model)\n",
    "                )\n",
    "            self.extra_gate = nn.Sequential(\n",
    "                nn.Linear(2*d_model, d_model), act\n",
    "                )\n",
    "\n",
    "        # local GNN stack\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        if use_mixed_edges:\n",
    "            self.edge_enc = EdgeEncoderMixed(d_model, cont_dim=cont_dim, activation=activation)\n",
    "        else:\n",
    "            self.edge_enc = BondEncoder(emb_dim=d_model)\n",
    "        self.local_blocks = nn.ModuleList([GINEBlock(d_model, activation=activation, dropout=dropout) for _ in range(local_layers)])\n",
    "\n",
    "        # transformer stack (PyTorch encoder)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=4*d_model,\n",
    "            dropout=dropout, \n",
    "            activation=activation, \n",
    "            batch_first=True, \n",
    "            norm_first=True\n",
    "            )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers, enable_nested_tensor=False)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        # readout: concat mean + max + (optional) CLS + attention pool\n",
    "        self.gate_pool = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model//2), act, \n",
    "            nn.Linear(d_model//2, 1)\n",
    "            )\n",
    "        # features: mean(d), max(d), attn(d) = 3d, (+cls d) optional, + rdkit, + has_xyz\n",
    "        pooled_dim = 3*d_model + (d_model if use_cls else 0)\n",
    "        head_in = pooled_dim + rdkit_dim + (1 if use_has_xyz else 0)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(head_in),\n",
    "            nn.Linear(head_in, head_hidden), act, \n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden, head_hidden//2), act, \n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden//2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # 1. atom encoding  and optional per-atom extras\n",
    "        x = self.atom_enc(data.x) # (N,D)\n",
    "        if getattr(self, \"use_extra_atom_feats\", False) and hasattr(data, \"extra_atom_feats\"):\n",
    "            xa = self.extra_proj(data.extra_atom_feats.float()) # (N,D)\n",
    "            x  = self.extra_gate(torch.cat([x, xa], dim=1)) # (N,D)\n",
    "\n",
    "        # 2. local GNN over sparse graph\n",
    "        e = self.edge_enc(data.edge_attr)\n",
    "        for blk in self.local_blocks:\n",
    "            x = blk(x, data.edge_index, e) # (N,D)\n",
    "\n",
    "        # 3. pack to dense (no CLS yet)\n",
    "        x_pad, valid = to_dense_batch(x, data.batch) # (B,L0,D)\n",
    "        B, L0, D = x_pad.shape\n",
    "        key_padding = ~valid # (B,L0) True == PAD\n",
    "\n",
    "        # 4, head-wise attention bias on L0 tokens (B,H,L0,L0), pre-CLS\n",
    "        # AttnBiasFull supports SPD, geometry, adjacency, edges\n",
    "        hops = getattr(data, \"hops\", None) # (B,MAX_NODES,MAX_NODES) or None\n",
    "        ptr = getattr(data, \"ptr\", None)\n",
    "        attn_bias = self.bias_builder(\n",
    "            pos=(data.pos if hasattr(data, \"pos\") else None),\n",
    "            edge_index=data.edge_index,\n",
    "            edge_attr=(data.edge_attr if hasattr(data, \"edge_attr\") else None),\n",
    "            batch=data.batch,\n",
    "            key_padding_mask=key_padding, # (B,L0), True=PAD\n",
    "            hops=getattr(data, \"hops\", None),\n",
    "            ptr=ptr\n",
    "        )  # (B,H,L0,L0)                                              \n",
    "\n",
    "        # 5. finalize bias (mask PAD rows/cols, keep diagonal 0), then optionally append CLS\n",
    "        B, H, L = attn_bias.shape[0], attn_bias.shape[1], attn_bias.shape[-1]\n",
    "        pad = key_padding # (B,L)\n",
    "        huge = attn_bias.new_tensor(-1e4)\n",
    "\n",
    "        # rows FROM PAD, cols TO PAD\n",
    "        attn_bias = attn_bias.masked_fill(pad.view(B, 1, L, 1), huge)\n",
    "        attn_bias = attn_bias.masked_fill(pad.view(B, 1, 1, L), huge)\n",
    "\n",
    "        # keep diagonal = 0 on valid tokens\n",
    "        I = torch.eye(L, device=attn_bias.device, dtype=torch.bool).view(1, 1, L, L)\n",
    "        attn_bias = torch.where(I, attn_bias.new_zeros(()), attn_bias)\n",
    "\n",
    "        # (optional) append CLS token at the end\n",
    "        if getattr(self, \"use_cls\", False):\n",
    "            # append CLS embedding\n",
    "            cls = self.cls_token.expand(B, 1, D)# (B,1,D)\n",
    "            x_pad = torch.cat([x_pad, cls], dim=1) # (B,L+1,D)\n",
    "\n",
    "            # extend key_padding: CLS is always valid (False)\n",
    "            key_padding = torch.cat(\n",
    "                [key_padding, torch.zeros(B, 1, dtype=torch.bool, device=x_pad.device)],\n",
    "                dim=1\n",
    "            ) # (B,L+1)\n",
    "\n",
    "            # pad bias by one row/col with zeros for CLS -> (B,H,L+1,L+1)\n",
    "            attn_bias = F.pad(attn_bias, (0, 1, 0, 1), value=0.0)\n",
    "            L = L + 1\n",
    "\n",
    "        # 6. transformer encoder with 3D additive mask (B*H,L,L)\n",
    "        attn_mask_3d = attn_bias.reshape(B * H, L, L).to(x_pad.dtype)\n",
    "        h = self.encoder( # returns (B,L,D) when batch_first=True\n",
    "            x_pad,\n",
    "            mask=attn_mask_3d, # additive float mask \n",
    "        )\n",
    "\n",
    "        # 7. pooling (mean + max + gated attention), plus optional CLS and then RDKit/has_xyz and head \n",
    "        # exclude CLS from token pools\n",
    "        h_tok = h[:, :L0, :] # (B,L0,D)\n",
    "        mask_f = valid.float()# (B,L0)\n",
    "\n",
    "        mean = (h_tok * mask_f.unsqueeze(-1)).sum(1) / (mask_f.sum(1, keepdim=True) + 1e-8)  # (B,D)\n",
    "        mmax, _ = (h_tok + (1.0 - mask_f.unsqueeze(-1)) * (-1e4)).max(dim=1) # (B,D)\n",
    "\n",
    "        gate_logits = self.gate_pool(h_tok).squeeze(-1)# (B,L0)\n",
    "        gate = torch.softmax(gate_logits.masked_fill(~valid, -1e4), dim=1)\n",
    "        attn_pool = (h_tok * gate.unsqueeze(-1)).sum(1) # (B,D)\n",
    "\n",
    "        parts = [mean, mmax, attn_pool]\n",
    "\n",
    "        if getattr(self, \"use_cls\", False):\n",
    "            parts.append(h[:, L-1, :]) # CLS vector (B,D)\n",
    "\n",
    "        # RDKit globals\n",
    "        rd = data.rdkit_feats.view(B, -1).float() # (B, rdkit_dim)\n",
    "        parts.append(rd)\n",
    "\n",
    "        # optional has_xyz scalar if present\n",
    "        if getattr(self, \"use_has_xyz\", False) and hasattr(data, \"has_xyz\"):\n",
    "            parts.append(data.has_xyz.view(B, 1).float())\n",
    "\n",
    "        out = torch.cat(parts, dim=1)\n",
    "        return self.head(out) # (B,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9f3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: n=10 tokens=960 sumL2=92160p50=96 p90=96 p95=96 p99=96 max=96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mem:after_epoch_1] allocated=105.3MB reserved=174.0MB\n",
      "Epoch 001 | tr_MSE 22743.43666 | val_MAE 100.13499 | val_RMSE 130.59550 | R2 -0.7866\n",
      "[mem:after_epoch_2] allocated=127.2MB reserved=168.0MB\n",
      "Epoch 002 | tr_MSE 22437.99427 | val_MAE 97.81074 | val_RMSE 127.86028 | R2 -0.7125\n",
      "[mem:after_epoch_3] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 003 | tr_MSE 20776.69352 | val_MAE 89.93953 | val_RMSE 117.81510 | R2 -0.4540\n",
      "[mem:after_epoch_4] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 004 | tr_MSE 16665.19699 | val_MAE 79.67222 | val_RMSE 100.28962 | R2 -0.0536\n",
      "[mem:after_epoch_5] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 005 | tr_MSE 13238.96151 | val_MAE 83.08064 | val_RMSE 99.82089 | R2 -0.0438\n",
      "[mem:after_epoch_6] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 006 | tr_MSE 13465.15711 | val_MAE 86.04630 | val_RMSE 102.33657 | R2 -0.0971\n",
      "[mem:after_epoch_7] allocated=126.8MB reserved=188.0MB\n",
      "Epoch 007 | tr_MSE 9569.35558 | val_MAE 58.30879 | val_RMSE 76.18587 | R2 0.3920\n",
      "[mem:after_epoch_8] allocated=127.2MB reserved=172.0MB\n",
      "Epoch 008 | tr_MSE 7992.38150 | val_MAE 75.56903 | val_RMSE 92.24654 | R2 0.1086\n",
      "[mem:after_epoch_9] allocated=127.2MB reserved=188.0MB\n",
      "Epoch 009 | tr_MSE 11958.34258 | val_MAE 57.89855 | val_RMSE 77.28654 | R2 0.3743\n",
      "[mem:after_epoch_10] allocated=126.7MB reserved=192.0MB\n",
      "Epoch 010 | tr_MSE 7523.08560 | val_MAE 62.34563 | val_RMSE 78.58416 | R2 0.3531\n",
      "[mem:after_epoch_11] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 011 | tr_MSE 7714.60725 | val_MAE 56.01862 | val_RMSE 72.83414 | R2 0.4443\n",
      "[mem:after_epoch_12] allocated=127.2MB reserved=172.0MB\n",
      "Epoch 012 | tr_MSE 7884.04507 | val_MAE 58.66296 | val_RMSE 75.06502 | R2 0.4097\n",
      "[mem:after_epoch_13] allocated=127.2MB reserved=184.0MB\n",
      "Epoch 013 | tr_MSE 7860.81037 | val_MAE 79.00042 | val_RMSE 107.63586 | R2 -0.2136\n",
      "[mem:after_epoch_14] allocated=127.4MB reserved=172.0MB\n",
      "Epoch 014 | tr_MSE 7726.92604 | val_MAE 60.67120 | val_RMSE 79.38448 | R2 0.3399\n",
      "[mem:after_epoch_15] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 015 | tr_MSE 7832.43807 | val_MAE 54.40394 | val_RMSE 72.18687 | R2 0.4541\n",
      "[mem:after_epoch_16] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 016 | tr_MSE 7716.98992 | val_MAE 57.74386 | val_RMSE 72.90991 | R2 0.4431\n",
      "[mem:after_epoch_17] allocated=126.8MB reserved=194.0MB\n",
      "Epoch 017 | tr_MSE 5968.43640 | val_MAE 54.99232 | val_RMSE 69.17044 | R2 0.4988\n",
      "[mem:after_epoch_18] allocated=126.7MB reserved=170.0MB\n",
      "Epoch 018 | tr_MSE 6714.48467 | val_MAE 59.11290 | val_RMSE 76.08173 | R2 0.3936\n",
      "[mem:after_epoch_19] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 019 | tr_MSE 6635.18486 | val_MAE 57.38637 | val_RMSE 71.61824 | R2 0.4627\n",
      "[mem:after_epoch_20] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 020 | tr_MSE 7827.31176 | val_MAE 63.65836 | val_RMSE 85.29588 | R2 0.2379\n",
      "[mem:after_epoch_21] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 021 | tr_MSE 6621.51905 | val_MAE 103.07732 | val_RMSE 124.81083 | R2 -0.6318\n",
      "[mem:after_epoch_22] allocated=126.7MB reserved=172.0MB\n",
      "Epoch 022 | tr_MSE 13003.81057 | val_MAE 70.14149 | val_RMSE 88.04841 | R2 0.1879\n",
      "[mem:after_epoch_23] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 023 | tr_MSE 9106.10539 | val_MAE 70.88667 | val_RMSE 87.77927 | R2 0.1929\n",
      "[mem:after_epoch_24] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 024 | tr_MSE 9191.54521 | val_MAE 59.16396 | val_RMSE 74.36352 | R2 0.4207\n",
      "[mem:after_epoch_25] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 025 | tr_MSE 8473.70719 | val_MAE 72.08868 | val_RMSE 92.67309 | R2 0.1003\n",
      "[mem:after_epoch_26] allocated=126.7MB reserved=170.0MB\n",
      "Epoch 026 | tr_MSE 8665.46951 | val_MAE 62.06993 | val_RMSE 80.58902 | R2 0.3197\n",
      "[mem:after_epoch_27] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 027 | tr_MSE 6674.20672 | val_MAE 66.92905 | val_RMSE 85.02922 | R2 0.2426\n",
      "[mem:after_epoch_28] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 028 | tr_MSE 6621.83741 | val_MAE 53.31760 | val_RMSE 66.83282 | R2 0.5321\n",
      "[mem:after_epoch_29] allocated=127.2MB reserved=172.0MB\n",
      "Epoch 029 | tr_MSE 6270.71933 | val_MAE 49.93450 | val_RMSE 63.53250 | R2 0.5772\n",
      "[mem:after_epoch_30] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 030 | tr_MSE 6404.77366 | val_MAE 56.35358 | val_RMSE 68.78466 | R2 0.5044\n",
      "[mem:after_epoch_31] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 031 | tr_MSE 5853.09618 | val_MAE 52.19372 | val_RMSE 65.39328 | R2 0.5520\n",
      "[mem:after_epoch_32] allocated=126.8MB reserved=190.0MB\n",
      "Epoch 032 | tr_MSE 6397.53270 | val_MAE 50.88629 | val_RMSE 66.14538 | R2 0.5417\n",
      "[mem:after_epoch_33] allocated=126.7MB reserved=168.0MB\n",
      "Epoch 033 | tr_MSE 5819.79262 | val_MAE 56.16352 | val_RMSE 69.02672 | R2 0.5009\n",
      "[mem:after_epoch_34] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 034 | tr_MSE 6834.60826 | val_MAE 52.08648 | val_RMSE 66.10616 | R2 0.5422\n",
      "[mem:after_epoch_35] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 035 | tr_MSE 5631.44508 | val_MAE 51.36248 | val_RMSE 64.31799 | R2 0.5667\n",
      "[mem:after_epoch_36] allocated=126.8MB reserved=190.0MB\n",
      "Epoch 036 | tr_MSE 7005.67394 | val_MAE 78.55319 | val_RMSE 102.56734 | R2 -0.1020\n",
      "[mem:after_epoch_37] allocated=126.7MB reserved=172.0MB\n",
      "Epoch 037 | tr_MSE 7453.74740 | val_MAE 58.48658 | val_RMSE 73.01393 | R2 0.4416\n",
      "[mem:after_epoch_38] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 038 | tr_MSE 6865.57134 | val_MAE 54.50996 | val_RMSE 69.05535 | R2 0.5005\n",
      "[mem:after_epoch_39] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 039 | tr_MSE 6132.84266 | val_MAE 63.20940 | val_RMSE 76.79006 | R2 0.3823\n",
      "[mem:after_epoch_40] allocated=126.8MB reserved=190.0MB\n",
      "Epoch 040 | tr_MSE 6834.37781 | val_MAE 55.89474 | val_RMSE 80.46604 | R2 0.3217\n",
      "[mem:after_epoch_41] allocated=126.7MB reserved=168.0MB\n",
      "Epoch 041 | tr_MSE 6163.69728 | val_MAE 53.09635 | val_RMSE 66.21275 | R2 0.5407\n",
      "[mem:after_epoch_42] allocated=126.7MB reserved=182.0MB\n",
      "Epoch 042 | tr_MSE 6388.68889 | val_MAE 64.92811 | val_RMSE 88.48468 | R2 0.1798\n",
      "[mem:after_epoch_43] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 043 | tr_MSE 6144.12848 | val_MAE 55.92867 | val_RMSE 76.45598 | R2 0.3877\n",
      "[mem:after_epoch_44] allocated=126.8MB reserved=190.0MB\n",
      "Epoch 044 | tr_MSE 5958.25078 | val_MAE 53.96611 | val_RMSE 67.58777 | R2 0.5215\n",
      "[mem:after_epoch_45] allocated=126.7MB reserved=168.0MB\n",
      "Epoch 045 | tr_MSE 7184.04823 | val_MAE 53.15918 | val_RMSE 68.39206 | R2 0.5100\n",
      "[mem:after_epoch_46] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 046 | tr_MSE 6013.94163 | val_MAE 51.74847 | val_RMSE 65.68095 | R2 0.5481\n",
      "[mem:after_epoch_47] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 047 | tr_MSE 6831.68439 | val_MAE 66.66019 | val_RMSE 82.77471 | R2 0.2823\n",
      "[mem:after_epoch_48] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 048 | tr_MSE 6446.71225 | val_MAE 57.65639 | val_RMSE 72.27160 | R2 0.4529\n",
      "[mem:after_epoch_49] allocated=126.7MB reserved=166.0MB\n",
      "Epoch 049 | tr_MSE 6661.84555 | val_MAE 49.88387 | val_RMSE 63.43606 | R2 0.5785\n",
      "[mem:after_epoch_50] allocated=127.2MB reserved=186.0MB\n",
      "Epoch 050 | tr_MSE 6123.49926 | val_MAE 49.35501 | val_RMSE 62.28357 | R2 0.5936\n",
      "[mem:after_epoch_51] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 051 | tr_MSE 6385.09256 | val_MAE 58.60353 | val_RMSE 73.45927 | R2 0.4347\n",
      "[mem:after_epoch_52] allocated=126.8MB reserved=190.0MB\n",
      "Epoch 052 | tr_MSE 6113.11387 | val_MAE 52.77769 | val_RMSE 66.97127 | R2 0.5302\n",
      "[mem:after_epoch_53] allocated=126.7MB reserved=176.0MB\n",
      "Epoch 053 | tr_MSE 5995.39178 | val_MAE 52.72565 | val_RMSE 67.00075 | R2 0.5297\n",
      "[mem:after_epoch_54] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 054 | tr_MSE 5873.75082 | val_MAE 55.19578 | val_RMSE 70.68126 | R2 0.4767\n",
      "[mem:after_epoch_55] allocated=126.7MB reserved=190.0MB\n",
      "Epoch 055 | tr_MSE 6187.61223 | val_MAE 53.50765 | val_RMSE 68.14268 | R2 0.5136\n",
      "[mem:after_epoch_56] allocated=126.8MB reserved=190.0MB\n",
      "Epoch 056 | tr_MSE 6370.46186 | val_MAE 52.77343 | val_RMSE 66.97607 | R2 0.5301\n",
      "[mem:after_epoch_57] allocated=126.7MB reserved=174.0MB\n",
      "Epoch 057 | tr_MSE 5930.05370 | val_MAE 53.50237 | val_RMSE 66.78238 | R2 0.5328\n",
      "[mem:after_epoch_58] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 058 | tr_MSE 6135.36871 | val_MAE 50.57993 | val_RMSE 64.08234 | R2 0.5698\n",
      "[mem:after_epoch_59] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 059 | tr_MSE 5507.62732 | val_MAE 52.08994 | val_RMSE 66.62324 | R2 0.5350\n",
      "[mem:after_epoch_60] allocated=126.8MB reserved=194.0MB\n",
      "Epoch 060 | tr_MSE 6074.24446 | val_MAE 57.37558 | val_RMSE 72.46550 | R2 0.4499\n",
      "[mem:after_epoch_61] allocated=126.7MB reserved=170.0MB\n",
      "Epoch 061 | tr_MSE 6017.87203 | val_MAE 52.34212 | val_RMSE 66.15954 | R2 0.5415\n",
      "[mem:after_epoch_62] allocated=126.7MB reserved=188.0MB\n",
      "Epoch 062 | tr_MSE 5784.56396 | val_MAE 55.07295 | val_RMSE 68.94546 | R2 0.5021\n",
      "[mem:after_epoch_63] allocated=126.7MB reserved=192.0MB\n",
      "Epoch 063 | tr_MSE 5880.00338 | val_MAE 51.89056 | val_RMSE 65.32556 | R2 0.5530\n",
      "[mem:after_epoch_64] allocated=126.8MB reserved=188.0MB\n",
      "Epoch 064 | tr_MSE 6306.06327 | val_MAE 50.92554 | val_RMSE 64.85198 | R2 0.5594\n",
      "[mem:after_epoch_65] allocated=126.7MB reserved=170.0MB\n",
      "Epoch 065 | tr_MSE 5987.11652 | val_MAE 51.23039 | val_RMSE 63.09172 | R2 0.5830\n",
      "[mem:after_epoch_66] allocated=126.7MB reserved=186.0MB\n",
      "Epoch 066 | tr_MSE 6235.86070 | val_MAE 57.04647 | val_RMSE 71.96075 | R2 0.4575\n",
      "[mem:after_epoch_67] allocated=126.7MB reserved=192.0MB\n",
      "Epoch 067 | tr_MSE 6356.39509 | val_MAE 51.49244 | val_RMSE 64.45718 | R2 0.5648\n",
      "[mem:after_epoch_68] allocated=126.8MB reserved=192.0MB\n",
      "Epoch 068 | tr_MSE 6470.90434 | val_MAE 49.69518 | val_RMSE 62.01041 | R2 0.5972\n",
      "[mem:after_epoch_69] allocated=126.7MB reserved=172.0MB\n",
      "Epoch 069 | tr_MSE 6076.75436 | val_MAE 54.46503 | val_RMSE 68.44391 | R2 0.5093\n",
      "[mem:after_epoch_70] allocated=126.7MB reserved=184.0MB\n",
      "Epoch 070 | tr_MSE 5851.12243 | val_MAE 51.44966 | val_RMSE 64.07404 | R2 0.5699\n",
      "Early stopping.\n",
      "[graphtransformer_tg_spd] Best Val — MAE 49.355015 | RMSE 62.283577 | R2 0.5936\n",
      "[mem:after_Tg] allocated=16.2MB reserved=40.0MB\n",
      "L: n=520 tokens=3840 sumL2=29700p50=8 p90=9 p95=9 p99=9 max=9\n",
      "[mem:after_epoch_1] allocated=102.4MB reserved=178.0MB\n",
      "Epoch 001 | tr_MSE 0.38554 | val_MAE 0.26228 | val_RMSE 0.31694 | R2 -4.7205\n",
      "[mem:after_epoch_2] allocated=124.2MB reserved=200.0MB\n",
      "Epoch 002 | tr_MSE 0.14461 | val_MAE 0.21958 | val_RMSE 0.28038 | R2 -3.4768\n",
      "[mem:after_epoch_3] allocated=123.8MB reserved=186.0MB\n",
      "Epoch 003 | tr_MSE 0.06896 | val_MAE 0.14401 | val_RMSE 0.19568 | R2 -1.1806\n",
      "[mem:after_epoch_4] allocated=124.2MB reserved=178.0MB\n",
      "Epoch 004 | tr_MSE 0.03990 | val_MAE 0.10260 | val_RMSE 0.14407 | R2 -0.1821\n",
      "[mem:after_epoch_5] allocated=123.8MB reserved=178.0MB\n",
      "Epoch 005 | tr_MSE 0.03386 | val_MAE 0.09249 | val_RMSE 0.11566 | R2 0.2383\n",
      "[mem:after_epoch_6] allocated=124.2MB reserved=174.0MB\n",
      "Epoch 006 | tr_MSE 0.02138 | val_MAE 0.07871 | val_RMSE 0.11935 | R2 0.1889\n",
      "[mem:after_epoch_7] allocated=123.8MB reserved=186.0MB\n",
      "Epoch 007 | tr_MSE 0.01788 | val_MAE 0.07517 | val_RMSE 0.10501 | R2 0.3721\n",
      "[mem:after_epoch_8] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 008 | tr_MSE 0.01479 | val_MAE 0.07502 | val_RMSE 0.10239 | R2 0.4030\n",
      "[mem:after_epoch_9] allocated=123.8MB reserved=176.0MB\n",
      "Epoch 009 | tr_MSE 0.01374 | val_MAE 0.15573 | val_RMSE 0.18782 | R2 -1.0089\n",
      "[mem:after_epoch_10] allocated=123.8MB reserved=182.0MB\n",
      "Epoch 010 | tr_MSE 0.02157 | val_MAE 0.07690 | val_RMSE 0.11270 | R2 0.2767\n",
      "[mem:after_epoch_11] allocated=123.8MB reserved=182.0MB\n",
      "Epoch 011 | tr_MSE 0.01249 | val_MAE 0.07700 | val_RMSE 0.11003 | R2 0.3106\n",
      "[mem:after_epoch_12] allocated=123.8MB reserved=182.0MB\n",
      "Epoch 012 | tr_MSE 0.01368 | val_MAE 0.07114 | val_RMSE 0.11327 | R2 0.2694\n",
      "[mem:after_epoch_13] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 013 | tr_MSE 0.01115 | val_MAE 0.10972 | val_RMSE 0.14323 | R2 -0.1682\n",
      "[mem:after_epoch_14] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 014 | tr_MSE 0.01464 | val_MAE 0.15021 | val_RMSE 0.18134 | R2 -0.8726\n",
      "[mem:after_epoch_15] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 015 | tr_MSE 0.02039 | val_MAE 0.05749 | val_RMSE 0.09174 | R2 0.5207\n",
      "[mem:after_epoch_16] allocated=123.8MB reserved=184.0MB\n",
      "Epoch 016 | tr_MSE 0.01218 | val_MAE 0.05589 | val_RMSE 0.09141 | R2 0.5241\n",
      "[mem:after_epoch_17] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 017 | tr_MSE 0.00733 | val_MAE 0.05702 | val_RMSE 0.09826 | R2 0.4502\n",
      "[mem:after_epoch_18] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 018 | tr_MSE 0.00749 | val_MAE 0.06368 | val_RMSE 0.09619 | R2 0.4730\n",
      "[mem:after_epoch_19] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 019 | tr_MSE 0.00893 | val_MAE 0.05892 | val_RMSE 0.09748 | R2 0.4589\n",
      "[mem:after_epoch_20] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 020 | tr_MSE 0.00636 | val_MAE 0.05463 | val_RMSE 0.09899 | R2 0.4419\n",
      "[mem:after_epoch_21] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 021 | tr_MSE 0.01013 | val_MAE 0.05572 | val_RMSE 0.09147 | R2 0.5236\n",
      "[mem:after_epoch_22] allocated=123.8MB reserved=180.0MB\n",
      "Epoch 022 | tr_MSE 0.00681 | val_MAE 0.04792 | val_RMSE 0.08842 | R2 0.5548\n",
      "[mem:after_epoch_23] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 023 | tr_MSE 0.00643 | val_MAE 0.04860 | val_RMSE 0.08515 | R2 0.5871\n",
      "[mem:after_epoch_24] allocated=124.2MB reserved=176.0MB\n",
      "Epoch 024 | tr_MSE 0.00642 | val_MAE 0.04485 | val_RMSE 0.07952 | R2 0.6399\n",
      "[mem:after_epoch_25] allocated=123.8MB reserved=180.0MB\n",
      "Epoch 025 | tr_MSE 0.00562 | val_MAE 0.04829 | val_RMSE 0.08510 | R2 0.5876\n",
      "[mem:after_epoch_26] allocated=123.8MB reserved=188.0MB\n",
      "Epoch 026 | tr_MSE 0.00512 | val_MAE 0.08057 | val_RMSE 0.11142 | R2 0.2930\n",
      "[mem:after_epoch_27] allocated=123.8MB reserved=182.0MB\n",
      "Epoch 027 | tr_MSE 0.00869 | val_MAE 0.04580 | val_RMSE 0.08948 | R2 0.5440\n",
      "[mem:after_epoch_28] allocated=123.8MB reserved=182.0MB\n",
      "Epoch 028 | tr_MSE 0.00636 | val_MAE 0.04840 | val_RMSE 0.08847 | R2 0.5543\n",
      "[mem:after_epoch_29] allocated=123.8MB reserved=182.0MB\n",
      "Epoch 029 | tr_MSE 0.00678 | val_MAE 0.08134 | val_RMSE 0.10983 | R2 0.3131\n",
      "[mem:after_epoch_30] allocated=123.8MB reserved=182.0MB\n",
      "Epoch 030 | tr_MSE 0.00845 | val_MAE 0.05295 | val_RMSE 0.08769 | R2 0.5621\n",
      "[mem:after_epoch_31] allocated=123.8MB reserved=186.0MB\n",
      "Epoch 031 | tr_MSE 0.00780 | val_MAE 0.04912 | val_RMSE 0.09286 | R2 0.5090\n",
      "[mem:after_epoch_32] allocated=123.8MB reserved=180.0MB\n",
      "Epoch 032 | tr_MSE 0.00544 | val_MAE 0.07782 | val_RMSE 0.11001 | R2 0.3108\n",
      "[mem:after_epoch_33] allocated=123.8MB reserved=182.0MB\n",
      "Epoch 033 | tr_MSE 0.00801 | val_MAE 0.08722 | val_RMSE 0.12154 | R2 0.1588\n",
      "[mem:after_epoch_34] allocated=123.8MB reserved=184.0MB\n",
      "Epoch 034 | tr_MSE 0.00837 | val_MAE 0.06969 | val_RMSE 0.09975 | R2 0.4334\n",
      "[mem:after_epoch_35] allocated=123.8MB reserved=180.0MB\n",
      "Epoch 035 | tr_MSE 0.00733 | val_MAE 0.05091 | val_RMSE 0.09379 | R2 0.4991\n",
      "[mem:after_epoch_36] allocated=123.8MB reserved=182.0MB\n",
      "Epoch 036 | tr_MSE 0.00449 | val_MAE 0.04583 | val_RMSE 0.08584 | R2 0.5804\n",
      "[mem:after_epoch_37] allocated=123.8MB reserved=186.0MB\n",
      "Epoch 037 | tr_MSE 0.00419 | val_MAE 0.03957 | val_RMSE 0.08717 | R2 0.5673\n",
      "[mem:after_epoch_38] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 038 | tr_MSE 0.00382 | val_MAE 0.04020 | val_RMSE 0.07815 | R2 0.6522\n",
      "[mem:after_epoch_39] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 039 | tr_MSE 0.00396 | val_MAE 0.04172 | val_RMSE 0.08833 | R2 0.5556\n",
      "[mem:after_epoch_40] allocated=124.2MB reserved=176.0MB\n",
      "Epoch 040 | tr_MSE 0.00364 | val_MAE 0.04274 | val_RMSE 0.08463 | R2 0.5921\n",
      "[mem:after_epoch_41] allocated=124.2MB reserved=178.0MB\n",
      "Epoch 041 | tr_MSE 0.00355 | val_MAE 0.03720 | val_RMSE 0.07842 | R2 0.6498\n",
      "[mem:after_epoch_42] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 042 | tr_MSE 0.00326 | val_MAE 0.03812 | val_RMSE 0.07719 | R2 0.6607\n",
      "[mem:after_epoch_43] allocated=123.8MB reserved=180.0MB\n",
      "Epoch 043 | tr_MSE 0.00319 | val_MAE 0.04062 | val_RMSE 0.08590 | R2 0.5798\n",
      "[mem:after_epoch_44] allocated=124.2MB reserved=178.0MB\n",
      "Epoch 044 | tr_MSE 0.00318 | val_MAE 0.04186 | val_RMSE 0.08749 | R2 0.5641\n",
      "[mem:after_epoch_45] allocated=123.8MB reserved=178.0MB\n",
      "Epoch 045 | tr_MSE 0.00328 | val_MAE 0.05554 | val_RMSE 0.08982 | R2 0.5406\n",
      "[mem:after_epoch_46] allocated=124.2MB reserved=178.0MB\n",
      "Epoch 046 | tr_MSE 0.00408 | val_MAE 0.04180 | val_RMSE 0.08291 | R2 0.6086\n",
      "[mem:after_epoch_47] allocated=123.8MB reserved=184.0MB\n",
      "Epoch 047 | tr_MSE 0.00488 | val_MAE 0.03913 | val_RMSE 0.07939 | R2 0.6411\n",
      "[mem:after_epoch_48] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 048 | tr_MSE 0.00397 | val_MAE 0.05540 | val_RMSE 0.08874 | R2 0.5516\n",
      "[mem:after_epoch_49] allocated=123.8MB reserved=180.0MB\n",
      "Epoch 049 | tr_MSE 0.00464 | val_MAE 0.06265 | val_RMSE 0.10379 | R2 0.3865\n",
      "[mem:after_epoch_50] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 050 | tr_MSE 0.00599 | val_MAE 0.08597 | val_RMSE 0.11174 | R2 0.2890\n",
      "[mem:after_epoch_51] allocated=123.8MB reserved=176.0MB\n",
      "Epoch 051 | tr_MSE 0.00754 | val_MAE 0.05621 | val_RMSE 0.09259 | R2 0.5118\n",
      "[mem:after_epoch_52] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 052 | tr_MSE 0.00488 | val_MAE 0.05289 | val_RMSE 0.08365 | R2 0.6015\n",
      "[mem:after_epoch_53] allocated=123.8MB reserved=180.0MB\n",
      "Epoch 053 | tr_MSE 0.00431 | val_MAE 0.04190 | val_RMSE 0.08572 | R2 0.5816\n",
      "[mem:after_epoch_54] allocated=124.2MB reserved=186.0MB\n",
      "Epoch 054 | tr_MSE 0.00413 | val_MAE 0.05252 | val_RMSE 0.08675 | R2 0.5714\n",
      "[mem:after_epoch_55] allocated=123.8MB reserved=182.0MB\n",
      "Epoch 055 | tr_MSE 0.00370 | val_MAE 0.03635 | val_RMSE 0.07966 | R2 0.6386\n",
      "[mem:after_epoch_56] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 056 | tr_MSE 0.00321 | val_MAE 0.04079 | val_RMSE 0.07865 | R2 0.6477\n",
      "[mem:after_epoch_57] allocated=124.2MB reserved=178.0MB\n",
      "Epoch 057 | tr_MSE 0.00304 | val_MAE 0.04061 | val_RMSE 0.08638 | R2 0.5750\n",
      "[mem:after_epoch_58] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 058 | tr_MSE 0.00299 | val_MAE 0.04301 | val_RMSE 0.08598 | R2 0.5790\n",
      "[mem:after_epoch_59] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 059 | tr_MSE 0.00291 | val_MAE 0.04760 | val_RMSE 0.08187 | R2 0.6183\n",
      "[mem:after_epoch_60] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 060 | tr_MSE 0.00319 | val_MAE 0.04802 | val_RMSE 0.08581 | R2 0.5807\n",
      "[mem:after_epoch_61] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 061 | tr_MSE 0.00396 | val_MAE 0.05417 | val_RMSE 0.08444 | R2 0.5940\n",
      "[mem:after_epoch_62] allocated=124.2MB reserved=188.0MB\n",
      "Epoch 062 | tr_MSE 0.00381 | val_MAE 0.03772 | val_RMSE 0.08008 | R2 0.6348\n",
      "[mem:after_epoch_63] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 063 | tr_MSE 0.00261 | val_MAE 0.04302 | val_RMSE 0.08680 | R2 0.5710\n",
      "[mem:after_epoch_64] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 064 | tr_MSE 0.00264 | val_MAE 0.03837 | val_RMSE 0.08210 | R2 0.6161\n",
      "[mem:after_epoch_65] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 065 | tr_MSE 0.00247 | val_MAE 0.04636 | val_RMSE 0.08170 | R2 0.6199\n",
      "[mem:after_epoch_66] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 066 | tr_MSE 0.00266 | val_MAE 0.03572 | val_RMSE 0.07708 | R2 0.6616\n",
      "[mem:after_epoch_67] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 067 | tr_MSE 0.00249 | val_MAE 0.03995 | val_RMSE 0.08597 | R2 0.5791\n",
      "[mem:after_epoch_68] allocated=123.8MB reserved=180.0MB\n",
      "Epoch 068 | tr_MSE 0.00233 | val_MAE 0.04901 | val_RMSE 0.08703 | R2 0.5687\n",
      "[mem:after_epoch_69] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 069 | tr_MSE 0.00260 | val_MAE 0.03863 | val_RMSE 0.08332 | R2 0.6046\n",
      "[mem:after_epoch_70] allocated=123.8MB reserved=184.0MB\n",
      "Epoch 070 | tr_MSE 0.00267 | val_MAE 0.03682 | val_RMSE 0.07127 | R2 0.7107\n",
      "[mem:after_epoch_71] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 071 | tr_MSE 0.00215 | val_MAE 0.04278 | val_RMSE 0.07618 | R2 0.6695\n",
      "[mem:after_epoch_72] allocated=123.8MB reserved=176.0MB\n",
      "Epoch 072 | tr_MSE 0.00228 | val_MAE 0.04366 | val_RMSE 0.07850 | R2 0.6491\n",
      "[mem:after_epoch_73] allocated=124.2MB reserved=178.0MB\n",
      "Epoch 073 | tr_MSE 0.00224 | val_MAE 0.03332 | val_RMSE 0.07672 | R2 0.6648\n",
      "[mem:after_epoch_74] allocated=123.8MB reserved=184.0MB\n",
      "Epoch 074 | tr_MSE 0.00230 | val_MAE 0.03281 | val_RMSE 0.06653 | R2 0.7480\n",
      "[mem:after_epoch_75] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 075 | tr_MSE 0.00221 | val_MAE 0.05289 | val_RMSE 0.07906 | R2 0.6440\n",
      "[mem:after_epoch_76] allocated=124.2MB reserved=186.0MB\n",
      "Epoch 076 | tr_MSE 0.00253 | val_MAE 0.04880 | val_RMSE 0.07511 | R2 0.6787\n",
      "[mem:after_epoch_77] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 077 | tr_MSE 0.00279 | val_MAE 0.05303 | val_RMSE 0.08994 | R2 0.5394\n",
      "[mem:after_epoch_78] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 078 | tr_MSE 0.00346 | val_MAE 0.07822 | val_RMSE 0.09881 | R2 0.4440\n",
      "[mem:after_epoch_79] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 079 | tr_MSE 0.00506 | val_MAE 0.03865 | val_RMSE 0.07569 | R2 0.6738\n",
      "[mem:after_epoch_80] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 080 | tr_MSE 0.00295 | val_MAE 0.04199 | val_RMSE 0.07889 | R2 0.6456\n",
      "[mem:after_epoch_81] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 081 | tr_MSE 0.00228 | val_MAE 0.04460 | val_RMSE 0.07773 | R2 0.6559\n",
      "[mem:after_epoch_82] allocated=124.2MB reserved=186.0MB\n",
      "Epoch 082 | tr_MSE 0.00230 | val_MAE 0.03649 | val_RMSE 0.07493 | R2 0.6803\n",
      "[mem:after_epoch_83] allocated=124.2MB reserved=186.0MB\n",
      "Epoch 083 | tr_MSE 0.00221 | val_MAE 0.04286 | val_RMSE 0.07268 | R2 0.6992\n",
      "[mem:after_epoch_84] allocated=124.2MB reserved=186.0MB\n",
      "Epoch 084 | tr_MSE 0.00219 | val_MAE 0.03916 | val_RMSE 0.07460 | R2 0.6831\n",
      "[mem:after_epoch_85] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 085 | tr_MSE 0.00184 | val_MAE 0.03347 | val_RMSE 0.07020 | R2 0.7194\n",
      "[mem:after_epoch_86] allocated=124.2MB reserved=188.0MB\n",
      "Epoch 086 | tr_MSE 0.00201 | val_MAE 0.05456 | val_RMSE 0.07842 | R2 0.6498\n",
      "[mem:after_epoch_87] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 087 | tr_MSE 0.00263 | val_MAE 0.04156 | val_RMSE 0.08350 | R2 0.6030\n",
      "[mem:after_epoch_88] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 088 | tr_MSE 0.00305 | val_MAE 0.04593 | val_RMSE 0.07845 | R2 0.6495\n",
      "[mem:after_epoch_89] allocated=124.2MB reserved=190.0MB\n",
      "Epoch 089 | tr_MSE 0.00233 | val_MAE 0.04178 | val_RMSE 0.07497 | R2 0.6800\n",
      "[mem:after_epoch_90] allocated=124.2MB reserved=182.0MB\n",
      "Epoch 090 | tr_MSE 0.00275 | val_MAE 0.03546 | val_RMSE 0.08430 | R2 0.5953\n",
      "[mem:after_epoch_91] allocated=124.2MB reserved=184.0MB\n",
      "Epoch 091 | tr_MSE 0.00228 | val_MAE 0.05087 | val_RMSE 0.08103 | R2 0.6261\n",
      "[mem:after_epoch_92] allocated=124.2MB reserved=188.0MB\n",
      "Epoch 092 | tr_MSE 0.00219 | val_MAE 0.04378 | val_RMSE 0.09289 | R2 0.5086\n",
      "[mem:after_epoch_93] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 093 | tr_MSE 0.00257 | val_MAE 0.05224 | val_RMSE 0.08059 | R2 0.6301\n",
      "[mem:after_epoch_94] allocated=124.2MB reserved=180.0MB\n",
      "Epoch 094 | tr_MSE 0.00286 | val_MAE 0.03753 | val_RMSE 0.07300 | R2 0.6965\n",
      "Early stopping.\n",
      "[graphtransformer_den_spd] Best Val — MAE 0.032809 | RMSE 0.066527 | R2 0.7480\n",
      "[mem:after_Density] allocated=16.2MB reserved=40.0MB\n",
      "L: n=510 tokens=3830 sumL2=29950p50=8 p90=9 p95=9 p99=9 max=9\n",
      "[mem:after_epoch_1] allocated=103.1MB reserved=178.0MB\n",
      "Epoch 001 | tr_MSE 288.57449 | val_MAE 16.11071 | val_RMSE 16.79092 | R2 -11.8101\n",
      "[mem:after_epoch_2] allocated=125.0MB reserved=370.0MB\n",
      "Epoch 002 | tr_MSE 241.84096 | val_MAE 11.54196 | val_RMSE 12.59178 | R2 -6.2041\n",
      "[mem:after_epoch_3] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 003 | tr_MSE 78.34864 | val_MAE 7.53548 | val_RMSE 8.75498 | R2 -2.4827\n",
      "[mem:after_epoch_4] allocated=124.7MB reserved=328.0MB\n",
      "Epoch 004 | tr_MSE 103.67880 | val_MAE 4.17199 | val_RMSE 4.80816 | R2 -0.0504\n",
      "[mem:after_epoch_5] allocated=124.6MB reserved=324.0MB\n",
      "Epoch 005 | tr_MSE 26.56506 | val_MAE 4.79496 | val_RMSE 5.60427 | R2 -0.4271\n",
      "[mem:after_epoch_6] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 006 | tr_MSE 38.41457 | val_MAE 4.08489 | val_RMSE 4.72275 | R2 -0.0134\n",
      "[mem:after_epoch_7] allocated=125.1MB reserved=182.0MB\n",
      "Epoch 007 | tr_MSE 29.94629 | val_MAE 4.22747 | val_RMSE 4.87954 | R2 -0.0818\n",
      "[mem:after_epoch_8] allocated=125.1MB reserved=182.0MB\n",
      "Epoch 008 | tr_MSE 28.33071 | val_MAE 4.10386 | val_RMSE 4.76513 | R2 -0.0317\n",
      "[mem:after_epoch_9] allocated=125.1MB reserved=178.0MB\n",
      "Epoch 009 | tr_MSE 29.04805 | val_MAE 4.84277 | val_RMSE 5.72002 | R2 -0.4866\n",
      "[mem:after_epoch_10] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 010 | tr_MSE 35.25097 | val_MAE 6.40822 | val_RMSE 7.26867 | R2 -1.4006\n",
      "[mem:after_epoch_11] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 011 | tr_MSE 57.69259 | val_MAE 3.32255 | val_RMSE 4.08505 | R2 0.2418\n",
      "[mem:after_epoch_12] allocated=124.7MB reserved=326.0MB\n",
      "Epoch 012 | tr_MSE 32.32410 | val_MAE 3.05878 | val_RMSE 4.06932 | R2 0.2476\n",
      "[mem:after_epoch_13] allocated=124.6MB reserved=328.0MB\n",
      "Epoch 013 | tr_MSE 20.50242 | val_MAE 2.75687 | val_RMSE 3.57576 | R2 0.4190\n",
      "[mem:after_epoch_14] allocated=125.1MB reserved=184.0MB\n",
      "Epoch 014 | tr_MSE 34.06466 | val_MAE 3.00368 | val_RMSE 4.09957 | R2 0.2364\n",
      "[mem:after_epoch_15] allocated=125.1MB reserved=180.0MB\n",
      "Epoch 015 | tr_MSE 17.46477 | val_MAE 4.62150 | val_RMSE 5.30542 | R2 -0.2789\n",
      "[mem:after_epoch_16] allocated=125.1MB reserved=182.0MB\n",
      "Epoch 016 | tr_MSE 21.67910 | val_MAE 2.60502 | val_RMSE 3.40219 | R2 0.4741\n",
      "[mem:after_epoch_17] allocated=124.7MB reserved=324.0MB\n",
      "Epoch 017 | tr_MSE 14.92548 | val_MAE 2.79070 | val_RMSE 3.51798 | R2 0.4377\n",
      "[mem:after_epoch_18] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 018 | tr_MSE 14.53221 | val_MAE 3.17463 | val_RMSE 3.86977 | R2 0.3196\n",
      "[mem:after_epoch_19] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 019 | tr_MSE 14.89127 | val_MAE 3.10443 | val_RMSE 3.77761 | R2 0.3516\n",
      "[mem:after_epoch_20] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 020 | tr_MSE 13.37348 | val_MAE 2.42164 | val_RMSE 3.22361 | R2 0.5278\n",
      "[mem:after_epoch_21] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 021 | tr_MSE 12.18353 | val_MAE 2.33666 | val_RMSE 3.21759 | R2 0.5296\n",
      "[mem:after_epoch_22] allocated=125.1MB reserved=178.0MB\n",
      "Epoch 022 | tr_MSE 13.13779 | val_MAE 2.33140 | val_RMSE 3.11789 | R2 0.5583\n",
      "[mem:after_epoch_23] allocated=124.7MB reserved=324.0MB\n",
      "Epoch 023 | tr_MSE 12.75346 | val_MAE 2.47643 | val_RMSE 3.31014 | R2 0.5022\n",
      "[mem:after_epoch_24] allocated=124.8MB reserved=320.0MB\n",
      "Epoch 024 | tr_MSE 11.77147 | val_MAE 2.46621 | val_RMSE 3.24182 | R2 0.5225\n",
      "[mem:after_epoch_25] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 025 | tr_MSE 10.89302 | val_MAE 2.20365 | val_RMSE 3.24507 | R2 0.5215\n",
      "[mem:after_epoch_26] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 026 | tr_MSE 11.08225 | val_MAE 2.58328 | val_RMSE 3.43701 | R2 0.4633\n",
      "[mem:after_epoch_27] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 027 | tr_MSE 16.25111 | val_MAE 2.40366 | val_RMSE 3.15568 | R2 0.5475\n",
      "[mem:after_epoch_28] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 028 | tr_MSE 11.17544 | val_MAE 2.23800 | val_RMSE 3.10278 | R2 0.5626\n",
      "[mem:after_epoch_29] allocated=124.6MB reserved=320.0MB\n",
      "Epoch 029 | tr_MSE 10.42655 | val_MAE 2.24048 | val_RMSE 3.10376 | R2 0.5623\n",
      "[mem:after_epoch_30] allocated=124.6MB reserved=326.0MB\n",
      "Epoch 030 | tr_MSE 10.81109 | val_MAE 2.24429 | val_RMSE 3.30917 | R2 0.5024\n",
      "[mem:after_epoch_31] allocated=124.6MB reserved=320.0MB\n",
      "Epoch 031 | tr_MSE 10.99703 | val_MAE 2.38953 | val_RMSE 3.36924 | R2 0.4842\n",
      "[mem:after_epoch_32] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 032 | tr_MSE 12.85687 | val_MAE 2.27092 | val_RMSE 3.11737 | R2 0.5584\n",
      "[mem:after_epoch_33] allocated=124.6MB reserved=324.0MB\n",
      "Epoch 033 | tr_MSE 9.26926 | val_MAE 2.99203 | val_RMSE 3.81185 | R2 0.3398\n",
      "[mem:after_epoch_34] allocated=124.6MB reserved=326.0MB\n",
      "Epoch 034 | tr_MSE 10.87145 | val_MAE 2.21718 | val_RMSE 3.24552 | R2 0.5214\n",
      "[mem:after_epoch_35] allocated=124.6MB reserved=324.0MB\n",
      "Epoch 035 | tr_MSE 10.50724 | val_MAE 2.77585 | val_RMSE 3.42547 | R2 0.4669\n",
      "[mem:after_epoch_36] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 036 | tr_MSE 9.79431 | val_MAE 2.49149 | val_RMSE 3.24300 | R2 0.5221\n",
      "[mem:after_epoch_37] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 037 | tr_MSE 9.27680 | val_MAE 2.26068 | val_RMSE 3.06649 | R2 0.5727\n",
      "[mem:after_epoch_38] allocated=124.6MB reserved=322.0MB\n",
      "Epoch 038 | tr_MSE 8.24129 | val_MAE 2.08872 | val_RMSE 3.01723 | R2 0.5864\n",
      "[mem:after_epoch_39] allocated=125.1MB reserved=182.0MB\n",
      "Epoch 039 | tr_MSE 8.73172 | val_MAE 2.34603 | val_RMSE 3.11845 | R2 0.5581\n",
      "[mem:after_epoch_40] allocated=125.1MB reserved=176.0MB\n",
      "Epoch 040 | tr_MSE 14.04312 | val_MAE 2.14010 | val_RMSE 3.01540 | R2 0.5869\n",
      "[mem:after_epoch_41] allocated=125.1MB reserved=178.0MB\n",
      "Epoch 041 | tr_MSE 15.78723 | val_MAE 2.02817 | val_RMSE 2.98836 | R2 0.5942\n",
      "[mem:after_epoch_42] allocated=124.7MB reserved=326.0MB\n",
      "Epoch 042 | tr_MSE 9.99645 | val_MAE 2.85365 | val_RMSE 3.65459 | R2 0.3931\n",
      "[mem:after_epoch_43] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 043 | tr_MSE 8.05778 | val_MAE 2.24099 | val_RMSE 3.23499 | R2 0.5245\n",
      "[mem:after_epoch_44] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 044 | tr_MSE 8.28073 | val_MAE 2.43829 | val_RMSE 3.40978 | R2 0.4717\n",
      "[mem:after_epoch_45] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 045 | tr_MSE 8.08190 | val_MAE 2.21222 | val_RMSE 3.19632 | R2 0.5358\n",
      "[mem:after_epoch_46] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 046 | tr_MSE 8.79288 | val_MAE 2.81664 | val_RMSE 3.75225 | R2 0.3603\n",
      "[mem:after_epoch_47] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 047 | tr_MSE 8.05934 | val_MAE 4.15691 | val_RMSE 4.83329 | R2 -0.0614\n",
      "[mem:after_epoch_48] allocated=124.8MB reserved=320.0MB\n",
      "Epoch 048 | tr_MSE 8.76568 | val_MAE 2.79713 | val_RMSE 3.67801 | R2 0.3853\n",
      "[mem:after_epoch_49] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 049 | tr_MSE 7.05386 | val_MAE 2.18667 | val_RMSE 3.16216 | R2 0.5457\n",
      "[mem:after_epoch_50] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 050 | tr_MSE 7.87109 | val_MAE 2.46511 | val_RMSE 3.45173 | R2 0.4586\n",
      "[mem:after_epoch_51] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 051 | tr_MSE 7.47483 | val_MAE 2.58292 | val_RMSE 3.54416 | R2 0.4293\n",
      "[mem:after_epoch_52] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 052 | tr_MSE 7.97091 | val_MAE 2.51616 | val_RMSE 3.36528 | R2 0.4854\n",
      "[mem:after_epoch_53] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 053 | tr_MSE 7.03657 | val_MAE 2.34771 | val_RMSE 3.25449 | R2 0.5187\n",
      "[mem:after_epoch_54] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 054 | tr_MSE 7.37665 | val_MAE 3.62539 | val_RMSE 4.37457 | R2 0.1305\n",
      "[mem:after_epoch_55] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 055 | tr_MSE 7.21482 | val_MAE 3.50100 | val_RMSE 4.25173 | R2 0.1786\n",
      "[mem:after_epoch_56] allocated=124.8MB reserved=324.0MB\n",
      "Epoch 056 | tr_MSE 8.05950 | val_MAE 3.44775 | val_RMSE 4.22409 | R2 0.1893\n",
      "[mem:after_epoch_57] allocated=124.8MB reserved=320.0MB\n",
      "Epoch 057 | tr_MSE 6.38603 | val_MAE 2.82101 | val_RMSE 3.73316 | R2 0.3668\n",
      "[mem:after_epoch_58] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 058 | tr_MSE 6.01819 | val_MAE 3.24044 | val_RMSE 4.12915 | R2 0.2253\n",
      "[mem:after_epoch_59] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 059 | tr_MSE 6.35085 | val_MAE 2.56784 | val_RMSE 3.50644 | R2 0.4414\n",
      "[mem:after_epoch_60] allocated=124.8MB reserved=322.0MB\n",
      "Epoch 060 | tr_MSE 6.58551 | val_MAE 3.25461 | val_RMSE 4.08376 | R2 0.2423\n",
      "[mem:after_epoch_61] allocated=124.8MB reserved=326.0MB\n",
      "Epoch 061 | tr_MSE 7.74827 | val_MAE 3.46920 | val_RMSE 4.31670 | R2 0.1533\n",
      "Early stopping.\n",
      "[graphtransformer_rg_spd] Best Val — MAE 2.028174 | RMSE 2.988360 | R2 0.5942\n",
      "[mem:after_Rg] allocated=16.2MB reserved=40.0MB\n",
      "L: n=620 tokens=4700 sumL2=36920p50=8 p90=9 p95=9 p99=9 max=9\n",
      "[mem:after_epoch_1] allocated=104.3MB reserved=146.0MB\n",
      "Epoch 001 | tr_MSE 0.02747 | val_MAE 0.12207 | val_RMSE 0.14629 | R2 -1.5061\n",
      "[mem:after_epoch_2] allocated=126.4MB reserved=358.0MB\n",
      "Epoch 002 | tr_MSE 0.01940 | val_MAE 0.12258 | val_RMSE 0.14478 | R2 -1.4548\n",
      "[mem:after_epoch_3] allocated=126.4MB reserved=172.0MB\n",
      "Epoch 003 | tr_MSE 0.01169 | val_MAE 0.08901 | val_RMSE 0.10918 | R2 -0.3960\n",
      "[mem:after_epoch_4] allocated=126.4MB reserved=358.0MB\n",
      "Epoch 004 | tr_MSE 0.00817 | val_MAE 0.07500 | val_RMSE 0.09530 | R2 -0.0635\n",
      "[mem:after_epoch_5] allocated=126.2MB reserved=344.0MB\n",
      "Epoch 005 | tr_MSE 0.00650 | val_MAE 0.05349 | val_RMSE 0.07556 | R2 0.3314\n",
      "[mem:after_epoch_6] allocated=126.4MB reserved=346.0MB\n",
      "Epoch 006 | tr_MSE 0.00513 | val_MAE 0.04744 | val_RMSE 0.06841 | R2 0.4520\n",
      "[mem:after_epoch_7] allocated=126.2MB reserved=170.0MB\n",
      "Epoch 007 | tr_MSE 0.00569 | val_MAE 0.06463 | val_RMSE 0.07939 | R2 0.2619\n",
      "[mem:after_epoch_8] allocated=126.3MB reserved=360.0MB\n",
      "Epoch 008 | tr_MSE 0.00604 | val_MAE 0.04158 | val_RMSE 0.05632 | R2 0.6286\n",
      "[mem:after_epoch_9] allocated=126.4MB reserved=360.0MB\n",
      "Epoch 009 | tr_MSE 0.00495 | val_MAE 0.06332 | val_RMSE 0.07607 | R2 0.3223\n",
      "[mem:after_epoch_10] allocated=126.4MB reserved=360.0MB\n",
      "Epoch 010 | tr_MSE 0.00376 | val_MAE 0.04778 | val_RMSE 0.06666 | R2 0.4796\n",
      "[mem:after_epoch_11] allocated=126.4MB reserved=360.0MB\n",
      "Epoch 011 | tr_MSE 0.00492 | val_MAE 0.05395 | val_RMSE 0.07248 | R2 0.3848\n",
      "[mem:after_epoch_12] allocated=126.4MB reserved=358.0MB\n",
      "Epoch 012 | tr_MSE 0.00416 | val_MAE 0.03279 | val_RMSE 0.04619 | R2 0.7501\n",
      "[mem:after_epoch_13] allocated=126.2MB reserved=346.0MB\n",
      "Epoch 013 | tr_MSE 0.00385 | val_MAE 0.04024 | val_RMSE 0.05519 | R2 0.6433\n",
      "[mem:after_epoch_14] allocated=126.2MB reserved=170.0MB\n",
      "Epoch 014 | tr_MSE 0.00364 | val_MAE 0.05346 | val_RMSE 0.06776 | R2 0.4623\n",
      "[mem:after_epoch_15] allocated=126.3MB reserved=358.0MB\n",
      "Epoch 015 | tr_MSE 0.00268 | val_MAE 0.04177 | val_RMSE 0.05533 | R2 0.6415\n",
      "[mem:after_epoch_16] allocated=126.2MB reserved=348.0MB\n",
      "Epoch 016 | tr_MSE 0.00277 | val_MAE 0.04146 | val_RMSE 0.05540 | R2 0.6406\n",
      "[mem:after_epoch_17] allocated=126.2MB reserved=168.0MB\n",
      "Epoch 017 | tr_MSE 0.00250 | val_MAE 0.03679 | val_RMSE 0.04992 | R2 0.7082\n",
      "[mem:after_epoch_18] allocated=126.3MB reserved=360.0MB\n",
      "Epoch 018 | tr_MSE 0.00206 | val_MAE 0.03917 | val_RMSE 0.05221 | R2 0.6808\n",
      "[mem:after_epoch_19] allocated=126.2MB reserved=348.0MB\n",
      "Epoch 019 | tr_MSE 0.00197 | val_MAE 0.03778 | val_RMSE 0.04902 | R2 0.7187\n",
      "[mem:after_epoch_20] allocated=126.2MB reserved=172.0MB\n",
      "Epoch 020 | tr_MSE 0.00269 | val_MAE 0.04408 | val_RMSE 0.05822 | R2 0.6031\n",
      "[mem:after_epoch_21] allocated=126.3MB reserved=358.0MB\n",
      "Epoch 021 | tr_MSE 0.00209 | val_MAE 0.03302 | val_RMSE 0.04607 | R2 0.7515\n",
      "[mem:after_epoch_22] allocated=126.2MB reserved=346.0MB\n",
      "Epoch 022 | tr_MSE 0.00204 | val_MAE 0.04205 | val_RMSE 0.05416 | R2 0.6565\n",
      "[mem:after_epoch_23] allocated=126.2MB reserved=170.0MB\n",
      "Epoch 023 | tr_MSE 0.00190 | val_MAE 0.03711 | val_RMSE 0.05209 | R2 0.6822\n",
      "[mem:after_epoch_24] allocated=126.3MB reserved=362.0MB\n",
      "Epoch 024 | tr_MSE 0.00198 | val_MAE 0.03816 | val_RMSE 0.05026 | R2 0.7041\n",
      "[mem:after_epoch_25] allocated=126.2MB reserved=348.0MB\n",
      "Epoch 025 | tr_MSE 0.00267 | val_MAE 0.04548 | val_RMSE 0.06016 | R2 0.5762\n",
      "[mem:after_epoch_26] allocated=126.2MB reserved=168.0MB\n",
      "Epoch 026 | tr_MSE 0.00259 | val_MAE 0.03828 | val_RMSE 0.04986 | R2 0.7088\n",
      "[mem:after_epoch_27] allocated=126.3MB reserved=360.0MB\n",
      "Epoch 027 | tr_MSE 0.00330 | val_MAE 0.04972 | val_RMSE 0.06601 | R2 0.4897\n",
      "[mem:after_epoch_28] allocated=126.2MB reserved=346.0MB\n",
      "Epoch 028 | tr_MSE 0.00405 | val_MAE 0.04710 | val_RMSE 0.06025 | R2 0.5748\n",
      "[mem:after_epoch_29] allocated=126.2MB reserved=168.0MB\n",
      "Epoch 029 | tr_MSE 0.00417 | val_MAE 0.05061 | val_RMSE 0.06255 | R2 0.5418\n",
      "[mem:after_epoch_30] allocated=126.3MB reserved=356.0MB\n",
      "Epoch 030 | tr_MSE 0.00262 | val_MAE 0.05205 | val_RMSE 0.07091 | R2 0.4111\n",
      "[mem:after_epoch_31] allocated=126.2MB reserved=346.0MB\n",
      "Epoch 031 | tr_MSE 0.00579 | val_MAE 0.04324 | val_RMSE 0.05508 | R2 0.6447\n",
      "[mem:after_epoch_32] allocated=126.2MB reserved=172.0MB\n",
      "Epoch 032 | tr_MSE 0.00465 | val_MAE 0.04937 | val_RMSE 0.06495 | R2 0.5060\n",
      "Early stopping.\n",
      "[graphtransformer_tc_spd] Best Val — MAE 0.032785 | RMSE 0.046193 | R2 0.7501\n",
      "[mem:after_Tc] allocated=16.2MB reserved=40.0MB\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(train_loader_tg))\n",
    "rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "model_tg = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "    use_adj_const=False, use_edge_bias=False,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_tg, ckpt_tg, met_tg = train_hybrid_gnn_sota(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=10, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=os.path.join(PROJECT_ROOT, \"saved_models\", \"transformer\", \"gt_tg_spd\"),\n",
    "    tag=\"graphtransformer_tg_spd\"\n",
    ")\n",
    "del model_tg, train_loader_tg, val_loader_tg\n",
    "free_cuda_memory(tag=\"after_Tg\")\n",
    "reset_cuda_stats()\n",
    "\n",
    "model_den = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=False, use_spd_bias=False, spd_max=5,\n",
    "    use_adj_const=False, use_edge_bias=False,\n",
    "    use_cls=False, use_has_xyz=False, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_den, ckpt_den, met_den = train_hybrid_gnn_sota(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=10, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=os.path.join(PROJECT_ROOT, \"saved_models\", \"transformer\", \"gt_den_spd\"),\n",
    "    tag=\"graphtransformer_den_spd\"\n",
    ")\n",
    "del model_den, train_loader_den, val_loader_den\n",
    "free_cuda_memory(tag=\"after_Density\")\n",
    "reset_cuda_stats()\n",
    "\n",
    "# Rg\n",
    "model_rg = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=False, use_edge_bias=False,\n",
    "    use_cls=False, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_rg, ckpt_rg, met_rg = train_hybrid_gnn_sota(\n",
    "    model_rg, train_loader_rg, val_loader_rg,\n",
    "    lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=10, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=os.path.join(PROJECT_ROOT, \"saved_models\", \"transformer\", \"gt_rg_spd\"),\n",
    "    tag=\"graphtransformer_rg_spd\"\n",
    ")\n",
    "del model_rg, train_loader_rg, val_loader_rg\n",
    "free_cuda_memory(tag=\"after_Rg\")\n",
    "reset_cuda_stats()\n",
    "# Tc\n",
    "model_tc = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=5, dropout=0.15,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=False, spd_max=5,\n",
    "    use_adj_const=False, use_edge_bias=False,\n",
    "    use_cls=True, use_has_xyz=False, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_tc, ckpt_tc, met_tc = train_hybrid_gnn_sota(\n",
    "    model_tc, train_loader_tc, val_loader_tc,\n",
    "    lr=5e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=200, warmup_epochs=10, patience=20,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"mse\",\n",
    "    save_dir=os.path.join(PROJECT_ROOT, \"saved_models\", \"transformer\", \"gt_tc_spd\"),\n",
    "    tag=\"graphtransformer_tc_spd\"\n",
    ")\n",
    "del model_tc, train_loader_tc, val_loader_tc\n",
    "free_cuda_memory(tag=\"after_Tc\")\n",
    "reset_cuda_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673460",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "| Model Type | Feature | MAE | RMSE | R2 |\n",
    "|---|---|---|---|---|\n",
    "| RF3D | FFV | 0.007621 | 0.017553 | 0.6605 |\n",
    "| RF3D_Aug | FFV | 0.007578 | 0.017404 | 0.6662 |\n",
    "| GNN2 | FFV | 0.013817 | 0.023902 | 0.4473 |\n",
    "| GNN2_Aug | FFV | 0.013092 | 0.022793 | 0.4974 |\n",
    "| ET | FFV | 0.006651 | 0.016818 | 0.6883 |\n",
    "| ET_Aug | FFV | 0.006635 | 0.016826 | 0.6880 |\n",
    "| **GT** | **FFV** | **0.005713** | **0.008959** | **0.9223** |\n",
    "| GT_Aug | FFV | 0.007025 | 0.011341 | 0.8756 |\n",
    "| RF3D | Tg | 58.315801 | 74.296699 | 0.5846 |\n",
    "| RF3D_Aug | Tg | 58.143107 | 74.521032 | 0.5821 |\n",
    "| **GNN2** | **Tg** | **47.105114** | **61.480179** | **0.6040** |\n",
    "| GNN2_Aug | Tg | 51.539692 | 70.575638 | 0.4782 |\n",
    "| ET | Tg | 58.973811 | 74.658978 | 0.5806 |\n",
    "| ET_Aug | Tg | 58.521052 | 74.475532 | 0.5826 |\n",
    "| GT | Tg | 78.903389 | 98.401192 |-0.0143 |\n",
    "| GT_Aug | Tg | 52.365578 | 67.529610 | 0.5223 |\n",
    "| RF3D | Tc | 0.029937 | 0.045036 | 0.7313 |\n",
    "| RF3D_Aug | Tc | 0.029675 | 0.044853 | 0.7335 |\n",
    "| **GNN2** | **Tc** | **0.025115** | **0.041331** | **0.8000** |\n",
    "| **GNN2_Aug** | **Tc** | **0.025252** | **0.039670** | **0.8157** |\n",
    "| ET | Tc | 0.028888 | 0.043469 | 0.7497 |\n",
    "| ET_Aug | Tc | 0.027990 | 0.042644 | 0.7591 |\n",
    "| GT | Tc | 0.032644 | 0.046613 | 0.7456 |\n",
    "| GT_Aug | Tc | 0.028590 | 0.043121 | 0.7822 |\n",
    "| RF3D | Rg | 1.648818 | 2.493712 | 0.7299 |\n",
    "| RF3D_Aug | Rg | 1.668425 | 2.517235 | 0.7248 |\n",
    "| GNN2 | Rg | 2.115880 | 2.801481 | 0.6434 |\n",
    "| GNN2_Aug | Rg | 1.532573 | 2.405382 | 0.7371 |\n",
    "| ET | Rg | 1.619464 | 2.522478 | 0.7237 |\n",
    "| **ET_Aug** | **Rg** | **1.609396** | **2.526705** | **0.7227** |\n",
    "| GT | Rg | 2.579300 | 3.521387 | 0.4366 |\n",
    "| GT_Aug | Rg | 2.134301 | 3.066199 | 0.5728 |\n",
    "| RF3D | Density | 0.037793 | 0.070932 | 0.7847 |\n",
    "| RF3D_Aug | Density | 0.037123 | 0.070212 | 0.7891 |\n",
    "| GNN2 | Density | 0.031735 | 0.067845 | 0.7379 |\n",
    "| GNN2_Aug | Density | 0.030458 | 0.070372 | 0.7180 |\n",
    "| ET | Density | 0.028492 | 0.052839 | 0.8805 |\n",
    "| **ET_Aug** | **Density** | **0.028135** | **0.051842** | **0.8850** |\n",
    "| GT | Density | 0.104749 | 0.134771 | -0.0343 |\n",
    "| GT_Aug | Density | 0.087159 | 0.126079 | 0.0948 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
