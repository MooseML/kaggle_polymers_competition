{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c615e0e2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-23T05:01:54.324857Z",
     "iopub.status.busy": "2025-07-23T05:01:54.324606Z",
     "iopub.status.idle": "2025-07-23T05:01:56.365975Z",
     "shell.execute_reply": "2025-07-23T05:01:56.365092Z"
    },
    "papermill": {
     "duration": 2.048016,
     "end_time": "2025-07-23T05:01:56.367363",
     "exception": false,
     "start_time": "2025-07-23T05:01:54.319347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb22f53",
   "metadata": {
    "papermill": {
     "duration": 0.002914,
     "end_time": "2025-07-23T05:01:56.374004",
     "exception": false,
     "start_time": "2025-07-23T05:01:56.371090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NeurIPS 2025 Open Polymer Prediction: Baseline Pipeline\n",
    "\n",
    "This notebook runs the full baseline pipeline for the competition, including:\n",
    "- Data preparation (LMDB creation)\n",
    "- Model training and validation\n",
    "- Test prediction and submission generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc8011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T05:01:56.381100Z",
     "iopub.status.busy": "2025-07-23T05:01:56.380771Z",
     "iopub.status.idle": "2025-07-23T05:03:41.036564Z",
     "shell.execute_reply": "2025-07-23T05:03:41.035578Z"
    },
    "papermill": {
     "duration": 104.661117,
     "end_time": "2025-07-23T05:03:41.038168",
     "exception": false,
     "start_time": "2025-07-23T05:01:56.377051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torch_geometric\n",
    "# !pip install rdkit \n",
    "# !pip install ogb\n",
    "# !pip install lmdb\n",
    "# !pip install lz4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47963b58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T05:03:41.090980Z",
     "iopub.status.busy": "2025-07-23T05:03:41.090724Z",
     "iopub.status.idle": "2025-07-23T05:03:58.507303Z",
     "shell.execute_reply": "2025-07-23T05:03:58.506190Z"
    },
    "papermill": {
     "duration": 17.444366,
     "end_time": "2025-07-23T05:03:58.508474",
     "exception": true,
     "start_time": "2025-07-23T05:03:41.064108",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/polymer')\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "from polymer_model import PolymerPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08cd22",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2. Check/Create LMDBs (train & test)\n",
    "If LMDBs are missing, run the builder scripts. Comment out after first run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  # Writable directory\n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/best_gnn_transformer_hybrid.pt'\n",
    "else:\n",
    "    DATA_ROOT = 'data'\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = 'best_gnn_transformer_hybrid.pt'\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"LMDB directory: {CHUNK_DIR}\")\n",
    "print(f\"Train LMDB: {TRAIN_LMDB}\")\n",
    "print(f\"Test LMDB: {TEST_LMDB}\")\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    # Run the LMDB builders\n",
    "    !python build_polymer_lmdb_fixed.py train\n",
    "    !python build_polymer_lmdb_fixed.py test\n",
    "    print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fcea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with correct backbone path\n",
    "model = PolymerPredictor(\n",
    "    backbone_ckpt=BACKBONE_PATH,\n",
    "    n_out=5,\n",
    "    freeze=True,  # Keep backbone frozen initially\n",
    "    use_gap=True  # Don't use HOMO-LUMO gap head\n",
    ").to(device)\n",
    "\n",
    "model.backbone.eval()\n",
    "\n",
    "print('Model initialized with frozen backbone')\n",
    "print(f'Total parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(f'Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cccba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helper functions\n",
    "def weighted_mae_loss(pred, target, mask):\n",
    "    \"\"\"Compute weighted MAE loss for polymer properties\"\"\"\n",
    "    valid_pred = pred[mask]\n",
    "    valid_target = target[mask]\n",
    "    \n",
    "    if len(valid_pred) == 0:\n",
    "        return torch.tensor(0.0, device=pred.device, requires_grad=True)\n",
    "    \n",
    "    return F.l1_loss(valid_pred, valid_target)\n",
    "\n",
    "def compute_metrics(pred, target, mask):\n",
    "    \"\"\"Compute validation metrics\"\"\"\n",
    "    label_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "    results = {}\n",
    "    \n",
    "    for i, col in enumerate(label_cols):\n",
    "        col_mask = mask[:, i]\n",
    "        if col_mask.sum() > 0:\n",
    "            col_mae = F.l1_loss(pred[col_mask, i], target[col_mask, i]).item()\n",
    "            results[f'{col}_MAE'] = col_mae\n",
    "        else:\n",
    "            results[f'{col}_MAE'] = float('nan')\n",
    "    \n",
    "    # Overall MAE\n",
    "    if mask.sum() > 0:\n",
    "        overall_mae = F.l1_loss(pred[mask], target[mask]).item()\n",
    "        results['Overall_MAE'] = overall_mae\n",
    "    else:\n",
    "        results['Overall_MAE'] = float('nan')\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60ded8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3. Train/Validation Split\n",
    "Create a stratified split based on label availability (how many non-NaN values each row has).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca56a9e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load train CSV and create split\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, 'train.csv'))\n",
    "label_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "ids = train_csv['id'].values\n",
    "\n",
    "# Stratify by number of non-NaN labels per row\n",
    "stratify_col = train_csv[label_cols].notna().sum(axis=1)\n",
    "train_ids, val_ids = train_test_split(\n",
    "    ids, \n",
    "    test_size=0.1,  \n",
    "    random_state=42,\n",
    "    stratify=stratify_col\n",
    ")\n",
    "\n",
    "print(f'Train/val split: {len(train_ids)} / {len(val_ids)} samples')\n",
    "\n",
    "# Show label availability stats for both splits\n",
    "def print_split_stats(split_ids, name):\n",
    "    split_df = train_csv[train_csv['id'].isin(split_ids)]\n",
    "    avail = split_df[label_cols].notna().sum()\n",
    "    print(f'\\n{name} set label counts:')\n",
    "    for col, count in avail.items():\n",
    "        print(f'{col:>8}: {count:>4} ({count/len(split_ids):>6.1%})')\n",
    "\n",
    "print_split_stats(train_ids, 'Train')\n",
    "print_split_stats(val_ids, 'Val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7d142",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4. Data Loading\n",
    "Create DataLoaders using the fast LMDB datasets for both train and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebcffae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DataLoader parameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4  \n",
    "\n",
    "# Create train and validation loaders\n",
    "train_loader = DataLoader(\n",
    "    LMDBDataset(train_ids, TRAIN_LMDB),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    LMDBDataset(val_ids, TRAIN_LMDB),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f'Created DataLoaders with {BATCH_SIZE} batch size')\n",
    "print(f'Train: {len(train_loader)} batches')\n",
    "print(f'Val: {len(val_loader)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afaeca7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5. Weighted MAE (wMAE) Utility\n",
    "Implement the competition's weighted MAE metric. We approximate test ranges using the train set since true test ranges are hidden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e156f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "LABEL_COLS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']  # single source of truth\n",
    "\n",
    "def make_wmae_fn(train_df, label_cols=LABEL_COLS, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Precompute n_t and delta_t from the *train* split and\n",
    "    return a closure compute_wmae(pred, true, mask=None).\n",
    "    \"\"\"\n",
    "    train_labels = train_df[label_cols].values.astype(np.float64)\n",
    "\n",
    "    # counts ignoring NaN\n",
    "    n_t = np.sum(~np.isnan(train_labels), axis=0).astype(np.float64)\n",
    "\n",
    "    # robust range via percentiles; fallback to max-min; final floor\n",
    "    p1 = np.nanpercentile(train_labels, 1,  axis=0)\n",
    "    p99 = np.nanpercentile(train_labels, 99, axis=0)\n",
    "    delta_t = p99 - p1\n",
    "    fallback = (np.nanmax(train_labels, axis=0) - np.nanmin(train_labels, axis=0))\n",
    "    delta_t = np.where((~np.isfinite(delta_t)) | (delta_t <= 0), fallback, delta_t)\n",
    "    delta_t = np.where((~np.isfinite(delta_t)) | (delta_t <= 0), 1.0, delta_t).astype(np.float64)\n",
    "\n",
    "    def compute_wmae(pred, true, mask=None):\n",
    "        \"\"\"\n",
    "        Robust weighted MAE w/ protections against NaN/Inf.\n",
    "        Returns: wmae (float), mae_per_task (5,), weights (5,)\n",
    "        \"\"\"\n",
    "        # to numpy\n",
    "        if isinstance(pred, torch.Tensor): pred = pred.detach().cpu().numpy()\n",
    "        if isinstance(true, torch.Tensor): true = true.detach().cpu().numpy()\n",
    "        if mask is None:\n",
    "            mask = ~np.isnan(true)\n",
    "        elif isinstance(mask, torch.Tensor):\n",
    "            mask = mask.detach().cpu().numpy().astype(bool)\n",
    "        else:\n",
    "            mask = mask.astype(bool)\n",
    "\n",
    "        pred = pred.astype(np.float64)\n",
    "        true = true.astype(np.float64)\n",
    "\n",
    "        # abs error with mask\n",
    "        abs_err = np.abs(pred - true)\n",
    "        abs_err[~mask] = 0.0\n",
    "\n",
    "        # per-task counts present in this eval split\n",
    "        counts_val = mask.sum(axis=0)\n",
    "\n",
    "        # MAE per task (safe divide)\n",
    "        mae_per_task = abs_err.sum(axis=0) / np.maximum(counts_val, 1)\n",
    "\n",
    "        # sanitize precomputed arrays\n",
    "        n_t_s = np.where((~np.isfinite(n_t)) | (n_t <= 0), 0.0, n_t)\n",
    "        delta_t_s = np.where((~np.isfinite(delta_t)) | (delta_t <= 0), eps, delta_t)\n",
    "\n",
    "        # raw weights (1/sqrt(n_t * delta_t)) for tasks with train labels\n",
    "        weight_raw = np.zeros_like(mae_per_task, dtype=np.float64)\n",
    "        valid_train = (n_t_s > 0)\n",
    "        weight_raw[valid_train] = 1.0 / np.sqrt(n_t_s[valid_train] * delta_t_s[valid_train])\n",
    "\n",
    "        # drop tasks not present in this val split\n",
    "        present_val = (counts_val > 0)\n",
    "        weight_raw[~present_val] = 0.0\n",
    "\n",
    "        denom = weight_raw.sum()\n",
    "        if denom <= 0 or not np.isfinite(denom):\n",
    "            # fallback: equal weights over tasks present in val\n",
    "            weights = np.zeros_like(weight_raw)\n",
    "            m = present_val.sum()\n",
    "            if m > 0:\n",
    "                weights[present_val] = 1.0 / m\n",
    "        else:\n",
    "            weights = weight_raw / denom\n",
    "\n",
    "        wmae = float((mae_per_task * weights).sum())\n",
    "        return wmae, mae_per_task.astype(np.float64), weights.astype(np.float64)\n",
    "\n",
    "    # small log so you can see what the fixed weights would be on a dummy\n",
    "    print('Label statistics from train set:')\n",
    "    print('\\nSample counts (n_t):')\n",
    "    for col, count in zip(label_cols, n_t):\n",
    "        print(f'{col:>8}: {int(count):>6}')\n",
    "    print('\\nValue ranges (delta_t):')\n",
    "    for col, delta in zip(label_cols, delta_t):\n",
    "        print(f'{col:>8}: {delta:>8.3f}')\n",
    "\n",
    "    # example weights (for visibility)\n",
    "    dummy = np.zeros((1, len(label_cols)), dtype=np.float64)\n",
    "    _, _, weights = compute_wmae(dummy, dummy, np.ones_like(dummy, dtype=bool))\n",
    "    print('\\nResulting (normalized) weights to be used at eval time:')\n",
    "    for col, w in zip(label_cols, weights):\n",
    "        print(f'{col:>8}: {w:>8.3f}')\n",
    "\n",
    "    return compute_wmae\n",
    "\n",
    "# Build the single canonical function bound to train statistics\n",
    "wmae_fn = make_wmae_fn(train_csv, LABEL_COLS)\n",
    "\n",
    "# (Optional) Shadow any older definitions to avoid accidental use\n",
    "compute_wmae = wmae_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf84a42c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6. Model Setup\n",
    "Initialize the PolymerPredictor with frozen backbone weights and configure training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dddba0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = PolymerPredictor(\n",
    "    backbone_ckpt='best_gnn_transformer_hybrid.pt',\n",
    "    freeze=True,  # Freeze backbone weights\n",
    "    use_gap=True  # Don't use gap feature initially\n",
    ").to(device)\n",
    "\n",
    "model.backbone.eval()\n",
    "\n",
    "# Optimizer (only train the head)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.head.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# Training config\n",
    "EPOCHS = 100\n",
    "EARLY_STOP_PATIENCE = 20  \n",
    "print(f'Model initialized on {device}')\n",
    "print('Trainable parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27acd772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, numpy as np, torch\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "try:\n",
    "    import torch_scatter\n",
    "    print(\"torch_scatter:\", getattr(torch_scatter, \"__version__\", \"import ok\"))\n",
    "except Exception as e:\n",
    "    print(\"torch_scatter import failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3c69c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 7. Training Loop\n",
    "Train for ~30 epochs with early stopping based on validation wMAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b692cb9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "best_val_wmae = float('inf')\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "val_wmaes = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS} - Training'):\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        \n",
    "        # Compute loss only on non-NaN labels\n",
    "        mask = ~torch.isnan(batch.y)\n",
    "        loss = torch.abs(pred - batch.y)[mask].mean()  # MAE loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch.num_graphs\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_trues, val_masks = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validation'):\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            val_preds.append(pred.cpu())\n",
    "            val_trues.append(batch.y.cpu())\n",
    "            val_masks.append(~torch.isnan(batch.y.cpu()))\n",
    "    \n",
    "    val_preds = torch.cat(val_preds, dim=0)\n",
    "    val_trues = torch.cat(val_trues, dim=0)\n",
    "    val_masks = torch.cat(val_masks, dim=0)\n",
    "    \n",
    "    # Compute validation wMAE\n",
    "    val_wmae, val_maes, w = wmae_fn(val_preds, val_trues, val_masks)\n",
    "\n",
    "    if not np.isfinite(val_wmae):\n",
    "        print(\"Warning: non-finite val_wmae; setting to +inf for early stopping.\")\n",
    "        val_wmae = float('inf')\n",
    "      \n",
    "    val_wmaes.append(val_wmae)\n",
    "    \n",
    "    # Print epoch stats\n",
    "    print(f'\\nEpoch {epoch+1}/{EPOCHS}:')\n",
    "    print(f'Train loss: {train_loss:.4f}')\n",
    "    print(f'Val wMAE: {val_wmae:.4f}')\n",
    "    print('Val MAEs:', ' '.join(f'{mae:.4f}' for mae in val_maes))\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_wmae < best_val_wmae:\n",
    "        best_val_wmae = val_wmae\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_wmae': val_wmae,\n",
    "        }, 'best_polymer_model.pt')\n",
    "        print('Saved new best model!')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "            print(f'\\nEarly stopping after {epoch+1} epochs!')\n",
    "            break\n",
    "\n",
    "print(f'\\nTraining complete! Best validation wMAE: {best_val_wmae:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20321c69",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8. Plot Training History\n",
    "Visualize the training loss and validation wMAE over epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455ac99",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, 'b-', label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation wMAE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_wmaes, 'r-', label='Val wMAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('wMAE')\n",
    "plt.title('Validation wMAE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc99ca",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde04dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from torch import nn\n",
    "from math import pi\n",
    "\n",
    "def set_trainable(module: nn.Module, trainable: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = trainable\n",
    "\n",
    "def freeze_batchnorm(module: nn.Module):\n",
    "    \"\"\"Keep BN in eval + frozen (common when unfreezing deep nets).\"\"\"\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "            m.eval()\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "def find_modules_by_name(model: nn.Module, name_patterns):\n",
    "    \"\"\"Return list of modules whose qualified name matches any regex in name_patterns.\"\"\"\n",
    "    if isinstance(name_patterns, str):\n",
    "        name_patterns = [name_patterns]\n",
    "    pats = [re.compile(p) for p in name_patterns]\n",
    "    hits = []\n",
    "    for qname, module in model.named_modules():\n",
    "        if any(p.search(qname) for p in pats):\n",
    "            hits.append((qname, module))\n",
    "    # dedupe by module identity, keep order\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for q, m in hits:\n",
    "        if id(m) not in seen:\n",
    "            uniq.append((q, m))\n",
    "            seen.add(id(m))\n",
    "    return uniq\n",
    "\n",
    "def backbone_module(model):\n",
    "    for attr in [\"backbone\", \"encoder\", \"gnn\", \"trunk\"]:\n",
    "        if hasattr(model, attr):\n",
    "            return getattr(model, attr)\n",
    "    # fallback to model itself\n",
    "    return model\n",
    "\n",
    "def head_module(model):\n",
    "    for attr in [\"head\", \"classifier\", \"regressor\", \"readout\"]:\n",
    "        if hasattr(model, attr):\n",
    "            return getattr(model, attr)\n",
    "    return None\n",
    "\n",
    "def last_k_blocks(backbone: nn.Module, k: int = 2):\n",
    "    \"\"\"\n",
    "    Try to extract the last K blocks from common containers like nn.Sequential or lists of layers.\n",
    "    If not found, returns empty list.\n",
    "    \"\"\"\n",
    "    # Common containers\n",
    "    candidates = []\n",
    "    for name in [\"layers\", \"blocks\", \"stages\", \"encoder_layers\"]:\n",
    "        if hasattr(backbone, name):\n",
    "            seq = getattr(backbone, name)\n",
    "            try:\n",
    "                # nn.Sequential, ModuleList, python list\n",
    "                tail = list(seq)[-k:] if len(seq) >= k else list(seq)\n",
    "                candidates = tail\n",
    "            except Exception:\n",
    "                pass\n",
    "    # Fallback: take last K child modules\n",
    "    if not candidates:\n",
    "        kids = list(backbone.children())\n",
    "        if kids:\n",
    "            candidates = kids[-k:] if len(kids) >= k else kids\n",
    "    return [m for m in candidates if isinstance(m, nn.Module)]\n",
    "\n",
    "def param_groups_for(model: nn.Module, trainable_scopes, base_lr=1e-4, wd=1e-5):\n",
    "    \"\"\"\n",
    "    Build optimizer groups with de-duplication across scopes.\n",
    "    Earlier groups have priority over later ones.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    groups = []\n",
    "\n",
    "    for module, lr_mult in trainable_scopes:\n",
    "        params = []\n",
    "        for p in module.parameters():\n",
    "            if p.requires_grad and id(p) not in seen:\n",
    "                params.append(p)\n",
    "                seen.add(id(p))\n",
    "        if params:  # skip empty buckets\n",
    "            groups.append({\"params\": params, \"lr\": base_lr * lr_mult, \"weight_decay\": wd})\n",
    "\n",
    "    # Catch-all (rare): any remaining trainable params not covered explicitly\n",
    "    rest = [p for p in model.parameters() if p.requires_grad and id(p) not in seen]\n",
    "    if rest:\n",
    "        groups.append({\"params\": rest, \"lr\": base_lr * 0.1, \"weight_decay\": wd})\n",
    "\n",
    "    # Optional: sanity print\n",
    "    # print(\"Param groups:\", [len(g[\"params\"]) for g in groups])\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "class CosineWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Cosine decay with linear warmup, reset per phase.\n",
    "    T_total = num_steps for the phase.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps=100, total_steps=1000, last_epoch=-1):\n",
    "        self.warmup_steps = max(1, warmup_steps)\n",
    "        self.total_steps = max(self.warmup_steps + 1, total_steps)\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "        lrs = []\n",
    "        for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups):\n",
    "            if step <= self.warmup_steps:\n",
    "                scale = step / float(self.warmup_steps)\n",
    "            else:\n",
    "                t = (step - self.warmup_steps) / float(self.total_steps - self.warmup_steps)\n",
    "                scale = 0.5 * (1.0 + torch.cos(torch.tensor(pi * t))).item()\n",
    "            lrs.append(base_lr * scale)\n",
    "        return lrs\n",
    "\n",
    "def configure_phase(model, phase_idx, K_last=2):\n",
    "    \"\"\"\n",
    "    Returns: list of (module, lr_multiplier), and a human-readable name\n",
    "    Phase 1: head only (1.0x)\n",
    "    Phase 2: \"attention\" modules (0.33x) + head (1.0x)  [backbone still frozen]\n",
    "    Phase 3: last K backbone blocks (0.2x) + attention (0.33x) + head (1.0x)\n",
    "    Phase 4: all trainable (0.1x) except BatchNorm stays frozen\n",
    "    \"\"\"\n",
    "    bb = backbone_module(model)\n",
    "    hd = head_module(model)\n",
    "\n",
    "    # first freeze everything\n",
    "    set_trainable(model, False)\n",
    "    freeze_batchnorm(model)\n",
    "\n",
    "    # try to find attention-like modules (transformer/GAT/etc.)\n",
    "    attn_hits = find_modules_by_name(\n",
    "        model,\n",
    "        name_patterns=[r\"\\b(attn|attention|multihead|gat|gine|transformer)\\b\", r\"self_attn\", r\"mhsa\", r\"encoder\\.layers\\.\\d+\\.self_attn\"]\n",
    "    )\n",
    "    attn_modules = [m for _, m in attn_hits]\n",
    "\n",
    "    scopes = []\n",
    "    name = \"\"\n",
    "\n",
    "    if phase_idx == 1:\n",
    "        if hd is not None:\n",
    "            set_trainable(hd, True)\n",
    "            scopes = [(hd, 1.0)]\n",
    "        name = \"Phase 1: head only\"\n",
    "\n",
    "    elif phase_idx == 2:\n",
    "        if hd is not None:\n",
    "            set_trainable(hd, True)\n",
    "            scopes.append((hd, 1.0))\n",
    "        # unfreeze attention modules only\n",
    "        if attn_modules:\n",
    "            for m in attn_modules:\n",
    "                set_trainable(m, True)\n",
    "            scopes.append((nn.ModuleList(attn_modules), 1/3))\n",
    "        name = \"Phase 2: attention + head\"\n",
    "\n",
    "    elif phase_idx == 3:\n",
    "        if hd is not None:\n",
    "            set_trainable(hd, True)\n",
    "            scopes.append((hd, 1.0))\n",
    "        if attn_modules:\n",
    "            for m in attn_modules:\n",
    "                set_trainable(m, True)\n",
    "            scopes.append((nn.ModuleList(attn_modules), 1/3))\n",
    "        last_blocks = last_k_blocks(bb, k=K_last)\n",
    "        if last_blocks:\n",
    "            for m in last_blocks:\n",
    "                set_trainable(m, True)\n",
    "            scopes.append((nn.ModuleList(last_blocks), 1/5))\n",
    "        name = f\"Phase 3: last {K_last} backbone blocks + attention + head\"\n",
    "\n",
    "    elif phase_idx == 4:\n",
    "        # all layers except BN\n",
    "        set_trainable(model, True)\n",
    "        freeze_batchnorm(model)  # re-freeze BN after enabling everything\n",
    "        # Discriminative LRs: head > attention > backbone\n",
    "        if hd is not None:\n",
    "            scopes.append((hd, 1.0))\n",
    "        if attn_modules:\n",
    "            scopes.append((nn.ModuleList(attn_modules), 1/3))\n",
    "        scopes.append((backbone_module(model), 1/10))\n",
    "        name = \"Phase 4: full model (BN frozen)\"\n",
    "\n",
    "    return scopes, name\n",
    "\n",
    "def run_phase(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    compute_wmae_fn,   \n",
    "    device,\n",
    "    base_lr=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    epochs=10,\n",
    "    warmup_steps=200,\n",
    "    max_grad_norm=0.2,\n",
    "    early_patience=None,\n",
    "    phase_idx=1\n",
    "):\n",
    "    scopes, phase_name = configure_phase(model, phase_idx=phase_idx, K_last=2)\n",
    "    assert scopes, f\"No trainable scopes resolved for {phase_name}. Check module/attr names.\"\n",
    "\n",
    "    # Build optimizer param groups\n",
    "    groups = param_groups_for(model, scopes, base_lr=base_lr, wd=weight_decay)\n",
    "    optimizer = torch.optim.AdamW(groups)\n",
    "    total_steps = max(1, epochs * len(train_loader))\n",
    "    scheduler = CosineWithWarmup(optimizer, warmup_steps=min(warmup_steps, total_steps//4), total_steps=total_steps)\n",
    "\n",
    "    print(f\"\\n=== {phase_name} ===\")\n",
    "    print(\"Trainable parameters:\",\n",
    "          sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    patience = 0\n",
    "\n",
    "    global_step = 0\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            pred  = model(batch)\n",
    "            mask  = ~torch.isnan(batch.y)\n",
    "            loss  = weighted_mae_loss(pred, batch.y, mask)  # uses your existing loss\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item() * batch.num_graphs\n",
    "            global_step += 1\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vp, vt, vm = [], [], []\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                pred = model(batch)\n",
    "                vp.append(pred.cpu()); vt.append(batch.y.cpu()); vm.append(~torch.isnan(batch.y.cpu()))\n",
    "            vp = torch.cat(vp, 0); vt = torch.cat(vt, 0); vm = torch.cat(vm, 0)\n",
    "            val_wmae, _, _ = compute_wmae_fn(vp.numpy(), vt.numpy(), vm.numpy())\n",
    "\n",
    "        print(f\"[{phase_name}] Epoch {ep}/{epochs} \"\n",
    "              f\"train_loss={train_loss/len(train_loader.dataset):.5f}  val_wMAE={val_wmae:.5f}\")\n",
    "\n",
    "        improved = val_wmae < best_val - 1e-6\n",
    "        if improved:\n",
    "            best_val = val_wmae\n",
    "            patience = 0\n",
    "            torch.save({\"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"val_wmae\": val_wmae,\n",
    "                        \"phase\": phase_idx,\n",
    "                        \"epoch\": ep}, f\"best_phase{phase_idx}.pt\")\n",
    "        else:\n",
    "            patience += 1\n",
    "            if early_patience is not None and patience >= early_patience:\n",
    "                print(f\"Early stop {phase_name} at epoch {ep}. Best val_wMAE={best_val:.5f}\")\n",
    "                break\n",
    "\n",
    "    return best_val\n",
    "\n",
    "def run_all_phases(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    compute_wmae_fn,\n",
    "    device,\n",
    "    base_lr=1e-4,\n",
    "    phase_epochs=10,\n",
    "    patience_per_phase=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes the 4-phase schedule you described.\n",
    "    You can tweak base_lr or phase_epochs to taste.\n",
    "    \"\"\"\n",
    "    # Phase LRs relative to your description:\n",
    "    # P1 = 1.0 * base_lr; P2 = base_lr/3; P3 = base_lr/5; P4 = base_lr/10\n",
    "    # We implement this via lr_multipliers in configure_phase() and pass base_lr here.\n",
    "\n",
    "    bests = []\n",
    "    for i in [1,2,3,4]:\n",
    "        best_val = run_phase(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            compute_wmae_fn=compute_wmae_fn,\n",
    "            device=device,\n",
    "            base_lr=base_lr,           \n",
    "            weight_decay=1e-5,\n",
    "            epochs=phase_epochs,\n",
    "            warmup_steps=min(200, phase_epochs * max(1, len(train_loader)//4)),\n",
    "            max_grad_norm=0.2,\n",
    "            early_patience=patience_per_phase,\n",
    "            phase_idx=i\n",
    "        )\n",
    "        bests.append(best_val)\n",
    "    print(\"Phase bests:\", bests)\n",
    "    return bests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: X epochs per phase = 10; feel free to adjust\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "phase_bests = run_all_phases(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    compute_wmae_fn=wmae_fn,  # your existing function\n",
    "    device=device,\n",
    "    base_lr=1e-5,                  # your “initial” LR for the head\n",
    "    phase_epochs=10,               # X\n",
    "    patience_per_phase=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a54630",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 9. Generate Submission\n",
    "Load the best model, predict on test set, and create submission file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83fcb0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_finetuned_polymer_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']+1} with val_wmae: {checkpoint['val_wmae']:.4f}\")\n",
    "\n",
    "# Load test data\n",
    "test_csv = pd.read_csv(os.path.join(DATA_ROOT, 'test.csv'))\n",
    "test_ids = test_csv['id'].values\n",
    "test_loader = DataLoader(\n",
    "    LMDBDataset(test_ids, TEST_LMDB),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Predict on test set\n",
    "model.eval()\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Predicting test set'):\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        test_preds.append(pred.cpu().numpy())\n",
    "\n",
    "test_preds = np.concatenate(test_preds, axis=0)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame(test_preds, columns=label_cols)\n",
    "submission.insert(0, 'id', test_ids)\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('\\nSubmission file created: submission.csv')\n",
    "print('Shape:', submission.shape)\n",
    "print('\\nFirst few rows:')\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5b95c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "Possible improvements:\n",
    "1. Unfreeze backbone for fine-tuning\n",
    "2. Add gap feature (`use_gap=True`)\n",
    "3. Try different learning rates\n",
    "4. Add data augmentation\n",
    "5. Experiment with model architecture\n",
    "6. Ensemble multiple models\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12966160,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "datasetId": 7923968,
     "sourceId": 12550608,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 132.263567,
   "end_time": "2025-07-23T05:04:00.360011",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-23T05:01:48.096444",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
