{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61979795",
   "metadata": {},
   "source": [
    "# Polymer Property Predictions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ace_tools_open as tools\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import pickle\n",
    "import joblib\n",
    "import os \n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Module, Sequential, Dropout\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# OGB dataset \n",
    "from ogb.lsc import PygPCQM4Mv2Dataset, PCQM4Mv2Dataset\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "# RDKit\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "\n",
    "# ChemML\n",
    "from chemml.chem import Molecule, RDKitFingerprint, CoulombMatrix, tensorise_molecules\n",
    "from chemml.models import MLP, NeuralGraphHidden, NeuralGraphOutput\n",
    "from chemml.utils import regression_metrics\n",
    "\n",
    "# SKlearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589db70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Built with CUDA: True\n",
      "CUDA available: True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device: /physical_device:GPU:0\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA available:\", tf.test.is_built_with_gpu_support())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# list all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# check compute capability if GPU available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Device: {gpu.name}\")\n",
    "        print(f\"Compute Capability: {details.get('compute_capability')}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b585ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: data\n",
      "LMDB directory: data\\processed_chunks\n",
      "Train LMDB: data\\processed_chunks\\polymer_train3d_dist.lmdb\n",
      "Test LMDB: data\\processed_chunks\\polymer_test3d_dist.lmdb\n",
      "LMDBs already exist.\n"
     ]
    }
   ],
   "source": [
    "# Paths - Fixed for Kaggle environment\n",
    "if os.path.exists('/kaggle'):\n",
    "    DATA_ROOT = '/kaggle/input/neurips-open-polymer-prediction-2025'\n",
    "    CHUNK_DIR = '/kaggle/working/processed_chunks'  # Writable directory\n",
    "    BACKBONE_PATH = '/kaggle/input/polymer/best_gnn_transformer_hybrid.pt'\n",
    "else:\n",
    "    DATA_ROOT = 'data'\n",
    "    CHUNK_DIR = os.path.join(DATA_ROOT, 'processed_chunks')\n",
    "    BACKBONE_PATH = 'best_gnn_transformer_hybrid.pt'\n",
    "\n",
    "TRAIN_LMDB = os.path.join(CHUNK_DIR, 'polymer_train3d_dist.lmdb')\n",
    "TEST_LMDB = os.path.join(CHUNK_DIR, 'polymer_test3d_dist.lmdb')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"LMDB directory: {CHUNK_DIR}\")\n",
    "print(f\"Train LMDB: {TRAIN_LMDB}\")\n",
    "print(f\"Test LMDB: {TEST_LMDB}\")\n",
    "\n",
    "# Create LMDBs if they don't exist\n",
    "if not os.path.exists(TRAIN_LMDB) or not os.path.exists(TEST_LMDB):\n",
    "    print('Building LMDBs...')\n",
    "    os.makedirs(CHUNK_DIR, exist_ok=True)\n",
    "    # Run the LMDB builders\n",
    "    !python build_polymer_lmdb_fixed.py train\n",
    "    !python build_polymer_lmdb_fixed.py test\n",
    "    print('LMDB creation complete.')\n",
    "else:\n",
    "    print('LMDBs already exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c34b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Tg → parents train=  408 val=  103 | aug rows train=  4080 val=  1030\n",
      "    FFV → parents train= 5624 val= 1406 | aug rows train= 56240 val= 14060\n",
      "     Tc → parents train=  589 val=  148 | aug rows train=  5890 val=  1480\n",
      "Density → parents train=  490 val=  123 | aug rows train=  4900 val=  1230\n",
      "     Rg → parents train=  491 val=  123 | aug rows train=  4910 val=  1230\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 1: parent-aware wiring (works for both GNN + ET) ====\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "AUG_KEY_MULT = 1000  # must match the LMDB builder\n",
    "\n",
    "# Paths expected: DATA_ROOT, TRAIN_LMDB\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_csv[\"id\"] = train_csv[\"id\"].astype(int)\n",
    "\n",
    "# LMDB ids (augmented key_ids)\n",
    "lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "lmdb_ids = np.loadtxt(lmdb_ids_path, dtype=np.int64)\n",
    "if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "\n",
    "# Parent map (preferred); fallback derives from key structure\n",
    "pmap_path = TRAIN_LMDB + \".parent_map.tsv\"\n",
    "if os.path.exists(pmap_path):\n",
    "    pmap = pd.read_csv(pmap_path, sep=\"\\t\")  # cols: key_id, parent_id, aug_idx, seed\n",
    "    pmap[\"key_id\"] = pmap[\"key_id\"].astype(np.int64)\n",
    "    pmap[\"parent_id\"] = pmap[\"parent_id\"].astype(np.int64)\n",
    "else:\n",
    "    pmap = pd.DataFrame({\n",
    "        \"key_id\": lmdb_ids.astype(np.int64),\n",
    "        \"parent_id\": (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "    })\n",
    "\n",
    "parents_in_lmdb = np.sort(pmap[\"parent_id\"].unique().astype(np.int64))\n",
    "\n",
    "def parents_with_label(task: str) -> np.ndarray:\n",
    "    m = ~train_csv[task].isna()\n",
    "    have = train_csv.loc[m, \"id\"].astype(int).values\n",
    "    return np.intersect1d(have, parents_in_lmdb, assume_unique=False)\n",
    "\n",
    "def task_parent_split(task: str, test_size=0.2, seed=42):\n",
    "    parents_labeled = parents_with_label(task)\n",
    "    if parents_labeled.size == 0:\n",
    "        raise ValueError(f\"No parents with labels for {task}\")\n",
    "    p_tr, p_va = train_test_split(parents_labeled, test_size=test_size, random_state=seed)\n",
    "    tr_keys = pmap.loc[pmap.parent_id.isin(p_tr), \"key_id\"].astype(np.int64).values\n",
    "    va_keys = pmap.loc[pmap.parent_id.isin(p_va), \"key_id\"].astype(np.int64).values\n",
    "    return np.sort(tr_keys), np.sort(va_keys), np.sort(p_tr), np.sort(p_va)\n",
    "\n",
    "# Pools for all tasks (augmented key_ids for GNN)\n",
    "task_pools = {}\n",
    "task_parent_splits = {}\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys, p_tr, p_va = task_parent_split(t, test_size=0.2, seed=42)\n",
    "    task_pools[t] = (tr_keys, va_keys)\n",
    "    task_parent_splits[t] = (p_tr, p_va)\n",
    "\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys = task_pools[t]\n",
    "    p_tr, p_va = task_parent_splits[t]\n",
    "    print(f\"{t:>7} → parents train={len(p_tr):5d} val={len(p_va):5d} | aug rows train={len(tr_keys):6d} val={len(va_keys):6d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd3c3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# --- CONSTANT RDF EDGES: 12 edges -> 11 bins (ALWAYS) ---\n",
    "RDF_EDGES = torch.tensor([0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 6], dtype=torch.float32)\n",
    "RDF_NUM_BINS = len(RDF_EDGES) - 1  # 11\n",
    "\n",
    "def _hist_fixed(x: torch.Tensor, edges: torch.Tensor = RDF_EDGES):\n",
    "    \"\"\"Normalized histogram with a FIXED number of bins (len(edges) - 1).\"\"\"\n",
    "    if x.numel() == 0:\n",
    "        return [0.0] * (len(edges) - 1)\n",
    "    h = torch.histc(x, bins=len(edges) - 1, min=float(edges[0]), max=float(edges[-1]))\n",
    "    h = (h / (h.sum() + 1e-8)).tolist()\n",
    "    return h\n",
    "\n",
    "def _rbf(d: torch.Tensor, K: int = 32, beta: float = 5.0, dmax: float = 6.0, device=None):\n",
    "    c = torch.linspace(0.0, dmax, K, device=device)\n",
    "    return torch.exp(-beta * (d.unsqueeze(-1) - c) ** 2)  # [M,K]\n",
    "\n",
    "def geom_features_from_rec(\n",
    "    rec,\n",
    "    rdkit_dim_expected: int = 15,\n",
    "    rbf_K: int = 32,\n",
    "    max_pairs: int = 20000\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns a FIXED-LENGTH (120) feature vector per LMDB record:\n",
    "      15 RDKit globals\n",
    "      5  sizes/degree/has_xyz     : [n_atoms, n_bonds, deg_mean, deg_max, has_xyz]\n",
    "      3  inertia eigenvalues      : λ1..λ3 (descending)\n",
    "      2  shape                    : [Rg_geom, anisotropy]\n",
    "      3  bbox extents             : [dx, dy, dz]\n",
    "      3  radius-from-centroid     : [mean, std, max]\n",
    "      4  bond distance stats      : [mean, std, min, max]\n",
    "      5  SPD histogram            : [hop0, hop1, hop2, hop3, hop>=4] (normalized)\n",
    "      5  extra atom mean (if 5-D; else zeros)\n",
    "      32 RBF(bond distances) mean\n",
    "      32 RBF(pairwise distances) mean (sampled if too large)\n",
    "      11 RDF histogram over pairwise distances (0..6Å, fixed bins)\n",
    "      Total = 120 dims\n",
    "    \"\"\"\n",
    "    # ---- RDKit globals (expected 15) ----\n",
    "    rd = getattr(rec, \"rdkit_feats\", None)\n",
    "    if rd is not None:\n",
    "        rd = torch.as_tensor(rd).view(-1).float().detach().cpu().numpy()\n",
    "    else:\n",
    "        rd = np.zeros((rdkit_dim_expected,), dtype=np.float32)\n",
    "    if rd.size != rdkit_dim_expected:\n",
    "        rd = np.zeros((rdkit_dim_expected,), dtype=np.float32)\n",
    "\n",
    "    # ---- Graph sizes & degree ----\n",
    "    x  = torch.as_tensor(getattr(rec, \"x\", np.zeros((0, 1), np.float32)))\n",
    "    ei = torch.as_tensor(getattr(rec, \"edge_index\", np.zeros((2, 0), np.int64)))\n",
    "    n  = int(x.shape[0])\n",
    "    e  = int(ei.shape[1]) if ei.ndim == 2 else 0\n",
    "    deg = torch.bincount(ei[0], minlength=n) if e > 0 else torch.zeros(n, dtype=torch.long)\n",
    "    deg_mean = deg.float().mean().item() if n > 0 else 0.0\n",
    "    deg_max  = deg.max().item() if n > 0 else 0.0\n",
    "\n",
    "    # ---- has_xyz ----\n",
    "    has_xyz = 0\n",
    "    if hasattr(rec, \"has_xyz\"):\n",
    "        hz = getattr(rec, \"has_xyz\")\n",
    "        has_xyz = int(bool(hz[0].item() if isinstance(hz, torch.Tensor) else hz))\n",
    "\n",
    "    # ---- Geometry from pos ----\n",
    "    pos = getattr(rec, \"pos\", None)\n",
    "    inertia = np.zeros(3, dtype=np.float32)\n",
    "    rg_geom = 0.0\n",
    "    anisotropy = 0.0\n",
    "    extents = np.zeros(3, dtype=np.float32)\n",
    "    rad_stats = np.zeros(3, dtype=np.float32)\n",
    "    bond_stats = np.zeros(4, dtype=np.float32)  # mean, std, min, max\n",
    "\n",
    "    rbf_pair_mean = np.zeros(rbf_K, dtype=np.float32)\n",
    "    rbf_bond_mean = np.zeros(rbf_K, dtype=np.float32)\n",
    "    rdf_hist = [0.0] * RDF_NUM_BINS  # ALWAYS 11 bins\n",
    "    dists = torch.tensor([])  # keep a handle for later checks\n",
    "\n",
    "    if pos is not None and n > 0 and has_xyz:\n",
    "        P = torch.as_tensor(pos).float()\n",
    "        ctr = P.mean(0, keepdim=True)\n",
    "        C = P - ctr\n",
    "\n",
    "        # inertia tensor (mass = 1 per atom)\n",
    "        I = torch.zeros(3, 3, dtype=P.dtype, device=P.device)\n",
    "        for r in C:\n",
    "            x_, y_, z_ = r\n",
    "            I += torch.tensor([[y_*y_ + z_*z_, -x_*y_,        -x_*z_],\n",
    "                               [ -x_*y_,       x_*x_ + z_*z_, -y_*z_],\n",
    "                               [ -x_*z_,       -y_*z_,        x_*x_ + y_*y_]],\n",
    "                              dtype=P.dtype, device=P.device)\n",
    "        evals, _ = torch.linalg.eigh(I)   # ascending\n",
    "        lam1, lam2, lam3 = evals.flip(0)  # descending\n",
    "        inertia = torch.stack([lam1, lam2, lam3]).detach().cpu().numpy()\n",
    "        rg_geom = float(torch.sqrt(evals.sum() / max(1, n)))\n",
    "        anisotropy = float((lam1 - (lam2 + lam3) / 2.0) / (evals.sum() + 1e-8))\n",
    "\n",
    "        # bbox extents\n",
    "        mn, mx = P.min(0).values, P.max(0).values\n",
    "        extents = (mx - mn).detach().cpu().numpy()\n",
    "\n",
    "        # radii from centroid\n",
    "        r = C.norm(dim=1)\n",
    "        rad_stats = np.array([\n",
    "            r.mean().item(),\n",
    "            r.std(unbiased=False).item(),\n",
    "            r.max().item()\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        # pairwise distances (cap for speed)\n",
    "        if n >= 2:\n",
    "            total_pairs = n * (n - 1) // 2\n",
    "            if total_pairs > max_pairs:\n",
    "                # kNN-style sampling to approximate the distribution\n",
    "                k = int(math.sqrt(max_pairs))\n",
    "                a = min(n, k)\n",
    "                anchors = torch.randperm(n)[:a]\n",
    "                dmat = torch.cdist(P[anchors], P)\n",
    "                _, nn = torch.topk(dmat, k=min(n, k), largest=False)\n",
    "                dists = (P[anchors].unsqueeze(1) - P[nn]).norm(dim=2).reshape(-1)\n",
    "            else:\n",
    "                dists = torch.pdist(P, p=2)\n",
    "\n",
    "            if dists.numel() > 0:\n",
    "                # FIXED-LENGTH RDF\n",
    "                rdf_hist = _hist_fixed(dists, RDF_EDGES)\n",
    "                # RBF over pairs\n",
    "                rbf_pair = _rbf(dists, K=rbf_K, beta=5.0, dmax=float(RDF_EDGES[-1]), device=P.device)\n",
    "                rbf_pair_mean = rbf_pair.mean(0).detach().cpu().numpy()\n",
    "\n",
    "        # bond distances + RBF\n",
    "        if e > 0:\n",
    "            d_bond = (P[ei[0]] - P[ei[1]]).norm(dim=1)\n",
    "            bond_stats = np.array([\n",
    "                d_bond.mean().item(),\n",
    "                d_bond.std(unbiased=False).item(),\n",
    "                d_bond.min().item(),\n",
    "                d_bond.max().item(),\n",
    "            ], dtype=np.float32)\n",
    "            rbf_bond = _rbf(d_bond, K=rbf_K, beta=5.0, dmax=float(RDF_EDGES[-1]), device=P.device)\n",
    "            rbf_bond_mean = rbf_bond.mean(0).detach().cpu().numpy()\n",
    "\n",
    "    # ---- SPD histogram (prefer 'hops', fallback 'dist') ----\n",
    "    spd_hist = np.zeros(5, dtype=np.float32)  # [0,1,2,3,>=4]\n",
    "    H = getattr(rec, \"hops\", None)\n",
    "    if H is None:\n",
    "        H = getattr(rec, \"dist\", None)\n",
    "    if H is not None:\n",
    "        H = torch.as_tensor(H).float()\n",
    "        if H.ndim == 2:\n",
    "            H = H[:n, :n]\n",
    "            finite = H[torch.isfinite(H) & (H >= 0)]\n",
    "            if finite.numel() > 0:\n",
    "                counts = [\n",
    "                    (finite == 0).float().sum(),\n",
    "                    (finite == 1).float().sum(),\n",
    "                    (finite == 2).float().sum(),\n",
    "                    (finite == 3).float().sum(),\n",
    "                    (finite >= 4).float().sum(),\n",
    "                ]\n",
    "                total = sum(counts) + 1e-8\n",
    "                spd_hist = np.array([float(c / total) for c in counts], dtype=np.float32)\n",
    "\n",
    "    # ---- extra atom features mean (expect 5 dims if present) ----\n",
    "    extra_mean = np.zeros(5, dtype=np.float32)\n",
    "    if hasattr(rec, \"extra_atom_feats\") and getattr(rec, \"extra_atom_feats\") is not None:\n",
    "        EA = torch.as_tensor(rec.extra_atom_feats).float()\n",
    "        if EA.ndim == 2 and EA.shape[1] == 5:\n",
    "            extra_mean = EA.mean(0).detach().cpu().numpy()\n",
    "\n",
    "    scalars = np.array([n, e, deg_mean, deg_max, float(has_xyz)], dtype=np.float32)\n",
    "    rdf_flat = np.array(rdf_hist, dtype=np.float32)  # ALWAYS length 11\n",
    "\n",
    "    vec = np.concatenate([\n",
    "        rd,                     # 15\n",
    "        scalars,                # 5  -> 20\n",
    "        inertia,                # 3  -> 23\n",
    "        np.array([rg_geom, anisotropy], dtype=np.float32),  # 2 -> 25\n",
    "        extents,                # 3  -> 28\n",
    "        rad_stats,              # 3  -> 31\n",
    "        bond_stats,             # 4  -> 35\n",
    "        spd_hist,               # 5  -> 40\n",
    "        extra_mean,             # 5  -> 45\n",
    "        rbf_bond_mean,          # 32 -> 77\n",
    "        rbf_pair_mean,          # 32 -> 109\n",
    "        rdf_flat                # 11 -> 120\n",
    "    ], axis=0)\n",
    "\n",
    "    # Safety: enforce fixed size 120 (pad/truncate if anything drifts)\n",
    "    if vec.shape[0] != 120:\n",
    "        if vec.shape[0] < 120:\n",
    "            vec = np.pad(vec, (0, 120 - vec.shape[0]), mode='constant')\n",
    "        else:\n",
    "            vec = vec[:120]\n",
    "    return vec.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a0a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors as rdmd, DataStructs\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def morgan_bits(smiles_list, n_bits=1024, radius=3):\n",
    "    X = np.zeros((len(smiles_list), n_bits), dtype=np.uint8)\n",
    "    for i, s in enumerate(smiles_list):\n",
    "        arr = np.zeros((n_bits,), dtype=np.uint8)\n",
    "        m = Chem.MolFromSmiles(s)\n",
    "        if m is not None:\n",
    "            fp = rdmd.GetMorganFingerprintAsBitVect(m, radius=radius, nBits=n_bits)\n",
    "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        X[i] = arr\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "def build_rf_features_from_lmdb(ids: np.ndarray, lmdb_path: str, smiles_list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns X = [Morgan1024 | LMDB-3D-global(69)] for each id/smiles.\n",
    "    Assumes ids and smiles_list are aligned with the CSV used to build LMDB.\n",
    "    \"\"\"\n",
    "    base = LMDBDataset(ids, lmdb_path)\n",
    "    # 3D/global block\n",
    "    feats3d = []\n",
    "    for i in range(len(base)):\n",
    "        rec = base[i]\n",
    "        feats3d.append(geom_features_from_rec(rec))  # shape (69,)\n",
    "    X3d = np.vstack(feats3d).astype(np.float32) if feats3d else np.zeros((0, 69), dtype=np.float32)\n",
    "\n",
    "    # Morgan FP block (2D)\n",
    "    Xfp = morgan_bits(smiles_list, n_bits=1024, radius=3)   # (N,1024)\n",
    "\n",
    "    # concat\n",
    "    X = np.hstack([Xfp, X3d]).astype(np.float32)            # (N, 1024+69)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0467213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 4: fp3d features aggregated per parent for ET ====\n",
    "AUG_KEY_MULT = 1000  # must match builder\n",
    "\n",
    "def build_fp3d_features_from_lmdb_parents(parent_ids, lmdb_path, smiles_list, *, agg=\"mean\"):\n",
    "    \"\"\"\n",
    "    Expands each parent -> its augmented key_ids, calls your existing\n",
    "    build_rf_features_from_lmdb(key_ids, lmdb_path, smiles_for_each_key),\n",
    "    then aggregates per parent (mean/median/max) -> one row per parent.\n",
    "    Returns X_parent, keep_idx (indices into parent_ids/smiles_list).\n",
    "    \"\"\"\n",
    "    # parent_map\n",
    "    pmap_path = lmdb_path + \".parent_map.tsv\"\n",
    "    if os.path.exists(pmap_path):\n",
    "        pmap = pd.read_csv(pmap_path, sep=\"\\t\")\n",
    "        pmap['key_id'] = pmap['key_id'].astype(np.int64)\n",
    "        pmap['parent_id'] = pmap['parent_id'].astype(np.int64)\n",
    "        group = pmap.groupby('parent_id')['key_id'].apply(list).to_dict()\n",
    "    else:\n",
    "        lmdb_ids = np.loadtxt(lmdb_path + \".ids.txt\", dtype=np.int64)\n",
    "        if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "        dfmap = pd.DataFrame({\n",
    "            'parent_id': (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "            'key_id': lmdb_ids.astype(np.int64),\n",
    "        })\n",
    "        group = dfmap.groupby('parent_id')['key_id'].apply(list).to_dict()\n",
    "\n",
    "    # expand\n",
    "    flat_keys, flat_smiles, seg_sizes = [], [], []\n",
    "    for pid, smi in zip(parent_ids, smiles_list):\n",
    "        keys = group.get(int(pid), [])\n",
    "        seg_sizes.append(len(keys))\n",
    "        if len(keys):\n",
    "            flat_keys.extend(keys)\n",
    "            flat_smiles.extend([smi] * len(keys))\n",
    "\n",
    "    if len(flat_keys) == 0:\n",
    "        raise ValueError(\"No augmented key_ids found for provided parent ids.\")\n",
    "\n",
    "    # IMPORTANT: this uses your existing function\n",
    "    X_all = build_rf_features_from_lmdb(np.array(flat_keys, dtype=np.int64),\n",
    "                                        lmdb_path,\n",
    "                                        flat_smiles)  # -> (sum_augs, D)\n",
    "\n",
    "    # fold back per parent\n",
    "    rows, keep_idx = [], []\n",
    "    i0 = 0\n",
    "    for i, k in enumerate(seg_sizes):\n",
    "        if k == 0: continue\n",
    "        Xi = X_all[i0:i0+k]\n",
    "        i0 += k\n",
    "        if   agg == \"mean\":   rows.append(Xi.mean(axis=0))\n",
    "        elif agg == \"median\": rows.append(np.median(Xi, axis=0))\n",
    "        elif agg == \"max\":    rows.append(Xi.max(axis=0))\n",
    "        else: raise ValueError(f\"agg={agg} not supported\")\n",
    "        keep_idx.append(i)\n",
    "\n",
    "    X_parent = np.vstack(rows).astype(np.float32)\n",
    "    keep_idx = np.asarray(keep_idx, dtype=int)\n",
    "    return X_parent, keep_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e663914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors as rdmd, DataStructs\n",
    "\n",
    "def smiles_to_morgan_fp(smi: str, n_bits=1024, radius=3) -> Optional[np.ndarray]:\n",
    "    m = Chem.MolFromSmiles(smi)\n",
    "    if m is None: return None\n",
    "    bv = rdmd.GetMorganFingerprintAsBitVect(m, radius, nBits=n_bits)\n",
    "    arr = np.zeros((n_bits,), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "def build_features_for_rows(\n",
    "    ids: np.ndarray,\n",
    "    smiles: List[str],\n",
    "    *,\n",
    "    feature_backend: str,           # \"fp\" or \"fp3d\"\n",
    "    lmdb_path: Optional[str] = None,\n",
    "    rbf_K: int = 32,\n",
    "    cache_npz: Optional[str] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return X for rows in the given order.\n",
    "    If feature_backend==\"fp3d\", requires lmdb_path and uses LMDBDataset.\n",
    "    Optionally caches to an .npz file keyed by a hash of ids+backend.\n",
    "    \"\"\"\n",
    "    assert feature_backend in {\"fp\", \"fp3d\"}\n",
    "    N = len(smiles)\n",
    "\n",
    "    # Optional cache\n",
    "    if cache_npz and os.path.exists(cache_npz):\n",
    "        try:\n",
    "            z = np.load(cache_npz, allow_pickle=False)\n",
    "            return z[\"X\"]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # FP block\n",
    "    Xfp = np.zeros((N, 1024), dtype=np.float32)\n",
    "    keep = np.ones(N, dtype=bool)\n",
    "    for i, s in enumerate(smiles):\n",
    "        arr = smiles_to_morgan_fp(s)\n",
    "        if arr is None:\n",
    "            keep[i] = False\n",
    "        else:\n",
    "            Xfp[i] = arr\n",
    "\n",
    "    if feature_backend == \"fp\":\n",
    "        X = Xfp[keep]\n",
    "    else:\n",
    "        assert lmdb_path is not None, \"lmdb_path required for feature_backend='fp3d'\"\n",
    "        from dataset_polymer_fixed import LMDBDataset\n",
    "        ds = LMDBDataset(ids.astype(int), lmdb_path)\n",
    "        feats3d = []\n",
    "        for i in range(len(ds)):\n",
    "            rec = ds[i]\n",
    "            feats3d.append(geom_features_from_rec(rec, rbf_K=rbf_K))\n",
    "        X3d = np.vstack(feats3d).astype(np.float32) if feats3d else np.zeros((0, 1), dtype=np.float32)\n",
    "        X = np.hstack([Xfp, X3d])[keep]\n",
    "\n",
    "    if cache_npz:\n",
    "        np.savez_compressed(cache_npz, X=X)\n",
    "    return X\n",
    "\n",
    "\n",
    "# ==== Cell 5: override prepare_features_for_target for fp3d backend ====\n",
    "def prepare_features_for_target(\n",
    "    df: pd.DataFrame, target_col: str, *,\n",
    "    lmdb_path: str, feature_backend: str, cache_dir: str = None, agg: str = \"mean\"\n",
    "):\n",
    "    # filter to labeled parents present in LMDB\n",
    "    mask = ~df[target_col].isna()\n",
    "    parent_ids = df.loc[mask, 'id'].astype(int).values\n",
    "    smiles     = df.loc[mask, 'SMILES'].astype(str).tolist()\n",
    "    y          = df.loc[mask, target_col].astype(float).values\n",
    "\n",
    "    if feature_backend == \"fp3d\":\n",
    "        # aggregate augmented features -> one row per parent\n",
    "        X, keep_idx = build_fp3d_features_from_lmdb_parents(parent_ids, lmdb_path, smiles, agg=agg)\n",
    "        y = y[keep_idx]\n",
    "        df_clean = df.loc[mask].iloc[keep_idx].reset_index(drop=True)\n",
    "        return df_clean, y, X\n",
    "\n",
    "    # else: add your other backends here as you had before\n",
    "    raise ValueError(f\"Unknown feature_backend={feature_backend}\")\n",
    "\n",
    "    return work[[\"SMILES\", target_col, \"id\"]], y, X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe69f3",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff620911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "@dataclass\n",
    "class TabularSplits:\n",
    "    # unscaled (for RF)\n",
    "    X_train: np.ndarray\n",
    "    X_test:  np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    y_test:  np.ndarray\n",
    "    # scaled (for KRR/MLP)\n",
    "    X_train_scaled: Optional[np.ndarray] = None\n",
    "    X_test_scaled:  Optional[np.ndarray] = None\n",
    "    y_train_scaled: Optional[np.ndarray] = None  # shape (N,1)\n",
    "    y_test_scaled:  Optional[np.ndarray] = None\n",
    "    x_scaler: Optional[StandardScaler] = None\n",
    "    y_scaler: Optional[StandardScaler] = None\n",
    "\n",
    "def _make_regression_stratify_bins(y: np.ndarray, n_bins: int = 10) -> np.ndarray:\n",
    "    \"\"\"Return integer bins for approximate stratification in regression.\"\"\"\n",
    "    y = y.ravel()\n",
    "    # handle degenerate case\n",
    "    if np.unique(y).size < n_bins:\n",
    "        n_bins = max(2, np.unique(y).size)\n",
    "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "    bins = np.unique(np.quantile(y, quantiles))\n",
    "    # ensure strictly increasing\n",
    "    bins = np.unique(bins)\n",
    "    # np.digitize expects right-open intervals by default\n",
    "    strat = np.digitize(y, bins[1:-1], right=False)\n",
    "    return strat\n",
    "\n",
    "def make_tabular_splits(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    scale_X: bool = True,\n",
    "    scale_y: bool = True,\n",
    "    stratify_regression: bool = False,\n",
    "    n_strat_bins: int = 10,\n",
    "    # if you already decided splits (e.g., scaffold split), pass indices:\n",
    "    train_idx: Optional[np.ndarray] = None,\n",
    "    test_idx: Optional[np.ndarray] = None,\n",
    ") -> TabularSplits:\n",
    "    \"\"\"\n",
    "    Split and (optionally) scale tabular features/targets for a single target.\n",
    "    Returns both scaled and unscaled arrays, plus fitted scalers.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    if train_idx is not None and test_idx is not None:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "    else:\n",
    "        strat = None\n",
    "        if stratify_regression:\n",
    "            strat = _make_regression_stratify_bins(y, n_bins=n_strat_bins)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "        )\n",
    "\n",
    "    # Unscaled outputs (for RF, tree models)\n",
    "    splits = TabularSplits(\n",
    "        X_train=X_train, X_test=X_test,\n",
    "        y_train=y_train, y_test=y_test\n",
    "    )\n",
    "\n",
    "    # Scaled versions (for KRR/MLP)\n",
    "    if scale_X:\n",
    "        xscaler = StandardScaler()\n",
    "        splits.X_train_scaled = xscaler.fit_transform(X_train)\n",
    "        splits.X_test_scaled  = xscaler.transform(X_test)\n",
    "        splits.x_scaler = xscaler\n",
    "    if scale_y:\n",
    "        yscaler = StandardScaler()\n",
    "        splits.y_train_scaled = yscaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        splits.y_test_scaled  = yscaler.transform(y_test.reshape(-1, 1))\n",
    "        splits.y_scaler = yscaler\n",
    "\n",
    "    # Shapes summary\n",
    "    print(\"Splits:\")\n",
    "    print(\"X_train:\", splits.X_train.shape, \"| X_test:\", splits.X_test.shape)\n",
    "    if splits.X_train_scaled is not None:\n",
    "        print(\"X_train_scaled:\", splits.X_train_scaled.shape, \"| X_test_scaled:\", splits.X_test_scaled.shape)\n",
    "    print(\"y_train:\", splits.y_train.shape, \"| y_test:\", splits.y_test.shape)\n",
    "    if splits.y_train_scaled is not None:\n",
    "        print(\"y_train_scaled:\", splits.y_train_scaled.shape, \"| y_test_scaled:\", splits.y_test_scaled.shape)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c284cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import ExtraTreesRegressor as ETR\n",
    "def train_eval_et(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    et_params: Dict[str, Any],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    stratify_regression: bool = True,\n",
    "    n_strat_bins: int = 10,\n",
    "    save_dir: str = \"saved_models/et\",\n",
    "    tag: str = \"model\",\n",
    ") -> Tuple[ExtraTreesRegressor, Dict[str, float], TabularSplits, str]:\n",
    "    \"\"\"\n",
    "    Trains a RandomForest on unscaled features; returns (model, metrics, splits, path).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # Pick a safe number of bins based on dataset size\n",
    "    if stratify_regression:\n",
    "        adaptive_bins = min(n_strat_bins, max(3, int(np.sqrt(len(y)))))\n",
    "    else:\n",
    "        adaptive_bins = n_strat_bins\n",
    "    splits = make_tabular_splits(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        scale_X=False, scale_y=False,                 # RF doesn't need scaling\n",
    "        stratify_regression=stratify_regression,\n",
    "        n_strat_bins=adaptive_bins\n",
    "    )\n",
    "\n",
    "    et = ETR(random_state=random_state, n_jobs=-1, **et_params)\n",
    "    et.fit(splits.X_train, splits.y_train)\n",
    "\n",
    "    pred_tr = et.predict(splits.X_train)\n",
    "    pred_te = et.predict(splits.X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_MAE\": mean_absolute_error(splits.y_train, pred_tr),\n",
    "        \"train_RMSE\": mean_squared_error(splits.y_train, pred_tr, squared=False),\n",
    "        \"train_R2\": r2_score(splits.y_train, pred_tr),\n",
    "        \"val_MAE\": mean_absolute_error(splits.y_test, pred_te),\n",
    "        \"val_RMSE\": mean_squared_error(splits.y_test, pred_te, squared=False),\n",
    "        \"val_R2\": r2_score(splits.y_test, pred_te),\n",
    "    }\n",
    "    print(f\"[ET/{tag}] val_MAE={metrics['val_MAE']:.6f}  val_RMSE={metrics['val_RMSE']:.6f}  val_R2={metrics['val_R2']:.4f}\")\n",
    "\n",
    "    path = os.path.join(save_dir, f\"et_{tag}.joblib\")\n",
    "    joblib.dump({\"model\": et, \"metrics\": metrics, \"et_params\": et_params}, path)\n",
    "    return et, metrics, splits, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ac98b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> ET (FFV) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (5624, 1144) | X_test: (1406, 1144)\n",
      "y_train: (5624,) | y_test: (1406,)\n",
      "[ET/aug3D_fp3d_FFV] val_MAE=0.006635  val_RMSE=0.016826  val_R2=0.6880\n",
      "[ET+3D/FFV] val_MAE=0.006635  val_RMSE=0.016826  val_R2=0.6880\n",
      "\n",
      ">>> ET (Tg) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (408, 1144) | X_test: (103, 1144)\n",
      "y_train: (408,) | y_test: (103,)\n",
      "[ET/aug3D_fp3d_Tg] val_MAE=58.521052  val_RMSE=74.475532  val_R2=0.5826\n",
      "[ET+3D/Tg] val_MAE=58.521052  val_RMSE=74.475532  val_R2=0.5826\n",
      "\n",
      ">>> ET (Tc) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (589, 1144) | X_test: (148, 1144)\n",
      "y_train: (589,) | y_test: (148,)\n",
      "[ET/aug3D_fp3d_Tc] val_MAE=0.027990  val_RMSE=0.042644  val_R2=0.7591\n",
      "[ET+3D/Tc] val_MAE=0.027990  val_RMSE=0.042644  val_R2=0.7591\n",
      "\n",
      ">>> ET (Rg) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (491, 1144) | X_test: (123, 1144)\n",
      "y_train: (491,) | y_test: (123,)\n",
      "[ET/aug3D_fp3d_Rg] val_MAE=1.609396  val_RMSE=2.526705  val_R2=0.7227\n",
      "[ET+3D/Rg] val_MAE=1.609396  val_RMSE=2.526705  val_R2=0.7227\n",
      "\n",
      ">>> ET (Density) with backend=fp3d\n",
      "Splits:\n",
      "X_train: (490, 1144) | X_test: (123, 1144)\n",
      "y_train: (490,) | y_test: (123,)\n",
      "[ET/aug3D_fp3d_Density] val_MAE=0.028135  val_RMSE=0.051842  val_R2=0.8850\n",
      "[ET+3D/Density] val_MAE=0.028135  val_RMSE=0.051842  val_R2=0.8850\n"
     ]
    }
   ],
   "source": [
    "def train_et_for_target(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    et_params: dict,\n",
    "    *,\n",
    "    lmdb_path: Optional[str],\n",
    "    feature_backend: str = \"fp3d\",   # default to augmented\n",
    "    save_dir: str = \"saved_models/et\",\n",
    "    tag_prefix: str = \"et\",\n",
    "    **split_kwargs\n",
    "):\n",
    "    df_clean, y, X = prepare_features_for_target(\n",
    "        df, target_col,\n",
    "        lmdb_path=lmdb_path,\n",
    "        feature_backend=feature_backend,\n",
    "        cache_dir=os.path.join(save_dir, \"cache\")\n",
    "    )\n",
    "    model, metrics, splits, path = train_eval_et(\n",
    "        X, y,\n",
    "        et_params=et_params,\n",
    "        save_dir=save_dir,\n",
    "        tag=f\"{tag_prefix}_{feature_backend}_{target_col}\",\n",
    "        **split_kwargs\n",
    "    )\n",
    "    return model, metrics, splits, path\n",
    "\n",
    "# rf_cfg_aug = {\n",
    "#     \"FFV\":     {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.2, \"bootstrap\": True},\n",
    "#     \"Tc\":      {\"n_estimators\": 800, \"max_depth\": 20, \"min_samples_split\": 6, \"min_samples_leaf\": 2, \"max_features\": \"sqrt\", \"bootstrap\": False},\n",
    "#     \"Rg\":      {\"n_estimators\": 400, \"max_depth\": 260, \"min_samples_split\": 6, \"min_samples_leaf\": 4, \"max_features\": 1.0, \"bootstrap\": True},\n",
    "#     \"Tg\":      {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.2, \"bootstrap\": True},\n",
    "#     \"Density\": {\"n_estimators\": 600, \"max_depth\": 40, \"min_samples_leaf\": 1, \"max_features\": \"sqrt\"},\n",
    "# }\n",
    "\n",
    "etr_cfg_full = {\n",
    "  \"FFV\":     {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.2, \"bootstrap\": False},\n",
    "  \"Tc\":      {\"n_estimators\": 1500, \"max_depth\": None, \"min_samples_leaf\": 3, \"max_features\": 0.15, \"bootstrap\": False},\n",
    "  \"Rg\":      {\"n_estimators\": 400, \"max_depth\": 260, \"min_samples_split\": 6, \"min_samples_leaf\": 4, \"max_features\": 1.0, \"bootstrap\": True},\n",
    "  \"Tg\":      {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.2, \"bootstrap\": False},\n",
    "  \"Density\": {\"n_estimators\": 1200, \"max_depth\": None, \"min_samples_leaf\": 2, \"max_features\": 0.25, \"bootstrap\": False},\n",
    "}\n",
    "\n",
    "\n",
    "TRAIN_CSV = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "df_all = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "et_models, et_metrics = {}, {}\n",
    "for t in [\"FFV\", \"Tg\", \"Tc\", \"Rg\", \"Density\"]:\n",
    "    print(f\"\\n>>> ET ({t}) with backend=fp3d\")\n",
    "    m, met, sp, p = train_et_for_target(\n",
    "        df_all, t, etr_cfg_full[t],\n",
    "        lmdb_path=TRAIN_LMDB,\n",
    "        feature_backend=\"fp3d\",\n",
    "        save_dir=\"saved_models/et_aug3d\",\n",
    "        tag_prefix=\"aug3D\",\n",
    "        test_size=0.2, random_state=42, stratify_regression=True, n_strat_bins=10,\n",
    "    )\n",
    "    et_models[t], et_metrics[t] = m, met\n",
    "    print(f\"[ET+3D/{t}] val_MAE={met['val_MAE']:.6f}  val_RMSE={met['val_RMSE']:.6f}  val_R2={met['val_R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a0004",
   "metadata": {},
   "source": [
    "# Graph Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d599b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Tg → parents train=  408 val=  103 | aug rows train=  4080 val=  1030\n",
      "    FFV → parents train= 5624 val= 1406 | aug rows train= 56240 val= 14060\n",
      "     Tc → parents train=  589 val=  148 | aug rows train=  5890 val=  1480\n",
      "Density → parents train=  490 val=  123 | aug rows train=  4900 val=  1230\n",
      "     Rg → parents train=  491 val=  123 | aug rows train=  4910 val=  1230\n"
     ]
    }
   ],
   "source": [
    "# ==== Parent-aware wiring (CSV parents -> augmented LMDB key_ids) ====\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_cols = ['Tg','FFV','Tc','Density','Rg']\n",
    "task2idx   = {k:i for i,k in enumerate(label_cols)}\n",
    "AUG_KEY_MULT = 1000  # must match your LMDB builder\n",
    "\n",
    "# CSV (parents)\n",
    "train_csv = pd.read_csv(os.path.join(DATA_ROOT, \"train.csv\"))\n",
    "train_csv[\"id\"] = train_csv[\"id\"].astype(int)\n",
    "\n",
    "# LMDB ids (augmented key_ids)\n",
    "lmdb_ids_path = TRAIN_LMDB + \".ids.txt\"\n",
    "lmdb_ids = np.loadtxt(lmdb_ids_path, dtype=np.int64)\n",
    "if lmdb_ids.ndim == 0: lmdb_ids = lmdb_ids.reshape(1)\n",
    "\n",
    "# Parent map (preferred) → key_id list per parent\n",
    "pmap_path = TRAIN_LMDB + \".parent_map.tsv\"\n",
    "if os.path.exists(pmap_path):\n",
    "    pmap = pd.read_csv(pmap_path, sep=\"\\t\")  # cols: key_id, parent_id, aug_idx, seed\n",
    "    pmap[\"key_id\"] = pmap[\"key_id\"].astype(np.int64)\n",
    "    pmap[\"parent_id\"] = pmap[\"parent_id\"].astype(np.int64)\n",
    "else:\n",
    "    # fallback if parent_map missing: derive parent by integer division\n",
    "    pmap = pd.DataFrame({\n",
    "        \"key_id\": lmdb_ids.astype(np.int64),\n",
    "        \"parent_id\": (lmdb_ids // AUG_KEY_MULT).astype(np.int64),\n",
    "    })\n",
    "parents_in_lmdb = np.sort(pmap[\"parent_id\"].unique().astype(np.int64))\n",
    "\n",
    "def parents_with_label(task: str) -> np.ndarray:\n",
    "    m = ~train_csv[task].isna()\n",
    "    have = train_csv.loc[m, \"id\"].astype(int).values  # parents that have this label\n",
    "    return np.intersect1d(have, parents_in_lmdb, assume_unique=False)\n",
    "\n",
    "# Split BY PARENT (no leakage), then expand to augmented key_ids\n",
    "def task_parent_split_keys(task: str, test_size=0.2, seed=42):\n",
    "    parents_labeled = parents_with_label(task)\n",
    "    if parents_labeled.size == 0:\n",
    "        raise ValueError(f\"No parents with labels for {task}\")\n",
    "    p_tr, p_va = train_test_split(parents_labeled, test_size=test_size, random_state=seed)\n",
    "    tr_keys = pmap.loc[pmap.parent_id.isin(p_tr), \"key_id\"].astype(np.int64).values\n",
    "    va_keys = pmap.loc[pmap.parent_id.isin(p_va), \"key_id\"].astype(np.int64).values\n",
    "    return np.sort(tr_keys), np.sort(va_keys), np.sort(p_tr), np.sort(p_va)\n",
    "\n",
    "# Build pools (augmented key_ids) per task\n",
    "task_pools = {}\n",
    "task_parent_splits = {}\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys, p_tr, p_va = task_parent_split_keys(t, test_size=0.2, seed=42)\n",
    "    task_pools[t] = (tr_keys, va_keys)\n",
    "    task_parent_splits[t] = (p_tr, p_va)\n",
    "\n",
    "for t in label_cols:\n",
    "    tr_keys, va_keys = task_pools[t]\n",
    "    p_tr, p_va = task_parent_splits[t]\n",
    "    print(f\"{t:>7} → parents train={len(p_tr):5d} val={len(p_va):5d} | aug rows train={len(tr_keys):6d} val={len(va_keys):6d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3efce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import torch, numpy as np\n",
    "from dataset_polymer_fixed import LMDBDataset\n",
    "\n",
    "def _get_rdkit_feats_from_record(rec):\n",
    "    arr = getattr(rec, \"rdkit_feats\", None)\n",
    "    if arr is None:\n",
    "        return torch.zeros(1, 15, dtype=torch.float32)  # keep (1, D)\n",
    "    v = torch.as_tensor(np.asarray(arr, np.float32).reshape(1, -1), dtype=torch.float32)\n",
    "    return v  # (1, D)\n",
    "\n",
    "class LMDBtoPyGSingleTask(Dataset):\n",
    "    def __init__(self,\n",
    "                 ids,                # <<< must be augmented key_ids\n",
    "                 lmdb_path,\n",
    "                 target_index=None,\n",
    "                 *,\n",
    "                 use_mixed_edges: bool = True,\n",
    "                 include_extra_atom_feats: bool = True):\n",
    "        self.ids = np.asarray(ids, dtype=np.int64)\n",
    "        self.base = LMDBDataset(self.ids, lmdb_path)\n",
    "        self.t = target_index\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        self.include_extra_atom_feats = include_extra_atom_feats\n",
    "\n",
    "    def __len__(self): return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.base[idx]\n",
    "        x  = torch.as_tensor(rec.x, dtype=torch.long)\n",
    "        ei = torch.as_tensor(rec.edge_index, dtype=torch.long)\n",
    "        ea = torch.as_tensor(rec.edge_attr)\n",
    "\n",
    "        # Mixed edges: 3 categorical + 32 RBF; categorical-only if disabled\n",
    "        edge_attr = ea.to(torch.float32) if self.use_mixed_edges else ea[:, :3].to(torch.long)\n",
    "\n",
    "        d = Data(x=x, edge_index=ei, edge_attr=edge_attr,\n",
    "                 rdkit_feats=_get_rdkit_feats_from_record(rec))  # (1, D)\n",
    "\n",
    "        if hasattr(rec, \"pos\"):                d.pos  = torch.as_tensor(rec.pos, dtype=torch.float32)\n",
    "        if self.include_extra_atom_feats and hasattr(rec, \"extra_atom_feats\"):\n",
    "                                               d.extra_atom_feats = torch.as_tensor(rec.extra_atom_feats, dtype=torch.float32)\n",
    "        if hasattr(rec, \"has_xyz\"):            d.has_xyz = torch.as_tensor(rec.has_xyz, dtype=torch.float32)  # (1,)\n",
    "        if hasattr(rec, \"dist\"):               d.hops = torch.as_tensor(rec.dist, dtype=torch.long).unsqueeze(0)  # (1,L,L)\n",
    "\n",
    "        if (self.t is not None) and hasattr(rec, \"y\"):\n",
    "            yv = torch.as_tensor(rec.y, dtype=torch.float32).view(-1)\n",
    "            if self.t < yv.numel(): d.y = yv[self.t:self.t+1]  # (1,)\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "694612d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "def make_loaders_for_task_from_pools(task, task_pools, *,\n",
    "                                     batch_size=64,\n",
    "                                     use_mixed_edges=True,\n",
    "                                     include_extra_atom_feats=True):\n",
    "    t = task2idx[task]\n",
    "    tr_keys, va_keys = task_pools[task]\n",
    "    if len(tr_keys) == 0 or len(va_keys) == 0:\n",
    "        raise ValueError(f\"Empty pools for {task}. Check splits.\")\n",
    "    tr_ds = LMDBtoPyGSingleTask(tr_keys, TRAIN_LMDB, target_index=t,\n",
    "                                use_mixed_edges=use_mixed_edges,\n",
    "                                include_extra_atom_feats=include_extra_atom_feats)\n",
    "    va_ds = LMDBtoPyGSingleTask(va_keys, TRAIN_LMDB, target_index=t,\n",
    "                                use_mixed_edges=use_mixed_edges,\n",
    "                                include_extra_atom_feats=include_extra_atom_feats)\n",
    "    tr = GeoDataLoader(tr_ds, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "    va = GeoDataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    return tr, va\n",
    "\n",
    "# Build loaders (no leakage; parent-split → aug-expanded)\n",
    "train_loader_tg,  val_loader_tg  = make_loaders_for_task_from_pools(\"Tg\",      task_pools, batch_size=512)\n",
    "train_loader_den, val_loader_den = make_loaders_for_task_from_pools(\"Density\", task_pools, batch_size=512)\n",
    "train_loader_rg,  val_loader_rg  = make_loaders_for_task_from_pools(\"Rg\",      task_pools, batch_size=512)\n",
    "train_loader_ffv, val_loader_ffv = make_loaders_for_task_from_pools(\"FFV\",     task_pools, batch_size=512)\n",
    "train_loader_tc,  val_loader_tc  = make_loaders_for_task_from_pools(\"Tc\",      task_pools, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983db98",
   "metadata": {},
   "source": [
    "## Step 5: Define the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc992041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numpy as np, torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, RMSprop\n",
    "from torch.amp import GradScaler, autocast\n",
    "from copy import deepcopy\n",
    "\n",
    "def train_hybrid_gnn_sota(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    *,\n",
    "    lr: float = 5e-4,\n",
    "    optimizer: str = \"AdamW\",\n",
    "    weight_decay: float = 1e-5,\n",
    "    epochs: int = 120,\n",
    "    warmup_epochs: int = 5,\n",
    "    patience: int = 15,\n",
    "    clip_norm: float = 1.0,\n",
    "    amp: bool = True,\n",
    "    loss_name: str = \"mse\",   # \"mse\" or \"huber\"\n",
    "    save_dir: str = \"saved_models/gnn\",\n",
    "    tag: str = \"model_sota\",\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    opt_name = optimizer.lower()\n",
    "    if opt_name == \"rmsprop\":\n",
    "        opt = RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.0)\n",
    "    else:\n",
    "        opt = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # cosine schedule w/ warmup\n",
    "    def lr_factor(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / max(1, warmup_epochs)\n",
    "        t = (epoch - warmup_epochs) / max(1, (epochs - warmup_epochs))\n",
    "        return 0.5 * (1 + math.cos(math.pi * t))\n",
    "    scaler = GradScaler(\"cuda\", enabled=amp)\n",
    "\n",
    "    def loss_fn(pred, target):\n",
    "        if loss_name.lower() == \"huber\":\n",
    "            return F.huber_loss(pred, target, delta=1.0)\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_once(loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        for b in loader:\n",
    "            b = b.to(device)\n",
    "            p = model(b)\n",
    "            preds.append(p.detach().cpu())\n",
    "            trues.append(b.y.view(-1,1).cpu())\n",
    "        preds = torch.cat(preds).numpy(); trues = torch.cat(trues).numpy()\n",
    "        mae = np.mean(np.abs(preds - trues))\n",
    "        rmse = float(np.sqrt(np.mean((preds - trues)**2)))\n",
    "        r2 = float(1 - np.sum((preds - trues)**2) / np.sum((trues - trues.mean())**2))\n",
    "        return mae, rmse, r2\n",
    "\n",
    "    best_mae = float(\"inf\")\n",
    "    best = None\n",
    "    best_path = os.path.join(save_dir, f\"{tag}.pt\")\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # schedule\n",
    "        for g in opt.param_groups:\n",
    "            g[\"lr\"] = lr * lr_factor(ep-1)\n",
    "\n",
    "        model.train()\n",
    "        total, count = 0.0, 0\n",
    "        for b in train_loader:\n",
    "            b = b.to(device)\n",
    "            with autocast(\"cuda\", enabled=amp):\n",
    "                pred = model(b)\n",
    "                loss = loss_fn(pred, b.y.view(-1,1))\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            if clip_norm is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "            scaler.step(opt); scaler.update()\n",
    "\n",
    "            total += loss.item() * b.num_graphs\n",
    "            count += b.num_graphs\n",
    "\n",
    "        tr_mse = total / max(1, count)\n",
    "        mae, rmse, r2 = eval_once(val_loader)\n",
    "        print(f\"Epoch {ep:03d} | tr_MSE {tr_mse:.5f} | val_MAE {mae:.5f} | val_RMSE {rmse:.5f} | R2 {r2:.4f}\")\n",
    "\n",
    "        if mae < best_mae - 1e-6:\n",
    "            best_mae = mae\n",
    "            best = deepcopy(model.state_dict())\n",
    "            torch.save(best, best_path)\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best is not None:\n",
    "        model.load_state_dict(best)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "    final_mae, final_rmse, final_r2 = eval_once(val_loader)\n",
    "    print(f\"[{tag}] Best Val — MAE {final_mae:.6f} | RMSE {final_rmse:.6f} | R2 {final_r2:.4f}\")\n",
    "    return model, best_path, {\"MAE\": final_mae, \"RMSE\": final_rmse, \"R2\": final_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9449bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "def _act(name: str):\n",
    "    name = (name or \"relu\").lower()\n",
    "    if name == \"gelu\": return nn.GELU()\n",
    "    if name in (\"silu\", \"swish\"): return nn.SiLU()\n",
    "    return nn.ReLU()\n",
    "\n",
    "\n",
    "class AttnBiasFull(nn.Module):\n",
    "    \"\"\"\n",
    "    Produces additive per-head attention bias of shape (B, H, L0, L0)\n",
    "    from geometry (xyz), adjacency, SPD buckets, and categorical edge types.\n",
    "\n",
    "    Accepts both old arg names (use_geo/use_adj_const/spd_max/rbf_K) and\n",
    "    new ones (use_geo_bias/use_adj_bias/spd_buckets/rbf_k/edge_cats).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_heads: int,\n",
    "        *,\n",
    "        # old names\n",
    "        use_geo: bool = None, use_adj_const: bool = None, use_spd: bool = True,\n",
    "        spd_max: int = None, rbf_K: int = None,\n",
    "        # new alias names\n",
    "        use_geo_bias: bool = None, use_adj_bias: bool = None,\n",
    "        spd_buckets: int = None, rbf_k: int = None,\n",
    "        edge_cats: tuple = (5, 6, 2),\n",
    "        use_edge_bias: bool = True,\n",
    "        # shared\n",
    "        rbf_beta: float = 5.0, activation: str = \"relu\",\n",
    "        edge_cont_dim: int = 32,  # (kept for compatibility; not used here)\n",
    "        use_headnorm: bool = True,\n",
    "        bound_scale: float = 0.1,   # tanh scale for gentle bounding\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_heads = int(n_heads)\n",
    "        self.bound_scale = float(bound_scale)\n",
    "        self.use_headnorm = bool(use_headnorm)\n",
    "\n",
    "        # ---- resolve aliases / defaults ----\n",
    "        def pick(*vals, default):\n",
    "            for v in vals:\n",
    "                if v is not None:\n",
    "                    return v\n",
    "            return default\n",
    "\n",
    "        self.use_geo = bool(pick(use_geo, use_geo_bias, default=True))\n",
    "        self.use_adj_const = bool(pick(use_adj_const, use_adj_bias, default=True))\n",
    "\n",
    "        # SPD: if spd_buckets given, use exactly that; else spd_max + 2 (0..spd_max + catch-all)\n",
    "        if spd_buckets is not None:\n",
    "            self.spd_buckets = int(spd_buckets)\n",
    "        else:\n",
    "            smax = 5 if spd_max is None else int(spd_max)\n",
    "            self.spd_buckets = smax + 2  # 0..smax + 1(>=)\n",
    "\n",
    "        K = int(pick(rbf_K, rbf_k, default=16))\n",
    "        self.rbf_beta = float(rbf_beta)\n",
    "\n",
    "        # ---- geometry → per-head bias ----\n",
    "        if self.use_geo:\n",
    "            centers = torch.linspace(0.0, 10.0, K)\n",
    "            self.register_buffer(\"centers\", centers, persistent=False)\n",
    "            self.geo_mlp = nn.Sequential(\n",
    "                nn.Linear(K, self.n_heads),  # simple per-head projection\n",
    "            )\n",
    "\n",
    "        # ---- adjacency constant per head ----\n",
    "        if self.use_adj_const:\n",
    "            self.adj_bias = nn.Parameter(torch.zeros(self.n_heads))\n",
    "\n",
    "        # ---- SPD buckets → per-head bias ----\n",
    "        self.use_spd = bool(use_spd)\n",
    "        if self.use_spd:\n",
    "            self.spd_emb = nn.Embedding(self.spd_buckets, self.n_heads)\n",
    "\n",
    "        # ---- edge categorical bias (configurable widths) ----\n",
    "        t, s, c = edge_cats\n",
    "        self.use_edge_bias = bool(use_edge_bias)\n",
    "        if self.use_edge_bias:\n",
    "            self.edge_emb0 = nn.Embedding(int(t), self.n_heads)\n",
    "            self.edge_emb1 = nn.Embedding(int(s), self.n_heads)\n",
    "            self.edge_emb2 = nn.Embedding(int(c), self.n_heads)\n",
    "        else:\n",
    "            self.edge_emb0 = self.edge_emb1 = self.edge_emb2 = None\n",
    "\n",
    "        # ---- per-component learnable scalers ----\n",
    "        self.alpha_geo  = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_spd  = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_adj  = nn.Parameter(torch.tensor(0.2))\n",
    "        self.alpha_edge = nn.Parameter(torch.tensor(0.2))\n",
    "\n",
    "        # ---- simple head-wise LayerNorms (normalize across H) ----\n",
    "        if self.use_headnorm:\n",
    "            self.ln_geo  = nn.LayerNorm(self.n_heads)\n",
    "            self.ln_spd  = nn.LayerNorm(self.n_heads)\n",
    "            self.ln_edge = nn.LayerNorm(self.n_heads)\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def _apply_ln_heads(self, t: torch.Tensor, ln: nn.LayerNorm) -> torch.Tensor:\n",
    "        \"\"\"Apply LayerNorm across heads for a (B,H,L,L) tensor.\"\"\"\n",
    "        # (B,H,L,L) -> (B,L,L,H) -> LN(H) -> (B,H,L,L)\n",
    "        t = t.permute(0, 2, 3, 1)\n",
    "        t = ln(t)\n",
    "        t = t.permute(0, 3, 1, 2).contiguous()\n",
    "        return t\n",
    "\n",
    "    def _bound(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Bound magnitudes to avoid dominating softmax; keeps gradients smooth.\"\"\"\n",
    "        return self.bound_scale * torch.tanh(t)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _spd_bias(self, hops: torch.Tensor, valid_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        hops: (B, MAX_NODES, MAX_NODES) or (B, L0, L0) shortest-path distances (uint8/long)\n",
    "        valid_mask: (B, L0, L0) bool, True where both tokens are real (not PAD)\n",
    "        returns: (B, H, L0, L0) additive per-head bias\n",
    "        \"\"\"\n",
    "        if hops.dim() == 2:  # (L,L) -> (1,L,L)\n",
    "            hops = hops.unsqueeze(0)\n",
    "\n",
    "        B, L0, _ = valid_mask.shape\n",
    "\n",
    "        # align SPD to current L0 (top-left block)\n",
    "        if hops.size(1) != L0 or hops.size(2) != L0:\n",
    "            hops = hops[:, :L0, :L0]\n",
    "\n",
    "        # bucketize SPD: last bucket = catch-all (>= last)\n",
    "        last = self.spd_buckets - 1\n",
    "        raw = hops.to(valid_mask.device).long().clamp_min_(0)\n",
    "        catch_all = raw >= last\n",
    "        raw = raw.clamp_max(last - 1)\n",
    "        bucket = torch.where(catch_all, raw.new_full(raw.shape, last), raw)\n",
    "\n",
    "        # wipe invalid pairs\n",
    "        bucket = torch.where(valid_mask, bucket, torch.zeros_like(bucket))\n",
    "\n",
    "        emb = self.spd_emb(bucket)              # (B, L0, L0, H)\n",
    "        return emb.permute(0, 3, 1, 2).contiguous()  # (B, H, L0, L0)\n",
    "\n",
    "    def _edge_bias(self, edge_index, edge_attr, batch, L0, ptr=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-head additive bias from categorical bond attributes.\n",
    "        Returns: (B, H, L0, L0)\n",
    "        \"\"\"\n",
    "        u, v = edge_index\n",
    "        be   = batch[u]  # graph id per edge\n",
    "\n",
    "        if ptr is None:\n",
    "            B = int(batch.max().item()) + 1\n",
    "            counts = torch.bincount(batch, minlength=B)\n",
    "            ptr = torch.zeros(B + 1, dtype=torch.long, device=batch.device)\n",
    "            ptr[1:] = torch.cumsum(counts, dim=0)\n",
    "        B = int(ptr.numel() - 1)\n",
    "\n",
    "        start = ptr[be]\n",
    "        u_loc = (u - start).long()\n",
    "        v_loc = (v - start).long()\n",
    "\n",
    "        cat = edge_attr[:, :3].long()\n",
    "        eh  = ( self.edge_emb0(cat[:, 0])\n",
    "              + self.edge_emb1(cat[:, 1])\n",
    "              + self.edge_emb2(cat[:, 2]) )  # (E,H)\n",
    "\n",
    "        H = self.n_heads\n",
    "        eb = torch.zeros((B, H, L0, L0), device=edge_attr.device, dtype=torch.float32)\n",
    "        for b in range(B):\n",
    "            m = (be == b)\n",
    "            if not torch.any(m):\n",
    "                continue\n",
    "            eb[b, :, u_loc[m], v_loc[m]] += eh[m].T\n",
    "        return eb\n",
    "\n",
    "    # ---------- forward ----------\n",
    "    def forward(self, pos, edge_index, edge_attr, batch, key_padding_mask, hops=None, ptr=None):\n",
    "        \"\"\"\n",
    "        Returns (B, H, L0, L0) additive bias. PAD rows/cols are filled with large negative.\n",
    "        \"\"\"\n",
    "        A = to_dense_adj(edge_index, batch=batch).squeeze(1)  # (B,L0,L0)\n",
    "        B, L0, _ = A.shape\n",
    "        H = self.n_heads\n",
    "        device = A.device\n",
    "\n",
    "        valid = ~key_padding_mask                             # (B,L0)\n",
    "        valid2d = valid.unsqueeze(2) & valid.unsqueeze(1)     # (B,L0,L0)\n",
    "\n",
    "        # geometry\n",
    "        if self.use_geo and (pos is not None):\n",
    "            pad_pos, _ = to_dense_batch(pos, batch)           # (B,L0,3)\n",
    "            diff = pad_pos.unsqueeze(2) - pad_pos.unsqueeze(1)      # (B,L0,L0,3)\n",
    "            dist = torch.sqrt(torch.clamp((diff**2).sum(-1), min=0.0))  # (B,L0,L0)\n",
    "            centers = self.centers.to(dist.device)\n",
    "            rbf = torch.exp(-self.rbf_beta * (dist.unsqueeze(-1) - centers)**2)\n",
    "            geo = self.geo_mlp(rbf).permute(0, 3, 1, 2).contiguous()    # (B,H,L0,L0)\n",
    "        else:\n",
    "            geo = torch.zeros((B, H, L0, L0), device=device)\n",
    "\n",
    "        # adjacency constant per head\n",
    "        if self.use_adj_const:\n",
    "            adj = A.unsqueeze(1) * self.adj_bias.view(1, H, 1, 1)       # (B,H,L0,L0)\n",
    "        else:\n",
    "            adj = torch.zeros_like(geo)\n",
    "\n",
    "        # SPD\n",
    "        if self.use_spd and (hops is not None):\n",
    "            spd = self._spd_bias(hops, valid2d)                          # (B,H,L0,L0)\n",
    "        else:\n",
    "            spd = torch.zeros_like(geo)\n",
    "\n",
    "        # edge categorical\n",
    "        if self.use_edge_bias and (edge_attr is not None):\n",
    "            edg = self._edge_bias(edge_index, edge_attr, batch, L0, ptr) # (B,H,L0,L0)\n",
    "        else:\n",
    "            edg = torch.zeros_like(geo)\n",
    "\n",
    "        # ---- normalize & bound each component, then scale ----\n",
    "        if self.use_headnorm:\n",
    "            if self.use_geo:  geo = self._apply_ln_heads(geo,  self.ln_geo)\n",
    "            if self.use_spd:  spd = self._apply_ln_heads(spd,  self.ln_spd)\n",
    "            if self.use_edge_bias: edg = self._apply_ln_heads(edg, self.ln_edge)\n",
    "\n",
    "        # gently bound to keep attention stable\n",
    "        if self.use_geo:       geo = self._bound(geo)\n",
    "        if self.use_spd:       spd = self._bound(spd)\n",
    "        if self.use_edge_bias: edg = self._bound(edg)\n",
    "        # typically don't bound adj; it’s already a small learned scalar per head\n",
    "\n",
    "        bias = (self.alpha_geo  * geo\n",
    "              + self.alpha_spd  * spd\n",
    "              + self.alpha_adj  * adj\n",
    "              + self.alpha_edge * edg)\n",
    "\n",
    "        # mask PAD rows/cols; keep diagonal 0 for valid tokens\n",
    "        pad = key_padding_mask\n",
    "        big_neg = torch.tensor(-1e4, device=bias.device, dtype=bias.dtype)\n",
    "        bias = bias.masked_fill(pad.view(B, 1, L0, 1), big_neg)\n",
    "        bias = bias.masked_fill(pad.view(B, 1, 1, L0), big_neg)\n",
    "        I = torch.eye(L0, device=device, dtype=torch.bool).view(1, 1, L0, L0)\n",
    "        bias = torch.where(I, bias.new_zeros(()), bias)\n",
    "\n",
    "        return bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "026345f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINEConv\n",
    "\n",
    "class GINEBlock(nn.Module):\n",
    "    def __init__(self, dim, activation=\"silu\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.conv = GINEConv(nn.Sequential(\n",
    "            nn.Linear(dim, dim), act, nn.Linear(dim, dim)\n",
    "        ))\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(nn.Linear(dim, 2*dim), act, nn.Dropout(dropout), nn.Linear(2*dim, dim))\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_emb):\n",
    "        h = self.conv(self.norm1(x), edge_index, edge_emb)\n",
    "        x = x + self.drop1(h)\n",
    "        x = x + self.drop2(self.ffn(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class EdgeEncoderMixed(nn.Module):\n",
    "    def __init__(self, emb_dim: int, cont_dim: int = 32, activation=\"silu\"):\n",
    "        super().__init__()\n",
    "        act = _act(activation)\n",
    "        self.emb0 = nn.Embedding(5, emb_dim)\n",
    "        self.emb1 = nn.Embedding(6, emb_dim)\n",
    "        self.emb2 = nn.Embedding(2, emb_dim)\n",
    "        self.mlp_cont = nn.Sequential(\n",
    "            nn.Linear(cont_dim, emb_dim),\n",
    "            act,\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.LayerNorm(emb_dim),       # <<< add\n",
    "        )\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        cat  = edge_attr[:, :3].long()\n",
    "        cont = edge_attr[:, 3:].float()\n",
    "        e_cat  = self.emb0(cat[:,0]) + self.emb1(cat[:,1]) + self.emb2(cat[:,2])\n",
    "        e_cont = self.mlp_cont(cont)\n",
    "        return e_cat + 0.5 * e_cont     # <<< gentle scale on cont branch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f224f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class GraphTransformerGPS(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 8,\n",
    "        nlayers: int = 6,\n",
    "        dropout: float = 0.2,\n",
    "        drop_path: float = 0.0,   # (kept for extensibility)\n",
    "        activation: str = \"silu\",\n",
    "        rdkit_dim: int = 15,\n",
    "        use_extra_atom_feats: bool = True,\n",
    "        extra_atom_dim: int = 5,\n",
    "        # local GNN (GPS) settings\n",
    "        local_layers: int = 2,\n",
    "        use_mixed_edges: bool = True,\n",
    "        cont_dim: int = 32,\n",
    "        # bias knobs\n",
    "        use_geo_bias: bool = True,\n",
    "        use_spd_bias: bool = True,\n",
    "        spd_max: int = 5,\n",
    "        use_adj_const: bool = True,\n",
    "        use_edge_bias: bool = True,\n",
    "        # readout\n",
    "        use_cls: bool = True,\n",
    "        use_has_xyz: bool = True,\n",
    "        head_hidden: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead   = nhead\n",
    "        self.use_cls = use_cls\n",
    "        self.use_has_xyz = use_has_xyz\n",
    "        self.use_extra_atom_feats = use_extra_atom_feats\n",
    "        self.bias_builder = AttnBiasFull(\n",
    "            n_heads=nhead,\n",
    "            rbf_k=32,\n",
    "            rbf_beta=5.0,\n",
    "            use_geo_bias=use_geo_bias,          # was use_geo\n",
    "            use_adj_bias=use_adj_const,         # was use_adj_const (name matches here)\n",
    "            use_spd=use_spd_bias,               # was use_spd\n",
    "            spd_buckets=(spd_max + 1),          # was spd_max; +1 gives the \">= spd_max\" bucket\n",
    "            use_edge_bias=use_edge_bias,\n",
    "            edge_cats=(5, 6, 2),\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "\n",
    "        act = _act(activation)\n",
    "\n",
    "        # encoders\n",
    "        self.atom_enc = AtomEncoder(emb_dim=d_model)\n",
    "        if use_extra_atom_feats:\n",
    "            self.extra_proj = nn.Sequential(nn.Linear(extra_atom_dim, d_model), act, nn.Linear(d_model, d_model))\n",
    "            self.extra_gate = nn.Sequential(nn.Linear(2*d_model, d_model), act)\n",
    "\n",
    "        # local GNN stack\n",
    "        self.use_mixed_edges = use_mixed_edges\n",
    "        if use_mixed_edges:\n",
    "            self.edge_enc = EdgeEncoderMixed(d_model, cont_dim=cont_dim, activation=activation)\n",
    "        else:\n",
    "            from ogb.graphproppred.mol_encoder import BondEncoder\n",
    "            self.edge_enc = BondEncoder(emb_dim=d_model)\n",
    "        self.local_blocks = nn.ModuleList([GINEBlock(d_model, activation=activation, dropout=dropout) \n",
    "                                           for _ in range(local_layers)])\n",
    "\n",
    "        # transformer stack (PyTorch encoder)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=4*d_model,\n",
    "                                               dropout=dropout, activation=activation, batch_first=True, \n",
    "                                               norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers, enable_nested_tensor=False)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        # readout: concat mean + max + (optional) CLS + attention pool\n",
    "        self.gate_pool = nn.Sequential(nn.Linear(d_model, d_model//2), act, nn.Linear(d_model//2, 1))\n",
    "        # features: mean(d), max(d), attn(d) = 3d, (+cls d) optional, + rdkit, + has_xyz\n",
    "        pooled_dim = 3*d_model + (d_model if use_cls else 0)\n",
    "        head_in = pooled_dim + rdkit_dim + (1 if use_has_xyz else 0)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(head_in),\n",
    "            nn.Linear(head_in, head_hidden), act, nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden, head_hidden//2), act, nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden//2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # 1) atom encoding + optional per-atom extras\n",
    "        x = self.atom_enc(data.x)  # (N,D)\n",
    "        if getattr(self, \"use_extra_atom_feats\", False) and hasattr(data, \"extra_atom_feats\"):\n",
    "            xa = self.extra_proj(data.extra_atom_feats.float())          # (N,D)\n",
    "            x  = self.extra_gate(torch.cat([x, xa], dim=1))              # (N,D)\n",
    "\n",
    "        # 2) local GNN over sparse graph\n",
    "        e = self.edge_enc(data.edge_attr)\n",
    "        for blk in self.local_blocks:\n",
    "            x = blk(x, data.edge_index, e)                               # (N,D)\n",
    "\n",
    "        # 3) pack to dense (no CLS yet)\n",
    "        x_pad, valid = to_dense_batch(x, data.batch)                     # (B,L0,D)\n",
    "        B, L0, D = x_pad.shape\n",
    "        key_padding = ~valid                                             # (B,L0) True == PAD\n",
    "\n",
    "        # 4) head-wise attention bias on L0 tokens (B,H,L0,L0), pre-CLS\n",
    "        #    Your AttnBiasFull typically supports SPD, geometry, adjacency, edges\n",
    "        hops = getattr(data, \"hops\", None)                               # (B,MAX_NODES,MAX_NODES) or None\n",
    "        ptr = getattr(data, \"ptr\", None)\n",
    "        attn_bias = self.bias_builder(\n",
    "            pos=(data.pos if hasattr(data, \"pos\") else None),\n",
    "            edge_index=data.edge_index,\n",
    "            edge_attr=(data.edge_attr if hasattr(data, \"edge_attr\") else None),\n",
    "            batch=data.batch,\n",
    "            key_padding_mask=key_padding,   # (B,L0), True=PAD\n",
    "            hops=getattr(data, \"hops\", None),\n",
    "            ptr=ptr\n",
    "        )  # (B,H,L0,L0)                                                # (B,H,L0,L0)\n",
    "\n",
    "        # 5) finalize bias (mask PAD rows/cols, keep diagonal 0), then optionally append CLS\n",
    "        B, H, L = attn_bias.shape[0], attn_bias.shape[1], attn_bias.shape[-1]\n",
    "        pad = key_padding                                                 # (B,L)\n",
    "        huge = attn_bias.new_tensor(-1e4)\n",
    "\n",
    "        # rows FROM PAD, cols TO PAD\n",
    "        attn_bias = attn_bias.masked_fill(pad.view(B, 1, L, 1), huge)\n",
    "        attn_bias = attn_bias.masked_fill(pad.view(B, 1, 1, L), huge)\n",
    "\n",
    "        # keep diagonal = 0 on valid tokens\n",
    "        I = torch.eye(L, device=attn_bias.device, dtype=torch.bool).view(1, 1, L, L)\n",
    "        attn_bias = torch.where(I, attn_bias.new_zeros(()), attn_bias)\n",
    "\n",
    "        # (optional) append CLS token at the end\n",
    "        if getattr(self, \"use_cls\", False):\n",
    "            # append CLS embedding\n",
    "            cls = self.cls_token.expand(B, 1, D)                         # (B,1,D)\n",
    "            x_pad = torch.cat([x_pad, cls], dim=1)                       # (B,L+1,D)\n",
    "\n",
    "            # extend key_padding: CLS is always valid (False)\n",
    "            key_padding = torch.cat(\n",
    "                [key_padding, torch.zeros(B, 1, dtype=torch.bool, device=x_pad.device)],\n",
    "                dim=1\n",
    "            )                                                             # (B,L+1)\n",
    "\n",
    "            # pad bias by one row/col with zeros for CLS -> (B,H,L+1,L+1)\n",
    "            attn_bias = F.pad(attn_bias, (0, 1, 0, 1), value=0.0)\n",
    "            L = L + 1\n",
    "\n",
    "        # 6) transformer encoder with 3D additive mask (B*H,L,L)\n",
    "        attn_mask_3d = attn_bias.reshape(B * H, L, L).to(x_pad.dtype)\n",
    "        h = self.encoder(                                                # returns (B,L,D) when batch_first=True\n",
    "            x_pad,\n",
    "            mask=attn_mask_3d, # additive float mask \n",
    "        )\n",
    "\n",
    "        # 7) pooling (mean + max + gated attention), plus optional CLS; then RDKit/has_xyz and head\n",
    "        # exclude CLS from token pools\n",
    "        h_tok = h[:, :L0, :]                                             # (B,L0,D)\n",
    "        mask_f = valid.float()                                           # (B,L0)\n",
    "\n",
    "        mean = (h_tok * mask_f.unsqueeze(-1)).sum(1) / (mask_f.sum(1, keepdim=True) + 1e-8)  # (B,D)\n",
    "        mmax, _ = (h_tok + (1.0 - mask_f.unsqueeze(-1)) * (-1e4)).max(dim=1)                 # (B,D)\n",
    "\n",
    "        gate_logits = self.gate_pool(h_tok).squeeze(-1)                  # (B,L0)\n",
    "        gate = torch.softmax(gate_logits.masked_fill(~valid, -1e4), dim=1)\n",
    "        attn_pool = (h_tok * gate.unsqueeze(-1)).sum(1)                  # (B,D)\n",
    "\n",
    "        parts = [mean, mmax, attn_pool]\n",
    "\n",
    "        if getattr(self, \"use_cls\", False):\n",
    "            parts.append(h[:, L-1, :])                                   # CLS vector (B,D)\n",
    "\n",
    "        # RDKit globals\n",
    "        rd = data.rdkit_feats.view(B, -1).float()                        # (B, rdkit_dim)\n",
    "        parts.append(rd)\n",
    "\n",
    "        # optional has_xyz scalar if present\n",
    "        if getattr(self, \"use_has_xyz\", False) and hasattr(data, \"has_xyz\"):\n",
    "            parts.append(data.has_xyz.view(B, 1).float())\n",
    "\n",
    "        out = torch.cat(parts, dim=1)\n",
    "        return self.head(out)                                            # (B,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9de38ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mattg\\anaconda3\\envs\\chemml_env\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | tr_MSE 111.21163 | val_MAE 99.77571 | val_RMSE 130.17746 | R2 -0.7752\n",
      "Epoch 002 | tr_MSE 109.52632 | val_MAE 96.56107 | val_RMSE 126.40446 | R2 -0.6738\n",
      "Epoch 003 | tr_MSE 104.14766 | val_MAE 88.61504 | val_RMSE 116.01917 | R2 -0.4100\n",
      "Epoch 004 | tr_MSE 91.76187 | val_MAE 79.57697 | val_RMSE 97.99117 | R2 -0.0059\n",
      "Epoch 005 | tr_MSE 89.84983 | val_MAE 78.82456 | val_RMSE 97.68488 | R2 0.0004\n",
      "Epoch 006 | tr_MSE 87.84224 | val_MAE 78.43917 | val_RMSE 97.59521 | R2 0.0022\n",
      "Epoch 007 | tr_MSE 87.08742 | val_MAE 78.35258 | val_RMSE 101.79829 | R2 -0.0856\n",
      "Epoch 008 | tr_MSE 82.36361 | val_MAE 71.31493 | val_RMSE 89.36398 | R2 0.1634\n",
      "Epoch 009 | tr_MSE 83.91591 | val_MAE 83.48821 | val_RMSE 105.87797 | R2 -0.1743\n",
      "Epoch 010 | tr_MSE 75.38746 | val_MAE 63.33444 | val_RMSE 80.77422 | R2 0.3165\n",
      "Epoch 011 | tr_MSE 67.52360 | val_MAE 62.32416 | val_RMSE 78.33080 | R2 0.3573\n",
      "Epoch 012 | tr_MSE 64.58598 | val_MAE 66.45879 | val_RMSE 81.78575 | R2 0.2993\n",
      "Epoch 013 | tr_MSE 62.30015 | val_MAE 68.03371 | val_RMSE 83.13867 | R2 0.2759\n",
      "Epoch 014 | tr_MSE 62.95259 | val_MAE 58.20344 | val_RMSE 75.81189 | R2 0.3979\n",
      "Epoch 015 | tr_MSE 62.68440 | val_MAE 57.92185 | val_RMSE 73.24044 | R2 0.4381\n",
      "Epoch 016 | tr_MSE 60.80616 | val_MAE 60.12296 | val_RMSE 76.76177 | R2 0.3828\n",
      "Epoch 017 | tr_MSE 59.06883 | val_MAE 55.62342 | val_RMSE 71.16451 | R2 0.4695\n",
      "Epoch 018 | tr_MSE 58.04861 | val_MAE 55.62420 | val_RMSE 70.79221 | R2 0.4750\n",
      "Epoch 019 | tr_MSE 58.33931 | val_MAE 61.15619 | val_RMSE 76.34292 | R2 0.3895\n",
      "Epoch 020 | tr_MSE 56.24565 | val_MAE 55.90358 | val_RMSE 70.94347 | R2 0.4728\n",
      "Epoch 021 | tr_MSE 54.04998 | val_MAE 52.68010 | val_RMSE 66.66499 | R2 0.5345\n",
      "Epoch 022 | tr_MSE 52.98305 | val_MAE 54.60399 | val_RMSE 67.65652 | R2 0.5205\n",
      "Epoch 023 | tr_MSE 52.00375 | val_MAE 58.51939 | val_RMSE 72.79786 | R2 0.4449\n",
      "Epoch 024 | tr_MSE 52.33347 | val_MAE 53.09828 | val_RMSE 68.01624 | R2 0.5154\n",
      "Epoch 025 | tr_MSE 51.38965 | val_MAE 52.36558 | val_RMSE 67.52960 | R2 0.5223\n",
      "Epoch 026 | tr_MSE 50.06548 | val_MAE 53.08396 | val_RMSE 67.19653 | R2 0.5270\n",
      "Epoch 027 | tr_MSE 50.59815 | val_MAE 52.38611 | val_RMSE 66.89146 | R2 0.5313\n",
      "Epoch 028 | tr_MSE 50.10315 | val_MAE 53.29118 | val_RMSE 67.01781 | R2 0.5295\n",
      "Epoch 029 | tr_MSE 49.91818 | val_MAE 53.68365 | val_RMSE 69.25204 | R2 0.4976\n",
      "Epoch 030 | tr_MSE 49.16831 | val_MAE 53.03281 | val_RMSE 67.73287 | R2 0.5194\n",
      "Epoch 031 | tr_MSE 49.15307 | val_MAE 53.87975 | val_RMSE 68.22748 | R2 0.5124\n",
      "Epoch 032 | tr_MSE 50.36481 | val_MAE 54.06193 | val_RMSE 67.53996 | R2 0.5221\n",
      "Epoch 033 | tr_MSE 48.11993 | val_MAE 54.85189 | val_RMSE 71.18127 | R2 0.4692\n",
      "Epoch 034 | tr_MSE 47.86843 | val_MAE 53.26428 | val_RMSE 68.20919 | R2 0.5126\n",
      "Epoch 035 | tr_MSE 47.71037 | val_MAE 53.43321 | val_RMSE 68.70760 | R2 0.5055\n",
      "Early stopping.\n",
      "[graphtransformer_tg_spd] Best Val — MAE 52.365578 | RMSE 67.529610 | R2 0.5223\n",
      "Epoch 001 | tr_MSE 0.09679 | val_MAE 0.10526 | val_RMSE 0.13217 | R2 0.0053\n",
      "Epoch 002 | tr_MSE 0.02031 | val_MAE 0.11548 | val_RMSE 0.13868 | R2 -0.0952\n",
      "Epoch 003 | tr_MSE 0.01313 | val_MAE 0.08716 | val_RMSE 0.12608 | R2 0.0948\n",
      "Epoch 004 | tr_MSE 0.00959 | val_MAE 0.09386 | val_RMSE 0.11602 | R2 0.2334\n",
      "Epoch 005 | tr_MSE 0.00733 | val_MAE 0.12287 | val_RMSE 0.14407 | R2 -0.1821\n",
      "Epoch 006 | tr_MSE 0.00524 | val_MAE 0.19133 | val_RMSE 0.21957 | R2 -1.7454\n",
      "Epoch 007 | tr_MSE 0.00397 | val_MAE 0.14944 | val_RMSE 0.18046 | R2 -0.8545\n",
      "Epoch 008 | tr_MSE 0.00313 | val_MAE 0.21547 | val_RMSE 0.24466 | R2 -2.4086\n",
      "Epoch 009 | tr_MSE 0.00308 | val_MAE 0.22492 | val_RMSE 0.25685 | R2 -2.7569\n",
      "Epoch 010 | tr_MSE 0.00291 | val_MAE 0.20781 | val_RMSE 0.23857 | R2 -2.2411\n",
      "[graphtransformer_den_spd] Best Val — MAE 0.087159 | RMSE 0.126079 | R2 0.0948\n",
      "Epoch 001 | tr_MSE 15.50024 | val_MAE 15.86416 | val_RMSE 16.55955 | R2 -11.4595\n",
      "Epoch 002 | tr_MSE 14.03399 | val_MAE 13.26251 | val_RMSE 14.15985 | R2 -8.1101\n",
      "Epoch 003 | tr_MSE 9.56203 | val_MAE 5.29926 | val_RMSE 7.26641 | R2 -1.3991\n",
      "Epoch 004 | tr_MSE 4.46820 | val_MAE 4.51781 | val_RMSE 5.10047 | R2 -0.1820\n",
      "Epoch 005 | tr_MSE 4.01092 | val_MAE 3.89526 | val_RMSE 4.84366 | R2 -0.0660\n",
      "Epoch 006 | tr_MSE 3.51412 | val_MAE 3.79901 | val_RMSE 4.97056 | R2 -0.1226\n",
      "Epoch 007 | tr_MSE 3.31861 | val_MAE 3.69222 | val_RMSE 5.06999 | R2 -0.1679\n",
      "Epoch 008 | tr_MSE 3.01125 | val_MAE 2.99481 | val_RMSE 4.01324 | R2 0.2682\n",
      "Epoch 009 | tr_MSE 2.90935 | val_MAE 6.87664 | val_RMSE 7.52109 | R2 -1.5702\n",
      "Epoch 010 | tr_MSE 4.87123 | val_MAE 2.75454 | val_RMSE 3.62706 | R2 0.4023\n",
      "Epoch 011 | tr_MSE 2.73725 | val_MAE 2.67116 | val_RMSE 3.60139 | R2 0.4107\n",
      "Epoch 012 | tr_MSE 2.32325 | val_MAE 2.30470 | val_RMSE 3.36744 | R2 0.4848\n",
      "Epoch 013 | tr_MSE 2.09433 | val_MAE 2.56620 | val_RMSE 3.42826 | R2 0.4660\n",
      "Epoch 014 | tr_MSE 2.06558 | val_MAE 2.40029 | val_RMSE 3.45987 | R2 0.4561\n",
      "Epoch 015 | tr_MSE 2.29632 | val_MAE 2.48321 | val_RMSE 3.30835 | R2 0.5027\n",
      "Epoch 016 | tr_MSE 2.17596 | val_MAE 2.20666 | val_RMSE 3.09198 | R2 0.5656\n",
      "Epoch 017 | tr_MSE 2.03538 | val_MAE 2.22689 | val_RMSE 3.27985 | R2 0.5112\n",
      "Epoch 018 | tr_MSE 2.00274 | val_MAE 2.87837 | val_RMSE 3.50779 | R2 0.4409\n",
      "Epoch 019 | tr_MSE 2.07049 | val_MAE 3.73753 | val_RMSE 4.27966 | R2 0.1678\n",
      "Epoch 020 | tr_MSE 2.09790 | val_MAE 2.13430 | val_RMSE 3.06620 | R2 0.5728\n",
      "Epoch 021 | tr_MSE 2.37723 | val_MAE 3.45861 | val_RMSE 3.99609 | R2 0.2744\n",
      "Epoch 022 | tr_MSE 2.59088 | val_MAE 2.88739 | val_RMSE 3.51677 | R2 0.4381\n",
      "Epoch 023 | tr_MSE 1.79072 | val_MAE 2.64016 | val_RMSE 3.29734 | R2 0.5060\n",
      "Epoch 024 | tr_MSE 2.09275 | val_MAE 2.32280 | val_RMSE 3.10173 | R2 0.5629\n",
      "Epoch 025 | tr_MSE 1.81745 | val_MAE 2.19663 | val_RMSE 3.02564 | R2 0.5841\n",
      "Epoch 026 | tr_MSE 1.74846 | val_MAE 3.02412 | val_RMSE 3.54674 | R2 0.4284\n",
      "Epoch 027 | tr_MSE 1.76253 | val_MAE 2.28244 | val_RMSE 3.01807 | R2 0.5861\n",
      "Epoch 028 | tr_MSE 1.75366 | val_MAE 2.54074 | val_RMSE 3.16885 | R2 0.5437\n",
      "Epoch 029 | tr_MSE 1.61810 | val_MAE 3.71307 | val_RMSE 4.18944 | R2 0.2025\n",
      "Epoch 030 | tr_MSE 1.91541 | val_MAE 2.41312 | val_RMSE 3.09981 | R2 0.5634\n",
      "Early stopping.\n",
      "[graphtransformer_rg_spd] Best Val — MAE 2.134301 | RMSE 3.066199 | R2 0.5728\n",
      "Epoch 001 | tr_MSE 0.01689 | val_MAE 0.08997 | val_RMSE 0.11528 | R2 -0.5562\n",
      "Epoch 002 | tr_MSE 0.00399 | val_MAE 0.06800 | val_RMSE 0.09384 | R2 -0.0313\n",
      "Epoch 003 | tr_MSE 0.00284 | val_MAE 0.05450 | val_RMSE 0.07088 | R2 0.4116\n",
      "Epoch 004 | tr_MSE 0.00210 | val_MAE 0.05059 | val_RMSE 0.06858 | R2 0.4492\n",
      "Epoch 005 | tr_MSE 0.00156 | val_MAE 0.04066 | val_RMSE 0.05534 | R2 0.6413\n",
      "Epoch 006 | tr_MSE 0.00120 | val_MAE 0.03688 | val_RMSE 0.04890 | R2 0.7200\n",
      "Epoch 007 | tr_MSE 0.00105 | val_MAE 0.04069 | val_RMSE 0.05533 | R2 0.6415\n",
      "Epoch 008 | tr_MSE 0.00098 | val_MAE 0.03947 | val_RMSE 0.05453 | R2 0.6518\n",
      "Epoch 009 | tr_MSE 0.00106 | val_MAE 0.04753 | val_RMSE 0.06309 | R2 0.5339\n",
      "Epoch 010 | tr_MSE 0.00108 | val_MAE 0.03621 | val_RMSE 0.04833 | R2 0.7264\n",
      "Epoch 011 | tr_MSE 0.00089 | val_MAE 0.03249 | val_RMSE 0.04669 | R2 0.7447\n",
      "Epoch 012 | tr_MSE 0.00082 | val_MAE 0.03255 | val_RMSE 0.04590 | R2 0.7532\n",
      "Epoch 013 | tr_MSE 0.00078 | val_MAE 0.03501 | val_RMSE 0.05039 | R2 0.7027\n",
      "Epoch 014 | tr_MSE 0.00075 | val_MAE 0.03440 | val_RMSE 0.04973 | R2 0.7104\n",
      "Epoch 015 | tr_MSE 0.00074 | val_MAE 0.03664 | val_RMSE 0.05170 | R2 0.6869\n",
      "Epoch 016 | tr_MSE 0.00075 | val_MAE 0.03222 | val_RMSE 0.04769 | R2 0.7337\n",
      "Epoch 017 | tr_MSE 0.00068 | val_MAE 0.03499 | val_RMSE 0.04986 | R2 0.7089\n",
      "Epoch 018 | tr_MSE 0.00070 | val_MAE 0.03364 | val_RMSE 0.04873 | R2 0.7219\n",
      "Epoch 019 | tr_MSE 0.00069 | val_MAE 0.02901 | val_RMSE 0.04355 | R2 0.7778\n",
      "Epoch 020 | tr_MSE 0.00074 | val_MAE 0.02931 | val_RMSE 0.04381 | R2 0.7753\n",
      "Epoch 021 | tr_MSE 0.00063 | val_MAE 0.02859 | val_RMSE 0.04312 | R2 0.7822\n",
      "Epoch 022 | tr_MSE 0.00060 | val_MAE 0.03811 | val_RMSE 0.05341 | R2 0.6659\n",
      "Epoch 023 | tr_MSE 0.00059 | val_MAE 0.03441 | val_RMSE 0.04950 | R2 0.7131\n",
      "Epoch 024 | tr_MSE 0.00054 | val_MAE 0.03023 | val_RMSE 0.04571 | R2 0.7553\n",
      "Epoch 025 | tr_MSE 0.00050 | val_MAE 0.03097 | val_RMSE 0.04614 | R2 0.7507\n",
      "Epoch 026 | tr_MSE 0.00050 | val_MAE 0.02887 | val_RMSE 0.04343 | R2 0.7791\n",
      "Epoch 027 | tr_MSE 0.00048 | val_MAE 0.02931 | val_RMSE 0.04389 | R2 0.7744\n",
      "Epoch 028 | tr_MSE 0.00047 | val_MAE 0.03128 | val_RMSE 0.04534 | R2 0.7593\n",
      "Epoch 029 | tr_MSE 0.00044 | val_MAE 0.03068 | val_RMSE 0.04510 | R2 0.7618\n",
      "Epoch 030 | tr_MSE 0.00047 | val_MAE 0.03113 | val_RMSE 0.04603 | R2 0.7519\n",
      "Epoch 031 | tr_MSE 0.00048 | val_MAE 0.03260 | val_RMSE 0.04705 | R2 0.7407\n",
      "Early stopping.\n",
      "[graphtransformer_tc_spd] Best Val — MAE 0.028590 | RMSE 0.043121 | R2 0.7822\n",
      "Epoch 001 | tr_MSE 0.00307 | val_MAE 0.01749 | val_RMSE 0.02747 | R2 0.2698\n",
      "Epoch 002 | tr_MSE 0.00047 | val_MAE 0.02237 | val_RMSE 0.02972 | R2 0.1455\n",
      "Epoch 003 | tr_MSE 0.00035 | val_MAE 0.02151 | val_RMSE 0.02833 | R2 0.2235\n",
      "Epoch 004 | tr_MSE 0.00027 | val_MAE 0.03323 | val_RMSE 0.03848 | R2 -0.4323\n",
      "Epoch 005 | tr_MSE 0.00022 | val_MAE 0.04709 | val_RMSE 0.05160 | R2 -1.5759\n",
      "Epoch 006 | tr_MSE 0.00017 | val_MAE 0.04470 | val_RMSE 0.04982 | R2 -1.4010\n",
      "Epoch 007 | tr_MSE 0.00015 | val_MAE 0.04307 | val_RMSE 0.04895 | R2 -1.3184\n",
      "Epoch 008 | tr_MSE 0.00015 | val_MAE 0.05235 | val_RMSE 0.05836 | R2 -2.2954\n",
      "Epoch 009 | tr_MSE 0.00014 | val_MAE 0.03825 | val_RMSE 0.04336 | R2 -0.8192\n"
     ]
    }
   ],
   "source": [
    "# introspect dims\n",
    "b = next(iter(train_loader_tg))\n",
    "rd_dim = int(b.rdkit_feats.shape[-1])\n",
    "\n",
    "model_tg = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=6, dropout=0.1,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=True, use_edge_bias=True,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_tg, ckpt_tg, met_tg = train_hybrid_gnn_sota(\n",
    "    model_tg, train_loader_tg, val_loader_tg,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=100, warmup_epochs=5, patience=10,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "    save_dir=\"saved_models/gt_tg_spd\", tag=\"graphtransformer_tg_spd\"\n",
    ")\n",
    "\n",
    "\n",
    "model_den = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=6, dropout=0.2,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=True, use_edge_bias=True,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_den, ckpt_den, met_den = train_hybrid_gnn_sota(\n",
    "    model_den, train_loader_den, val_loader_den,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=10, warmup_epochs=3, patience=10,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "    save_dir=\"saved_models/gt_den_spd\", tag=\"graphtransformer_den_spd\"\n",
    ")\n",
    "\n",
    "# Rg\n",
    "model_rg = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=6, dropout=0.1,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=True, use_edge_bias=True,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_rg, ckpt_rg, met_rg = train_hybrid_gnn_sota(\n",
    "    model_rg, train_loader_rg, val_loader_rg,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=100, warmup_epochs=10, patience=10,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "    save_dir=\"saved_models/gt_rg_spd\", tag=\"graphtransformer_rg_spd\"\n",
    ")\n",
    "\n",
    "# Tc\n",
    "model_tc = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=6, dropout=0.1,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=True, use_edge_bias=True,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_tc, ckpt_tc, met_tc = train_hybrid_gnn_sota(\n",
    "    model_tc, train_loader_tc, val_loader_tc,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=100, warmup_epochs=10, patience=10,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "    save_dir=\"saved_models/gt_tc_spd\", tag=\"graphtransformer_tc_spd\"\n",
    ")\n",
    "\n",
    "model_ffv = GraphTransformerGPS(\n",
    "    d_model=256, nhead=8, nlayers=6, dropout=0.1,\n",
    "    rdkit_dim=rd_dim, activation=\"gelu\",\n",
    "    use_extra_atom_feats=True, extra_atom_dim=5,\n",
    "    local_layers=2, use_mixed_edges=True, cont_dim=32,\n",
    "    use_geo_bias=True, use_spd_bias=True, spd_max=5,\n",
    "    use_adj_const=True, use_edge_bias=True,\n",
    "    use_cls=True, use_has_xyz=True, head_hidden=512\n",
    ").to(b.x.device)\n",
    "\n",
    "model_ffv, ckpt_ffv, met_ffv = train_hybrid_gnn_sota(\n",
    "    model_ffv, train_loader_ffv, val_loader_ffv,\n",
    "    lr=6e-4, optimizer=\"AdamW\", weight_decay=1e-5,\n",
    "    epochs=100, warmup_epochs=10, patience=10,\n",
    "    clip_norm=1.0, amp=True, loss_name=\"huber\",\n",
    "    save_dir=\"saved_models/gt_ffv_spd\", tag=\"graphtransformer_ffv_spd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428bf75",
   "metadata": {},
   "source": [
    "\n",
    "[graphtransformer_tg_spd] Best Val — MAE 62.587055 | RMSE 78.081322 | R2 0.3613\n",
    "\n",
    "[graphtransformer_den_spd] Best Val — MAE 0.097926 | RMSE 0.126593 | R2 0.0874\n",
    "\n",
    "[graphtransformer_rg_spd] Best Val — MAE 1.952458 | RMSE 2.967103 | R2 0.6000\n",
    "\n",
    "[graphtransformer_tc_spd] Best Val — MAE 0.028464 | RMSE 0.043981 | R2 0.7735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895acd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0  loss 0.0193\n"
     ]
    }
   ],
   "source": [
    "import torch, random\n",
    "from torch.utils.data import Subset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "train_ds = train_loader_den.dataset\n",
    "idx = random.sample(range(len(train_ds)), 64)\n",
    "tiny = Subset(train_ds, idx)\n",
    "tiny_loader = type(train_loader_den)(tiny, batch_size=32, shuffle=True)\n",
    "\n",
    "model = model_den  # reuse\n",
    "model.train()\n",
    "opt = AdamW(model.parameters(), lr=2e-4, weight_decay=0.05)\n",
    "crit = torch.nn.SmoothL1Loss(beta=1.0)  # Huber\n",
    "\n",
    "dev = next(model.parameters()).device\n",
    "for step, b in enumerate(tiny_loader):\n",
    "    b = b.to(dev)\n",
    "    if b.x.dtype != torch.long: b.x = b.x.long()\n",
    "    p = model(b)\n",
    "    loss = crit(p.view_as(b.y), b.y)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    opt.step()\n",
    "    if step % 5 == 0:\n",
    "        print(f\"step {step}  loss {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673460",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "| Model Type | Feature | MAE | RMSE | R2 |\n",
    "|---|---|---|---|---|\n",
    "| RF3D | FFV | 0.007621 | 0.017553 | 0.6605 |\n",
    "| RF3D_Aug | FFV | 0.007578 | 0.017404 | 0.6662 |\n",
    "| GNN2 | FFV | 0.013817 | 0.023902 | 0.4473 |\n",
    "| GNN2_Aug | FFV | 0.013092 | 0.022793 | 0.4974 |\n",
    "| ET | FFV | 0.006651 | 0.016818 | 0.6883 |\n",
    "| **ET_Aug** | **FFV** | **0.006635** | **0.016826** | **0.6880** |\n",
    "| GT | FFV | 0.XXXXXX | 0.XXXXXX | 0.XXXX |\n",
    "| GT_Aug | FFV | 0.XXXXXX | 0.XXXXXX | 0.XXXX |\n",
    "| RF3D | Tg | 58.315801 | 74.296699 | 0.5846 |\n",
    "| RF3D_Aug | Tg | 58.143107 | 74.521032 | 0.5821 |\n",
    "| **GNN2** | **Tg** | **47.105114** | **61.480179** | **0.6040** |\n",
    "| GNN2_Aug | Tg | 51.539692 | 70.575638 | 0.4782 |\n",
    "| ET | Tg | 58.973811 | 74.658978 | 0.5806 |\n",
    "| ET_Aug | Tg | 58.521052 | 74.475532 | 0.5826 |\n",
    "| GT | Tg | 78.903389 | 98.401192 |-0.0143 |\n",
    "| GT_Aug | Tg | 52.365578 | 67.529610 | 0.5223 |\n",
    "| RF3D | Tc | 0.029937 | 0.045036 | 0.7313 |\n",
    "| RF3D_Aug | Tc | 0.029675 | 0.044853 | 0.7335 |\n",
    "| **GNN2** | **Tc** | **0.025115** | **0.041331** | **0.8000** |\n",
    "| **GNN2_Aug** | **Tc** | **0.025252** | **0.039670** | **0.8157** |\n",
    "| ET | Tc | 0.028888 | 0.043469 | 0.7497 |\n",
    "| ET_Aug | Tc | 0.027990 | 0.042644 | 0.7591 |\n",
    "| GT | Tc | 0.032644 | 0.046613 | 0.7456 |\n",
    "| GT_Aug | Tc | 0.028590 | 0.043121 | 0.7822 |\n",
    "| RF3D | Rg | 1.648818 | 2.493712 | 0.7299 |\n",
    "| RF3D_Aug | Rg | 1.668425 | 2.517235 | 0.7248 |\n",
    "| GNN2 | Rg | 2.115880 | 2.801481 | 0.6434 |\n",
    "| GNN2_Aug | Rg | 1.532573 | 2.405382 | 0.7371 |\n",
    "| ET | Rg | 1.619464 | 2.522478 | 0.7237 |\n",
    "| **ET_Aug** | **Rg** | **1.609396** | **2.526705** | **0.7227** |\n",
    "| GT | Rg | 2.579300 | 3.521387 | 0.4366 |\n",
    "| GT_Aug | Rg | 2.134301 | 3.066199 | 0.5728 |\n",
    "| RF3D | Density | 0.037793 | 0.070932 | 0.7847 |\n",
    "| RF3D_Aug | Density | 0.037123 | 0.070212 | 0.7891 |\n",
    "| GNN2 | Density | 0.031735 | 0.067845 | 0.7379 |\n",
    "| GNN2_Aug | Density | 0.030458 | 0.070372 | 0.7180 |\n",
    "| ET | Density | 0.028492 | 0.052839 | 0.8805 |\n",
    "| **ET_Aug** | **Density** | **0.028135** | **0.051842** | **0.8850** |\n",
    "| GT | Density | 0.104749 | 0.134771 | -0.0343 |\n",
    "| GT_Aug | Density | 0.087159 | 0.126079 | 0.0948 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f53569",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
